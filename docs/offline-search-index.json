[{"body":"Generally: the main engine in Clickhouse is called MergeTree. It allows to store and process data on one server and feel all the advantages of Clickhouse. Basic usage of MergeTree does not require any special configuration, and you can start using it ‘out of the box’.\nBut one server and one copy of data are not fault-tolerant - something can happen with the server itself, with datacenter availability, etc. So you need to have the replica(s) - i.e. server(s) with the same data and which can ‘substitute’ the original server at any moment.\nTo have an extra copy (replica) of your data you need to use ReplicatedMergeTree engine. It can be used instead of MergeTree engine, and you can always upgrade from MergeTree to ReplicatedMergeTree (and downgrade back) if you need. To use that you need to have ZooKeeper installed and running. For tests, you can use one standalone Zookeeper instance, but for production usage, you should have zookeeper ensemble at least of 3 servers.\nWhen you use ReplicatedMergeTree then the inserted data is copied automatically to all the replicas, but all the SELECTs are executed on the single server you have connected to. So you can have 5 replicas of your data, but if you will always connect to one replica - it will not ‘share’ / ‘balance’ that traffic automatically between all the replicas, one server will be loaded and the rest will generally do nothing. If you need that balancing of load between multiple replicas - you can use the internal ’loadbalancer’ mechanism which is provided by Distributed engine of Clickhouse. As an alternative in that scenario you can work without Distributed table, but with some external load balancer that will balance the requests between several replicas according to your specific rules or preferences, or just cluster-aware client which will pick one of the servers for the query time.\nThe Distributed engine does not store any data, but it can ‘point’ to the same ReplicatedMergeTree/MergeTree table on multiple servers. To use Distributed engine you need to configure \u003ccluser\u003e settings in your ClickHouse server config file.\nSo let’s say you have 3 replicas of table my_replicated_data with ReplicatedMergeTree engine. You can create a table with Distributed engine called my_distributed_replicated_data which will ‘point’ to all of that 3 servers, and when you will select from that my_distributed_replicated_data table the select will be forwarded and executed on one of the replicas. So in that scenario, each replica will get 1/3 of requests (but each request still will be fully executed on one chosen replica).\nAll that is great, and will work well while one copy of your data is fitting on a single physical server, and can be processed by the resources of one server. When you have too much data to be stored/processed on one server - you need to use sharding (it’s just a way to split the data into smaller parts). Sharding is the mechanism also provided by Distributed engine.\nWith sharding data is divided into parts (shards) according to some sharding key. You can just use random distribution, so let’s say - throw a coin to decide on each of the servers the data should be stored, or you can use some ‘smarter’ sharding scheme, to make the data connected to the same subject (let’s say to the same customer) stored on one server, and to another subject on another. So in that case all the shards should be requested at the same time and later the ‘common’ result should be calculated.\nIn ClickHouse each shard works independently and process its part of data, inside each shard replication can work. And later to query all the shards at the same time and combine the final result - Distributed engine is used. So Distributed work as load balancer inside each shard, and can combine the data coming from different shards together to make the ‘common’ result.\nYou can use Distributed table for inserts, in that case, it will pass the data to one of the shards according to the sharding key. Or you can insert to the underlying table on one of the shards bypassing the Distributed table.\nShort summary start with MergeTree to have several copies of data use ReplicatedMergeTree if your data is too big to fit/ to process on one server - use sharding to balance the load between replicas and to combine the result of selects from different shards - use Distributed table. More Please check @alex-zaitsev presentation, which covers that subject: https://www.youtube.com/watch?v=zbjub8BQPyE ( Slides are here: https://yadi.sk/i/iLA5ssAv3NdYGy )\nP.S. Actually you can create replication without Zookeeper and ReplicatedMergeTree, just by using the Distributed table above MergeTree and internal_replication=false cluster setting, but in that case, there will be no guarantee that all the replicas will have 100% the same data, so I rather would not recommend that scenario.\nSee also: ReplacingMergeTree does not collapse duplicates\nBased on my original answer on github: https://github.com/ClickHouse/ClickHouse/issues/2161\n","categories":"","description":"Learn about ClickHouse engines, from MergeTree, Atomic Database to RocksDB.\n","excerpt":"Learn about ClickHouse engines, from MergeTree, Atomic Database to …","ref":"/engines/","tags":"","title":"Engines"},{"body":"Internal implementation Code\nClickHouse uses non-blocking? hash tables, so each thread has at least one hash table.\nIt makes easier to not care about sync between multiple threads, but has such disadvantages as:\nBigger memory usage. Needs to merge those per-thread hash tables afterwards. Because second step can be a bottleneck in case of a really big GROUP BY with a lot of distinct keys, another solution has been made.\nTwo-Level https://youtu.be/SrucFOs8Y6c?t=2132\n┌─name───────────────────────────────┬─value────┬─changed─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┬─type───┐ │ group_by_two_level_threshold │ 100000 │ 0 │ From what number of keys, a two-level aggregation starts. 0 - the threshold is not set. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ UInt64 │ │ group_by_two_level_threshold_bytes │ 50000000 │ 0 │ From what size of the aggregation state in bytes, a two-level aggregation begins to be used. 0 - the threshold is not set. Two-level aggregation is used when at least one of the thresholds is triggered. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ UInt64 │ └────────────────────────────────────┴──────────┴─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┴────────┘ In order to parallelize merging of hash tables, ie execute such merge via multiple threads, ClickHouse use two-level approach:\nOn the first step ClickHouse creates 256 buckets for each thread. (determined by one byte of hash function) On the second step ClickHouse can merge those 256 buckets independently by multiple threads.\nhttps://github.com/ClickHouse/ClickHouse/blob/1ea637d996715d2a047f8cd209b478e946bdbfb0/src/Common/HashTable/TwoLevelHashTable.h#L6\nGROUP BY in external memory It utilizes a two-level group by and dumps those buckets on disk. And at the last stage ClickHouse will read those buckets from disk one by one and merge them. So you should have enough RAM to hold one bucket (1/256 of whole GROUP BY size).\nhttps://clickhouse.com/docs/en/sql-reference/statements/select/group-by/#select-group-by-in-external-memory\noptimize_aggregation_in_order GROUP BY Usually it works slower than regular GROUP BY, because ClickHouse need’s to read and process data in specific ORDER, which makes it much more complicated to parallelize reading and aggregating.\nBut it use much less memory, because ClickHouse can stream resultset and there is no need to keep it in memory.\nLast item cache ClickHouse saves value of previous hash calculation, just in case next value will be the same.\nhttps://github.com/ClickHouse/ClickHouse/pull/5417 https://github.com/ClickHouse/ClickHouse/blob/808d9afd0f8110faba5ae027051bf0a64e506da3/src/Common/ColumnsHashingImpl.h#L40\nStringHashMap Actually uses 5 different hash tables\nFor empty strings For strings \u003c 8 bytes For strings \u003c 16 bytes For strings \u003c 24 bytes For strings \u003e 24 bytes SELECT count() FROM ( SELECT materialize('1234567890123456') AS key -- length(key) = 16 FROM numbers(1000000000) ) GROUP BY key Aggregator: Aggregation method: key_string Elapsed: 8.888 sec. Processed 1.00 billion rows, 8.00 GB (112.51 million rows/s., 900.11 MB/s.) SELECT count() FROM ( SELECT materialize('12345678901234567') AS key -- length(key) = 17 FROM numbers(1000000000) ) GROUP BY key Aggregator: Aggregation method: key_string Elapsed: 9.089 sec. Processed 1.00 billion rows, 8.00 GB (110.03 million rows/s., 880.22 MB/s.) SELECT count() FROM ( SELECT materialize('123456789012345678901234') AS key -- length(key) = 24 FROM numbers(1000000000) ) GROUP BY key Aggregator: Aggregation method: key_string Elapsed: 9.134 sec. Processed 1.00 billion rows, 8.00 GB (109.49 million rows/s., 875.94 MB/s.) SELECT count() FROM ( SELECT materialize('1234567890123456789012345') AS key -- length(key) = 25 FROM numbers(1000000000) ) GROUP BY key Aggregator: Aggregation method: key_string Elapsed: 12.566 sec. Processed 1.00 billion rows, 8.00 GB (79.58 million rows/s., 636.67 MB/s.) length\n16 8.89 17 9.09 24 9.13 25 12.57\nFor what GROUP BY statement use memory Hash tables It will grow with:\nAmount of unique combinations of keys participated in GROUP BY\nSize of keys participated in GROUP BY\nStates of aggregation functions: Be careful with function, which state can use unrestricted amount of memory and grow indefenetely:\ngroupArray (groupArray(1000)()) uniqExact (uniq,uniqCombined) quantileExact (medianExact) (quantile,quantileTDigest) windowFunnel groupBitmap sequenceCount (sequenceMatch) *Map Why my GROUP BY eat all the RAM run your query with set send_logs_level='trace'\nRemove all aggregation functions from the query, try to understand how many memory simple GROUP BY will take.\nOne by one remove aggregation functions from query in order to understand which one is taking most of memory\n","categories":"","description":"Learn about GROUP BY clause in ClickHouse.\n","excerpt":"Learn about GROUP BY clause in ClickHouse.\n","ref":"/altinity-kb-queries-and-syntax/group-by/","tags":"","title":"GROUP BY"},{"body":"","categories":"","description":"Learn about ClickHouse queries \u0026 syntax, including Joins \u0026 Window Functions.\n","excerpt":"Learn about ClickHouse queries \u0026 syntax, including Joins \u0026 Window …","ref":"/altinity-kb-queries-and-syntax/","tags":"","title":"Queries \u0026 Syntax"},{"body":"","categories":"","description":"Functions\n","excerpt":"Functions\n","ref":"/altinity-kb-functions/","tags":"","title":"Functions"},{"body":"","categories":"","description":"Tips and tricks for using ClickHouse with different cloud services.\n","excerpt":"Tips and tricks for using ClickHouse with different cloud services.\n","ref":"/altinity-kb-integrations/altinity-cloud/","tags":"","title":"Cloud Services"},{"body":"","categories":"","description":"Learn how you can integrate cloud services, BI tools, kafka, MySQL, Spark, MindsDB, and more with ClickHouse.\n","excerpt":"Learn how you can integrate cloud services, BI tools, kafka, MySQL, …","ref":"/altinity-kb-integrations/","tags":"","title":"Integrations"},{"body":"Organizations that want to enable administrative users in their Altinity.Cloud ClickHouse servers can do so by enabling access_management manually. This allows for administrative users to be created on the specific ClickHouse Cluster.\nWARNING Modifying the ClickHouse cluster settings manually can lead to the cluster not loading or other issues. Change settings only with full consultation with an Altinity.Cloud support team member, and be ready to remove settings if they cause any disruption of service. To add the access_management setting to an Altinity.Cloud ClickHouse Cluster:\nLog into your Altinity.Cloud account.\nFor the cluster to modify, select Configure -\u003e Settings.\nCluster setting configure From the Settings page, select +ADD SETTING.\nAdd cluster setting Set the following options:\nSetting Type: Select users.d file.\nFilename: access_management.xml\nContents: Enter the following to allow the clickhouse_operator that controls the cluster through the clickhouse-operator the ability to set administrative options:\n\u003cyandex\u003e \u003cusers\u003e \u003cadmin\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/admin\u003e \u003cclickhouse_operator\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/clickhouse_operator\u003e \u003c/users\u003e \u003c/yandex\u003e access_management=1 means that users admin, clickhouse_operator are able to create users and grant them privileges using SQL.\nSelect OK. The cluster will restart, and users can now be created in the cluster that can be granted administrative access.\nIf you are running ClickHouse 21.9 and above you can enable storing access management in ZooKeeper. in this case it will be automatically propagated to the cluster. This requires yet another configuration file:\nSetting Type: Select config.d file\nFilename: user_directories.xml\nContents:\n\u003cyandex\u003e \u003cuser_directories replace=\"replace\"\u003e \u003cusers_xml\u003e \u003cpath\u003e/etc/clickhouse-server/users.xml\u003c/path\u003e \u003c/users_xml\u003e \u003creplicated\u003e \u003czookeeper_path\u003e/clickhouse/access/\u003c/zookeeper_path\u003e \u003c/replicated\u003e \u003c/user_directories\u003e \u003c/yandex\u003e ","categories":"","description":"Enabling access_management for Altinity.Cloud databases.\n","excerpt":"Enabling access_management for Altinity.Cloud databases.\n","ref":"/altinity-kb-integrations/altinity-cloud/altinity-cloud-access-management/","tags":"","title":"Altinity Cloud Access Management"},{"body":"","categories":"","description":"S3 \u0026 object storage\n","excerpt":"S3 \u0026 object storage\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/","tags":"","title":"S3 \u0026 object storage"},{"body":"","categories":"","description":"Learn how to set up, deploy, monitor, and backup ClickHouse with step-by-step guides.\n","excerpt":"Learn how to set up, deploy, monitor, and backup ClickHouse with …","ref":"/altinity-kb-setup-and-maintenance/","tags":"","title":"Setup \u0026 maintenance"},{"body":"","categories":"","description":"Access useful ClickHouse queries, from finding database size, missing blocks, checking table metadata in Zookeeper, and more.\n","excerpt":"Access useful ClickHouse queries, from finding database size, missing …","ref":"/altinity-kb-useful-queries/","tags":"","title":"Useful queries"},{"body":"","categories":"","description":"All you need to know about ClickHouse schema design, including materialized view, limitations, lowcardinality, codecs.\n","excerpt":"All you need to know about ClickHouse schema design, including …","ref":"/altinity-kb-schema-design/","tags":"","title":"Schema design"},{"body":"clickhouse-backup setup-example.yaml\n","categories":"","description":"Run ClickHouse in Kubernetes without any issues.\n","excerpt":"Run ClickHouse in Kubernetes without any issues.\n","ref":"/altinity-kb-kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"See the frequent questions users have about clickhouse-client.\n","excerpt":"See the frequent questions users have about clickhouse-client.\n","ref":"/altinity-kb-interfaces/","tags":"","title":"Interfaces"},{"body":"Normally the upgrade procedure looks like that:\npick the release to upgrade check the release notes/changelog between the release you use currently and the target release sometimes you may need to change some configuration settings to change the defaults (for better compatibility, etc) upgrade itself is simple: upgrade package (it doesn’t trigger the restart of clickhouse-server automatically) restart clickhouse-server check healthchecks / logs repeat on other nodes Mixing several versions working together in the same cluster may often lead to different degradations. Usually, it’s not recommended to have a big delay between upgrading different nodes on the same cluster. Usually, you do upgrade on the odd replicas first, and after they were back online - restart the even replicas. upgrade the dev / staging first ensure your schema/queries work properly on the staging env do the production upgrade. ","categories":"","description":"Upgrade notes.\n","excerpt":"Upgrade notes.\n","ref":"/upgrade/","tags":"","title":"Upgrade"},{"body":"For more information on ClickHouse Dictionaries, see\nthe presentation https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup34/clickhouse_integration.pdf, slides 82-95, video https://youtu.be/728Yywcd5ys?t=10642\nWe have also couple of articles about dictionaries in our blog: https://altinity.com/blog/dictionaries-explained https://altinity.com/blog/2020/5/19/clickhouse-dictionaries-reloaded\nAnd some videos: https://www.youtube.com/watch?v=FsVrFbcyb84\nAlso there 3rd party articles on the same subj. https://prog.world/how-to-create-and-use-dictionaries-in-clickhouse/\n","categories":"","description":"All you need to know about creating and using ClickHouse dictionaries.\n","excerpt":"All you need to know about creating and using ClickHouse dictionaries. …","ref":"/altinity-kb-dictionaries/","tags":"","title":"Dictionaries"},{"body":"This Knowledge Base now supports Mermaid, a handy way to create charts from text. The following example shows a very simple chart, and the code to use.\nTo add a Mermaid chart, encase the Mermaid code between {{\u003c mermaid \u003e}}, as follows:\n{{\u003cmermaid\u003e}} graph TD; A--\u003eB; A--\u003eC; B--\u003eD; C--\u003eD; {{\u003c/mermaid\u003e}} And it renders as so:\ngraph TD; A--\u003eB; A--\u003eC; B--\u003eD; C--\u003eD; ","categories":"","description":"A short example of using the Mermaid library to add charts.\n","excerpt":"A short example of using the Mermaid library to add charts.\n","ref":"/using-this-knowledgebase/mermaid_example/","tags":"","title":"Mermaid Example"},{"body":"The Altinity Knowledge Base is built on GitHub Pages, using Hugo and Docsy. This guide provides a brief description on how to make updates and add to this knowledge base.\nPage and Section Basics The knowledge base is structured in a simple directory format, with the content of the Knowledge Base stored under the directory content/en.\nEach section is a directory, with the file _index.md that provides that sections information. For example, the Upgrade section has the following layout:\n├── upgrade │ ├── _index.md │ └── removing-empty-parts.md Each Markdown file provides the section’s information and the title that is displayed on the left navigation panel, with the file _index.md providing the top level information on the section.\nEach page is set in the following format that sets the page attributes:\n--- title: \"Using This Knowledge Base\" linkTitle: \"Using This Knowledge Base\" description: \u003e How to add pages, make updates, and expand this knowledge base. weight: 11 --- The content of the page in Markdown format. The attributes are as follows:\ntitle: The title of the page displayed at the top of the page. linkTitle: The title used in the left navigation panel. description: A short description of the page listed under the title. weight: The placement of the page in the hierarchy in the left navigation panel. The higher the weight, the higher in the display order it will be. For example, the file engines/_index.md has a weight of 1, pushing its display to the top of the list. Create Pages and Sections Create or Edit A Page To create a new page or edit an existing one in the knowledge base:\nFrom the page to start from: To create a new page, select Create child page. To edit an existing page, select Edit this page. This will open the page’s location in the GitHub repository. Update the page using Markdown. See the Docsy Formatting Options section below for tips and details. View how the page will look Preview. The GitHub Preview is not 100% the same as the page will be displayed on the knowledge base, but it is a close enough approximation. Saving the file will depend on your role. For those who have been granted Knowledgebase Contributor status, select Commit New File. The changes will be automatically applied to the GitHub repository, and the additions will be displayed to the knowledge base within 1-5 minutes.\nFor those who have not been granted Knowledgebase Contributor status, they will have to fork the changes and then create a new pull request through the following process:\nWhen editing is complete, select Propose New File. This will being you to the GitHub Pull Request page.\nVerify the new file is accurate, then select Create Pull Request.\nName the Pull Request, then select Create pull request.\nFirst time contributors will be required to review and sign the Contributor License Agreement(CLA). To signify they agree with the CLA, the following comment must be left as part of the pull request:\nI have read the CLA Document and I hereby sign the CLA This signature will be stored as part of the GitHub repository indicating the GitHub username, the date of the agreement, and the pull request where the signer indicated their consent with the CLA.\nThe Pull Request will be reviewed and if approved, the changes will be applied to the Knowledge Base. Create a New Section To create a new section in the knowledge base, add a new directory under content/en from either the GitHub Repository or through some other GitHub related method., and add the file index.md. The same submission process will be followed as outlined in Create or Edit A Page.\nDocsy Formatting Options Docsy uses Markdown, providing a simple method of formatting documents. Refer to the Markdown documentation for how to edit pages and achieve the display results.\nThe following guide recommendations should be followed:\nCode should should be code segments, which uses three back tics to start and end a code section, with the type of code used. For example, if the code segment is regarding SQL then the section would start with ```sql` . Display text should be in bold. For example, when requesting someone click Create New Page on a page, Create New Page is in bold. Adding Images New images and other static files are stored in the directory static, with the following categories:\nImages are stored under static/assets. Pdf files are stored under static/assets ","categories":"","description":"Add pages, make updates, and contribute to this ClickHouse knowledge base.\n","excerpt":"Add pages, make updates, and contribute to this ClickHouse knowledge …","ref":"/using-this-knowledgebase/","tags":"","title":"Using This Knowledge Base"},{"body":"In that example, partitioning is being calculated via MATERIALIZED column expression toDate(toStartOfInterval(ts, toIntervalT(...))), but partition id also can be generated on application side and inserted to ClickHouse as is.\nCREATE TABLE tbl ( `ts` DateTime, `key` UInt32, `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalYear(1))) ) ENGINE = MergeTree PARTITION BY (partition_key, ignore(ts)) ORDER BY key; SET send_logs_level = 'trace'; INSERT INTO tbl SELECT toDateTime(toDate('2020-01-01') + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20200101-0_1_1_0 to 20200101-0_1_1_0 INSERT INTO tbl SELECT toDateTime(toDate('2021-01-01') + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20210101-0_2_2_0 to 20210101-0_2_2_0 ALTER TABLE tbl MODIFY COLUMN `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalMonth(1))); INSERT INTO tbl SELECT toDateTime(toDate('2022-01-01') + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20220101-0_3_3_0 to 20220101-0_3_3_0 Renaming temporary part tmp_insert_20220201-0_4_4_0 to 20220201-0_4_4_0 Renaming temporary part tmp_insert_20220301-0_5_5_0 to 20220301-0_5_5_0 Renaming temporary part tmp_insert_20220401-0_6_6_0 to 20220401-0_6_6_0 Renaming temporary part tmp_insert_20220501-0_7_7_0 to 20220501-0_7_7_0 Renaming temporary part tmp_insert_20220601-0_8_8_0 to 20220601-0_8_8_0 Renaming temporary part tmp_insert_20220701-0_9_9_0 to 20220701-0_9_9_0 Renaming temporary part tmp_insert_20220801-0_10_10_0 to 20220801-0_10_10_0 Renaming temporary part tmp_insert_20220901-0_11_11_0 to 20220901-0_11_11_0 Renaming temporary part tmp_insert_20221001-0_12_12_0 to 20221001-0_12_12_0 ALTER TABLE tbl MODIFY COLUMN `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalDay(1))); INSERT INTO tbl SELECT toDateTime(toDate('2023-01-01') + number) as ts, number as key FROM numbers(5); Renaming temporary part tmp_insert_20230101-0_13_13_0 to 20230101-0_13_13_0 Renaming temporary part tmp_insert_20230102-0_14_14_0 to 20230102-0_14_14_0 Renaming temporary part tmp_insert_20230103-0_15_15_0 to 20230103-0_15_15_0 Renaming temporary part tmp_insert_20230104-0_16_16_0 to 20230104-0_16_16_0 Renaming temporary part tmp_insert_20230105-0_17_17_0 to 20230105-0_17_17_0 SELECT _partition_id, min(ts), max(ts), count() FROM tbl GROUP BY _partition_id ORDER BY _partition_id; ┌─_partition_id─┬─────────────min(ts)─┬─────────────max(ts)─┬─count()─┐ │ 20200101-0 │ 2020-01-01 00:00:00 │ 2020-10-26 00:00:00 │ 300 │ │ 20210101-0 │ 2021-01-01 00:00:00 │ 2021-10-27 00:00:00 │ 300 │ │ 20220101-0 │ 2022-01-01 00:00:00 │ 2022-01-31 00:00:00 │ 31 │ │ 20220201-0 │ 2022-02-01 00:00:00 │ 2022-02-28 00:00:00 │ 28 │ │ 20220301-0 │ 2022-03-01 00:00:00 │ 2022-03-31 00:00:00 │ 31 │ │ 20220401-0 │ 2022-04-01 00:00:00 │ 2022-04-30 00:00:00 │ 30 │ │ 20220501-0 │ 2022-05-01 00:00:00 │ 2022-05-31 00:00:00 │ 31 │ │ 20220601-0 │ 2022-06-01 00:00:00 │ 2022-06-30 00:00:00 │ 30 │ │ 20220701-0 │ 2022-07-01 00:00:00 │ 2022-07-31 00:00:00 │ 31 │ │ 20220801-0 │ 2022-08-01 00:00:00 │ 2022-08-31 00:00:00 │ 31 │ │ 20220901-0 │ 2022-09-01 00:00:00 │ 2022-09-30 00:00:00 │ 30 │ │ 20221001-0 │ 2022-10-01 00:00:00 │ 2022-10-27 00:00:00 │ 27 │ │ 20230101-0 │ 2023-01-01 00:00:00 │ 2023-01-01 00:00:00 │ 1 │ │ 20230102-0 │ 2023-01-02 00:00:00 │ 2023-01-02 00:00:00 │ 1 │ │ 20230103-0 │ 2023-01-03 00:00:00 │ 2023-01-03 00:00:00 │ 1 │ │ 20230104-0 │ 2023-01-04 00:00:00 │ 2023-01-04 00:00:00 │ 1 │ │ 20230105-0 │ 2023-01-05 00:00:00 │ 2023-01-05 00:00:00 │ 1 │ └───────────────┴─────────────────────┴─────────────────────┴─────────┘ SELECT count() FROM tbl WHERE ts \u003e '2023-01-04'; Key condition: unknown MinMax index condition: (column 0 in [1672758001, +Inf)) Selected 1/17 parts by partition key, 1 parts by primary key, 1/1 marks by primary key, 1 marks to read from 1 ranges Spreading mark ranges among streams (default reading) Reading 1 ranges in order from part 20230105-0_17_17_0, approx. 1 rows starting from 0 ","categories":"","description":"Approach, which allow you to redefine partitioning without table creation.","excerpt":"Approach, which allow you to redefine partitioning without table …","ref":"/altinity-kb-queries-and-syntax/variable-partitioning/","tags":"","title":"Adjustable table partitioning"},{"body":"What happened After ClickHouse upgrade from version pre 21.6 to version after 21.6, count of unique UUID in AggregatingMergeTree tables nearly doubled in case of merging of data which was generated in different ClickHouse versions.\nWhy happened In pull request which changed the internal representation of big integers data types (and UUID). SipHash64 hash-function used for uniq aggregation function for UUID data type was replaced with intHash64, which leads to different result for the same UUID value across different ClickHouse versions. Therefore, it results in doubling of counts, when uniqState created by different ClickHouse versions being merged together.\nRelated issue.\nSolution You need to replace any occurrence of uniqState(uuid) in MATERIALIZED VIEWs with uniqState(sipHash64(uuid)) and change data type for already saved data from AggregateFunction(uniq, UUID) to AggregateFunction(uniq, UInt64), because result data type of sipHash64 is UInt64.\n-- On ClickHouse version 21.3 CREATE TABLE uniq_state ( `key` UInt32, `value` AggregateFunction(uniq, UUID) ) ENGINE = MergeTree ORDER BY key INSERT INTO uniq_state SELECT number % 10000 AS key, uniqState(reinterpretAsUUID(number)) FROM numbers(1000000) GROUP BY key Ok. 0 rows in set. Elapsed: 0.404 sec. Processed 1.05 million rows, 8.38 MB (2.59 million rows/s., 20.74 MB/s.) SELECT key % 20, uniqMerge(value) FROM uniq_state GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┐ │ 0 │ 50000 │ │ 1 │ 50000 │ │ 2 │ 50000 │ │ 3 │ 50000 │ │ 4 │ 50000 │ │ 5 │ 50000 │ │ 6 │ 49999 │ │ 7 │ 50000 │ │ 8 │ 49999 │ │ 9 │ 50000 │ │ 10 │ 50000 │ │ 11 │ 50000 │ │ 12 │ 50000 │ │ 13 │ 50000 │ │ 14 │ 50000 │ │ 15 │ 50000 │ │ 16 │ 50000 │ │ 17 │ 50000 │ │ 18 │ 50000 │ │ 19 │ 50000 │ └─────────────────┴──────────────────┘ -- After upgrade of ClickHouse to 21.8 SELECT key % 20, uniqMerge(value) FROM uniq_state GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┐ │ 0 │ 50000 │ │ 1 │ 50000 │ │ 2 │ 50000 │ │ 3 │ 50000 │ │ 4 │ 50000 │ │ 5 │ 50000 │ │ 6 │ 49999 │ │ 7 │ 50000 │ │ 8 │ 49999 │ │ 9 │ 50000 │ │ 10 │ 50000 │ │ 11 │ 50000 │ │ 12 │ 50000 │ │ 13 │ 50000 │ │ 14 │ 50000 │ │ 15 │ 50000 │ │ 16 │ 50000 │ │ 17 │ 50000 │ │ 18 │ 50000 │ │ 19 │ 50000 │ └─────────────────┴──────────────────┘ 20 rows in set. Elapsed: 0.240 sec. Processed 10.00 thousand rows, 1.16 MB (41.72 thousand rows/s., 4.86 MB/s.) CREATE TABLE uniq_state_2 ENGINE = MergeTree ORDER BY key AS SELECT * FROM uniq_state Ok. 0 rows in set. Elapsed: 0.128 sec. Processed 10.00 thousand rows, 1.16 MB (78.30 thousand rows/s., 9.12 MB/s.) INSERT INTO uniq_state_2 SELECT number % 10000 AS key, uniqState(reinterpretAsUUID(number)) FROM numbers(1000000) GROUP BY key Ok. 0 rows in set. Elapsed: 0.266 sec. Processed 1.05 million rows, 8.38 MB (3.93 million rows/s., 31.48 MB/s.) SELECT key % 20, uniqMerge(value) FROM uniq_state_2 GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┐ │ 0 │ 99834 │ \u003c- Count of unique values nearly doubled. │ 1 │ 100219 │ │ 2 │ 100128 │ │ 3 │ 100457 │ │ 4 │ 100272 │ │ 5 │ 100279 │ │ 6 │ 99372 │ │ 7 │ 99450 │ │ 8 │ 99974 │ │ 9 │ 99632 │ │ 10 │ 99562 │ │ 11 │ 100660 │ │ 12 │ 100439 │ │ 13 │ 100252 │ │ 14 │ 100650 │ │ 15 │ 99320 │ │ 16 │ 100095 │ │ 17 │ 99632 │ │ 18 │ 99540 │ │ 19 │ 100098 │ └─────────────────┴──────────────────┘ 20 rows in set. Elapsed: 0.356 sec. Processed 20.00 thousand rows, 2.33 MB (56.18 thousand rows/s., 6.54 MB/s.) CREATE TABLE uniq_state_3 ENGINE = MergeTree ORDER BY key AS SELECT * FROM uniq_state 0 rows in set. Elapsed: 0.126 sec. Processed 10.00 thousand rows, 1.16 MB (79.33 thousand rows/s., 9.24 MB/s.) -- Option 1, create separate column ALTER TABLE uniq_state_3 ADD COLUMN `value_2` AggregateFunction(uniq, UInt64) DEFAULT unhex(hex(value)); ALTER TABLE uniq_state_3 UPDATE value_2 = value_2 WHERE 1; SELECT * FROM system.mutations WHERE is_done = 0; Ok. 0 rows in set. Elapsed: 0.008 sec. INSERT INTO uniq_state_3 (key, value_2) SELECT number % 10000 AS key, uniqState(sipHash64(reinterpretAsUUID(number))) FROM numbers(1000000) GROUP BY key Ok. 0 rows in set. Elapsed: 0.337 sec. Processed 1.05 million rows, 8.38 MB (3.11 million rows/s., 24.89 MB/s.) SELECT key % 20, uniqMerge(value), uniqMerge(value_2) FROM uniq_state_3 GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┬─uniqMerge(value_2)─┐ │ 0 │ 50000 │ 50000 │ │ 1 │ 50000 │ 50000 │ │ 2 │ 50000 │ 50000 │ │ 3 │ 50000 │ 50000 │ │ 4 │ 50000 │ 50000 │ │ 5 │ 50000 │ 50000 │ │ 6 │ 49999 │ 49999 │ │ 7 │ 50000 │ 50000 │ │ 8 │ 49999 │ 49999 │ │ 9 │ 50000 │ 50000 │ │ 10 │ 50000 │ 50000 │ │ 11 │ 50000 │ 50000 │ │ 12 │ 50000 │ 50000 │ │ 13 │ 50000 │ 50000 │ │ 14 │ 50000 │ 50000 │ │ 15 │ 50000 │ 50000 │ │ 16 │ 50000 │ 50000 │ │ 17 │ 50000 │ 50000 │ │ 18 │ 50000 │ 50000 │ │ 19 │ 50000 │ 50000 │ └─────────────────┴──────────────────┴────────────────────┘ 20 rows in set. Elapsed: 0.768 sec. Processed 20.00 thousand rows, 4.58 MB (26.03 thousand rows/s., 5.96 MB/s.) -- Option 2, modify column in-place with String as intermediate data type. ALTER TABLE uniq_state_3 MODIFY COLUMN `value` String Ok. 0 rows in set. Elapsed: 0.280 sec. ALTER TABLE uniq_state_3 MODIFY COLUMN `value` AggregateFunction(uniq, UInt64) Ok. 0 rows in set. Elapsed: 0.254 sec. INSERT INTO uniq_state_3 (key, value) SELECT number % 10000 AS key, uniqState(sipHash64(reinterpretAsUUID(number))) FROM numbers(1000000) GROUP BY key Ok. 0 rows in set. Elapsed: 0.554 sec. Processed 1.05 million rows, 8.38 MB (1.89 million rows/s., 15.15 MB/s.) SELECT key % 20, uniqMerge(value), uniqMerge(value_2) FROM uniq_state_3 GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┬─uniqMerge(value_2)─┐ │ 0 │ 50000 │ 50000 │ │ 1 │ 50000 │ 50000 │ │ 2 │ 50000 │ 50000 │ │ 3 │ 50000 │ 50000 │ │ 4 │ 50000 │ 50000 │ │ 5 │ 50000 │ 50000 │ │ 6 │ 49999 │ 49999 │ │ 7 │ 50000 │ 50000 │ │ 8 │ 49999 │ 49999 │ │ 9 │ 50000 │ 50000 │ │ 10 │ 50000 │ 50000 │ │ 11 │ 50000 │ 50000 │ │ 12 │ 50000 │ 50000 │ │ 13 │ 50000 │ 50000 │ │ 14 │ 50000 │ 50000 │ │ 15 │ 50000 │ 50000 │ │ 16 │ 50000 │ 50000 │ │ 17 │ 50000 │ 50000 │ │ 18 │ 50000 │ 50000 │ │ 19 │ 50000 │ 50000 │ └─────────────────┴──────────────────┴────────────────────┘ 20 rows in set. Elapsed: 0.589 sec. Processed 30.00 thousand rows, 6.87 MB (50.93 thousand rows/s., 11.66 MB/s.) SHOW CREATE TABLE uniq_state_3; CREATE TABLE default.uniq_state_3 ( `key` UInt32, `value` AggregateFunction(uniq, UInt64), `value_2` AggregateFunction(uniq, UInt64) DEFAULT unhex(hex(value)) ) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity = 8192 -- Option 3, CAST uniqState(UInt64) to String. CREATE TABLE uniq_state_4 ENGINE = MergeTree ORDER BY key AS SELECT * FROM uniq_state Ok. 0 rows in set. Elapsed: 0.146 sec. Processed 10.00 thousand rows, 1.16 MB (68.50 thousand rows/s., 7.98 MB/s.) INSERT INTO uniq_state_4 (key, value) SELECT number % 10000 AS key, CAST(uniqState(sipHash64(reinterpretAsUUID(number))), 'String') FROM numbers(1000000) GROUP BY key Ok. 0 rows in set. Elapsed: 0.476 sec. Processed 1.05 million rows, 8.38 MB (2.20 million rows/s., 17.63 MB/s.) SELECT key % 20, uniqMerge(value) FROM uniq_state_4 GROUP BY key % 20 ┌─modulo(key, 20)─┬─uniqMerge(value)─┐ │ 0 │ 50000 │ │ 1 │ 50000 │ │ 2 │ 50000 │ │ 3 │ 50000 │ │ 4 │ 50000 │ │ 5 │ 50000 │ │ 6 │ 49999 │ │ 7 │ 50000 │ │ 8 │ 49999 │ │ 9 │ 50000 │ │ 10 │ 50000 │ │ 11 │ 50000 │ │ 12 │ 50000 │ │ 13 │ 50000 │ │ 14 │ 50000 │ │ 15 │ 50000 │ │ 16 │ 50000 │ │ 17 │ 50000 │ │ 18 │ 50000 │ │ 19 │ 50000 │ └─────────────────┴──────────────────┘ 20 rows in set. Elapsed: 0.281 sec. Processed 20.00 thousand rows, 2.33 MB (71.04 thousand rows/s., 8.27 MB/s.) SHOW CREATE TABLE uniq_state_4; CREATE TABLE default.uniq_state_4 ( `key` UInt32, `value` AggregateFunction(uniq, UUID) ) ENGINE = MergeTree ORDER BY key SETTINGS index_granularity = 8192 ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/altinity-kb-setup-and-maintenance/uniq-uuid-doubled-clickhouse-upgrade/","tags":"","title":"AggregateFunction(uniq, UUID) doubled after ClickHouse upgrade"},{"body":"Using AWS IAM — Identity and Access Management roles For EC2 instance, there is an option to configure an IAM role:\nRole shall contain a policy with permissions like:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"allow-put-and-get\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\" ], \"Resource\": \"arn:aws:s3:::BUCKET_NAME/test_s3_disk/*\" } ] } Corresponding configuration of ClickHouse:\n\u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cdisk_s3\u003e \u003ctype\u003es3\u003c/type\u003e \u003cendpoint\u003ehttp://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/\u003c/endpoint\u003e \u003cuse_environment_credentials\u003etrue\u003c/use_environment_credentials\u003e \u003c/disk_s3\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cpolicy_s3_only\u003e \u003cvolumes\u003e \u003cvolume_s3\u003e \u003cdisk\u003edisk_s3\u003c/disk\u003e \u003c/volume_s3\u003e \u003c/volumes\u003e \u003c/policy_s3_only\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e Small check:\nCREATE TABLE table_s3 (number Int64) ENGINE=MergeTree() ORDER BY tuple() PARTITION BY tuple() SETTINGS storage_policy='policy_s3_only'; INSERT INTO table_s3 SELECT * FROM system.numbers LIMIT 100000000; SELECT * FROM table_s3; DROP TABLE table_s3; How to use AWS IRSA and IAM in Altinity Kubernetes clickhouse-operator to allow S3 backup without Explicit credentials Install clickhouse-operator https://github.com/Altinity/clickhouse-operator/tree/master/docs/operator_installation_details.md\nCreate Role and IAM Policy, look details in https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html\nCreate service account with annotations\napiVersion: v1 kind: ServiceAccount metadata: name: \u003cSERVICE ACOUNT NAME\u003e namespace: \u003cNAMESPACE\u003e annotations: eks.amazonaws.com/role-arn: arn:aws:iam::\u003cACCOUNT_ID\u003e:role/\u003cROLE_NAME\u003e Link service account to podTemplate it will create AWS_ROLE_ARN and AWS_WEB_IDENTITY_TOKEN_FILE environment variables.\napiVersion: \"clickhouse.altinity.com/v1\" kind: \"ClickHouseInstallation\" metadata: name: \u003cNAME\u003e namespace: \u003cNAMESPACE\u003e spec: defaults: templates: podTemplate: \u003cPOD_TEMPLATE_NAME\u003e templates: podTemplates: - name: \u003cPOD_TEMPLATE_NAME\u003e spec: serviceAccountName: \u003cSERVICE ACCOUNT NAME\u003e containers: - name: clickhouse-backup ","categories":"","description":"AWS S3 Recipes","excerpt":"AWS S3 Recipes","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/aws-s3-recipes/","tags":"","title":"AWS S3 Recipes"},{"body":"Can not connect to my ClickHouse server Errors like “Connection reset by peer, while reading from socket”\nEnsure that the clickhouse-server is running\nsystemctl status clickhouse-server If server was restarted recently and don’t accept the connections after the restart - most probably it still just starting. During the startup sequence it need to iterate over all data folders in /var/lib/clickhouse-server In case if you have a very high number of folders there (usually caused by a wrong partitioning, or a very high number of tables / databases) that startup time can take a lot of time (same can happen if disk is very slow, for example NFS).\nYou can check that by looking for ‘Ready for connections’ line in /var/log/clickhouse-server/clickhouse-server.log (Information log level neede)\nEnsure you use the proper port ip / interface?\nEnsure you’re not trying to connect to secure port without tls / https or vice versa.\nFor clickhouse-client - pay attention on host / port / secure flags.\nEnsure the interface you’re connecting to is the one which clickhouse listens (by default clickhouse listens only localhost).\nNote: If you uncomment line \u003clisten_host\u003e0.0.0.0\u003c/listen_host\u003e only - clickhouse will listen only ipv4 interfaces, while the localhost (used by clickhouse-client) may be resolved to ipv6 address. And clickhouse-client may be failing to connect.\nHow to check which interfaces / ports do clickhouse listen?\nsudo lsof -i -P -n | grep LISTEN echo listen_host sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=listen_host echo tcp_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=tcp_port echo tcp_port_secure sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=tcp_port_secure echo http_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=http_port echo https_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=https_port For secure connection:\nensure that server uses some certificate which can be validated by the client OR disable certificate checks on the client (UNSECURE) Check for errors in /var/log/clickhouse-server/clickhouse-server.err.log ?\nIs clickhouse able to serve some trivial tcp / http requests from localhost?\ncurl 127.0.0.1:9200 curl 127.0.0.1:8123 Check number of sockets opened by clickhouse\nsudo lsof -i -a -p $(pidof clickhouse-server) # or (adjust 9000 / 8123 ports if needed) netstat -tn 2\u003e/dev/null | tail -n +3 | awk '{ printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, $3, $4, $5, $6) }' | clickhouse-local -S \"Proto String, RecvQ Int64, SendQ Int64, LocalAddress String, ForeignAddress String, State LowCardinality(String)\" --query=\"SELECT * FROM table WHERE LocalAddress like '%:9000' FORMAT PrettyCompact\" netstat -tn 2\u003e/dev/null | tail -n +3 | awk '{ printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, $3, $4, $5, $6) }' | clickhouse-local -S \"Proto String, RecvQ Int64, SendQ Int64, LocalAddress String, ForeignAddress String, State LowCardinality(String)\" --query=\"SELECT * FROM table WHERE LocalAddress like '%:8123' FORMAT PrettyCompact\" ClickHouse has a limit of number of open connections (4000 by default).\nCheck also:\n# system overall support limited number of connections it can handle netstat # you can also be reaching of of the process ulimits (Max open files) cat /proc/$(pidof -s clickhouse-server)/limits Check firewall / selinux rules (if used)\n","categories":"","description":"Can not connect to my ClickHouse server.","excerpt":"Can not connect to my ClickHouse server.","ref":"/altinity-kb-setup-and-maintenance/connection-problems/","tags":"","title":"Can not connect to my ClickHouse server"},{"body":"cgroups and kubernetes cloud providers Why my ClickHouse is slow after upgrade to version 22.2 and higher?\nThe probable reason is that ClickHouse 22.2 started to respect cgroups (Respect cgroups limits in max_threads autodetection. #33342 (JaySon).\nYou can observe that max_threads = 1\nSELECT name, value FROM system.settings WHERE name = 'max_threads' ┌─name────────┬─value─────┐ │ max_threads │ 'auto(1)' │ └─────────────┴───────────┘ This makes ClickHouse to execute all queries with a single thread (normal behavior is half of available CPU cores, cores = 64, then ‘auto(32)’).\nWe observe this cgroups behavior with AWS EKS (Kubernetes) environment and Altinity ClickHouse Operator in case if requests.cpu and limits.cpu are not set for a resource.\nWorkaround We suggest to set requests.cpu = half of available CPU cores, and limits.cpu = CPU cores.\nFor example in case of 16 CPU cores:\nresources: requests: memory: ... cpu: 8 limits: memory: .... cpu: 16 Then you should get a new result:\nSELECT name, value FROM system.settings WHERE name = 'max_threads' ┌─name────────┬─value─────┐ │ max_threads │ 'auto(8)' │ └─────────────┴───────────┘ in depth For some reason AWS EKS sets cgroup kernel parameters in case of empty requests.cpu \u0026 limits.cpu into these:\n# cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us -1 # cat /sys/fs/cgroup/cpu/cpu.cfs_period_us 100000 # cat /sys/fs/cgroup/cpu/cpu.shares 2 This makes ClickHouse to set max_threads = 1 because of\ncgroup_share = /sys/fs/cgroup/cpu/cpu.shares (2) PER_CPU_SHARES = 1024 share_count = ceil( cgroup_share / PER_CPU_SHARES ) ---\u003e ceil(2 / 1024) ---\u003e 1 Fix Incorrect calculation was fixed in https://github.com/ClickHouse/ClickHouse/pull/35815 and will work correctly on newer releases.\n","categories":"","description":"cgroups and kubernetes cloud providers.","excerpt":"cgroups and kubernetes cloud providers.","ref":"/altinity-kb-setup-and-maintenance/cgroups_k8s/","tags":"","title":"cgroups and kubernetes cloud providers"},{"body":"Compare table metadata of different replicas in zookeeper Metadata on replica is not up to date with common metadata in Zookeeper\nSELECT *, if( neighbor(name, -1) == name and name != 'is_active', neighbor(value, -1) == value , 1) as looks_good FROM ( SELECT name, path, ctime, mtime, value FROM system.zookeeper WHERE (path IN ( SELECT arrayJoin(groupUniqArray(if(path LIKE '%/replicas', concat(path, '/', name), path))) FROM system.zookeeper WHERE path IN ( SELECT arrayJoin([zookeeper_path, concat(zookeeper_path, '/replicas')]) FROM system.replicas WHERE table = 'test_repl' ) )) AND (name IN ('metadata', 'columns', 'is_active')) ORDER BY name = 'is_active', name ASC, path ASC ) vs.\nSELECT metadata_modification_time, create_table_query FROM system.tables WHERE name = 'test_repl' ","categories":"","description":"Check table metadata in zookeeper.","excerpt":"Check table metadata in zookeeper.","ref":"/altinity-kb-useful-queries/table-meta-in-zookeeper/","tags":"","title":"Check table metadata in zookeeper"},{"body":"In general ClickHouse should work with any POSIX-compatible filesystem.\nhard links and soft links support is mandatory. clickhouse can use O_DIRECT mode to bypass the cache (and async io) clickhouse can use renameat2 command for some atomic operations (not all the filesystems support that). depending on the schema and details of the usage the filesystem load can vary between the setup. The most natural load - is high throughput, with low or moderate IOPS. data is compressed in clickhouse (LZ4 by default), while indexes / marks / metadata files - no. Enabling disk-level compression can sometimes improve the compression, but can affect read / write speed. ext4 no issues, fully supported.\nThe minimum kernel version required is 3.15 (newer are recommended)\nXFS Performance issues reported by users, use on own risk. Old kernels are not recommended (4.0 or newer is recommended).\nAccording to the users’ feedback, XFS behaves worse with ClickHouse under heavy load. We don’t have real proofs/benchmarks though, example reports:\nIn GitHub there are complaints about XFS from Cloudflare. Recently my colleague discovered that two of ClickHouse servers perform worse in a cluster than others and they found that they accidentally set up those servers with XFS instead of Ext4. in the system journal you can sometimes see reports like ’task XYZ blocked for more than 120 seconds’ and stack trace pointing to XFS code (example: https://gist.github.com/filimonov/85b894268f978c2ccc18ea69bae5adbd ) system goes to 99% io kernel under load sometimes. we have XFS, sometimes clickhouse goes to “sleep” because XFS daemon is doing smth unknown Maybe the above problem can be workaround by some tuning/settings, but so far we do not have a working and confirmed way to do this.\nZFS Limitations exist, extra tuning may be needed, and having more RAM is recommended. Old kernels are not recommended.\nMemory usage control - ZFS adaptive replacement cache (ARC) can take a lot of RAM. It can be the reason of out-of-memory issues when memory is also requested by the ClickHouse.\nIt seems that the most important thing is zfs_arc_max - you just need to limit the maximum size of the ARC so that the sum of the maximum size of the arc + the CH itself does not exceed the size of the available RAM. For example, we set a limit of 80% RAM for Clickhouse and 10% for ARC. 10% will remain for the system and other applications Tuning:\nanother potentially interesting setting is primarycache=metadata, see benchmark example: https://www.ikus-soft.com/en/blog/2018-05-23-proxmox-primarycache-all-metadata/ examples of tuning ZFS for MySQL https://wiki.freebsd.org/ZFSTuningGuide - perhaps some of this can also be useful (atime, recordsize) but everything needs to be carefully checked with benchmarks (I have no way). best practices: https://efim360.ru/zfs-best-practices-guide/ important note: ZFS does not support the renameat2 command, which is used by the Atomic database engine, and therefore some of the Atomic functionality will not be available.\nIn old versions of clickhouse, you can face issues with the O_DIRECT mode.\nAlso there is a well-known (and controversional) Linus Torvalds opinion: “Don’t Use ZFS on Linux” [1], [2], [3].\nBTRFS Not enough information. Some users report performance improvement for their use case.\nReiserFS Not enough information.\nLustre There are reports that some people successfully use it in their setups. A fast network is required.\nThere were some reports about data damage on the disks on older clickhouse versions, which could be caused by the issues with O_DIRECT or async io support on Lustre.\nNFS (and EFS) Accouding to the reports - it works, throughput depends a lot on the network speed. IOPS / number of file operations per seconds can be super low (due to the locking mechanism).\nhttps://github.com/ClickHouse/ClickHouse/issues/31113\nMooseFS There are installations using that. No extra info.\nGlusterFS There are installations using that. No extra info.\nCeph There are installations using that. Some information: https://github.com/ClickHouse/ClickHouse/issues/8315\n","categories":"","description":"ClickHouse and different filesystems.","excerpt":"ClickHouse and different filesystems.","ref":"/altinity-kb-setup-and-maintenance/filesystems/","tags":"","title":"ClickHouse and different filesystems"},{"body":"ClickHouse 22.8 Starting from 22.8 version, ClickHouse support writing logs in JSON format:\n\u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003clogger\u003e \u003c!-- Structured log formatting: You can specify log format(for now, JSON only). In that case, the console log will be printed in specified format like JSON. For example, as below: {\"date_time\":\"1650918987.180175\",\"thread_name\":\"#1\",\"thread_id\":\"254545\",\"level\":\"Trace\",\"query_id\":\"\",\"logger_name\":\"BaseDaemon\",\"message\":\"Received signal 2\",\"source_file\":\"../base/daemon/BaseDaemon.cpp; virtual void SignalListener::run()\",\"source_line\":\"192\"} To enable JSON logging support, just uncomment \u003cformatting\u003e tag below. --\u003e \u003cformatting\u003ejson\u003c/formatting\u003e \u003c/logger\u003e \u003c/clickhouse\u003e Transformation Clickhouse logs to ndjson using Vector.dev\" Installation of vector.dev # arm64 wget https://packages.timber.io/vector/0.15.2/vector_0.15.2-1_arm64.deb # amd64 wget https://packages.timber.io/vector/0.15.2/vector_0.15.2-1_amd64.deb dpkg -i vector_0.15.2-1_*.deb systemctl stop vector mkdir /var/log/clickhouse-server-json chown vector.vector /var/log/clickhouse-server-json usermod -a -G clickhouse vector vector config # cat /etc/vector/vector.toml data_dir = \"/var/lib/vector\" [sources.clickhouse-log] type = \"file\" include = [ \"/var/log/clickhouse-server/clickhouse-server.log\" ] fingerprinting.strategy = \"device_and_inode\" message_start_indicator = '^\\d+\\.\\d+\\.\\d+ \\d+:\\d+:\\d+' multi_line_timeout = 1000 [transforms.clickhouse-log-text] inputs = [ \"clickhouse-log\" ] type = \"remap\" source = ''' . |= parse_regex!(.message, r'^(?P\u003ctimestamp\u003e\\d+\\.\\d+\\.\\d+ \\d+:\\d+:\\d+\\.\\d+) \\[\\s?(?P\u003cthread_id\u003e\\d+)\\s?\\] \\{(?P\u003cquery_id\u003e.*)\\} \u003c(?P\u003cseverity\u003e\\w+)\u003e (?s)(?P\u003cmessage\u003e.*$)') ''' [sinks.emit-clickhouse-log-json] type = \"file\" inputs = [ \"clickhouse-log-text\" ] compression = \"none\" path = \"/var/log/clickhouse-server-json/clickhouse-server.%Y-%m-%d.ndjson\" encoding.only_fields = [\"timestamp\", \"thread_id\", \"query_id\", \"severity\", \"message\" ] encoding.codec = \"ndjson\" start systemctl start vector tail /var/log/clickhouse-server-json/clickhouse-server.2022-04-21.ndjson {\"message\":\"DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 166.80 GiB.\",\"query_id\":\"\",\"severity\":\"Debug\",\"thread_id\":\"283239\",\"timestamp\":\"2022.04.21 13:43:21.164660\"} {\"message\":\"MergedBlockOutputStream: filled checksums 202204_67118_67118_0 (state Temporary)\",\"query_id\":\"\",\"severity\":\"Trace\",\"thread_id\":\"283239\",\"timestamp\":\"2022.04.21 13:43:21.166810\"} {\"message\":\"system.metric_log (e3365172-4c9b-441b-b803-756ae030e741): Renaming temporary part tmp_insert_202204_67118_67118_0 to 202204_171703_171703_0.\",\"query_id\":\"\",\"severity\":\"Trace\",\"thread_id\":\"283239\",\"timestamp\":\"2022.04.21 13:43:21.167226\"} .... sink logs into ClickHouse table Be carefull with logging ClickHouse messages into the same ClickHouse instance, it will cause endless recursive self-logging.\ncreate table default.clickhouse_logs( timestamp DateTime64(3), host LowCardinality(String), thread_id LowCardinality(String), severity LowCardinality(String), query_id String, message String) Engine = MergeTree Partition by toYYYYMM(timestamp) Order by (toStartOfHour(timestamp), host, severity, query_id); create user vector identified by 'vector1234'; grant insert on default.clickhouse_logs to vector; create settings profile or replace profile_vector settings log_queries=0 readonly TO vector; [sinks.clickhouse-output-clickhouse] inputs = [\"clickhouse-log-text\"] type = \"clickhouse\" host = \"http://localhost:8123\" database = \"default\" auth.strategy = \"basic\" auth.user = \"vector\" auth.password = \"vector1234\" healthcheck = true table = \"clickhouse_logs\" encoding.timestamp_format = \"unix\" buffer.type = \"disk\" buffer.max_size = 104900000 buffer.when_full = \"block\" request.in_flight_limit = 20 encoding.only_fields = [\"host\", \"timestamp\", \"thread_id\", \"query_id\", \"severity\", \"message\"] select * from default.clickhouse_logs limit 10; ┌───────────────timestamp─┬─host───────┬─thread_id─┬─severity─┬─query_id─┬─message───────────────────────────────────────────────────── │ 2022-04-21 19:08:13.443 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: 13e87050-7824-46b0-9bd5-29469a1b102f Authentic │ 2022-04-21 19:08:13.443 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: 13e87050-7824-46b0-9bd5-29469a1b102f Authentic │ 2022-04-21 19:08:13.443 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: 13e87050-7824-46b0-9bd5-29469a1b102f Creating │ 2022-04-21 19:08:13.447 │ clickhouse │ 283155 │ Debug │ │ MemoryTracker: Peak memory usage (for query): 4.00 MiB. │ 2022-04-21 19:08:13.447 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: 13e87050-7824-46b0-9bd5-29469a1b102f Destroyin │ 2022-04-21 19:08:13.495 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: f7eb829f-7b3a-4c43-8a41-a2e6676177fb Authentic │ 2022-04-21 19:08:13.495 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: f7eb829f-7b3a-4c43-8a41-a2e6676177fb Authentic │ 2022-04-21 19:08:13.495 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: f7eb829f-7b3a-4c43-8a41-a2e6676177fb Creating │ 2022-04-21 19:08:13.496 │ clickhouse │ 283155 │ Debug │ │ MemoryTracker: Peak memory usage (for query): 4.00 MiB. │ 2022-04-21 19:08:13.496 │ clickhouse │ 283155 │ Debug │ │ HTTP-Session: f7eb829f-7b3a-4c43-8a41-a2e6676177fb Destroyin └─────────────────────────┴────────────┴───────────┴──────────┴──────────┴───────────────────────────────────────────────────────────── ","categories":"","description":"Transformation Clickhouse logs to ndjson using Vector.dev","excerpt":"Transformation Clickhouse logs to ndjson using Vector.dev","ref":"/altinity-kb-setup-and-maintenance/ch-logs-2-json-vectordev/","tags":"","title":"Transformation Clickhouse logs to ndjson using Vector.dev"},{"body":"ClickHouse operator https://github.com/Altinity/clickhouse-operator/blob/master/docs/README.md\n","categories":"","description":"ClickHouse operator","excerpt":"ClickHouse operator","ref":"/altinity-kb-setup-and-maintenance/clickhouse-operator/","tags":"","title":"ClickHouse operator"},{"body":"ClickHouse row-level deduplication. (Block level deduplication exists in Replicated tables, and is not the subject of that article).\nThere is quite common requirement to do deduplication on a record level in ClickHouse.\nSometimes duplicates are appear naturally on collector side. Sometime they appear due the the fact that message queue system (Kafka/Rabbit/etc) offers at-least-once guarantees. Sometimes you just expect insert idempotency on row level. For now that problem has no good solution in general case using ClickHouse only.\nThe reason in simple: to check if the row already exists you need to do some lookup (key-value) alike (ClickHouse is bad for key-value lookups), in general case - across the whole huge table (which can be terabyte/petabyte size).\nBut there many usecase when you can archive something like row-level deduplication in ClickHouse:\nApproach 0. Make deduplication before ingesting data to ClickHouse\nyou have full control extra coding and ‘moving parts’, storing some ids somewhere clean and simple schema and selects in ClickHouse ! check if row exists in clickhouse before insert can give non-satisfing results if you use ClickHouse cluster (i.e. Replicated / Distributed tables) - due to eventual consistency. Approach 1. Allow duplicates during ingestion. Remove them on SELECT level (by things like GROUP BY)\nsimple inserts complicate selects all selects will be significantly slower Approach 2. Eventual deduplication using Replacing\nsimple can force you to use suboptimal primary key (which will guarantee record uniqueness) deduplication is eventual - you never know when it will happen, and you will get some duplicates if you don’t use FINAL clause selects with FINAL clause (select * from table_name FINAL) are much slower and may require tricky manual optimization https://github.com/ClickHouse/ClickHouse/issues/31411 can work with acceptable speed in some special conditions: https://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/ Approach 3. Eventual deduplication using Collapsing\ncomplicated can force you to use suboptimal primary key (which will guarantee record uniqueness) you need to store previous state of the record somewhere, or extract it before ingestion from clickhouse deduplication is eventual (same as with Replacing) you can make the proper aggregations of last state w/o FINAL (bookkeeping-alike sums, counts etc) Approach 4. Eventual deduplication using Summing with SimpleAggregateFunction( anyLast, …), Aggregating with argMax etc.\nquite complicated can force you to use suboptimal primary key (which will guarantee record uniqueness) deduplication is eventual (same as with Replacing) but you can finish deduplication with GROUP BY instead if FINAL (it’s faster) Approach 5. Keep data fragment where duplicates are possible isolated. Usually you can expect the duplicates only in some time window (like 5 minutes, or one hour, or something like that). You can put that ‘dirty’ data in separate place, and put it to final MergeTree table after deduplication window timeout. For example - you insert data in some tiny tables (Engine=StripeLog) with minute suffix, and move data from tinytable older that X minutes to target MergeTree (with some external queries). In the meanwhile you can see realtime data using Engine=Merge / VIEWs etc.\nquite complicated good control no duplicated in target table perfect ingestion speed Approach 6. Deduplication using MV pipeline. You insert into some temporary table (even with Engine=Null) and MV do join or subselect (which will check the existence of arrived rows in some time frame of target table) and copy new only rows to destination table.\ndon’t impact the select speed complicated for clusters can be inaccurate due to eventual consistency slow down inserts significantly (every insert will need to do lookup in target table first) In all case: due to eventual consistency of ClickHouse replication you can still get duplicates if you insert into different replicas/shards.\n","categories":"","description":"ClickHouse row-level deduplication.","excerpt":"ClickHouse row-level deduplication.","ref":"/altinity-kb-schema-design/row-level-deduplication/","tags":"","title":"ClickHouse row-level deduplication"},{"body":"clickhouse-keeper-initd An init.d script for clickhouse-keeper. This example is based on zkServer.sh\n#!/bin/bash ### BEGIN INIT INFO # Provides: clickhouse-keeper # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Required-Start: # Required-Stop: # Short-Description: Start keeper daemon # Description: Start keeper daemon ### END INIT INFO NAME=clickhouse-keeper ZOOCFGDIR=/etc/$NAME ZOOCFG=\"$ZOOCFGDIR/keeper.xml\" ZOO_LOG_DIR=/var/log/$NAME USER=clickhouse GROUP=clickhouse ZOOPIDDIR=/var/run/$NAME ZOOPIDFILE=$ZOOPIDDIR/$NAME.pid SCRIPTNAME=/etc/init.d/$NAME #echo \"Using config: $ZOOCFG\" \u003e\u00262 ZOOCMD=\"clickhouse-keeper -C ${ZOOCFG} start --daemon\" # ensure PIDDIR exists, otw stop will fail mkdir -p \"$(dirname \"$ZOOPIDFILE\")\" if [ ! -w \"$ZOO_LOG_DIR\" ] ; then mkdir -p \"$ZOO_LOG_DIR\" fi case $1 in start) echo -n \"Starting keeper ... \" if [ -f \"$ZOOPIDFILE\" ]; then if kill -0 `cat \"$ZOOPIDFILE\"` \u003e /dev/null 2\u003e\u00261; then echo already running as process `cat \"$ZOOPIDFILE\"`. exit 0 fi fi sudo -u clickhouse `echo \"$ZOOCMD\"` if [ $? -eq 0 ] then pgrep -f \"$ZOOCMD\" \u003e \"$ZOOPIDFILE\" echo \"PID:\" `cat $ZOOPIDFILE` if [ $? -eq 0 ]; then sleep 1 echo STARTED else echo FAILED TO WRITE PID exit 1 fi else echo SERVER DID NOT START exit 1 fi ;; start-foreground) sudo -u clickhouse clickhouse-keeper -C \"$ZOOCFG\" start ;; print-cmd) echo \"sudo -u clickhouse ${ZOOCMD}\" ;; stop) echo -n \"Stopping keeper ... \" if [ ! -f \"$ZOOPIDFILE\" ] then echo \"no keeper to stop (could not find file $ZOOPIDFILE)\" else ZOOPID=$(cat \"$ZOOPIDFILE\") echo $ZOOPID kill $ZOOPID while true; do sleep 3 if kill -0 $ZOOPID \u003e /dev/null 2\u003e\u00261; then echo $ZOOPID is still running else break fi done rm \"$ZOOPIDFILE\" echo STOPPED fi exit 0 ;; restart) shift \"$0\" stop ${@} sleep 3 \"$0\" start ${@} ;; status) clientPortAddress=\"localhost\" clientPort=2181 STAT=`echo srvr | nc $clientPortAddress $clientPort 2\u003e /dev/null | grep Mode` if [ \"x$STAT\" = \"x\" ] then echo \"Error contacting service. It is probably not running.\" exit 1 else echo $STAT exit 0 fi ;; *) echo \"Usage: $0 {start|start-foreground|stop|restart|status|print-cmd}\" \u003e\u00262 esac ","categories":"","description":"clickhouse-keeper-initd","excerpt":"clickhouse-keeper-initd","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-initd/","tags":"","title":"clickhouse-keeper-initd"},{"body":"clickhouse-keeper-service installation Need to install clickhouse-common-static + clickhouse-keeper OR clickhouse-common-static + clickhouse-server. Both OK, use the first if you don’t need clickhouse server locally.\ndpkg -i clickhouse-common-static_{%version}.deb clickhouse-keeper_{%version}.deb dpkg -i clickhouse-common-static_{%version}.deb clickhouse-server_{%version}.deb clickhouse-client_{%version}.deb Create directories\nmkdir -p /etc/clickhouse-keeper/config.d mkdir -p /var/log/clickhouse-keeper mkdir -p /var/lib/clickhouse-keeper/coordination/log mkdir -p /var/lib/clickhouse-keeper/coordination/snapshots mkdir -p /var/lib/clickhouse-keeper/cores chown -R clickhouse.clickhouse /etc/clickhouse-keeper /var/log/clickhouse-keeper /var/lib/clickhouse-keeper config cat /etc/clickhouse-keeper/config.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003clogger\u003e \u003c!-- Possible levels [1]: - none (turns off logging) - fatal - critical - error - warning - notice - information - debug - trace - test (not for production usage) [1]: https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/Logger.h#L105-L114 --\u003e \u003clevel\u003etrace\u003c/level\u003e \u003clog\u003e/var/log/clickhouse-keeper/clickhouse-keeper.log\u003c/log\u003e \u003cerrorlog\u003e/var/log/clickhouse-keeper/clickhouse-keeper.err.log\u003c/errorlog\u003e \u003c!-- Rotation policy See https://github.com/pocoproject/poco/blob/poco-1.9.4-release/Foundation/include/Poco/FileChannel.h#L54-L85 --\u003e \u003csize\u003e1000M\u003c/size\u003e \u003ccount\u003e10\u003c/count\u003e \u003c!-- \u003cconsole\u003e1\u003c/console\u003e --\u003e \u003c!-- Default behavior is autodetection (log to console if not daemon mode and is tty) --\u003e \u003c!-- Per level overrides (legacy): For example to suppress logging of the ConfigReloader you can use: NOTE: levels.logger is reserved, see below. --\u003e \u003c!-- \u003clevels\u003e \u003cConfigReloader\u003enone\u003c/ConfigReloader\u003e \u003c/levels\u003e --\u003e \u003c!-- Per level overrides: For example to suppress logging of the RBAC for default user you can use: (But please note that the logger name maybe changed from version to version, even after minor upgrade) --\u003e \u003c!-- \u003clevels\u003e \u003clogger\u003e \u003cname\u003eContextAccess (default)\u003c/name\u003e \u003clevel\u003enone\u003c/level\u003e \u003c/logger\u003e \u003clogger\u003e \u003cname\u003eDatabaseOrdinary (test)\u003c/name\u003e \u003clevel\u003enone\u003c/level\u003e \u003c/logger\u003e \u003c/levels\u003e --\u003e \u003c!-- Structured log formatting: You can specify log format(for now, JSON only). In that case, the console log will be printed in specified format like JSON. For example, as below: {\"date_time\":\"1650918987.180175\",\"thread_name\":\"#1\",\"thread_id\":\"254545\",\"level\":\"Trace\",\"query_id\":\"\",\"logger_name\":\"BaseDaemon\",\"message\":\"Received signal 2\",\"source_file\":\"../base/daemon/BaseDaemon.cpp; virtual void SignalListener::run()\",\"source_line\":\"192\"} To enable JSON logging support, just uncomment \u003cformatting\u003e tag below. --\u003e \u003c!-- \u003cformatting\u003ejson\u003c/formatting\u003e --\u003e \u003c/logger\u003e \u003c!-- Listen specified address. Use :: (wildcard IPv6 address), if you want to accept connections both with IPv4 and IPv6 from everywhere. Notes: If you open connections from wildcard address, make sure that at least one of the following measures applied: - server is protected by firewall and not accessible from untrusted networks; - all users are restricted to subset of network addresses (see users.xml); - all users have strong passwords, only secure (TLS) interfaces are accessible, or connections are only made via TLS interfaces. - users without password have readonly access. See also: https://www.shodan.io/search?query=clickhouse --\u003e \u003c!-- \u003clisten_host\u003e::\u003c/listen_host\u003e --\u003e \u003c!-- Same for hosts without support for IPv6: --\u003e \u003c!-- \u003clisten_host\u003e0.0.0.0\u003c/listen_host\u003e --\u003e \u003c!-- Default values - try listen localhost on IPv4 and IPv6. --\u003e \u003c!-- \u003clisten_host\u003e::1\u003c/listen_host\u003e \u003clisten_host\u003e127.0.0.1\u003c/listen_host\u003e --\u003e \u003c!-- \u003cinterserver_listen_host\u003e::\u003c/interserver_listen_host\u003e --\u003e \u003c!-- Listen host for communication between replicas. Used for data exchange --\u003e \u003c!-- Default values - equal to listen_host --\u003e \u003c!-- Don't exit if IPv6 or IPv4 networks are unavailable while trying to listen. --\u003e \u003c!-- \u003clisten_try\u003e0\u003c/listen_try\u003e --\u003e \u003c!-- Allow multiple servers to listen on the same address:port. This is not recommended. --\u003e \u003c!-- \u003clisten_reuse_port\u003e0\u003c/listen_reuse_port\u003e --\u003e \u003c!-- \u003clisten_backlog\u003e4096\u003c/listen_backlog\u003e --\u003e \u003cpath\u003e/var/lib/clickhouse-keeper/\u003c/path\u003e \u003ccore_path\u003e/var/lib/clickhouse-keeper/cores\u003c/core_path\u003e \u003ckeeper_server\u003e \u003ctcp_port\u003e2181\u003c/tcp_port\u003e \u003cserver_id\u003e1\u003c/server_id\u003e \u003clog_storage_path\u003e/var/lib/clickhouse-keeper/coordination/log\u003c/log_storage_path\u003e \u003csnapshot_storage_path\u003e/var/lib/clickhouse-keeper/coordination/snapshots\u003c/snapshot_storage_path\u003e \u003ccoordination_settings\u003e \u003coperation_timeout_ms\u003e10000\u003c/operation_timeout_ms\u003e \u003csession_timeout_ms\u003e30000\u003c/session_timeout_ms\u003e \u003craft_logs_level\u003etrace\u003c/raft_logs_level\u003e \u003crotate_log_storage_interval\u003e10000\u003c/rotate_log_storage_interval\u003e \u003c/coordination_settings\u003e \u003craft_configuration\u003e \u003cserver\u003e \u003cid\u003e1\u003c/id\u003e \u003chostname\u003elocalhost\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003c/raft_configuration\u003e \u003c/keeper_server\u003e \u003c/clickhouse\u003e cat /etc/clickhouse-keeper/config.d/keeper.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003clisten_host\u003e::\u003c/listen_host\u003e \u003ckeeper_server\u003e \u003ctcp_port\u003e2181\u003c/tcp_port\u003e \u003cserver_id\u003e1\u003c/server_id\u003e \u003craft_configuration\u003e \u003cserver\u003e \u003cid\u003e1\u003c/id\u003e \u003chostname\u003ekeeper-host-1\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003cserver\u003e \u003cid\u003e2\u003c/id\u003e \u003chostname\u003ekeeper-host-2\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003cserver\u003e \u003cid\u003e3\u003c/id\u003e \u003chostname\u003ekeeper-host-3\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003c/raft_configuration\u003e \u003c/keeper_server\u003e \u003c/clickhouse\u003e systemd service cat /lib/systemd/system/clickhouse-keeper.service [Unit] Description=ClickHouse Keeper (analytic DBMS for big data) Requires=network-online.target # NOTE: that After/Wants=time-sync.target is not enough, you need to ensure # that the time was adjusted already, if you use systemd-timesyncd you are # safe, but if you use ntp or some other daemon, you should configure it # additionaly. After=time-sync.target network-online.target Wants=time-sync.target [Service] Type=simple User=clickhouse Group=clickhouse Restart=always RestartSec=30 RuntimeDirectory=clickhouse-keeper ExecStart=/usr/bin/clickhouse-keeper --config=/etc/clickhouse-keeper/config.xml --pid-file=/run/clickhouse-keeper/clickhouse-keeper.pid # Minus means that this file is optional. EnvironmentFile=-/etc/default/clickhouse LimitCORE=infinity LimitNOFILE=500000 CapabilityBoundingSet=CAP_NET_ADMIN CAP_IPC_LOCK CAP_SYS_NICE CAP_NET_BIND_SERVICE [Install] # ClickHouse should not start from the rescue shell (rescue.target). WantedBy=multi-user.target systemctl daemon-reload systemctl status clickhouse-keeper systemctl start clickhouse-keeper debug start without service (as foreground application) sudo -u clickhouse /usr/bin/clickhouse-keeper --config=/etc/clickhouse-keeper/config.xml ","categories":"","description":"clickhouse-keeper-service","excerpt":"clickhouse-keeper-service","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-service/","tags":"","title":"clickhouse-keeper-service"},{"body":"It’s possible to expose clickhouse-server metrics in clickhouse-operator style. It’s for clickhouse-operator grafana dashboard.\nCREATE VIEW system.operator_compatible_metrics ( `name` String, `value` Float64, `help` String, `labels` Map(String, String), `type` String ) AS SELECT concat('chi_clickhouse_event_', event) AS name, CAST(value, 'Float64') AS value, description AS help, map('hostname', hostName()) AS labels, 'counter' AS type FROM system.events UNION ALL SELECT concat('chi_clickhouse_metric_', metric) AS name, CAST(value, 'Float64') AS value, description AS help, map('hostname', hostName()) AS labels, 'gauge' AS type FROM system.metrics UNION ALL SELECT concat('chi_clickhouse_metric_', metric) AS name, value, '' AS help, map('hostname', hostName()) AS labels, 'gauge' AS type FROM system.asynchronous_metrics UNION ALL SELECT 'chi_clickhouse_metric_MemoryDictionaryBytesAllocated' AS name, CAST(sum(bytes_allocated), 'Float64') AS value, 'Memory size allocated for dictionaries' AS help, map('hostname', hostName()) AS labels, 'gauge' AS type FROM system.dictionaries UNION ALL SELECT 'chi_clickhouse_metric_LongestRunningQuery' AS name, CAST(max(elapsed), 'Float64') AS value, 'Longest running query time' AS help, map('hostname', hostName()) AS labels, 'gauge' AS type FROM system.processes UNION ALL WITH ['chi_clickhouse_table_partitions', 'chi_clickhouse_table_parts', 'chi_clickhouse_table_parts_bytes', 'chi_clickhouse_table_parts_bytes_uncompressed', 'chi_clickhouse_table_parts_rows', 'chi_clickhouse_metric_DiskDataBytes', 'chi_clickhouse_metric_MemoryPrimaryKeyBytesAllocated'] AS names, [uniq(partition), count(), sum(bytes), sum(data_uncompressed_bytes), sum(rows), sum(bytes_on_disk), sum(primary_key_bytes_in_memory_allocated)] AS values, arrayJoin(arrayZip(names, values)) AS tpl SELECT tpl.1 AS name, CAST(tpl.2, 'Float64') AS value, '' AS help, map('database', database, 'table', table, 'active', toString(active), 'hostname', hostName()) AS labels, 'gauge' AS type FROM system.parts GROUP BY active, database, table UNION ALL WITH ['chi_clickhouse_table_mutations', 'chi_clickhouse_table_mutations_parts_to_do'] AS names, [CAST(count(), 'Float64'), CAST(sum(parts_to_do), 'Float64')] AS values, arrayJoin(arrayZip(names, values)) AS tpl SELECT tpl.1 AS name, tpl.2 AS value, '' AS help, map('database', database, 'table', table, 'hostname', hostName()) AS labels, 'gauge' AS type FROM system.mutations WHERE is_done = 0 GROUP BY database, table UNION ALL WITH if(coalesce(reason, 'unknown') = '', 'detached_by_user', coalesce(reason, 'unknown')) AS detach_reason SELECT 'chi_clickhouse_metric_DetachedParts' AS name, CAST(count(), 'Float64') AS value, '' AS help, map('database', database, 'table', table, 'disk', disk, 'hostname', hostName()) AS labels, 'gauge' AS type FROM system.detached_parts GROUP BY database, table, disk, reason ORDER BY name ASC nano /etc/clickhouse-server/config.d/operator_metrics.xml \u003cclickhouse\u003e \u003chttp_handlers\u003e \u003crule\u003e \u003curl\u003e/metrics\u003c/url\u003e \u003cmethods\u003ePOST,GET\u003c/methods\u003e \u003chandler\u003e \u003ctype\u003epredefined_query_handler\u003c/type\u003e \u003cquery\u003eSELECT * FROM system.operator_compatible_metrics FORMAT Prometheus\u003c/query\u003e \u003ccontent_type\u003etext/plain; charset=utf-8\u003c/content_type\u003e \u003c/handler\u003e \u003c/rule\u003e \u003cdefaults/\u003e \u003crule\u003e \u003curl\u003e/\u003c/url\u003e \u003cmethods\u003ePOST,GET\u003c/methods\u003e \u003cheaders\u003e\u003cpragma\u003eno-cache\u003c/pragma\u003e\u003c/headers\u003e \u003chandler\u003e \u003ctype\u003edynamic_query_handler\u003c/type\u003e \u003cquery_param_name\u003equery\u003c/query_param_name\u003e \u003c/handler\u003e \u003c/rule\u003e \u003c/http_handlers\u003e \u003c/clickhouse\u003e curl http://localhost:8123/metrics # HELP chi_clickhouse_metric_Query Number of executing queries # TYPE chi_clickhouse_metric_Query gauge chi_clickhouse_metric_Query{hostname=\"LAPTOP\"} 1 # HELP chi_clickhouse_metric_Merge Number of executing background merges # TYPE chi_clickhouse_metric_Merge gauge chi_clickhouse_metric_Merge{hostname=\"LAPTOP\"} 0 # HELP chi_clickhouse_metric_PartMutation Number of mutations (ALTER DELETE/UPDATE) # TYPE chi_clickhouse_metric_PartMutation gauge chi_clickhouse_metric_PartMutation{hostname=\"LAPTOP\"} 0 ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/altinity-kb-setup-and-maintenance/monitoring-operator-exporter-compatibility/","tags":"","title":"Compatibility layer for clickhouse-operator metric exporter"},{"body":"CollapsingMergeTree vs ReplacingMergeTree ReplacingMergeTree CollapsingMergeTree + very easy to use (always replace) - more complex (accounting-alike, put ‘rollback’ records to fix something) + you don’t need to store the previous state of the row - you need to the store (somewhere) the previous state of the row, OR extract it from the table itself (point queries is not nice for ClickHouse) - no deletes + support deletes - w/o FINAL - you can can always see duplicates, you need always to ‘pay’ FINAL performance penalty + properly crafted query can give correct results without final (i.e. sum(amount * sign) will be correct, no matter of you have duplicated or not) - only uniq()-alike things can be calculated in materialied views + you can do basic counts \u0026 sums in materialized views ","categories":"","description":"CollapsingMergeTree vs ReplacingMergeTree.","excerpt":"CollapsingMergeTree vs ReplacingMergeTree.","ref":"/engines/mergetree-table-engine-family/collapsing-vs-replacing/","tags":"","title":"CollapsingMergeTree vs ReplacingMergeTree"},{"body":"Column backfilling Sometimes you need to add a column into a huge table and backfill it with a data from another source, without reingesting all data.\nReplicated setup In case of a replicated / sharded setup you need to have the dictionary and source table (dict_table / item_dict) on all nodes and they have to all have EXACTLY the same data. The easiest way to do this is to make dict_table replicated.\nIn this case, you will need to set the setting allow_nondeterministic_mutations=1 on the user that runs the ALTER TABLE. See the ClickHouse docs for more information about this setting.\nHere is an example.\ncreate database test; use test; -- table with an existing data, we need to backfill / update S column create table fact ( key1 UInt64, key2 String, key3 String, D Date, S String) Engine MergeTree partition by D order by (key1, key2, key3); -- example data insert into fact select number, toString(number%103), toString(number%13), today(), toString(number) from numbers(1e9); 0 rows in set. Elapsed: 155.066 sec. Processed 1.00 billion rows, 8.00 GB (6.45 million rows/s., 51.61 MB/s.) insert into fact select number, toString(number%103), toString(number%13), today() - 30, toString(number)　from numbers(1e9); 0 rows in set. Elapsed: 141.594 sec. Processed 1.00 billion rows, 8.00 GB (7.06 million rows/s., 56.52 MB/s.) insert into fact select number, toString(number%103), toString(number%13), today() - 60, toString(number)　from numbers(1e10); 0 rows in set. Elapsed: 1585.549 sec. Processed 10.00 billion rows, 80.01 GB (6.31 million rows/s., 50.46 MB/s.) select count() from fact; 12000000000 -- 12 billions rows. -- table - source of the info to update create table dict_table ( key1 UInt64, key2 String, key3 String, S String) Engine MergeTree order by (key1, key2, key3); -- example data insert into dict_table select number, toString(number%103), toString(number%13), toString(number)||'xxx'　from numbers(1e10); 0 rows in set. Elapsed: 1390.121 sec. Processed 10.00 billion rows, 80.01 GB (7.19 million rows/s., 57.55 MB/s.) -- DICTIONARY witch will be the source for update / we cannot query dict_table directly CREATE DICTIONARY item_dict ( key1 UInt64, key2 String, key3 String, S String ) PRIMARY KEY key1,key2,key3 SOURCE(CLICKHOUSE(TABLE dict_table DB 'test' USER 'default')) LAYOUT(complex_key_cache(size_in_cells 50000000)) Lifetime(60000); -- let's test that the dictionary is working select dictGetString('item_dict', 'S', tuple(toUInt64(1),'1','1')); ┌─dictGetString('item_dict', 'S', tuple(toUInt64(1), '1', '1'))─┐ │ 1xxx │ └───────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 0.080 sec. SELECT dictGetString('item_dict', 'S', (toUInt64(1111111), '50', '1')) ┌─dictGetString('item_dict', 'S', tuple(toUInt64(1111111), '50', '1'))─┐ │ 1111111xxx │ └──────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 0.004 sec. -- Now let's lower number of simultaneous updates/mutations select value from system.settings where name like '%background_pool_size%'; ┌─value─┐ │ 16 │ └───────┘ alter table fact modify setting number_of_free_entries_in_pool_to_execute_mutation=15; -- only one mutation is possible per time / 16 - 15 = 1 -- the mutation itself alter table test.fact update S = dictGetString('test.item_dict', 'S', tuple(key1,key2,key3)) where 1; -- mutation took 26 hours and item_dict used bytes_allocated: 8187277280 select * from system.mutations where not is_done \\G Row 1: ────── database: test table: fact mutation_id: mutation_11452.txt command: UPDATE S = dictGetString('test.item_dict', 'S', (key1, key2, key3)) WHERE 1 create_time: 2022-01-29 20:21:00 block_numbers.partition_id: [''] block_numbers.number: [11452] parts_to_do_names: ['20220128_1_954_4','20211230_955_1148_3','20211230_1149_1320_3','20211230_1321_1525_3','20211230_1526_1718_3','20211230_1719_1823_3','20211230_1824_1859_2','20211230_1860_1895_2','20211230_1896_1900_1','20211230_1901_1906_1','20211230_1907_1907_0','20211230_1908_1908_0','20211130_2998_9023_5','20211130_9024_10177_4','20211130_10178_11416_4','20211130_11417_11445_2','20211130_11446_11446_0'] parts_to_do: 17 is_done: 0 latest_failed_part: latest_fail_time: 1970-01-01 00:00:00 latest_fail_reason: SELECT table, (elapsed * (1 / progress)) - elapsed, elapsed, progress, is_mutation, formatReadableSize(total_size_bytes_compressed) AS size, formatReadableSize(memory_usage) AS mem FROM system.merges ORDER BY progress DESC ┌─table────────────────────────┬─minus(multiply(elapsed, divide(1, progress)), elapsed)─┬─────────elapsed─┬────────────progress─┬─is_mutation─┬─size───────┬─mem───────┐ │ fact │ 7259.920140111059 │ 8631.476589565 │ 0.5431540560211632 │ 1 │ 1.89 GiB │ 0.00 B │ │ fact │ 60929.22808705666 │ 23985.610558929 │ 0.28246665649246827 │ 1 │ 9.86 GiB │ 4.25 MiB │ └──────────────────────────────┴────────────────────────────────────────────────────────┴─────────────────┴─────────────────────┴─────────────┴────────────┴───────────┘ SELECT *　FROM system.dictionaries　WHERE name = 'item_dict'　\\G Row 1: ────── database: test name: item_dict uuid: 28fda092-260f-430f-a8fd-a092260f330f status: LOADED origin: 28fda092-260f-430f-a8fd-a092260f330f type: ComplexKeyCache key.names: ['key1','key2','key3'] key.types: ['UInt64','String','String'] attribute.names: ['S'] attribute.types: ['String'] bytes_allocated: 8187277280 query_count: 12000000000 hit_rate: 1.6666666666666666e-10 found_rate: 1 element_count: 67108864 load_factor: 1 source: ClickHouse: test.dict_table lifetime_min: 0 lifetime_max: 60000 loading_start_time: 2022-01-29 20:20:50 last_successful_update_time: 2022-01-29 20:20:51 loading_duration: 0.829 last_exception: -- Check that data is updated SELECT * FROM test.fact WHERE key1 = 11111 ┌──key1─┬─key2─┬─key3─┬──────────D─┬─S────────┐ │ 11111 │ 90 │ 9 │ 2021-12-30 │ 11111xxx │ │ 11111 │ 90 │ 9 │ 2022-01-28 │ 11111xxx │ │ 11111 │ 90 │ 9 │ 2021-11-30 │ 11111xxx │ └───────┴──────┴──────┴────────────┴──────────┘ ","categories":"","description":"Column backfilling with alter/update using a dictionary","excerpt":"Column backfilling with alter/update using a dictionary","ref":"/altinity-kb-schema-design/backfill_column/","tags":"","title":"Column backfilling with alter/update using a dictionary"},{"body":"WITH toStartOfInterval(event_time, INTERVAL 5 MINUTE) = '2023-06-30 13:00:00' as before, toStartOfInterval(event_time, INTERVAL 5 MINUTE) = '2023-06-30 15:00:00' as after SELECT normalized_query_hash, anyIf(query, before) AS QueryBefore, anyIf(query, after) AS QueryAfter, countIf(before) as CountBefore, sumIf(query_duration_ms, before) / 1000 AS QueriesDurationBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'RealTimeMicroseconds')], before) / 1000000 AS RealTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'UserTimeMicroseconds')], before) / 1000000 AS UserTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SystemTimeMicroseconds')], before) / 1000000 AS SystemTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskReadElapsedMicroseconds')], before) / 1000000 AS DiskReadTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskWriteElapsedMicroseconds')], before) / 1000000 AS DiskWriteTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkSendElapsedMicroseconds')], before) / 1000000 AS NetworkSendTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkReceiveElapsedMicroseconds')], before) / 1000000 AS NetworkReceiveTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'ZooKeeperWaitMicroseconds')], before) / 1000000 AS ZooKeeperWaitTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSIOWaitMicroseconds')], before) / 1000000 AS OSIOWaitTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUWaitMicroseconds')], before) / 1000000 AS OSCPUWaitTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUVirtualTimeMicroseconds')], before) / 1000000 AS OSCPUVirtualTimeBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SelectedBytes')], before) AS SelectedBytesBefore, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SelectedRanges')], before) AS SelectedRangesBefore, sumIf(read_rows, before) AS ReadRowsBefore, formatReadableSize(sumIf(read_bytes, before) AS ReadBytesBefore), sumIf(written_rows, before) AS WrittenTowsBefore, formatReadableSize(sumIf(written_bytes, before)) AS WrittenBytesBefore, sumIf(result_rows, before) AS ResultRowsBefore, formatReadableSize(sumIf(result_bytes, before)) AS ResultBytesBefore, countIf(after) as CountAfter, sumIf(query_duration_ms, after) / 1000 AS QueriesDurationAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'RealTimeMicroseconds')], after) / 1000000 AS RealTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'UserTimeMicroseconds')], after) / 1000000 AS UserTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SystemTimeMicroseconds')], after) / 1000000 AS SystemTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskReadElapsedMicroseconds')], after) / 1000000 AS DiskReadTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskWriteElapsedMicroseconds')], after) / 1000000 AS DiskWriteTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkSendElapsedMicroseconds')], after) / 1000000 AS NetworkSendTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkReceiveElapsedMicroseconds')], after) / 1000000 AS NetworkReceiveTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'ZooKeeperWaitMicroseconds')], after) / 1000000 AS ZooKeeperWaitTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSIOWaitMicroseconds')], after) / 1000000 AS OSIOWaitTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUWaitMicroseconds')], after) / 1000000 AS OSCPUWaitTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUVirtualTimeMicroseconds')], after) / 1000000 AS OSCPUVirtualTimeAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SelectedBytes')], after) AS SelectedBytesAfter, sumIf(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SelectedRanges')], after) AS SelectedRangesAfter, sumIf(read_rows, after) AS ReadRowsAfter, formatReadableSize(sumIf(read_bytes, after) AS ReadBytesAfter), sumIf(written_rows, after) AS WrittenTowsAfter, formatReadableSize(sumIf(written_bytes, after)) AS WrittenBytesAfter, sumIf(result_rows, after) AS ResultRowsAfter, formatReadableSize(sumIf(result_bytes, after)) AS ResultBytesAfter FROM system.query_log WHERE (before OR after) AND type in (2,4) -- QueryFinish, ExceptionWhileProcessing GROUP BY normalized_query_hash WITH TOTALS ORDER BY SelectedRangesAfter- SelectedRangesBefore DESC LIMIT 10 FORMAT Vertical WITH toDateTime('2024-02-09 00:00:00') as timestamp_of_issue, event_time \u003c timestamp_of_issue as before, event_time \u003e= timestamp_of_issue as after select normalized_query_hash as h, any(query) as query_sample, round(quantileIf(0.9)(query_duration_ms, before)) as duration_q90_before, round(quantileIf(0.9)(query_duration_ms, after)) as duration_q90_after, countIf(before) as cnt_before, countIf(after) as cnt_after, sumIf(query_duration_ms,before) as duration_sum_before, sumIf(query_duration_ms,after) as duration_sum_after, sumIf(ProfileEvents['UserTimeMicroseconds'], before) as usertime_sum_before, sumIf(ProfileEvents['UserTimeMicroseconds'], after) as usertime_sum_after, sumIf(read_bytes,before) as sum_read_bytes_before, sumIf(read_bytes,after) as sum_read_bytes_after from system.query_log where event_time between timestamp_of_issue - INTERVAL 3 DAY and timestamp_of_issue + INTERVAL 3 DAY group by h HAVING cnt_after \u003e 1.1 * cnt_before OR sum_read_bytes_after \u003e 1.2 * sum_read_bytes_before OR usertime_sum_after \u003e 1.2 * usertime_sum_before ORDER BY sum_read_bytes_after - sum_read_bytes_before FORMAT Vertical ","categories":"","description":"","excerpt":"WITH toStartOfInterval(event_time, INTERVAL 5 MINUTE) = '2023-06-30 …","ref":"/altinity-kb-useful-queries/compare_query_log_for_2_intervals/","tags":"","title":"Compare query_log for 2 intervals"},{"body":"uniqExactState uniqExactState is stored in two parts: a count of values in LEB128 format + list values without a delimeter.\nIn our case, the value is sipHash128 of strings passed to uniqExact function.\n┌─hex(uniqExactState(toString(arrayJoin([1]))))─┐ │ 01E2756D8F7A583CA23016E03447724DE7 │ └───────────────────────────────────────────────┘ 01 E2756D8F7A583CA23016E03447724DE7 ^ ^ LEB128 sipHash128 ┌─hex(uniqExactState(toString(arrayJoin([1, 2]))))───────────────────┐ │ 024809CB4528E00621CF626BE9FA14E2BFE2756D8F7A583CA23016E03447724DE7 │ └────────────────────────────────────────────────────────────────────┘ 02 4809CB4528E00621CF626BE9FA14E2BF E2756D8F7A583CA23016E03447724DE7 ^ ^ ^ LEB128 sipHash128 sipHash128 So, our task is to find how we can generate such values by ourself. In case of String data type, it just the simple sipHash128 function.\n┌─hex(sipHash128(toString(2)))─────┬─hex(sipHash128(toString(1)))─────┐ │ 4809CB4528E00621CF626BE9FA14E2BF │ E2756D8F7A583CA23016E03447724DE7 │ └──────────────────────────────────┴──────────────────────────────────┘ The second task: it needs to read a state and split it into an array of values. Luckly for us, ClickHouse use the exact same serialization (LEB128 + list of values) for Arrays (in this case if uniqExactState and Array are serialized into RowBinary format).\nWe need one a helper – UDF function to do that conversion:\ncat /etc/clickhouse-server/pipe_function.xml \u003cclickhouse\u003e \u003cfunction\u003e \u003ctype\u003eexecutable\u003c/type\u003e \u003cexecute_direct\u003e0\u003c/execute_direct\u003e \u003cname\u003epipe\u003c/name\u003e \u003creturn_type\u003eArray(FixedString(16))\u003c/return_type\u003e \u003cargument\u003e \u003ctype\u003eString\u003c/type\u003e \u003c/argument\u003e \u003cformat\u003eRowBinary\u003c/format\u003e \u003ccommand\u003ecat\u003c/command\u003e \u003csend_chunk_header\u003e0\u003c/send_chunk_header\u003e \u003c/function\u003e \u003c/clickhouse\u003e This UDF – pipe converts uniqExactState to the Array(FixedString(16)).\n┌─arrayMap(x -\u003e hex(x), pipe(uniqExactState(toString(arrayJoin([1, 2])))))──────────────┐ │ ['4809CB4528E00621CF626BE9FA14E2BF','E2756D8F7A583CA23016E03447724DE7'] │ └───────────────────────────────────────────────────────────────────────────────────────┘ And here is the full example, how you can convert uniqExactState(string) to uniqState(string) or uniqCombinedState(string) using pipe UDF and arrayReduce('func', [..]).\n-- Generate demo with random data, uniqs are stored as heavy uniqExact CREATE TABLE aggregates ( `id` UInt32, `uniqExact` AggregateFunction(uniqExact, String) ) ENGINE = AggregatingMergeTree ORDER BY id as SELECT number % 10000 AS id, uniqExactState(toString(number)) FROM numbers(10000000) GROUP BY id; 0 rows in set. Elapsed: 2.042 sec. Processed 10.01 million rows, 80.06 MB (4.90 million rows/s., 39.21 MB/s.) -- Let's add a new columns to store optimized, approximate uniq \u0026 uniqCombined ALTER TABLE aggregates ADD COLUMN `uniq` AggregateFunction(uniq, FixedString(16)) default arrayReduce('uniqState', pipe(uniqExact)), ADD COLUMN `uniqCombined` AggregateFunction(uniqCombined, FixedString(16)) default arrayReduce('uniqCombinedState', pipe(uniqExact)); -- Materialize defaults in the new columns ALTER TABLE aggregates UPDATE uniqCombined = uniqCombined, uniq = uniq WHERE 1 settings mutations_sync=2; -- Let's reset defaults to remove the dependancy of the UDF from our table ALTER TABLE aggregates modify COLUMN `uniq` remove default, modify COLUMN `uniqCombined` remove default; -- Alternatively you can populate data in the new columns directly without using DEFAULT columns -- ALTER TABLE aggregates UPDATE -- uniqCombined = arrayReduce('uniqCombinedState', pipe(uniqExact)), -- uniq = arrayReduce('uniqState', pipe(uniqExact)) -- WHERE 1 settings mutations_sync=2; -- Check results, results are slighty different, because uniq \u0026 uniqCombined are approximate functions SELECT id % 20 AS key, uniqExactMerge(uniqExact), uniqCombinedMerge(uniqCombined), uniqMerge(uniq) FROM aggregates GROUP BY key ┌─key─┬─uniqExactMerge(uniqExact)─┬─uniqCombinedMerge(uniqCombined)─┬─uniqMerge(uniq)─┐ │ 0 │ 500000 │ 500195 │ 500455 │ │ 1 │ 500000 │ 502599 │ 501549 │ │ 2 │ 500000 │ 498058 │ 504428 │ │ 3 │ 500000 │ 499748 │ 500195 │ │ 4 │ 500000 │ 500791 │ 500836 │ │ 5 │ 500000 │ 502430 │ 497558 │ │ 6 │ 500000 │ 500262 │ 501785 │ │ 7 │ 500000 │ 501514 │ 495758 │ │ 8 │ 500000 │ 500121 │ 498597 │ │ 9 │ 500000 │ 502173 │ 500455 │ │ 10 │ 500000 │ 499144 │ 498386 │ │ 11 │ 500000 │ 500525 │ 503139 │ │ 12 │ 500000 │ 503624 │ 497103 │ │ 13 │ 500000 │ 499986 │ 497992 │ │ 14 │ 500000 │ 502027 │ 494833 │ │ 15 │ 500000 │ 498831 │ 500983 │ │ 16 │ 500000 │ 501103 │ 500836 │ │ 17 │ 500000 │ 499409 │ 496791 │ │ 18 │ 500000 │ 501641 │ 502991 │ │ 19 │ 500000 │ 500648 │ 500881 │ └─────┴───────────────────────────┴─────────────────────────────────┴─────────────────┘ 20 rows in set. Elapsed: 2.312 sec. Processed 10.00 thousand rows, 7.61 MB (4.33 thousand rows/s., 3.29 MB/s.) Now, lets repeat the same insert, but in that case we will also populate uniq \u0026 uniqCombined with values converted via sipHash128 function. If we did everything right, uniq counts will not change, because we inserted the exact same values.\nINSERT INTO aggregates SELECT number % 10000 AS id, uniqExactState(toString(number)), uniqState(sipHash128(toString(number))), uniqCombinedState(sipHash128(toString(number))) FROM numbers(10000000) GROUP BY id; 0 rows in set. Elapsed: 5.386 sec. Processed 10.01 million rows, 80.06 MB (1.86 million rows/s., 14.86 MB/s.) SELECT id % 20 AS key, uniqExactMerge(uniqExact), uniqCombinedMerge(uniqCombined), uniqMerge(uniq) FROM aggregates GROUP BY key ┌─key─┬─uniqExactMerge(uniqExact)─┬─uniqCombinedMerge(uniqCombined)─┬─uniqMerge(uniq)─┐ │ 0 │ 500000 │ 500195 │ 500455 │ │ 1 │ 500000 │ 502599 │ 501549 │ │ 2 │ 500000 │ 498058 │ 504428 │ │ 3 │ 500000 │ 499748 │ 500195 │ │ 4 │ 500000 │ 500791 │ 500836 │ │ 5 │ 500000 │ 502430 │ 497558 │ │ 6 │ 500000 │ 500262 │ 501785 │ │ 7 │ 500000 │ 501514 │ 495758 │ │ 8 │ 500000 │ 500121 │ 498597 │ │ 9 │ 500000 │ 502173 │ 500455 │ │ 10 │ 500000 │ 499144 │ 498386 │ │ 11 │ 500000 │ 500525 │ 503139 │ │ 12 │ 500000 │ 503624 │ 497103 │ │ 13 │ 500000 │ 499986 │ 497992 │ │ 14 │ 500000 │ 502027 │ 494833 │ │ 15 │ 500000 │ 498831 │ 500983 │ │ 16 │ 500000 │ 501103 │ 500836 │ │ 17 │ 500000 │ 499409 │ 496791 │ │ 18 │ 500000 │ 501641 │ 502991 │ │ 19 │ 500000 │ 500648 │ 500881 │ └─────┴───────────────────────────┴─────────────────────────────────┴─────────────────┘ 20 rows in set. Elapsed: 3.318 sec. Processed 20.00 thousand rows, 11.02 MB (6.03 thousand rows/s., 3.32 MB/s.) Let’s compare the data size, uniq won in this case, but check this article Functions to count uniqs, milage may vary.\noptimize table aggregates final; SELECT column, formatReadableSize(sum(column_data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(column_data_uncompressed_bytes) AS usize) AS uncompressed FROM system.parts_columns WHERE (active = 1) AND (table LIKE 'aggregates') and column like '%uniq%' GROUP BY column ORDER BY size DESC; ┌─column───────┬─compressed─┬─uncompressed─┐ │ uniqExact │ 153.21 MiB │ 152.61 MiB │ │ uniqCombined │ 76.62 MiB │ 76.32 MiB │ │ uniq │ 38.33 MiB │ 38.18 MiB │ └──────────────┴────────────┴──────────────┘ ","categories":"","description":"A way to convert to uniqExactState to other uniqStates (like uniqCombinedState) in Clickhouse. ","excerpt":"A way to convert to uniqExactState to other uniqStates (like …","ref":"/altinity-kb-setup-and-maintenance/uniqexact-to-uniq-combined/","tags":"","title":"How to convert uniqExact states to approximate uniq functions states"},{"body":"Using custom settings in config You can not use the custom settings in config file ‘as is’, because clickhouse don’t know which datatype should be used to parse it.\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003ccustom_data_version\u003e1\u003c/custom_data_version\u003e \u003c!-- will not work! see below --\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e That will end up with the following error:\n2021.09.24 12:50:37.369259 [ 264905 ] {} \u003cError\u003e ConfigReloader: Error updating configuration from '/etc/clickhouse-server/users.xml' config.: Code: 536. DB::Exception: Couldn't restore Field from dump: 1: while parsing value '1' for setting 'custom_data_version'. (CANNOT_RESTORE_FROM_FIELD_DUMP), Stack trace (when copying this message, always include the lines below): 0. DB::Exception::Exception(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, int, bool) @ 0x9440eba in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 1. DB::Field::restoreFromDump(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026)::$_4::operator()() const @ 0x10449da0 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 2. DB::Field::restoreFromDump(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026) @ 0x10449bf1 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 3. DB::BaseSettings\u003cDB::SettingsTraits\u003e::stringToValueUtil(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x1042e2bf in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 4. DB::UsersConfigAccessStorage::parseFromConfig(Poco::Util::AbstractConfiguration const\u0026) @ 0x1041a097 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 5. void std::__1::__function::__policy_invoker\u003cvoid (Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e, bool)\u003e::__call_impl\u003cstd::__1::__function::__default_alloc_func\u003cDB::UsersConfigAccessStorage::load(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::function\u003cstd::__1::shared_ptr\u003czkutil::ZooKeeper\u003e ()\u003e const\u0026)::$_0, void (Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e, bool)\u003e \u003e(std::__1::__function::__policy_storage const*, Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e\u0026\u0026, bool) @ 0x1042e7ff in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 6. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x11caf54e in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 7. DB::ConfigReloader::run() @ 0x11cb0f8f in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 8. ThreadFromGlobalPool::ThreadFromGlobalPool\u003cvoid (DB::ConfigReloader::*)(), DB::ConfigReloader*\u003e(void (DB::ConfigReloader::*\u0026\u0026)(), DB::ConfigReloader*\u0026\u0026)::'lambda'()::operator()() @ 0x11cb19f1 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 9. ThreadPoolImpl\u003cstd::__1::thread\u003e::worker(std::__1::__list_iterator\u003cstd::__1::thread, void*\u003e) @ 0x9481f5f in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 10. void* std::__1::__thread_proxy\u003cstd::__1::tuple\u003cstd::__1::unique_ptr\u003cstd::__1::__thread_struct, std::__1::default_delete\u003cstd::__1::__thread_struct\u003e \u003e, void ThreadPoolImpl\u003cstd::__1::thread\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda0'()\u003e \u003e(void*) @ 0x9485843 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 11. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so 12. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so (version 21.10.1.8002 (official build)) 2021.09.29 11:36:07.722213 [ 2090 ] {} \u003cError\u003e Application: DB::Exception: Couldn't restore Field from dump: 1: while parsing value '1' for setting 'custom_data_version' To make it work you need to change it an the following way:\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003ccustom_data_version\u003eUInt64_1\u003c/custom_data_version\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e or\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003ccustom_data_version\u003e'1'\u003c/custom_data_version\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e The list of recognized prefixes is in the sources: https://github.com/ClickHouse/ClickHouse/blob/ea13a8b562edbc422c07b5b4ecce353f79b6cb63/src/Core/Field.cpp#L253-L270\n","categories":"","description":"Using custom settings","excerpt":"Using custom settings","ref":"/altinity-kb-setup-and-maintenance/custom_settings/","tags":"","title":"Custom Settings"},{"body":"Substract fractional seconds WITH toDateTime64('2021-09-07 13:41:50.926', 3) AS time SELECT time - 1, time - 0.1 AS no_affect, time - toDecimal64(0.1, 3) AS uncorrect_result, time - toIntervalMillisecond(100) AS correct_result -- from 22.4 Query id: 696722bd-3c22-4270-babe-c6b124fee97f ┌──────────minus(time, 1)─┬───────────────no_affect─┬────────uncorrect_result─┬──────────correct_result─┐ │ 2021-09-07 13:41:49.926 │ 2021-09-07 13:41:50.926 │ 1970-01-01 00:00:00.000 │ 2021-09-07 13:41:50.826 │ └─────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┘ WITH toDateTime64('2021-03-03 09:30:00.100', 3) AS time, fromUnixTimestamp64Milli(toInt64(toUnixTimestamp64Milli(time) + (1.25 * 1000))) AS first, toDateTime64(toDecimal64(time, 3) + toDecimal64('1.25', 3), 3) AS second, reinterpret(reinterpret(time, 'Decimal64(3)') + toDecimal64('1.25', 3), 'DateTime64(3)') AS third, time + toIntervalMillisecond(1250) AS fourth, -- from 22.4 addMilliseconds(time, 1250) AS fifth -- from 22.4 SELECT first, second, third, fourth, fifth Query id: 176cd2e7-68bf-4e26-a492-63e0b5a87cc5 ┌───────────────────first─┬──────────────────second─┬───────────────────third─┬──────────────────fourth─┬───────────────────fifth─┐ │ 2021-03-03 09:30:01.350 │ 2021-03-03 09:30:01.350 │ 2021-03-03 09:30:01.350 │ 2021-03-03 09:30:01.350 │ 2021-03-03 09:30:01.350 │ └─────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┘ SET max_threads=1; Starting from 22.4 WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, time + toIntervalMillisecond(1250) AS fourth SELECT count() FROM numbers(100000000) WHERE NOT ignore(fourth) 1 rows in set. Elapsed: 0.215 sec. Processed 100.03 million rows, 800.21 MB (464.27 million rows/s., 3.71 GB/s.) WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, addMilliseconds(time, 1250) AS fifth SELECT count() FROM numbers(100000000) WHERE NOT ignore(fifth) 1 rows in set. Elapsed: 0.208 sec. Processed 100.03 million rows, 800.21 MB (481.04 million rows/s., 3.85 GB/s.) ########### WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, fromUnixTimestamp64Milli(reinterpretAsInt64(toUnixTimestamp64Milli(time) + (1.25 * 1000))) AS first SELECT count() FROM numbers(100000000) WHERE NOT ignore(first) 1 rows in set. Elapsed: 0.370 sec. Processed 100.03 million rows, 800.21 MB (270.31 million rows/s., 2.16 GB/s.) WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, fromUnixTimestamp64Milli(toUnixTimestamp64Milli(time) + toInt16(1.25 * 1000)) AS first SELECT count() FROM numbers(100000000) WHERE NOT ignore(first) 1 rows in set. Elapsed: 0.256 sec. Processed 100.03 million rows, 800.21 MB (391.06 million rows/s., 3.13 GB/s.) WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, toDateTime64(toDecimal64(time, 3) + toDecimal64('1.25', 3), 3) AS second SELECT count() FROM numbers(100000000) WHERE NOT ignore(second) 1 rows in set. Elapsed: 2.240 sec. Processed 100.03 million rows, 800.21 MB (44.65 million rows/s., 357.17 MB/s.) SET decimal_check_overflow=0; WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, toDateTime64(toDecimal64(time, 3) + toDecimal64('1.25', 3), 3) AS second SELECT count() FROM numbers(100000000) WHERE NOT ignore(second) 1 rows in set. Elapsed: 1.991 sec. Processed 100.03 million rows, 800.21 MB (50.23 million rows/s., 401.81 MB/s.) WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, reinterpret(reinterpret(time, 'Decimal64(3)') + toDecimal64('1.25', 3), 'DateTime64(3)') AS third SELECT count() FROM numbers(100000000) WHERE NOT ignore(third) 1 rows in set. Elapsed: 0.515 sec. Processed 100.03 million rows, 800.21 MB (194.39 million rows/s., 1.56 GB/s.) SET decimal_check_overflow=0; WITH materialize(toDateTime64('2021-03-03 09:30:00.100', 3)) AS time, reinterpret(reinterpret(time, 'Decimal64(3)') + toDecimal64('1.25', 3), 'DateTime64(3)') AS third SELECT count() FROM numbers(100000000) WHERE NOT ignore(third) 1 rows in set. Elapsed: 0.281 sec. Processed 100.03 million rows, 800.21 MB (356.21 million rows/s., 2.85 GB/s.) ","categories":"","description":"DateTime64 data type","excerpt":"DateTime64 data type","ref":"/altinity-kb-queries-and-syntax/datetime64/","tags":"","title":"DateTime64"},{"body":"Debug hanging / freezing things If ClickHouse is busy with something and you don’t know what’s happeing, you can easily check the stacktraces of all the thread which are working\nSELECT arrayStringConcat(arrayMap(x -\u003e demangle(addressToSymbol(x)), trace), '\\n') AS trace_functions, count() FROM system.stack_trace GROUP BY trace_functions ORDER BY count() DESC SETTINGS allow_introspection_functions=1 FORMAT Vertical; If you can’t start any queries, but you have access to the node, you can sent a singal\n# older versions for i in $(ls -1 /proc/$(pidof clickhouse-server)/task/); do kill -TSTP $i; done # even older versions for i in $(ls -1 /proc/$(pidof clickhouse-server)/task/); do kill -SIGPROF $i; done ","categories":"","description":"Debug hanging / freezing things","excerpt":"Debug hanging / freezing things","ref":"/altinity-kb-useful-queries/debug-hang/","tags":"","title":"Debug hanging thing"},{"body":"CompiledExpressionCacheCount -- number or compiled cached expression (if CompiledExpressionCache is enabled) jemalloc -- parameters of jemalloc allocator, they are not very useful, and not interesting MarkCacheBytes / MarkCacheFiles -- there are cache for .mrk files (default size is 5GB), you can see is it use all 5GB or not MemoryCode -- how much memory allocated for ClickHouse executable MemoryDataAndStack -- virtual memory allocated for data and stack MemoryResident -- real memory used by ClickHouse ( the same as top RES/RSS) MemoryShared -- shared memory used by ClickHouse MemoryVirtual -- virtual memory used by ClickHouse ( the same as top VIRT) NumberOfDatabases NumberOfTables ReplicasMaxAbsoluteDelay -- important parameter - replica max absolute delay in seconds ReplicasMaxRelativeDelay -- replica max relative delay (from other replicas) in seconds ReplicasMaxInsertsInQueue -- max number of parts to fetch for a single Replicated table ReplicasSumInsertsInQueue -- sum of parts to fetch for all Replicated tables ReplicasMaxMergesInQueue -- max number of merges in queue for a single Replicated table ReplicasSumMergesInQueue -- total number of merges in queue for all Replicated tables ReplicasMaxQueueSize -- max number of tasks for a single Replicated table ReplicasSumQueueSize -- total number of tasks in replication queue UncompressedCacheBytes/UncompressedCacheCells -- allocated memory for uncompressed cache (disabled by default) Uptime -- uptime seconds ","categories":"","description":"Description of asynchronous_metrics","excerpt":"Description of asynchronous_metrics","ref":"/altinity-kb-setup-and-maintenance/asynchronous_metrics_descr/","tags":"","title":"Description of asynchronous_metrics"},{"body":"Create folder mkdir /data/clickhouse_encrypted chown clickhouse.clickhouse /data/clickhouse_encrypted Configure encrypted disk and storage https://clickhouse.com/docs/en/operations/storing-data/#encrypted-virtual-file-system https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server-settings-encryption cat /etc/clickhouse-server/config.d/encrypted_storage.xml \u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cdisk1\u003e \u003ctype\u003elocal\u003c/type\u003e \u003cpath\u003e/data/clickhouse_encrypted/\u003c/path\u003e \u003c/disk1\u003e \u003cencrypted_disk\u003e \u003ctype\u003eencrypted\u003c/type\u003e \u003cdisk\u003edisk1\u003c/disk\u003e \u003cpath\u003eencrypted/\u003c/path\u003e \u003calgorithm\u003eAES_128_CTR\u003c/algorithm\u003e \u003ckey_hex id=\"0\"\u003e00112233445566778899aabbccddeeff\u003c/key_hex\u003e \u003ccurrent_key_id\u003e0\u003c/current_key_id\u003e \u003c/encrypted_disk\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cencrypted\u003e \u003cvolumes\u003e \u003cencrypted_volume\u003e \u003cdisk\u003eencrypted_disk\u003c/disk\u003e \u003c/encrypted_volume\u003e \u003c/volumes\u003e \u003c/encrypted\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e systemctl restart clickhouse-server select name, path, type, is_encrypted from system.disks; ┌─name───────────┬─path──────────────────────────────────┬─type──┬─is_encrypted─┐ │ default │ /var/lib/clickhouse/ │ local │ 0 │ │ disk1 │ /data/clickhouse_encrypted/ │ local │ 0 │ │ encrypted_disk │ /data/clickhouse_encrypted/encrypted/ │ local │ 1 │ └────────────────┴───────────────────────────────────────┴───────┴──────────────┘ select * from system.storage_policies; ┌─policy_name─┬─volume_name──────┬─volume_priority─┬─disks──────────────┬─volume_type─┬─max_data_part_size─┬─move_factor─┬─prefer_not_to_merge─┐ │ default │ default │ 1 │ ['default'] │ JBOD │ 0 │ 0 │ 0 │ │ encrypted │ encrypted_volume │ 1 │ ['encrypted_disk'] │ JBOD │ 0 │ 0 │ 0 │ └─────────────┴──────────────────┴─────────────────┴────────────────────┴─────────────┴────────────────────┴─────────────┴─────────────────────┘ Create table CREATE TABLE bench_encrypted(c_int Int64, c_str varchar(255), c_float Float64) engine=MergeTree order by c_int settings storage_policy = 'encrypted'; cat /data/clickhouse_encrypted/encrypted/store/906/9061167e-d5f7-45ea-8e54-eb6ba3b678dc/format_version.txt ENC�AdruM�˪h\"��^� Compare performance of encrypted and not encrypted tables CREATE TABLE bench_encrypted(c_int Int64, c_str varchar(255), c_float Float64) engine=MergeTree order by c_int settings storage_policy = 'encrypted'; insert into bench_encrypted select toInt64(cityHash64(number)), lower(hex(MD5(toString(number)))), number/cityHash64(number)*10000000 from numbers_mt(100000000); 0 rows in set. Elapsed: 33.357 sec. Processed 100.66 million rows, 805.28 MB (3.02 million rows/s., 24.14 MB/s.) CREATE TABLE bench_unencrypted(c_int Int64, c_str varchar(255), c_float Float64) engine=MergeTree order by c_int; insert into bench_unencrypted select toInt64(cityHash64(number)), lower(hex(MD5(toString(number)))), number/cityHash64(number)*10000000 from numbers_mt(100000000); 0 rows in set. Elapsed: 31.175 sec. Processed 100.66 million rows, 805.28 MB (3.23 million rows/s., 25.83 MB/s.) select avg(c_float) from bench_encrypted; 1 row in set. Elapsed: 0.195 sec. Processed 100.00 million rows, 800.00 MB (511.66 million rows/s., 4.09 GB/s.) select avg(c_float) from bench_unencrypted; 1 row in set. Elapsed: 0.150 sec. Processed 100.00 million rows, 800.00 MB (668.71 million rows/s., 5.35 GB/s.) select sum(c_int) from bench_encrypted; 1 row in set. Elapsed: 0.281 sec. Processed 100.00 million rows, 800.00 MB (355.74 million rows/s., 2.85 GB/s.) select sum(c_int) from bench_unencrypted; 1 row in set. Elapsed: 0.193 sec. Processed 100.00 million rows, 800.00 MB (518.88 million rows/s., 4.15 GB/s.) set max_threads=1; select avg(c_float) from bench_encrypted; 1 row in set. Elapsed: 0.934 sec. Processed 100.00 million rows, 800.00 MB (107.03 million rows/s., 856.23 MB/s.) select avg(c_float) from bench_unencrypted; 1 row in set. Elapsed: 0.874 sec. Processed 100.00 million rows, 800.00 MB (114.42 million rows/s., 915.39 MB/s.) read key_hex from environment variable https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server-settings-encryption https://serverfault.com/questions/413397/how-to-set-environment-variable-in-systemd-service cat /etc/clickhouse-server/config.d/encrypted_storage.xml \u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cdisk1\u003e \u003ctype\u003elocal\u003c/type\u003e \u003cpath\u003e/data/clickhouse_encrypted/\u003c/path\u003e \u003c/disk1\u003e \u003cencrypted_disk\u003e \u003ctype\u003eencrypted\u003c/type\u003e \u003cdisk\u003edisk1\u003c/disk\u003e \u003cpath\u003eencrypted/\u003c/path\u003e \u003calgorithm\u003eAES_128_CTR\u003c/algorithm\u003e \u003ckey_hex from_env=\"DiskKey\"/\u003e \u003c/encrypted_disk\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cencrypted\u003e \u003cvolumes\u003e \u003cencrypted_volume\u003e \u003cdisk\u003eencrypted_disk\u003c/disk\u003e \u003c/encrypted_volume\u003e \u003c/volumes\u003e \u003c/encrypted\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e cat /etc/default/clickhouse-server DiskKey=00112233445566778899aabbccddeeff systemctl restart clickhouse-server ","categories":"","description":"Example how to encrypt data in tables using storage policies.","excerpt":"Example how to encrypt data in tables using storage policies.","ref":"/altinity-kb-setup-and-maintenance/disk_encryption/","tags":"","title":"Clickhouse data/disk encryption (at rest)"},{"body":"DISTINCT SELECT DISTINCT number FROM numbers_mt(100000000) FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.00 GiB. 0 rows in set. Elapsed: 18.720 sec. Processed 100.03 million rows, 800.21 MB (5.34 million rows/s., 42.75 MB/s.) SELECT DISTINCT number FROM numbers_mt(100000000) SETTINGS max_threads = 1 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.00 GiB. 0 rows in set. Elapsed: 18.349 sec. Processed 100.03 million rows, 800.21 MB (5.45 million rows/s., 43.61 MB/s.) SELECT DISTINCT number FROM numbers_mt(100000000) LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 21.56 MiB. 0 rows in set. Elapsed: 0.014 sec. Processed 589.54 thousand rows, 4.72 MB (43.08 million rows/s., 344.61 MB/s.) SELECT DISTINCT number % 1000 FROM numbers_mt(1000000000) LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 1.80 MiB. 0 rows in set. Elapsed: 0.005 sec. Processed 589.54 thousand rows, 4.72 MB (127.23 million rows/s., 1.02 GB/s.) SELECT DISTINCT number % 1000 FROM numbers(1000000000) LIMIT 1001 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 847.05 KiB. 0 rows in set. Elapsed: 0.448 sec. Processed 1.00 billion rows, 8.00 GB (2.23 billion rows/s., 17.88 GB/s.) Final distinct step is single threaded Stream resultset GROUP BY SELECT number FROM numbers_mt(100000000) GROUP BY number FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.04 GiB. 0 rows in set. Elapsed: 8.212 sec. Processed 100.00 million rows, 800.00 MB (12.18 million rows/s., 97.42 MB/s.) SELECT number FROM numbers_mt(100000000) GROUP BY number SETTINGS max_threads = 1 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 6.00 GiB. 0 rows in set. Elapsed: 19.206 sec. Processed 100.03 million rows, 800.21 MB (5.21 million rows/s., 41.66 MB/s.) SELECT number FROM numbers_mt(100000000) GROUP BY number LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.05 GiB. 0 rows in set. Elapsed: 4.852 sec. Processed 100.00 million rows, 800.00 MB (20.61 million rows/s., 164.88 MB/s.) This query faster than first, because ClickHouse doesn't need to merge states for all keys, only for first 1000 (based on LIMIT) SELECT number % 1000 AS key FROM numbers_mt(1000000000) GROUP BY key LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 3.15 MiB. 0 rows in set. Elapsed: 0.770 sec. Processed 1.00 billion rows, 8.00 GB (1.30 billion rows/s., 10.40 GB/s.) SELECT number % 1000 AS key FROM numbers_mt(1000000000) GROUP BY key LIMIT 1001 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 3.77 MiB. 0 rows in set. Elapsed: 0.770 sec. Processed 1.00 billion rows, 8.00 GB (1.30 billion rows/s., 10.40 GB/s.) Multi threaded Will return result only after competion of aggregation LIMIT BY SELECT number FROM numbers_mt(100000000) LIMIT 1 BY number FORMAT `Null` MemoryTracker: Peak memory usage (for query): 6.00 GiB. 0 rows in set. Elapsed: 39.541 sec. Processed 100.00 million rows, 800.00 MB (2.53 million rows/s., 20.23 MB/s.) SELECT number FROM numbers_mt(100000000) LIMIT 1 BY number SETTINGS max_threads = 1 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 6.01 GiB. 0 rows in set. Elapsed: 36.773 sec. Processed 100.03 million rows, 800.21 MB (2.72 million rows/s., 21.76 MB/s.) SELECT number FROM numbers_mt(100000000) LIMIT 1 BY number LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 10.56 MiB. 0 rows in set. Elapsed: 0.019 sec. Processed 589.54 thousand rows, 4.72 MB (30.52 million rows/s., 244.20 MB/s.) SELECT number % 1000 AS key FROM numbers_mt(1000000000) LIMIT 1 BY key LIMIT 1000 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 5.14 MiB. 0 rows in set. Elapsed: 0.008 sec. Processed 589.54 thousand rows, 4.72 MB (71.27 million rows/s., 570.16 MB/s.) SELECT number % 1000 AS key FROM numbers_mt(1000000000) LIMIT 1 BY key LIMIT 1001 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 3.23 MiB. 0 rows in set. Elapsed: 36.027 sec. Processed 1.00 billion rows, 8.00 GB (27.76 million rows/s., 222.06 MB/s.) Single threaded Stream resultset Can return arbitrary amount of rows per each key ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/altinity-kb-queries-and-syntax/distinct-vs-group-by-vs-limit-by/","tags":"","title":"DISTINCT \u0026 GROUP BY \u0026 LIMIT 1 BY what the difference"},{"body":"quantileTDigestState quantileTDigestState is stored in two parts: a count of centroids in LEB128 format + list of centroids without a delimeter. Each centroid is represented as two Float32 values: Mean \u0026 Count.\nSELECT hex(quantileTDigestState(1)), hex(toFloat32(1)) ┌─hex(quantileTDigestState(1))─┬─hex(toFloat32(1))─┐ │ 010000803F0000803F │ 0000803F │ └──────────────────────────────┴───────────────────┘ 01 0000803F 0000803F ^ ^ ^ LEB128 Float32 Mean Float32 Count We need to make two helper UDF functions:\ncat /etc/clickhouse-server/decodeTDigestState_function.xml \u003cyandex\u003e \u003cfunction\u003e \u003ctype\u003eexecutable\u003c/type\u003e \u003cexecute_direct\u003e0\u003c/execute_direct\u003e \u003cname\u003edecodeTDigestState\u003c/name\u003e \u003creturn_type\u003eArray(Tuple(mean Float32, count Float32))\u003c/return_type\u003e \u003cargument\u003e \u003ctype\u003eAggregateFunction(quantileTDigest, UInt32)\u003c/type\u003e \u003c/argument\u003e \u003cformat\u003eRowBinary\u003c/format\u003e \u003ccommand\u003ecat\u003c/command\u003e \u003csend_chunk_header\u003e0\u003c/send_chunk_header\u003e \u003c/function\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/encodeTDigestState_function.xml \u003cyandex\u003e \u003cfunction\u003e \u003ctype\u003eexecutable\u003c/type\u003e \u003cexecute_direct\u003e0\u003c/execute_direct\u003e \u003cname\u003eencodeTDigestState\u003c/name\u003e \u003creturn_type\u003eAggregateFunction(quantileTDigest, UInt32)\u003c/return_type\u003e \u003cargument\u003e \u003ctype\u003eArray(Tuple(mean Float32, count Float32))\u003c/type\u003e \u003c/argument\u003e \u003cformat\u003eRowBinary\u003c/format\u003e \u003ccommand\u003ecat\u003c/command\u003e \u003csend_chunk_header\u003e0\u003c/send_chunk_header\u003e \u003c/function\u003e \u003c/yandex\u003e Those UDF – (encode/decode)TDigestState converts TDigestState to the Array(Tuple(Float32, Float32)) and back.\nSELECT quantileTDigest(CAST(number, 'UInt32')) AS result FROM numbers(10) ┌─result─┐ │ 4 │ └────────┘ SELECT decodeTDigestState(quantileTDigestState(CAST(number, 'UInt32'))) AS state FROM numbers(10) ┌─state─────────────────────────────────────────────────────────┐ │ [(0,1),(1,1),(2,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1)] │ └───────────────────────────────────────────────────────────────┘ SELECT finalizeAggregation(encodeTDigestState(CAST('[(0,1),(1,1),(2,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1)]', 'Array(Tuple(Float32, Float32))'))) AS result ┌─result─┐ │ 4 │ └────────┘ ","categories":"","description":"A way to export or import quantileTDigest states from/into ClickHouse.","excerpt":"A way to export or import quantileTDigest states from/into ClickHouse.","ref":"/altinity-kb-functions/how-to-encode-decode-quantiletdigest-state/","tags":"","title":"How to encode/decode quantileTDigest states from/to list of centroids"},{"body":"How to pipe data from bcp export tool for MSSQL database Prepare tables LAPTOP.localdomain :) CREATE TABLE tbl(key UInt32) ENGINE=MergeTree ORDER BY key; root@LAPTOP:/home/user# sqlcmd -U sa -P Password78 1\u003e WITH t0(i) AS (SELECT 0 UNION ALL SELECT 0), t1(i) AS (SELECT 0 FROM t0 a, t0 b), t2(i) AS (SELECT 0 FROM t1 a, t1 b), t3(i) AS (SELECT 0 FROM t2 a, t2 b), t4(i) AS (SELECT 0 FROM t3 a, t3 b), t5(i) AS (SELECT 0 FROM t4 a, t3 b),n(i) AS (SELECT ROW_NUMBER() OVER(ORDER BY (SELECT 0)) FROM t5) SELECT i INTO tbl FROM n WHERE i BETWEEN 1 AND 16777216 2\u003e GO (16777216 rows affected) root@LAPTOP:/home/user# sqlcmd -U sa -P Password78 -Q \"SELECT count(*) FROM tbl\" ----------- 16777216 (1 rows affected) Piping root@LAPTOP:/home/user# mkfifo import_pipe root@LAPTOP:/home/user# bcp \"SELECT * FROM tbl\" queryout import_pipe -t, -c -b 200000 -U sa -P Password78 -S localhost \u0026 [1] 6038 root@LAPTOP:/home/user# Starting copy... 1000 rows successfully bulk-copied to host-file. Total received: 1000 1000 rows successfully bulk-copied to host-file. Total received: 2000 1000 rows successfully bulk-copied to host-file. Total received: 3000 1000 rows successfully bulk-copied to host-file. Total received: 4000 1000 rows successfully bulk-copied to host-file. Total received: 5000 1000 rows successfully bulk-copied to host-file. Total received: 6000 1000 rows successfully bulk-copied to host-file. Total received: 7000 1000 rows successfully bulk-copied to host-file. Total received: 8000 1000 rows successfully bulk-copied to host-file. Total received: 9000 1000 rows successfully bulk-copied to host-file. Total received: 10000 1000 rows successfully bulk-copied to host-file. Total received: 11000 1000 rows successfully bulk-copied to host-file. Total received: 12000 1000 rows successfully bulk-copied to host-file. Total received: 13000 1000 rows successfully bulk-copied to host-file. Total received: 14000 1000 rows successfully bulk-copied to host-file. Total received: 15000 1000 rows successfully bulk-copied to host-file. Total received: 16000 1000 rows successfully bulk-copied to host-file. Total received: 17000 1000 rows successfully bulk-copied to host-file. Total received: 18000 1000 rows successfully bulk-copied to host-file. Total received: 19000 1000 rows successfully bulk-copied to host-file. Total received: 20000 1000 rows successfully bulk-copied to host-file. Total received: 21000 1000 rows successfully bulk-copied to host-file. Total received: 22000 1000 rows successfully bulk-copied to host-file. Total received: 23000 -- Enter root@LAPTOP:/home/user# cat import_pipe | clickhouse-client --query \"INSERT INTO tbl FORMAT CSV\" \u0026 ... 1000 rows successfully bulk-copied to host-file. Total received: 16769000 1000 rows successfully bulk-copied to host-file. Total received: 16770000 1000 rows successfully bulk-copied to host-file. Total received: 16771000 1000 rows successfully bulk-copied to host-file. Total received: 16772000 1000 rows successfully bulk-copied to host-file. Total received: 16773000 1000 rows successfully bulk-copied to host-file. Total received: 16774000 1000 rows successfully bulk-copied to host-file. Total received: 16775000 1000 rows successfully bulk-copied to host-file. Total received: 16776000 1000 rows successfully bulk-copied to host-file. Total received: 16777000 16777216 rows copied. Network packet size (bytes): 4096 Clock Time (ms.) Total : 11540 Average : (1453831.5 rows per sec.) [1]- Done bcp \"SELECT * FROM tbl\" queryout import_pipe -t, -c -b 200000 -U sa -P Password78 -S localhost [2]+ Done cat import_pipe | clickhouse-client --query \"INSERT INTO tbl FORMAT CSV\" Another shell root@LAPTOP:/home/user# for i in `seq 1 600`; do clickhouse-client -q \"select count() from tbl\";sleep 1; done 0 0 0 0 0 0 1048545 4194180 6291270 9436905 11533995 13631085 16777216 16777216 16777216 16777216 ","categories":"","description":"Export from MSSQL to ClickHouse","excerpt":"Export from MSSQL to ClickHouse","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/mssql-clickhouse/","tags":"","title":"MSSQL bcp pipe to clickhouse-client"},{"body":"Functions to count uniqs Function Function(State) StateSize Result QPS uniqExact uniqExactState 1600003 100000 59.23 uniq uniqState 200804 100315 85.55 uniqCombined uniqCombinedState 98505 100314 108.09 uniqCombined(12) uniqCombinedState(12) 3291 98160 151.64 uniqCombined(15) uniqCombinedState(15) 24783 100768 110.18 uniqCombined(18) uniqCombinedState(18) 196805 100332 101.56 uniqCombined(20) uniqCombinedState(20) 786621 100088 65.05 uniqCombined64(12) uniqCombined64State(12) 3291 98160 164.96 uniqCombined64(15) uniqCombined64State(15) 24783 100768 133.96 uniqCombined64(18) uniqCombined64State(18) 196805 100332 110.85 uniqCombined64(20) uniqCombined64State(20) 786621 100088 66.48 uniqHLL12 uniqHLL12State 2651 101344 177.91 uniqTheta uniqThetaState 32795 98045 144.05 uniqUpTo(100) uniqUpToState(100) 1 101 222.93 Stats collected via script below on 22.2\nfuncname=( \"uniqExact\" \"uniq\" \"uniqCombined\" \"uniqCombined(12)\" \"uniqCombined(15)\" \"uniqCombined(18)\" \"uniqCombined(20)\" \"uniqCombined64(12)\" \"uniqCombined64(15)\" \"uniqCombined64(18)\" \"uniqCombined64(20)\" \"uniqHLL12\" \"uniqTheta\" \"uniqUpTo(100)\") funcname2=( \"uniqExactState\" \"uniqState\" \"uniqCombinedState\" \"uniqCombinedState(12)\" \"uniqCombinedState(15)\" \"uniqCombinedState(18)\" \"uniqCombinedState(20)\" \"uniqCombined64State(12)\" \"uniqCombined64State(15)\" \"uniqCombined64State(18)\" \"uniqCombined64State(20)\" \"uniqHLL12State\" \"uniqThetaState\" \"uniqUpToState(100)\") length=${#funcname[@]} for (( j=0; j\u003clength; j++ )); do f1=\"${funcname[$j]}\" f2=\"${funcname2[$j]}\" size=$( clickhouse-client -q \"select ${f2}(toString(number)) from numbers_mt(100000) FORMAT RowBinary\" | wc -c ) result=\"$( clickhouse-client -q \"select ${f1}(toString(number)) from numbers_mt(100000)\" )\" time=$( rm /tmp/clickhouse-benchmark.json; echo \"select ${f1}(toString(number)) from numbers_mt(100000)\" | clickhouse-benchmark -i200 --json=/tmp/clickhouse-benchmark.json \u0026\u003e/dev/null; cat /tmp/clickhouse-benchmark.json | grep QPS ) printf \"|%s|%s,%s,%s,%s\\n\" \"$f1\" \"$f2\" \"$size\" \"$result\" \"$time\" done groupBitmap Use Roaring Bitmaps underneat. Return amount of uniq values.\nCan be used with Int* types Works really great when your values quite similar. (Low memory usage / state size)\nExample with blockchain data, block_number is monotonically increasing number.\nSELECT groupBitmap(block_number) FROM blockchain; ┌─groupBitmap(block_number)─┐ │ 48478157 │ └───────────────────────────┘ MemoryTracker: Peak memory usage (for query): 64.44 MiB. 1 row in set. Elapsed: 32.044 sec. Processed 4.77 billion rows, 38.14 GB (148.77 million rows/s., 1.19 GB/s.) SELECT uniqExact(block_number) FROM blockchain; ┌─uniqExact(block_number)─┐ │ 48478157 │ └─────────────────────────┘ MemoryTracker: Peak memory usage (for query): 4.27 GiB. 1 row in set. Elapsed: 70.058 sec. Processed 4.77 billion rows, 38.14 GB (68.05 million rows/s., 544.38 MB/s.) ","categories":"","description":"Functions to count uniqs.","excerpt":"Functions to count uniqs.","ref":"/altinity-kb-schema-design/uniq-functions/","tags":"","title":"Functions to count uniqs"},{"body":"Tricks Testing dataset\nCREATE TABLE sessions ( `app` LowCardinality(String), `user_id` String, `created_at` DateTime, `platform` LowCardinality(String), `clicks` UInt32, `session_id` UUID ) ENGINE = MergeTree PARTITION BY toYYYYMM(created_at) ORDER BY (app, user_id, session_id, created_at) INSERT INTO sessions WITH CAST(number % 4, 'Enum8(\\'Orange\\' = 0, \\'Melon\\' = 1, \\'Red\\' = 2, \\'Blue\\' = 3)') AS app, concat('UID: ', leftPad(toString(number % 20000000), 8, '0')) AS user_id, toDateTime('2021-10-01 10:11:12') + (number / 300) AS created_at, CAST((number + 14) % 3, 'Enum8(\\'Bat\\' = 0, \\'Mice\\' = 1, \\'Rat\\' = 2)') AS platform, number % 17 AS clicks, generateUUIDv4() AS session_id SELECT app, user_id, created_at, platform, clicks, session_id FROM numbers_mt(1000000000) 0 rows in set. Elapsed: 46.078 sec. Processed 1.00 billion rows, 8.00 GB (21.70 million rows/s., 173.62 MB/s.) ┌─database─┬─table────┬─column─────┬─type───────────────────┬───────rows─┬─compressed_bytes─┬─compressed─┬─uncompressed─┬──────────────ratio─┬─codec─┐ │ default │ sessions │ session_id │ UUID │ 1000000000 │ 16065918103 │ 14.96 GiB │ 14.90 GiB │ 0.9958970223439835 │ │ │ default │ sessions │ user_id │ String │ 1000000000 │ 3056977462 │ 2.85 GiB │ 13.04 GiB │ 4.57968701896828 │ │ │ default │ sessions │ clicks │ UInt32 │ 1000000000 │ 1859359032 │ 1.73 GiB │ 3.73 GiB │ 2.151278979023993 │ │ │ default │ sessions │ created_at │ DateTime │ 1000000000 │ 1332089630 │ 1.24 GiB │ 3.73 GiB │ 3.0028009451586226 │ │ │ default │ sessions │ platform │ LowCardinality(String) │ 1000000000 │ 329702248 │ 314.43 MiB │ 956.63 MiB │ 3.042446801879252 │ │ │ default │ sessions │ app │ LowCardinality(String) │ 1000000000 │ 4825544 │ 4.60 MiB │ 956.63 MiB │ 207.87333386660654 │ │ └──────────┴──────────┴────────────┴────────────────────────┴────────────┴──────────────────┴────────────┴──────────────┴────────────────────┴───────┘ All queries and datasets are unique, so in different situations different hacks could work better or worse.\nPreFilter values before GROUP BY SELECT user_id, sum(clicks) FROM sessions WHERE created_at \u003e '2021-11-01 00:00:00' GROUP BY user_id HAVING (argMax(clicks, created_at) = 16) AND (argMax(platform, created_at) = 'Rat') FORMAT `Null` \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 18.36 GiB. SELECT user_id, sum(clicks) FROM sessions WHERE user_id IN ( SELECT user_id FROM sessions WHERE (platform = 'Rat') AND (clicks = 16) AND (created_at \u003e '2021-11-01 00:00:00') -- So we will select user_id which could potentially match our HAVING clause in OUTER query. ) AND (created_at \u003e '2021-11-01 00:00:00') GROUP BY user_id HAVING (argMax(clicks, created_at) = 16) AND (argMax(platform, created_at) = 'Rat') FORMAT `Null` \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 4.43 GiB. Use Fixed-width data types instead of String For example, you have 2 strings which has values in special form like this\n‘ABX 1412312312313’\nYou can just remove the first 4 characters and convert the rest to UInt64\ntoUInt64(substr(‘ABX 1412312312313’,5))\nAnd you packed 17 bytes in 8, more than 2 times the improvement of size!\nSELECT user_id, sum(clicks) FROM sessions GROUP BY user_id, platform FORMAT `Null` Aggregator: Aggregation method: serialized \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 28.19 GiB. Elapsed: 7.375 sec. Processed 1.00 billion rows, 27.00 GB (135.60 million rows/s., 3.66 GB/s.) WITH CAST(user_id, 'FixedString(14)') AS user_fx, CAST(platform, 'FixedString(4)') AS platform_fx SELECT user_fx, sum(clicks) FROM sessions GROUP BY user_fx, platform_fx FORMAT `Null` Aggregator: Aggregation method: keys256 MemoryTracker: Peak memory usage (for query): 22.24 GiB. Elapsed: 6.637 sec. Processed 1.00 billion rows, 27.00 GB (150.67 million rows/s., 4.07 GB/s.) WITH CAST(user_id, 'FixedString(14)') AS user_fx, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 0)') AS platform_enum SELECT user_fx, sum(clicks) FROM sessions GROUP BY user_fx, platform_enum FORMAT `Null` Aggregator: Aggregation method: keys128 MemoryTracker: Peak memory usage (for query): 14.14 GiB. Elapsed: 5.335 sec. Processed 1.00 billion rows, 27.00 GB (187.43 million rows/s., 5.06 GB/s.) WITH toUInt32OrZero(trim( LEADING '0' FROM substr(user_id,6))) AS user_int, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 0)') AS platform_enum SELECT user_int, sum(clicks) FROM sessions GROUP BY user_int, platform_enum FORMAT `Null` Aggregator: Aggregation method: keys64 MemoryTracker: Peak memory usage (for query): 10.14 GiB. Elapsed: 8.549 sec. Processed 1.00 billion rows, 27.00 GB (116.97 million rows/s., 3.16 GB/s.) WITH toUInt32('1' || substr(user_id,6)) AS user_int, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 0)') AS platform_enum SELECT user_int, sum(clicks) FROM sessions GROUP BY user_int, platform_enum FORMAT `Null` Aggregator: Aggregation method: keys64 Peak memory usage (for query): 10.14 GiB. Elapsed: 6.247 sec. Processed 1.00 billion rows, 27.00 GB (160.09 million rows/s., 4.32 GB/s.) It can be especially useful when you tries to do GROUP BY lc_column_1, lc_column_2 and ClickHouse falls back to serialized algorithm.\nTwo LowCardinality Columns in GROUP BY SELECT app, sum(clicks) FROM sessions GROUP BY app FORMAT `Null` Aggregator: Aggregation method: low_cardinality_key_string MemoryTracker: Peak memory usage (for query): 43.81 MiB. Elapsed: 0.545 sec. Processed 1.00 billion rows, 5.00 GB (1.83 billion rows/s., 9.17 GB/s.) SELECT app, platform, sum(clicks) FROM sessions GROUP BY app, platform FORMAT `Null` Aggregator: Aggregation method: serialized -- Slowest method! MemoryTracker: Peak memory usage (for query): 222.86 MiB. Elapsed: 2.923 sec. Processed 1.00 billion rows, 6.00 GB (342.11 million rows/s., 2.05 GB/s.) SELECT CAST(app, 'FixedString(6)') AS app_fx, CAST(platform, 'FixedString(4)') AS platform_fx, sum(clicks) FROM sessions GROUP BY app_fx, platform_fx FORMAT `Null` Aggregator: Aggregation method: keys128 MemoryTracker: Peak memory usage (for query): 160.23 MiB. Elapsed: 1.632 sec. Processed 1.00 billion rows, 6.00 GB (612.63 million rows/s., 3.68 GB/s.) Split your query in multiple smaller queries and execute them one BY one SELECT user_id, sum(clicks) FROM sessions GROUP BY user_id, platform FORMAT `Null` MemoryTracker: Peak memory usage (for query): 28.19 GiB. Elapsed: 7.375 sec. Processed 1.00 billion rows, 27.00 GB (135.60 million rows/s., 3.66 GB/s.) SELECT user_id, sum(clicks) FROM sessions WHERE (cityHash64(user_id) % 4) = 0 GROUP BY user_id, platform FORMAT `Null` MemoryTracker: Peak memory usage (for query): 8.16 GiB. Elapsed: 2.910 sec. Processed 1.00 billion rows, 27.00 GB (343.64 million rows/s., 9.28 GB/s.) Shard your data by one of common high cardinal GROUP BY key So on each shard you will have 1/N of all unique combination and this will result in smaller hash tables.\nLet’s create 2 distributed tables with different distribution: rand() and by user_id\nCREATE TABLE sessions_distributed AS sessions ENGINE = Distributed('distr-groupby', default, sessions, rand()); INSERT INTO sessions_distributed WITH CAST(number % 4, 'Enum8(\\'Orange\\' = 0, \\'Melon\\' = 1, \\'Red\\' = 2, \\'Blue\\' = 3)') AS app, concat('UID: ', leftPad(toString(number % 20000000), 8, '0')) AS user_id, toDateTime('2021-10-01 10:11:12') + (number / 300) AS created_at, CAST((number + 14) % 3, 'Enum8(\\'Bat\\' = 0, \\'Mice\\' = 1, \\'Rat\\' = 2)') AS platform, number % 17 AS clicks, generateUUIDv4() AS session_id SELECT app, user_id, created_at, platform, clicks, session_id FROM numbers_mt(1000000000); CREATE TABLE sessions_2 ON CLUSTER 'distr-groupby' ( `app` LowCardinality(String), `user_id` String, `created_at` DateTime, `platform` LowCardinality(String), `clicks` UInt32, `session_id` UUID ) ENGINE = MergeTree PARTITION BY toYYYYMM(created_at) ORDER BY (app, user_id, session_id, created_at); CREATE TABLE sessions_distributed_2 AS sessions ENGINE = Distributed('distr-groupby', default, sessions_2, cityHash64(user_id)); INSERT INTO sessions_distributed_2 WITH CAST(number % 4, 'Enum8(\\'Orange\\' = 0, \\'Melon\\' = 1, \\'Red\\' = 2, \\'Blue\\' = 3)') AS app, concat('UID: ', leftPad(toString(number % 20000000), 8, '0')) AS user_id, toDateTime('2021-10-01 10:11:12') + (number / 300) AS created_at, CAST((number + 14) % 3, 'Enum8(\\'Bat\\' = 0, \\'Mice\\' = 1, \\'Rat\\' = 2)') AS platform, number % 17 AS clicks, generateUUIDv4() AS session_id SELECT app, user_id, created_at, platform, clicks, session_id FROM numbers_mt(1000000000); SELECT app, platform, sum(clicks) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS clicks FROM sessions_distributed GROUP BY user_id ) GROUP BY app, platform; [chi-distr-groupby-distr-groupby-0-0-0] MemoryTracker: Current memory usage (for query): 12.02 GiB. [chi-distr-groupby-distr-groupby-1-0-0] MemoryTracker: Current memory usage (for query): 12.05 GiB. [chi-distr-groupby-distr-groupby-2-0-0] MemoryTracker: Current memory usage (for query): 12.05 GiB. MemoryTracker: Peak memory usage (for query): 12.20 GiB. 12 rows in set. Elapsed: 28.345 sec. Processed 1.00 billion rows, 32.00 GB (35.28 million rows/s., 1.13 GB/s.) SELECT app, platform, sum(clicks) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS clicks FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform; [chi-distr-groupby-distr-groupby-0-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-1-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-2-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. MemoryTracker: Peak memory usage (for query): 5.61 GiB. 12 rows in set. Elapsed: 11.952 sec. Processed 1.00 billion rows, 32.00 GB (83.66 million rows/s., 2.68 GB/s.) SELECT app, platform, sum(clicks) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS clicks FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform SETTINGS optimize_distributed_group_by_sharding_key = 1 [chi-distr-groupby-distr-groupby-0-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-1-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-2-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. MemoryTracker: Peak memory usage (for query): 5.61 GiB. 12 rows in set. Elapsed: 11.916 sec. Processed 1.00 billion rows, 32.00 GB (83.92 million rows/s., 2.69 GB/s.) SELECT app, platform, sum(clicks) FROM cluster('distr-groupby', view( SELECT app, platform, sum(clicks) as clicks FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS clicks FROM sessions_2 GROUP BY user_id ) GROUP BY app, platform )) GROUP BY app, platform; [chi-distr-groupby-distr-groupby-0-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-1-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. [chi-distr-groupby-distr-groupby-2-0-0] MemoryTracker: Current memory usage (for query): 5.05 GiB. MemoryTracker: Peak memory usage (for query): 5.55 GiB. 12 rows in set. Elapsed: 9.491 sec. Processed 1.00 billion rows, 32.00 GB (105.36 million rows/s., 3.37 GB/s.) Query with bigger state:\nSELECT app, platform, sum(last_click) as sum, max(max_clicks) as max, min(min_clicks) as min, max(max_time) as max_time, min(min_time) as min_time FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS last_click, max(clicks) AS max_clicks, min(clicks) AS min_clicks, max(created_at) AS max_time, min(created_at) AS min_time FROM sessions_distributed GROUP BY user_id ) GROUP BY app, platform; MemoryTracker: Peak memory usage (for query): 19.95 GiB. 12 rows in set. Elapsed: 34.339 sec. Processed 1.00 billion rows, 32.00 GB (29.12 million rows/s., 932.03 MB/s.) SELECT app, platform, sum(last_click) as sum, max(max_clicks) as max, min(min_clicks) as min, max(max_time) as max_time, min(min_time) as min_time FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS last_click, max(clicks) AS max_clicks, min(clicks) AS min_clicks, max(created_at) AS max_time, min(created_at) AS min_time FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform; MemoryTracker: Peak memory usage (for query): 10.09 GiB. 12 rows in set. Elapsed: 13.220 sec. Processed 1.00 billion rows, 32.00 GB (75.64 million rows/s., 2.42 GB/s.) SELECT app, platform, sum(last_click) AS sum, max(max_clicks) AS max, min(min_clicks) AS min, max(max_time) AS max_time, min(min_time) AS min_time FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS last_click, max(clicks) AS max_clicks, min(clicks) AS min_clicks, max(created_at) AS max_time, min(created_at) AS min_time FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform SETTINGS optimize_distributed_group_by_sharding_key = 1; MemoryTracker: Peak memory usage (for query): 10.09 GiB. 12 rows in set. Elapsed: 13.361 sec. Processed 1.00 billion rows, 32.00 GB (74.85 million rows/s., 2.40 GB/s.) SELECT app, platform, sum(last_click) AS sum, max(max_clicks) AS max, min(min_clicks) AS min, max(max_time) AS max_time, min(min_time) AS min_time FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS last_click, max(clicks) AS max_clicks, min(clicks) AS min_clicks, max(created_at) AS max_time, min(created_at) AS min_time FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform SETTINGS distributed_group_by_no_merge=2; MemoryTracker: Peak memory usage (for query): 10.02 GiB. 12 rows in set. Elapsed: 9.789 sec. Processed 1.00 billion rows, 32.00 GB (102.15 million rows/s., 3.27 GB/s.) SELECT app, platform, sum(sum), max(max), min(min), max(max_time) AS max_time, min(min_time) AS min_time FROM cluster('distr-groupby', view( SELECT app, platform, sum(last_click) AS sum, max(max_clicks) AS max, min(min_clicks) AS min, max(max_time) AS max_time, min(min_time) AS min_time FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, argMax(clicks, created_at) AS last_click, max(clicks) AS max_clicks, min(clicks) AS min_clicks, max(created_at) AS max_time, min(created_at) AS min_time FROM sessions_2 GROUP BY user_id ) GROUP BY app, platform )) GROUP BY app, platform; MemoryTracker: Peak memory usage (for query): 10.09 GiB. 12 rows in set. Elapsed: 9.525 sec. Processed 1.00 billion rows, 32.00 GB (104.98 million rows/s., 3.36 GB/s.) SELECT app, platform, sum(sessions) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, uniq(session_id) as sessions FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform MemoryTracker: Peak memory usage (for query): 14.57 GiB. 12 rows in set. Elapsed: 37.730 sec. Processed 1.00 billion rows, 44.01 GB (26.50 million rows/s., 1.17 GB/s.) SELECT app, platform, sum(sessions) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, uniq(session_id) as sessions FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform SETTINGS optimize_distributed_group_by_sharding_key = 1; MemoryTracker: Peak memory usage (for query): 14.56 GiB. 12 rows in set. Elapsed: 37.792 sec. Processed 1.00 billion rows, 44.01 GB (26.46 million rows/s., 1.16 GB/s.) SELECT app, platform, sum(sessions) FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, uniq(session_id) as sessions FROM sessions_distributed_2 GROUP BY user_id ) GROUP BY app, platform SETTINGS distributed_group_by_no_merge = 2; MemoryTracker: Peak memory usage (for query): 14.54 GiB. 12 rows in set. Elapsed: 17.762 sec. Processed 1.00 billion rows, 44.01 GB (56.30 million rows/s., 2.48 GB/s.) SELECT app, platform, sum(sessions) FROM cluster('distr-groupby', view( SELECT app, platform, sum(sessions) as sessions FROM ( SELECT argMax(app, created_at) AS app, argMax(platform, created_at) AS platform, user_id, uniq(session_id) as sessions FROM sessions_2 GROUP BY user_id ) GROUP BY app, platform)) GROUP BY app, platform MemoryTracker: Peak memory usage (for query): 14.55 GiB. 12 rows in set. Elapsed: 17.574 sec. Processed 1.00 billion rows, 44.01 GB (56.90 million rows/s., 2.50 GB/s.) Reduce number of threads Because each thread uses an independent hash table, if you lower thread amount it will reduce number of hash tables as well and lower memory usage at the cost of slower query execution.\nSELECT user_id, sum(clicks) FROM sessions GROUP BY user_id, platform FORMAT `Null` MemoryTracker: Peak memory usage (for query): 28.19 GiB. Elapsed: 7.375 sec. Processed 1.00 billion rows, 27.00 GB (135.60 million rows/s., 3.66 GB/s.) SET max_threads = 2; SELECT user_id, sum(clicks) FROM sessions GROUP BY user_id, platform FORMAT `Null` MemoryTracker: Peak memory usage (for query): 13.26 GiB. Elapsed: 62.014 sec. Processed 1.00 billion rows, 27.00 GB (16.13 million rows/s., 435.41 MB/s.) UNION ALL SELECT user_id, sum(clicks) FROM sessions GROUP BY app, user_id FORMAT `Null` MemoryTracker: Peak memory usage (for query): 24.19 GiB. Elapsed: 5.043 sec. Processed 1.00 billion rows, 27.00 GB (198.29 million rows/s., 5.35 GB/s.) SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Orange' GROUP BY user_id UNION ALL SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Red' GROUP BY user_id UNION ALL SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Melon' GROUP BY user_id UNION ALL SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Blue' GROUP BY user_id FORMAT Null MemoryTracker: Peak memory usage (for query): 7.99 GiB. Elapsed: 2.852 sec. Processed 1.00 billion rows, 27.01 GB (350.74 million rows/s., 9.47 GB/s.) aggregation_in_order SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Orange' GROUP BY user_id FORMAT `Null` MemoryTracker: Peak memory usage (for query): 969.33 MiB. Elapsed: 2.494 sec. Processed 250.09 million rows, 6.75 GB (100.27 million rows/s., 2.71 GB/s.) SET optimize_aggregation_in_order = 1; SELECT user_id, sum(clicks) FROM sessions WHERE app = 'Orange' GROUP BY app, user_id FORMAT `Null` AggregatingInOrderTransform: Aggregating in order MemoryTracker: Peak memory usage (for query): 169.24 MiB. Elapsed: 4.925 sec. Processed 250.09 million rows, 6.75 GB (50.78 million rows/s., 1.37 GB/s.) Reduce dimensions from GROUP BY with functions like sumMap, *Resample One\nSELECT user_id, toDate(created_at) AS day, sum(clicks) FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange', 'Red', 'Blue')) GROUP BY user_id, day FORMAT `Null` MemoryTracker: Peak memory usage (for query): 50.74 GiB. Elapsed: 22.671 sec. Processed 594.39 million rows, 18.46 GB (26.22 million rows/s., 814.41 MB/s.) SELECT user_id, (toDate('2021-10-01') + date_diff) - 1 AS day, clicks FROM ( SELECT user_id, sumResample(0, 31, 1)(clicks, toDate(created_at) - toDate('2021-10-01')) AS clicks_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange', 'Red', 'Blue')) GROUP BY user_id ) ARRAY JOIN clicks_arr AS clicks, arrayEnumerate(clicks_arr) AS date_diff FORMAT `Null` Peak memory usage (for query): 8.24 GiB. Elapsed: 5.191 sec. Processed 594.39 million rows, 18.46 GB (114.50 million rows/s., 3.56 GB/s.) Multiple\nSELECT user_id, platform, toDate(created_at) AS day, sum(clicks) FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) AND user_id ='UID: 08525196' GROUP BY user_id, platform, day ORDER BY user_id, day, platform FORMAT `Null` Peak memory usage (for query): 29.50 GiB. Elapsed: 8.181 sec. Processed 198.14 million rows, 6.34 GB (24.22 million rows/s., 775.14 MB/s.) WITH arrayJoin(arrayZip(clicks_arr_lvl_2, range(3))) AS clicks_res SELECT user_id, CAST(clicks_res.2 + 1, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)') AS platform, (toDate('2021-10-01') + date_diff) - 1 AS day, clicks_res.1 AS clicks FROM ( SELECT user_id, sumResampleResample(1, 4, 1, 0, 31, 1)(clicks, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)'), toDate(created_at) - toDate('2021-10-01')) AS clicks_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id ) ARRAY JOIN clicks_arr AS clicks_arr_lvl_2, range(31) AS date_diff FORMAT `Null` Peak memory usage (for query): 9.92 GiB. Elapsed: 4.170 sec. Processed 198.14 million rows, 6.34 GB (47.52 million rows/s., 1.52 GB/s.) WITH arrayJoin(arrayZip(clicks_arr_lvl_2, range(3))) AS clicks_res SELECT user_id, CAST(clicks_res.2 + 1, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)') AS platform, (toDate('2021-10-01') + date_diff) - 1 AS day, clicks_res.1 AS clicks FROM ( SELECT user_id, sumResampleResample(1, 4, 1, 0, 31, 1)(clicks, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)'), toDate(created_at) - toDate('2021-10-01')) AS clicks_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id ) ARRAY JOIN clicks_arr AS clicks_arr_lvl_2, range(31) AS date_diff WHERE clicks \u003e 0 FORMAT `Null` Peak memory usage (for query): 10.14 GiB. Elapsed: 9.533 sec. Processed 198.14 million rows, 6.34 GB (20.78 million rows/s., 665.20 MB/s.) SELECT user_id, CAST(plat + 1, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)') AS platform, (toDate('2021-10-01') + date_diff) - 1 AS day, clicks FROM ( WITH (SELECT flatten(arrayMap(x -\u003e range(3) AS platforms, range(31) as days))) AS platform_arr, (SELECT flatten(arrayMap(x -\u003e [x, x, x], range(31) as days))) AS days_arr SELECT user_id, flatten(sumResampleResample(1, 4, 1, 0, 31, 1)(clicks, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)'), toDate(created_at) - toDate('2021-10-01'))) AS clicks_arr, platform_arr, days_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id ) ARRAY JOIN clicks_arr AS clicks, platform_arr AS plat, days_arr AS date_diff FORMAT `Null` Peak memory usage (for query): 9.95 GiB. Elapsed: 3.095 sec. Processed 198.14 million rows, 6.34 GB (64.02 million rows/s., 2.05 GB/s.) SELECT user_id, CAST(plat + 1, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)') AS platform, (toDate('2021-10-01') + date_diff) - 1 AS day, clicks FROM ( WITH (SELECT flatten(arrayMap(x -\u003e range(3) AS platforms, range(31) as days))) AS platform_arr, (SELECT flatten(arrayMap(x -\u003e [x, x, x], range(31) as days))) AS days_arr SELECT user_id, sumResampleResample(1, 4, 1, 0, 31, 1)(clicks, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)'), toDate(created_at) - toDate('2021-10-01')) AS clicks_arr, arrayFilter(x -\u003e ((x.1) \u003e 0), arrayZip(flatten(clicks_arr), platform_arr, days_arr)) AS result FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id ) ARRAY JOIN result.1 AS clicks, result.2 AS plat, result.3 AS date_diff FORMAT `Null` Peak memory usage (for query): 9.93 GiB. Elapsed: 4.717 sec. Processed 198.14 million rows, 6.34 GB (42.00 million rows/s., 1.34 GB/s.) SELECT user_id, CAST(range % 3, 'Enum8(\\'Rat\\' = 0, \\'Mice\\' = 1, \\'Bat\\' = 2)') AS platform, toDate('2021-10-01') + intDiv(range, 3) AS day, clicks FROM ( WITH ( SELECT range(93) ) AS range_arr SELECT user_id, sumResample(0, 93, 1)(clicks, ((toDate(created_at) - toDate('2021-10-01')) * 3) + toUInt8(CAST(platform, 'Enum8(\\'Rat\\' = 0, \\'Mice\\' = 1, \\'Bat\\' = 2)'))) AS clicks_arr, range_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id ) ARRAY JOIN clicks_arr AS clicks, range_arr AS range FORMAT `Null` Peak memory usage (for query): 8.24 GiB. Elapsed: 4.838 sec. Processed 198.14 million rows, 6.36 GB (40.95 million rows/s., 1.31 GB/s.) SELECT user_id, sumResampleResample(1, 4, 1, 0, 31, 1)(clicks, CAST(platform, 'Enum8(\\'Rat\\' = 1, \\'Mice\\' = 2, \\'Bat\\' = 3)'), toDate(created_at) - toDate('2021-10-01')) AS clicks_arr FROM sessions WHERE (created_at \u003e= toDate('2021-10-01')) AND (created_at \u003c toDate('2021-11-01')) AND (app IN ('Orange')) GROUP BY user_id FORMAT `Null` Peak memory usage (for query): 5.19 GiB. 0 rows in set. Elapsed: 1.160 sec. Processed 198.14 million rows, 6.34 GB (170.87 million rows/s., 5.47 GB/s.) ARRAY JOIN can be expensive\nhttps://kb.altinity.com/altinity-kb-functions/array-like-memory-usage/\nsumMap, *Resample\nhttps://kb.altinity.com/altinity-kb-functions/resample-vs-if-vs-map-vs-subquery/\nPlay with two-level Disable:\nSET group_by_two_level_threshold = 0, group_by_two_level_threshold_bytes = 0; From 22.4 ClickHouse can predict, when it make sense to initialize aggregation with two-level from start, instead of rehashing on fly. It can improve query time. https://github.com/ClickHouse/ClickHouse/pull/33439\nGROUP BY in external memory Slow!\nUse hash function for GROUP BY keys GROUP BY cityHash64(‘xxxx’)\nCan lead to incorrect results as hash functions is not 1 to 1 mapping.\nPerformance bugs https://github.com/ClickHouse/ClickHouse/issues/15005\nhttps://github.com/ClickHouse/ClickHouse/issues/29131\nhttps://github.com/ClickHouse/ClickHouse/issues/31120\nhttps://github.com/ClickHouse/ClickHouse/issues/35096 Fixed in 22.7\n","categories":"","description":"Tricks for GROUP BY memory usage optimization","excerpt":"Tricks for GROUP BY memory usage optimization","ref":"/altinity-kb-queries-and-syntax/group-by/tricks/","tags":"","title":"GROUP BY tricks"},{"body":"The most cpu / write / read-intensive queries from query_log SELECT normalized_query_hash, any(query), count(), sum(query_duration_ms) / 1000 AS QueriesDuration, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'RealTimeMicroseconds')]) / 1000000 AS RealTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'UserTimeMicroseconds')]) / 1000000 AS UserTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'SystemTimeMicroseconds')]) / 1000000 AS SystemTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskReadElapsedMicroseconds')]) / 1000000 AS DiskReadTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'DiskWriteElapsedMicroseconds')]) / 1000000 AS DiskWriteTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkSendElapsedMicroseconds')]) / 1000000 AS NetworkSendTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'NetworkReceiveElapsedMicroseconds')]) / 1000000 AS NetworkReceiveTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'ZooKeeperWaitMicroseconds')]) / 1000000 AS ZooKeeperWaitTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSIOWaitMicroseconds')]) / 1000000 AS OSIOWaitTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUWaitMicroseconds')]) / 1000000 AS OSCPUWaitTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, 'OSCPUVirtualTimeMicroseconds')]) / 1000000 AS OSCPUVirtualTime, sum(read_rows) AS ReadRows, formatReadableSize(sum(read_bytes)) AS ReadBytes, sum(written_rows) AS WrittenTows, formatReadableSize(sum(written_bytes)) AS WrittenBytes, sum(result_rows) AS ResultRows, formatReadableSize(sum(result_bytes)) AS ResultBytes FROM system.query_log WHERE (event_date \u003e= today()) AND (event_time \u003e (now() - 3600)) AND type in (2,4) -- QueryFinish, ExceptionWhileProcessing GROUP BY normalized_query_hash WITH TOTALS ORDER BY UserTime DESC LIMIT 30 FORMAT Vertical -- modern Clickhouse SELECT hostName() as host, normalized_query_hash, min(event_time), max(event_time), replace(substr(argMax(query, utime), 1, 80), '\\n', ' ') AS query, argMax(query_id, utime) AS sample_query_id, count(), sum(query_duration_ms) / 1000 AS QueriesDuration, /* wall clock */ sum(ProfileEvents['RealTimeMicroseconds']) / 1000000 AS RealTime, /* same as above but x number of thread */ sum(ProfileEvents['UserTimeMicroseconds'] as utime) / 1000000 AS UserTime, /* time when our query was doin some cpu-insense work, creating cpu load */ sum(ProfileEvents['SystemTimeMicroseconds']) / 1000000 AS SystemTime, /* time spend on waiting for some system operations */ sum(ProfileEvents['DiskReadElapsedMicroseconds']) / 1000000 AS DiskReadTime, sum(ProfileEvents['DiskWriteElapsedMicroseconds']) / 1000000 AS DiskWriteTime, sum(ProfileEvents['NetworkSendElapsedMicroseconds']) / 1000000 AS NetworkSendTime, /* check the other side of the network! */ sum(ProfileEvents['NetworkReceiveElapsedMicroseconds']) / 1000000 AS NetworkReceiveTime, /* check the other side of the network! */ sum(ProfileEvents['ZooKeeperWaitMicroseconds']) / 1000000 AS ZooKeeperWaitTime, sum(ProfileEvents['OSIOWaitMicroseconds']) / 1000000 AS OSIOWaitTime, /* IO waits, usually disks - that metric is 'orthogonal' to other */ sum(ProfileEvents['OSCPUWaitMicroseconds']) / 1000000 AS OSCPUWaitTime, /* waiting for a 'free' CPU - usually high when the other load on the server creates a lot of contention for cpu */ sum(ProfileEvents['OSCPUVirtualTimeMicroseconds']) / 1000000 AS OSCPUVirtualTime, /* similar to usertime + system time */ formatReadableSize(sum(ProfileEvents['NetworkReceiveBytes']) as network_receive_bytes) AS NetworkReceiveBytes, formatReadableSize(sum(ProfileEvents['NetworkSendBytes']) as network_send_bytes) AS NetworkSendBytes, sum(ProfileEvents['SelectedParts']) as SelectedParts, sum(ProfileEvents['SelectedRanges']) as SelectedRanges, sum(ProfileEvents['SelectedMarks']) as SelectedMarks, sum(ProfileEvents['SelectedRows']) as SelectedRows, /* those may different from read_rows - here the number or rows potentially matching the where conditions, not neccessary all will be read */ sum(ProfileEvents['SelectedBytes']) as SelectedBytes, sum(ProfileEvents['FileOpen']) as FileOpen, sum(ProfileEvents['ZooKeeperTransactions']) as ZooKeeperTransactions, formatReadableSize(sum(ProfileEvents['OSReadBytes'] ) as os_read_bytes ) as OSReadBytesExcludePageCache, formatReadableSize(sum(ProfileEvents['OSWriteBytes'] ) as os_write_bytes ) as OSWriteBytesExcludePageCache, formatReadableSize(sum(ProfileEvents['OSReadChars'] ) as os_read_chars ) as OSReadBytesIncludePageCache, formatReadableSize(sum(ProfileEvents['OSWriteChars'] ) as os_write_chars ) as OSWriteCharsIncludePageCache, formatReadableSize(quantile(0.97)(memory_usage) as memory_usage_q97) as MemoryUsageQ97 , sum(read_rows) AS ReadRows, formatReadableSize(sum(read_bytes) as read_bytes_sum) AS ReadBytes, sum(written_rows) AS WrittenRows, formatReadableSize(sum(written_bytes) as written_bytes_sum) AS WrittenBytes, /* */ sum(result_rows) AS ResultRows, formatReadableSize(sum(result_bytes) as result_bytes_sum) AS ResultBytes FROM clusterAllReplicas('{cluster}', system.query_log) WHERE event_date \u003e= today() AND type in (2,4)-- QueryFinish, ExceptionWhileProcessing GROUP BY GROUPING SETS ( (normalized_query_hash, host), (host), ()) ORDER BY OSCPUVirtualTime DESC LIMIT 30 FORMAT Vertical; Find queries which were started but not finished at some moment in time SELECT query_id, min(event_time) t, any(query) FROM system.query_log where event_date = today() and event_time \u003e '2021-11-25 02:29:12' GROUP BY query_id HAVING countIf(type='QueryFinish') = 0 OR sum(query_duration_ms) \u003e 100000 order by t; select query_id, any(query) from system.query_log where event_time between '2021-09-24 07:00:00' and '2021-09-24 09:00:00' group by query_id HAVING countIf(type=1) \u003c\u003e countIf(type!=1) ","categories":"","description":"Handy queries for a system.query_log.","excerpt":"Handy queries for a system.query_log.","ref":"/altinity-kb-useful-queries/query_log/","tags":"","title":"Handy queries for system.query_log"},{"body":"How ALTER’s works in ClickHouse: ADD (COLUMN/INDEX/PROJECTION) Lightweight, will only change table metadata. So new entity will be added in case of creation of new parts during INSERT’s OR during merges of old parts.\nIn case of COLUMN, ClickHouse will calculate column value on fly in query context.\nWarning CREATE TABLE test_materialization ( `key` UInt32, `value` UInt32 ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_materialization(key, value) SELECT 1, 1; INSERT INTO test_materialization(key, value) SELECT 2, 2; ALTER TABLE test_materialization ADD COLUMN inserted_at DateTime DEFAULT now(); SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:28:58 │ └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 2 │ 2022-09-01 03:28:58 │ └─────┴─────────────────────┘ SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:29:11 │ └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 2 │ 2022-09-01 03:29:11 │ └─────┴─────────────────────┘ Each query will return different inserted_at value, because each time now() function being executed. INSERT INTO test_materialization(key, value) SELECT 3, 3; SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 3 │ 2022-09-01 03:29:36 │ -- \u003c This value was materialized during ingestion, that's why it's smaller than value for keys 1 \u0026 2 └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:29:53 │ └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 2 │ 2022-09-01 03:29:53 │ └─────┴─────────────────────┘ OPTIMIZE TABLE test_materialization FINAL; SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:30:52 │ │ 2 │ 2022-09-01 03:30:52 │ │ 3 │ 2022-09-01 03:29:36 │ └─────┴─────────────────────┘ SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:30:52 │ │ 2 │ 2022-09-01 03:30:52 │ │ 3 │ 2022-09-01 03:29:36 │ └─────┴─────────────────────┘ So, data inserted after addition of column can have lower inserted_at value then old data without materialization. If you want to backpopulate data for old parts, you have multiple options:\nMATERIALIZE (COLUMN/INDEX/PROJECTION) (PART[ITION ID] ‘’) Will materialize this entity.\nOPTIMIZE TABLE xxxx (PART[ITION ID] ‘’) (FINAL) Will trigger merge, which will lead to materialization of all entities in affected parts.\nALTER TABLE xxxx UPDATE column_name = column_name WHERE 1; Will trigger mutation, which will materialize this column.\nDROP (COLUMN/INDEX/PROJECTION) Lightweight, it’s only about changing of table metadata and removing corresponding files from filesystem. For Compact parts it will trigger merge, which can be heavy. issue\nMODIFY COLUMN (DATE TYPE) Change column type in table schema. Schedule mutation to change type for old parts. Mutations Affected parts - parts with rows mathing condition.\nALTER TABLE xxxxx DELETE WHERE column_1 = 1; Will overwrite all column data in affected parts. For all part(ition)s will create new directories on disk and write new data to them or create hardlinks if they untouched. Register new parts names in ZooKeeper. ALTER TABLE xxxxx DELETE IN PARTITION ID ’’ WHERE column_1 = 1; Will do the same but only for specific partition.\nALTER TABLE xxxxx UPDATE SET column_2 = column_2, column_3 = column_3 WHERE column_1 = 1; Will overwrite column_2, column_3 data in affected parts. For all part(ition)s will create new directories on disk and write new data to them or create hardlinks if they untouched. Register new parts names in ZooKeeper. DELETE FROM xxxxx WHERE column_1 = 1; Will create \u0026 populate hidden boolean column in affected parts. (_row_exists column) For all part(ition)s will create new directories on disk and write new data to them or create hardlinks if they untouched. Register new parts names in ZooKeeper. Despite that LWD mutations will not rewrite all columns, steps 2 \u0026 3 in case of big tables can take significiant time.\n","categories":"","description":"","excerpt":"How ALTER’s works in ClickHouse: ADD (COLUMN/INDEX/PROJECTION) …","ref":"/altinity-kb-setup-and-maintenance/alters/","tags":"","title":"How ALTER's works in ClickHouse"},{"body":"How much is too much? In most of the cases clickhouse don’t have any hard limits. But obsiously there there are some practical limitation / barriers for different things - often they are caused by some system / network / filesystem limitation.\nSo after reaching some limits you can get different kind of problems, usually it never a failures / errors, but different kinds of degradations (slower queries / high cpu/memory usage, extra load on the network / zookeeper etc).\nWhile those numbers can vary a lot depending on your hardware \u0026 settings there is some safe ‘Goldilocks’ zone where ClickHouse work the best with default settings \u0026 usual hardware.\nNumber of tables (system-wide, across all databases) non-replicated MergeTree-family tables = few thousands is still acceptable, if you don’t do realtime inserts in more that few dozens of them. See #32259 ReplicatedXXXMergeTree = few hundreds is still acceptable, if you don’t do realtime inserts in more that few dozens of them. Every Replicated table comes with it’s own cost (need to do housekeepeing operations, monitoing replication queues etc). See #31919 Log family table = even dozens of thousands is still ok, especially if database engine = Lazy is used. Number of databases Fewer than number of tables (above). Dozens / hundreds is usually still acceptable.\nNumber of inserts per seconds For usual (non async) inserts - dozen is enough. Every insert creates a part, if you will create parts too often, clickhouse will not be able to merge them and you will be getting ’too many parts’.\nNumber of columns in the table Up to a few hundreds. With thousands of columns the inserts / background merges may become slower / require more of RAM. See for example https://github.com/ClickHouse/ClickHouse/issues/6943 https://github.com/ClickHouse/ClickHouse/issues/27502\nClickHouse instances on a single node / VM One is enough. Single ClickHouse can use resources of the node very efficiently, and it may require some complicated tuning to run several instances on a single node.\nNumber of parts / partitions (system-wide, across all databases) More than several dozens thousands may lead to performance degradation: slow starts (see https://github.com/ClickHouse/ClickHouse/issues/10087 ).\nNumber of tables \u0026 partitions touched by a single insert If you have realtime / frequent inserts no more than few.\nFor the inserts are rare - up to couple of dozens.\nNumber of parts in the single table More than ~ 5 thousands may lead to issues with alters in Replicated tables (caused by jute.maxbuffer overrun, see details ), and query speed degradation.\nDisk size per shard Less than 10TB of compressed data per server. Bigger disk are harder to replace / resync.\nNumber of shards Dozens is still ok. More may require having more complex (non-flat) routing.\nNumber of replicas in a single shard 2 is minimum for HA. 3 is a ‘golden standard’. Up to 6-8 is still ok. If you have more with realtime inserts - it can impact the zookeeper traffic.\nNumber of zookeeper nodes in the ensemble 3 (Three) for most of the cases is enough (you can loose one node). Using more nodes allows to scale up read throughput for zookeeper, but don’t improve writes at all.\nNumber of materialized view attached to a single table. Up to few. The less the better if the table is getting realtime inserts. (no matter if MV are chained or all are feeded from the same source table).\nThe more you have the more costy your inserts are, and the bigger risks to get some inconsitencies between some MV (inserts to MV and main table are not atomic).\nIf the table don’t have realtime inserts you can have more MV.\nNumber of projections inside a single table. Up to few. Similar to MV above.\nNumber of secondary indexes a single table. One to about a dozen. Different types of indexes has different penalty, bloom_filter is 100 times heavier than min_max index At some point your inserts will slow down. Try to create possible minimum of indexes. You can combine many columns into a single index and this index will work for any predicate but create less impact.\nNumber of Kafka tables / consumers inside High number of Kafka tables maybe quite expensive (every consumer = very expensive librdkafka object with several threads inside). Usually alternative approaches are preferrable (mixing several datastreams in one topic, denormalizing, consuming several topics of identical structure with a single Kafka table, etc).\nIf you really need a lot of Kafka tables you may need more ram / CPU on the node and increase background_message_broker_schedule_pool_size (default is 16) to the number of Kafka tables.\n","categories":"","description":"ClickHouse Limitations.","excerpt":"ClickHouse Limitations.","ref":"/altinity-kb-schema-design/how-much-is-too-much/","tags":"","title":"How much is too much?"},{"body":"Create a new table and copy data through an intermediate table. Step by step procedure. We want to add column3 to the ORDER BY in this table:\nCREATE TABLE example_table ( date Date, column1 String, column2 String, column3 String, column4 String ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/example_table', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY (column1, column2) Stop publishing/INSERT into example_table.\nRename table example_table to example_table_old\nCreate the new table with the old name. This will preserve all dependencies like materialized views.\nCREATE TABLE example_table as example_table_old ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/example_table_new', '{replica}') PARTITION BY toYYYYMM(date) ORDER BY (column1, column2, column3) Copy data from example_table_old into example_table_temp\na. Use this query to generate a list of INSERT statements\nselect concat('insert into example_table_temp select * from example_table_old where toYYYYMM(date)=',partition) as cmd, database, table, partition, sum(rows), sum(bytes_on_disk), count() from system.parts where database='default' and table='example_table_old' group by database, table, partition order by partition b. Create an intermediate table\nCREATE TABLE example_table_temp as example_table_old ENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY (column1, column2, column3) c. Run the queries one by one\nAfter each query compare the number of rows in both tables. If the INSERT statement was interrupted and failed to copy data, drop the partition in example_table and repeat the INSERT. If a partition was copied successfully, proceed to the next partition.\nHere is a query to compare the tables:\nselect database, table, partition, sum(rows), sum(bytes_on_disk), count() from system.parts where database='default' and table like 'example_table%' group by database, table, partition order by partition Attach data from the intermediate table to example_table\na. Use this query to generate a list of ATTACH statements\nselect concat('alter table example_table attach partition id ''',partition,''' from example_table_temp') as cmd, database, table, partition, sum(rows), sum(bytes_on_disk), count() from system.parts where database='default' and table='example_table_temp' group by database, table, partition order by partition b. Run the queries one by one\nHere is a query to compare the tables:\nselect hostName(), database, table, partition, sum(rows), sum(bytes_on_disk), count() from clusterAllReplicas('my-cluster',system.parts) where database='default' and table like 'example_table%' group by hostName(), database, table, partition order by partition Drop example_table_old and example_table_temp\n","categories":"","description":"How to change ORDER BY.","excerpt":"How to change ORDER BY.","ref":"/altinity-kb-schema-design/change-order-by/","tags":"","title":"How to change ORDER BY"},{"body":"New, official way Implemented automatic conversion of database engine from Ordinary to Atomic. Create empty convert_ordinary_to_atomic file in flags directory and all Ordinary databases will be converted automatically on next server start. Resolves #39546. #39933 (Alexander Tokmakov) Example How to Convert Ordinary to Atomic CREATE DATABASE db ENGINE=Ordinary; CREATE TABLE db.test(A Int64) ENGINE=MergeTree ORDER BY A; CREATE MATERIALIZED VIEW db.test_mv(A Int64) ENGINE=MergeTree ORDER BY A AS SELECT * FROM db.test; INSERT INTO db.test SELECT * FROM numbers(1000); CREATE DATABASE db_temp ENGINE=Atomic; RENAME TABLE db.test TO db_temp.test; RENAME TABLE db.test_mv TO db_temp.test_mv; DROP DATABASE db; RENAME DATABASE db_temp TO db; USE db; SHOW TABLES; ┌─name───────────────────────────────────────────┐ │ .inner_id.37db402c-fc46-421d-b7db-402cfc46921d │ │ test │ │ test_mv │ └────────────────────────────────────────────────┘ INSERT INTO db.test SELECT * FROM numbers(1000); SELECT count() FROM test; ┌─count()─┐ │ 2000 │ └─────────┘ SELECT count() FROM test_mv; ┌─count()─┐ │ 2000 │ └─────────┘ SHOW CREATE DATABASE db; ┌─statement─────────────────────────┐ │ CREATE DATABASE db ENGINE = Atomic │ └───────────────────────────────────┘ ","categories":"","description":"Clickhouse Howto Convert Ordinary to Atomic","excerpt":"Clickhouse Howto Convert Ordinary to Atomic","ref":"/engines/altinity-kb-atomic-database-engine/how-to-convert-ordinary-to-atomic/","tags":"","title":"How to Convert Ordinary to Atomic"},{"body":"How to fix a replication using hard-reset way Find the best replica (replica with the most fresh/consistent) data. Backup the table alter table mydatabase.mybadtable freeze; Stop all applications!!! Stop ingestion. Stop queries - table will be empty for some time. Check that detached folder is empty or clean it. SELECT concat('alter table ', database, '.', table, ' drop detached part \\'', name, '\\' settings allow_drop_detached=1;') FROM system.detached_parts WHERE (database = 'mydatabase') AND (table = 'mybadtable') FORMAT TSVRaw; Make sure that detached folder is empty select count() from system.detached_parts where database='mydatabase' and table ='mybadtable'; Detach all parts (table will became empty) SELECT concat('alter table ', database, '.', table, ' detach partition id \\'', partition_id, '\\';') AS detach FROM system.parts WHERE (active = 1) AND (database = 'mydatabase') AND (table = 'mybadtable') GROUP BY detach ORDER BY detach ASC FORMAT TSVRaw; Make sure that table is empty select count() from mydatabase.mybadtable; Attach all parts back SELECT concat('alter table ', database, '.', table, ' attach part \\'', a.name, '\\';') FROM system.detached_parts AS a WHERE (database = 'mydatabase') AND (table = 'mybadtable') FORMAT TSVRaw; Make sure that data is consistent at all replicas SELECT formatReadableSize(sum(bytes)) AS size, sum(rows), count() AS part_count, uniqExact(partition) AS partition_count FROM system.parts WHERE (active = 1) AND (database = 'mydatabase') AND (table = 'mybadtable'); ","categories":"","description":"How to recreate a table in case of total corruption of the replication queue.","excerpt":"How to recreate a table in case of total corruption of the replication …","ref":"/altinity-kb-setup-and-maintenance/how_to_recreate_table/","tags":"","title":"How to recreate a table in case of total corruption of the replication queue"},{"body":"http handler example (how to disable /play) # cat /etc/clickhouse-server/config.d/play_disable.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003chttp_handlers\u003e \u003crule\u003e \u003curl\u003e/play\u003c/url\u003e \u003cmethods\u003eGET\u003c/methods\u003e \u003chandler\u003e \u003ctype\u003estatic\u003c/type\u003e \u003cstatus\u003e403\u003c/status\u003e \u003ccontent_type\u003etext/plain; charset=UTF-8\u003c/content_type\u003e \u003cresponse_content\u003e\u003c/response_content\u003e \u003c/handler\u003e \u003c/rule\u003e \u003cdefaults/\u003e \u003c!-- handler to save default handlers ?query / ping --\u003e \u003c/http_handlers\u003e \u003c/yandex\u003e ","categories":"","description":"http handler example","excerpt":"http handler example","ref":"/altinity-kb-setup-and-maintenance/http_handlers/","tags":"","title":"http handler example"},{"body":"Why inserts into materialized views are not idempotent? ClickHouse still does not have transactions. They will be implemented around 2022Q2.\nBecause of Clickhouse materialized view is a trigger. And an insert into a table and an insert into a subordinate materialized view it’s two different inserts so they are not atomic alltogether.\nAnd insert into a materialized view may fail after the succesful insert into the table. In case of any failure a client gets the error about failed insertion. You may enable insert_deduplication (it’s enabled by default for Replciated engines) and repeate the insert with an idea to achive idempotate insertion, and insertion will be skipped into the source table becase of deduplication but it will be skipped for materialized view as well because by default materialized view inherites deduplication from the source table. It’s controlled by a parameter deduplicate_blocks_in_dependent_materialized_views https://clickhouse.com/docs/en/operations/settings/settings/#settings-deduplicate-blocks-in-dependent-materialized-views\nIf your materialized view is wide enought and always have enought data for constistent deduplication then you can enable deduplicate_blocks_in_dependent_materialized_views. Or you may add information for deduplication (some unique information / insert identifier).\nExample 1. Inconsistency with deduplicate_blocks_in_dependent_materialized_views 0 create table test (A Int64, D Date) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by toYYYYMM(D) order by A; create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT from test group by D; set max_partitions_per_insert_block=1; -- trick to fail insert into MV. insert into test select number, today()+number%3 from numbers(100); DB::Exception: Received from localhost:9000. DB::Exception: Too many partitions select count() from test; ┌─count()─┐ │ 100 │ -- Insert was successful into the test table └─────────┘ select sum(CNT) from test_mv; 0 rows in set. Elapsed: 0.001 sec. -- Insert was unsuccessful into the test_mv table (DB::Exception) -- Let's try to retry insertion set max_partitions_per_insert_block=100; -- disable trick insert into test select number, today()+number%3 from numbers(100); -- insert retry / No error select count() from test; ┌─count()─┐ │ 100 │ -- insert was deduplicated └─────────┘ select sum(CNT) from test_mv; 0 rows in set. Elapsed: 0.001 sec. -- Inconsistency! Unfortunatly insert into MV was deduplicated as well Example 2. Inconsistency with deduplicate_blocks_in_dependent_materialized_views 1 create table test (A Int64, D Date) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by toYYYYMM(D) order by A; create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT from test group by D; set deduplicate_blocks_in_dependent_materialized_views=1; insert into test select number, today() from numbers(100); -- insert 100 rows insert into test select number, today() from numbers(100,100); -- insert another 100 rows select count() from test; ┌─count()─┐ │ 200 │ -- 200 rows in the source test table └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 100 │ -- Inconsistency! The second insert was falsely deduplicated because count() was = 100 both times └──────────┘ Example 3. Solution: no inconsistency with deduplicate_blocks_in_dependent_materialized_views 1 Let’s add some artificial insert_id generated by the source of inserts:\ncreate table test (A Int64, D Date, insert_id Int64) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by toYYYYMM(D) order by A; create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT, any(insert_id) insert_id from test group by D; set deduplicate_blocks_in_dependent_materialized_views=1; insert into test select number, today(), 333 from numbers(100); insert into test select number, today(), 444 from numbers(100,100); select count() from test; ┌─count()─┐ │ 200 │ └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 200 │ -- no inconsistency, the second (100) was not deduplicated because 333\u003c\u003e444 └──────────┘ set max_partitions_per_insert_block=1; -- trick to fail insert into MV. insert into test select number, today()+number%3, 555 from numbers(100); DB::Exception: Too many partitions for single INSERT block (more than 1) select count() from test; ┌─count()─┐ │ 300 │ -- insert is successful into the test table └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 200 │ -- insert was unsuccessful into the test_mv table └──────────┘ set max_partitions_per_insert_block=100; insert into test select number, today()+number%3, 555 from numbers(100); -- insert retry select count() from test; ┌─count()─┐ │ 300 │ -- insert was deduplicated └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 300 │ -- No inconsistency! Insert was not deduplicated. └──────────┘ Idea how to fix it in Clickhouse source code https://github.com/ClickHouse/ClickHouse/issues/30240\nFake (unused) metric to add uniqueness. create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT, sum( cityHash(*) ) insert_id from test group by D; ","categories":"","description":"How to make idempotent inserts into a materialized view\".","excerpt":"How to make idempotent inserts into a materialized view\".","ref":"/altinity-kb-schema-design/materialized-views/idempotent_inserts_mv/","tags":"","title":"Idempotent inserts into a materialized view"},{"body":"Decimal SELECT 9.2::Decimal64(2) AS postgresql_cast, toDecimal64(9.2, 2) AS to_function, CAST(9.2, 'Decimal64(2)') AS cast_float_literal, CAST('9.2', 'Decimal64(2)') AS cast_string_literal ┌─postgresql_cast─┬─to_function─┬─cast_float_literal─┬─cast_string_literal─┐ │ 9.2 │ 9.19 │ 9.19 │ 9.2 │ └─────────────────┴─────────────┴────────────────────┴─────────────────────┘ When we try to type cast 64.32 to Decimal128(2) the resulted value is 64.31.\nWhen it sees a number with a decimal separator it interprets as Float64 literal (where 64.32 have no accurate representation, and actually you get something like 64.319999999999999999) and later that Float is casted to Decimal by removing the extra precision.\nWorkaround is very simple - wrap the number in quotes (and it will be considered as a string literal by query parser, and will be transformed to Decimal directly), or use postgres-alike casting syntax:\nselect cast(64.32,'Decimal128(2)') a, cast('64.32','Decimal128(2)') b, 64.32::Decimal128(2) c; ┌─────a─┬─────b─┬─────c─┐ │ 64.31 │ 64.32 │ 64.32 │ └───────┴───────┴───────┘ Float64 SELECT toFloat64(15008753.) AS to_func, toFloat64('1.5008753E7') AS to_func_scientific, CAST('1.5008753E7', 'Float64') AS cast_scientific ┌──to_func─┬─to_func_scientific─┬────cast_scientific─┐ │ 15008753 │ 15008753.000000002 │ 15008753.000000002 │ └──────────┴────────────────────┴────────────────────┘ ","categories":"","description":"Imprecise parsing of literal Decimal or Float64","excerpt":"Imprecise parsing of literal Decimal or Float64","ref":"/altinity-kb-queries-and-syntax/literal-decimal-or-float/","tags":"","title":"Imprecise parsing of literal Decimal or Float64"},{"body":"-- Insert rate select database, table, time_bucket, max(number_of_parts_per_insert) max_parts_pi, median(number_of_parts_per_insert) median_parts_pi, min(min_rows_per_part) min_rows_pp, max(max_rows_per_part) max_rows_pp, median(median_rows_per_part) median_rows_pp, min(rows_per_insert) min_rows_pi, median(rows_per_insert) median_rows_pi, max(rows_per_insert) max_rows_pi, sum(rows_per_insert) rows_inserted, sum(seconds_per_insert) parts_creation_seconds, count() inserts, sum(number_of_parts_per_insert) new_parts, max(last_part_pi) - min(first_part_pi) as insert_period, inserts*60/insert_period as inserts_per_minute from (SELECT database, table, toStartOfDay(event_time) AS time_bucket, count() AS number_of_parts_per_insert, min(rows) AS min_rows_per_part, max(rows) AS max_rows_per_part, median(rows) AS median_rows_per_part, sum(rows) AS rows_per_insert, min(size_in_bytes) AS min_bytes_per_part, max(size_in_bytes) AS max_bytes_per_part, median(size_in_bytes) AS median_bytes_per_part, sum(size_in_bytes) AS bytes_per_insert, median_bytes_per_part / median_rows_per_part AS avg_row_size, sum(duration_ms)/1000 as seconds_per_insert, max(event_time) as last_part_pi, min(event_time) as first_part_pi FROM system.part_log WHERE -- Enum8('NewPart' = 1, 'MergeParts' = 2, 'DownloadPart' = 3, 'RemovePart' = 4, 'MutatePart' = 5, 'MovePart' = 6) event_type = 1 AND -- change if another time period is desired event_date \u003e= today() GROUP BY query_id, database, table, time_bucket ) GROUP BY database, table, time_bucket ORDER BY time_bucket, database, table ASC -- New parts per partition select database, table, event_type, partition_id, count() c, round(avg(rows)) from system.part_log where event_date \u003e= today() and event_type = 'NewPart' group by database, table, event_type, partition_id order by c desc ","categories":"","description":"Query to gather information about ingestion rate from system.part_log. ","excerpt":"Query to gather information about ingestion rate from system.part_log. …","ref":"/altinity-kb-useful-queries/ingestion-rate-part_log/","tags":"","title":"Ingestion metrics from system.part_log"},{"body":"How to insert AggregateFunction data Ephemeral column CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name_stub String Ephemeral, name AggregateFunction(argMax, String, DateTime) default arrayReduce('argMaxState', [name_stub], [updated]) ) ENGINE=AggregatingMergeTree order by uid; INSERT INTO users(uid, updated, name_stub) VALUES (1231, '2020-01-02 00:00:00', 'Jane'); INSERT INTO users(uid, updated, name_stub) VALUES (1231, '2020-01-01 00:00:00', 'John'); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘ Input function CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name AggregateFunction(argMax, String, DateTime) ) ENGINE=AggregatingMergeTree order by uid; INSERT INTO users SELECT uid, updated, arrayReduce('argMaxState', [name], [updated]) FROM input('uid Int16, updated DateTime, name String') FORMAT Values (1231, '2020-01-02 00:00:00', 'Jane'); INSERT INTO users SELECT uid, updated, arrayReduce('argMaxState', [name], [updated]) FROM input('uid Int16, updated DateTime, name String') FORMAT Values (1231, '2020-01-01 00:00:00', 'John'); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid; ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘ Materialized View And Null Engine CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name AggregateFunction(argMax, String, DateTime) ) ENGINE=AggregatingMergeTree order by uid; CREATE TABLE users_null ( uid Int16, updated DateTime, name String ) ENGINE=Null; CREATE MATERIALIZED VIEW users_mv TO users AS SELECT uid, updated, arrayReduce('argMaxState', [name], [updated]) name FROM users_null; INSERT INTO users_null Values (1231, '2020-01-02 00:00:00', 'Jane'); INSERT INTO users_null Values (1231, '2020-01-01 00:00:00', 'John'); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid; ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘ ","categories":"","description":"ClickHouse. How to insert AggregateFunction data.","excerpt":"ClickHouse. How to insert AggregateFunction data.","ref":"/altinity-kb-schema-design/ingestion-aggregate-function/","tags":"","title":"Ingestion of AggregateFunction"},{"body":"Insert Deduplication Replicated tables have a special feature insert deduplication (enabled by default).\nDocumentation: Data blocks are deduplicated. For multiple writes of the same data block (data blocks of the same size containing the same rows in the same order), the block is only written once. The reason for this is in case of network failures when the client application does not know if the data was written to the DB, so the INSERT query can simply be repeated. It does not matter which replica INSERTs were sent to with identical data. INSERTs are idempotent. Deduplication parameters are controlled by merge_tree server settings.\nExample create table test_insert ( A Int64 ) Engine=ReplicatedMergeTree('/clickhouse/cluster_test/tables/{table}','{replica}') order by A; insert into test_insert values(1); insert into test_insert values(1); insert into test_insert values(1); insert into test_insert values(1); select * from test_insert; ┌─A─┐ │ 1 │ -- only one row has been inserted, the other rows were deduplicated └───┘ alter table test_insert delete where 1; -- that single row was removed insert into test_insert values(1); select * from test_insert; 0 rows in set. Elapsed: 0.001 sec. -- the last insert was deduplicated again, -- because `alter ... delete` does not clear deduplication checksums -- only `alter table drop partition` and `truncate` clear checksums In clickhouse-server.log you may see trace messages Block with ID ... already exists locally as part ... ignoring it\n# cat /var/log/clickhouse-server/clickhouse-server.log|grep test_insert|grep Block ..17:52:45.064974.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.068979.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.072883.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.076738.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. Deduplication checksums are stored in Zookeeper in /blocks table’s znode for each partition separately, so when you drop partition, they could be identified and removed for this partition. (during alter table delete it’s impossible to match checksums, that’s why checksums stay in Zookeeper).\nSELECT name, value FROM system.zookeeper WHERE path = '/clickhouse/cluster_test/tables/test_insert/blocks' ┌─name───────────────────────────────────────┬─value─────┐ │ all_7615936253566048997_747463735222236827 │ all_0_0_0 │ └────────────────────────────────────────────┴───────────┘ insert_deduplicate setting Insert deduplication is controled by the insert_deduplicate setting\nLet’s disable it:\nset insert_deduplicate = 0; -- insert_deduplicate is now disabled in this session insert into test_insert values(1); insert into test_insert values(1); insert into test_insert values(1); select * from test_insert format PrettyCompactMonoBlock; ┌─A─┐ │ 1 │ │ 1 │ │ 1 │ -- all 3 insterted rows are in the table └───┘ alter table test_insert delete where 1; insert into test_insert values(1); insert into test_insert values(1); select * from test_insert format PrettyCompactMonoBlock; ┌─A─┐ │ 1 │ │ 1 │ └───┘ Insert deduplication is a user-level setting, it can be disabled in a session or in a user’s profile (insert_deduplicate=0).\nclickhouse-client --insert_deduplicate=0 ....\nHow to disable insert_deduplicate by default for all queries:\n# cat /etc/clickhouse-server/users.d/insert_deduplicate.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cinsert_deduplicate\u003e0\u003c/insert_deduplicate\u003e \u003c/default\u003e \u003c/profile \u003c/yandex\u003e Other related settings: replicated_deduplication_window, replicated_deduplication_window_seconds, insert_deduplication_token.\nMore info: https://github.com/ClickHouse/ClickHouse/issues/16037 https://github.com/ClickHouse/ClickHouse/issues/3322\nNon-replicated MergeTree tables By default insert deduplication is disabled for non-replicated tables (for backward compatibility).\nIt can be enabled by the merge_tree setting non_replicated_deduplication_window.\nExample:\ncreate table test_insert ( A Int64 ) Engine=MergeTree order by A settings non_replicated_deduplication_window = 100; -- 100 - how many latest checksums to store insert into test_insert values(1); insert into test_insert values(1); insert into test_insert values(1); insert into test_insert values(2); insert into test_insert values(2); select * from test_insert format PrettyCompactMonoBlock; ┌─A─┐ │ 2 │ │ 1 │ └───┘ In case of non-replicated tables deduplication checksums are stored in files in the table’s folder:\ncat /var/lib/clickhouse/data/default/test_insert/deduplication_logs/deduplication_log_1.txt 1\tall_1_1_0\tall_7615936253566048997_747463735222236827 1\tall_4_4_0\tall_636943575226146954_4277555262323907666 Checksums calculation Checksums are calculated not from the inserted data but from formed parts.\nInsert data is separated to parts by table’s partitioning.\nParts contain rows sorted by the table’s order by and all values of functions (i.e. now()) or Default/Materialized columns are expanded.\nExample with partial insertion because of partitioning: create table test_insert ( A Int64, B Int64 ) Engine=MergeTree partition by B order by A settings non_replicated_deduplication_window = 100; insert into test_insert values (1,1); insert into test_insert values (1,1)(1,2); select * from test_insert format PrettyCompactMonoBlock; ┌─A─┬─B─┐ │ 1 │ 1 │ │ 1 │ 2 │ -- the second insert was skipped for only one partition!!! └───┴───┘ Example with deduplication despite the rows order: drop table test_insert; create table test_insert ( A Int64, B Int64 ) Engine=MergeTree order by (A, B) settings non_replicated_deduplication_window = 100; insert into test_insert values (1,1)(1,2); insert into test_insert values (1,2)(1,1); -- the order of rows is not equal with the first insert select * from test_insert format PrettyCompactMonoBlock; ┌─A─┬─B─┐ │ 1 │ 1 │ │ 1 │ 2 │ └───┴───┘ 2 rows in set. Elapsed: 0.001 sec. -- the second insert was skipped despite the rows order Example to demonstrate how Default/Materialize columns are expanded: drop table test_insert; create table test_insert ( A Int64, B Int64 Default rand() ) Engine=MergeTree order by A settings non_replicated_deduplication_window = 100; insert into test_insert(A) values (1); -- B calculated as rand() insert into test_insert(A) values (1); -- B calculated as rand() select * from test_insert format PrettyCompactMonoBlock; ┌─A─┬──────────B─┐ │ 1 │ 3467561058 │ │ 1 │ 3981927391 │ └───┴────────────┘ insert into test_insert(A, B) values (1, 3467561058); -- B is not calculated / will be deduplicated select * from test_insert format PrettyCompactMonoBlock; ┌─A─┬──────────B─┐ │ 1 │ 3981927391 │ │ 1 │ 3467561058 │ └───┴────────────┘ Example to demonstrate how functions are expanded: drop table test_insert; create table test_insert ( A Int64, B DateTime64 ) Engine=MergeTree order by A settings non_replicated_deduplication_window = 100; insert into test_insert values (1, now64()); .... insert into test_insert values (1, now64()); select * from test_insert format PrettyCompactMonoBlock; ┌─A─┬───────────────────────B─┐ │ 1 │ 2022-01-31 15:43:45.364 │ │ 1 │ 2022-01-31 15:43:41.944 │ └───┴─────────────────────────┘ insert_deduplication_token Since Clikhouse 22.2 there is a new setting insert_dedupplication_token. This setting allows you to define an explicit token that will be used for deduplication instead of calculating a checksum from the inserted data.\nCREATE TABLE test_table ( A Int64 ) ENGINE = MergeTree ORDER BY A SETTINGS non_replicated_deduplication_window = 100; INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1); -- the next insert won't be deduplicated because insert_deduplication_token is different INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1); -- the next insert will be deduplicated because insert_deduplication_token -- is the same as one of the previous INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2); SELECT * FROM test_table ┌─A─┐ │ 1 │ └───┘ ┌─A─┐ │ 1 │ └───┘ ","categories":"","description":"Insert Deduplication / Insert idempotency , insert_deduplicate setting.","excerpt":"Insert Deduplication / Insert idempotency , insert_deduplicate …","ref":"/altinity-kb-schema-design/insert_deduplication/","tags":"","title":"Insert Deduplication / Insert idempotency"},{"body":"Replication using MaterializeMySQL. https://clickhouse.tech/docs/en/engines/database-engines/materialized-mysql/ https://translate.google.com/translate?sl=auto\u0026tl=en\u0026u=https://www.jianshu.com/p/d0d4306411b3 https://raw.githubusercontent.com/ClickHouse/clickhouse-presentations/master/meetup47/materialize_mysql.pdf It reads mysql binlog directly and transform queries into something which clickhouse can support. Supports updates and deletes (under the hood implemented via something like ReplacingMergeTree with enforced FINAL and ‘deleted’ flag). Status is ’experimental’, there are quite a lot of known limitations and issues, but some people use it. The original author of that went to another project, and the main team don’t have a lot of resource to improve that for now (more important thing in the backlog)\nThe replication happens on the mysql database level.\nReplication using debezium + Kafka (+ Altinity Sink Connector) Debezium can read the binlog and transform it to Kafka messages.\nYou can later capture the stream of message on ClickHouse side and process it as you like. Please remember that currently Kafka engine supports only at-least-once delivery guarantees. It’s used by several companies, quite nice \u0026 flexible. But initial setup may require some efforts.\nAltinity Sink Connector Can handle transformation of debezium messages (with support for DELETEs and UPDATEs) and exactly-once delivery for you.\nLinks:\nhttps://altinity.com/blog/fast-mysql-to-clickhouse-replication-announcing-the-altinity-sink-connector-for-clickhouse https://altinity.com/mysql-to-clickhouse/ https://github.com/Altinity/clickhouse-sink-connector Same as above but using https://maxwells-daemon.io/ instead of debezium. Have no experience / feedback there, but should be very similar to debezium.\nReplication using clickhouse-mysql See https://altinity.com/blog/2018/6/30/realtime-mysql-clickhouse-replication-in-practice\nThat was done long time ago in altinity for one use-case, and it seem like it was never used outside of that. It’s a python application with lot of switches which can copy a schema or read binlog from mysql and put it to clickhouse. Not supported currently. But it’s just a python, so maybe can be adjusted to different needs.\nAccessing MySQL data via integration engines from inside clickhouse. MySQL table engine / table function, or MySQL database engine - clickhouse just connects to mysql server as a client, and can do normal selects.\nWe had webinar about that a year ago: https://www.youtube.com/watch?v=44kO3UzIDLI\nUsing that you can easily create some ETL script which will copy the data from mysql to clickhouse regularly, i.e. something like\nINSERT INTO clickhouse_table SELECT * FROM mysql_table WHERE id \u003e ... Works great if you have append only table in MySQL.\nIn newer clickhouse versions you can query this was also sharded / replicated MySQL cluster - see ExternalDistributed\nMySQL dictionaries There are also MySQL dictionaries, which can be very nice alternative for storing some dimensions information in star schema.\nhttps://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources/#dicts-external_dicts_dict_sources-mysql https://github.com/ClickHouse/ClickHouse/blob/9f5cd35a6963cc556a51218b46b0754dcac7306a/tests/testflows/aes_encryption/tests/compatibility/mysql/dictionary.py#L35-L51 ","categories":"","description":"Integration Clickhouse with MySQL","excerpt":"Integration Clickhouse with MySQL","ref":"/altinity-kb-integrations/mysql-clickhouse/","tags":"","title":"MySQL"},{"body":"Using JSONEachRow with Tuple() in Materialized views Sometimes we can have a nested json message with a fixed size structure like this:\n{\"s\": \"val1\", \"t\": {\"i\": 42, \"d\": \"2023-09-01 12:23:34.231\"}} Values can be NULL but the structure should be fixed. In this case we can use Tuple() to parse the JSON message:\nCREATE TABLE tests.nest_tuple_source ( `s` String, `t` Tuple(`i` UInt8, `d` DateTime64(3)) ) ENGINE = Null We can use the above table as a source for a materialized view, like it was a Kafka table and in case our message has unexpected keys we make the Kafka table ignore them with the setting (23.3+):\ninput_format_json_ignore_unknown_keys_in_named_tuple = 1\nCREATE MATERIALIZED VIEW tests.mv_nest_tuple TO tests.nest_tuple_destination AS SELECT s AS s, t.1 AS i, t.2 AS d FROM tests.nest_tuple_source Also, we need a destination table with an adapted structure as the source table:\nCREATE TABLE tests.nest_tuple_destination ( `s` String, `i` UInt8, `d` DateTime64(3) ) ENGINE = MergeTree ORDER BY tuple() INSERT INTO tests.nest_tuple_source FORMAT JSONEachRow {\"s\": \"val1\", \"t\": {\"i\": 42, \"d\": \"2023-09-01 12:23:34.231\"}} SELECT * FROM nest_tuple_destination ┌─s────┬──i─┬───────────────────────d─┐ │ val1 │ 42 │ 2023-09-01 12:23:34.231 │ └──────┴────┴─────────────────────────┘ Some hints:\n💡 Beware of column names in ClickHouse they are Case sensitive. If a JSON message has the key names in Capitals, the Kafka/Source table should have the same column names in Capitals.\n💡 Also this Tuple() approach is not for Dynamic json schemas as explained above. In the case of having a dynamic schema, use the classic approach using JSONExtract set of functions. If the schema is fixed, you can use Tuple() for JSONEachRow format but you need to use classic tuple notation (using index reference) inside the MV, because using named tuples inside the MV won’t work:\n💡 tuple.1 AS column1, tuple.2 AS column2 CORRECT!\n💡 tuple.column1 AS column1, tuple.column2 AS column2 WRONG!\n💡 use AS (alias) for aggregated columns or columns affected by functions because MV do not work by positional arguments like SELECTs,they work by names**\nExample:\nparseDateTime32BestEffort(t_date) WRONG! parseDateTime32BestEffort(t_date) AS t_date CORRECT! Using JSONEachRow with Map() in Materialized views Sometimes we can have a nested json message with a dynamic size like these and all elements inside the nested json must be of the same type:\n{\"k\": \"val1\", \"st\": {\"a\": 42, \"b\": 1.877363}} {\"k\": \"val2\", \"st\": {\"a\": 43, \"b\": 2.3343, \"c\": 34.4434}} {\"k\": \"val3\", \"st\": {\"a\": 66743}} In this case we can use Map() to parse the JSON message:\nCREATE TABLE tests.nest_map_source ( `k` String, `st` Map(String, Float64) ) Engine = Null CREATE MATERIALIZED VIEW tests.mv_nest_map TO tests.nest_map_destination AS SELECT k AS k, st['a'] AS st_a, st['b'] AS st_b, st['c'] AS st_c FROM tests.nest_map_source CREATE TABLE tests.nest_map_destination ( `k` String, `st_a` Float64, `st_b` Float64, `st_c` Float64 ) ENGINE = MergeTree ORDER BY tuple() By default, ClickHouse will ignore unknown keys in the Map() but if you want to fail the insert if there are unknown keys then use the setting:\ninput_format_skip_unknown_fields = 0\nINSERT INTO tests.nest_map_source FORMAT JSONEachRow {\"k\": \"val1\", \"st\": {\"a\": 42, \"b\": 1.877363}} INSERT INTO tests.nest_map_source FORMAT JSONEachRow {\"k\": \"val2\", \"st\": {\"a\": 43, \"b\": 2.3343, \"c\": 34.4434}} INSERT INTO tests.nest_map_source FORMAT JSONEachRow {\"k\": \"val3\", \"st\": {\"a\": 66743}} SELECT * FROM tests.nest_map_destination ┌─k────┬─st_a─┬─────st_b─┬─st_c─┐ │ val1 │ 42 │ 1.877363 │ 0 │ └──────┴──────┴──────────┴──────┘ ┌─k────┬──st_a─┬─st_b─┬─st_c─┐ │ val3 │ 66743 │ 0 │ 0 │ └──────┴───────┴──────┴──────┘ ┌─k────┬─st_a─┬───st_b─┬────st_c─┐ │ val2 │ 43 │ 2.3343 │ 34.4434 │ └──────┴──────┴────────┴─────────┘ See also:\nJSONExtract to parse many attributes at a time JSONAsString and Mat. View as JSON parser ","categories":"","description":"How to use Tuple() and Map() with nested JSON messages in MVs","excerpt":"How to use Tuple() and Map() with nested JSON messages in MVs","ref":"/altinity-kb-schema-design/altinity-kb-jsoneachrow-tuples-and-mvs/","tags":"","title":"JSONEachRow, Tuples, Maps and Materialized Views"},{"body":"from scipy.stats import skew, kurtosis # Creating a dataset dataset = [10,17,71,6,55,38,27,61,48,46,21,38,2,67,35,77,29,31,27,67,81,82,75,81,31,38,68,95,37,34,65,59,81,28,82,80,35,3,97,42,66,28,85,98,45,15,41,61,24,53,97,86,5,65,84,18,9,32,46,52,69,44,78,98,61,64,26,11,3,19,0,90,28,72,47,8,0,74,38,63,88,43,81,61,34,24,37,53,79,72,5,77,58,3,61,56,1,3,5,61] print(skew(dataset, axis=0, bias=True), skew(dataset)) # -0.05785361619432152 -0.05785361619432152 WITH arrayJoin([10,17,71,6,55,38,27,61,48,46,21,38,2,67,35,77,29,31,27,67,81,82,75,81,31,38,68,95,37,34,65,59,81,28,82,80,35,3,97,42,66,28,85,98,45,15,41,61,24,53,97,86,5,65,84,18,9,32,46,52,69,44,78,98,61,64,26,11,3,19,0,90,28,72,47,8,0,74,38,63,88,43,81,61,34,24,37,53,79,72,5,77,58,3,61,56,1,3,5,61]) AS value SELECT skewPop(value) AS ex_1 ┌──────────────────ex_1─┐ │ -0.057853616194321014 │ └───────────────────────┘ print(skew(dataset, bias=False)) # -0.05873838908626328 WITH arrayJoin([10, 17, 71, 6, 55, 38, 27, 61, 48, 46, 21, 38, 2, 67, 35, 77, 29, 31, 27, 67, 81, 82, 75, 81, 31, 38, 68, 95, 37, 34, 65, 59, 81, 28, 82, 80, 35, 3, 97, 42, 66, 28, 85, 98, 45, 15, 41, 61, 24, 53, 97, 86, 5, 65, 84, 18, 9, 32, 46, 52, 69, 44, 78, 98, 61, 64, 26, 11, 3, 19, 0, 90, 28, 72, 47, 8, 0, 74, 38, 63, 88, 43, 81, 61, 34, 24, 37, 53, 79, 72, 5, 77, 58, 3, 61, 56, 1, 3, 5, 61]) AS value SELECT skewSamp(value) AS ex_1, (pow(count(), 2) * ex_1) / ((count() - 1) * (count() - 2)) AS G ┌─────────────────ex_1─┬────────────────────G─┐ │ -0.05698798509149213 │ -0.05873838908626276 │ └──────────────────────┴──────────────────────┘ print(kurtosis(dataset, bias=True, fisher=False), kurtosis(dataset, bias=True, fisher=True), kurtosis(dataset)) # 1.9020275610791184 -1.0979724389208816 -1.0979724389208816 WITH arrayJoin([10, 17, 71, 6, 55, 38, 27, 61, 48, 46, 21, 38, 2, 67, 35, 77, 29, 31, 27, 67, 81, 82, 75, 81, 31, 38, 68, 95, 37, 34, 65, 59, 81, 28, 82, 80, 35, 3, 97, 42, 66, 28, 85, 98, 45, 15, 41, 61, 24, 53, 97, 86, 5, 65, 84, 18, 9, 32, 46, 52, 69, 44, 78, 98, 61, 64, 26, 11, 3, 19, 0, 90, 28, 72, 47, 8, 0, 74, 38, 63, 88, 43, 81, 61, 34, 24, 37, 53, 79, 72, 5, 77, 58, 3, 61, 56, 1, 3, 5, 61]) AS value SELECT kurtPop(value) AS pearson, pearson - 3 AS fisher ┌────────────pearson─┬──────────────fisher─┐ │ 1.9020275610791124 │ -1.0979724389208876 │ └────────────────────┴─────────────────────┘ print(kurtosis(dataset, bias=False)) # -1.0924286152713967 WITH arrayJoin([10, 17, 71, 6, 55, 38, 27, 61, 48, 46, 21, 38, 2, 67, 35, 77, 29, 31, 27, 67, 81, 82, 75, 81, 31, 38, 68, 95, 37, 34, 65, 59, 81, 28, 82, 80, 35, 3, 97, 42, 66, 28, 85, 98, 45, 15, 41, 61, 24, 53, 97, 86, 5, 65, 84, 18, 9, 32, 46, 52, 69, 44, 78, 98, 61, 64, 26, 11, 3, 19, 0, 90, 28, 72, 47, 8, 0, 74, 38, 63, 88, 43, 81, 61, 34, 24, 37, 53, 79, 72, 5, 77, 58, 3, 61, 56, 1, 3, 5, 61]) AS value SELECT kurtSamp(value) AS ex_1, (((pow(count(), 2) * (count() + 1)) / (((count() - 1) * (count() - 2)) * (count() - 3))) * ex_1) - ((3 * pow(count() - 1, 2)) / ((count() - 2) * (count() - 3))) AS G ┌──────────────ex_1─┬───────────────────G─┐ │ 1.864177212613638 │ -1.0924286152714027 │ └───────────────────┴─────────────────────┘ Google Collab\n","categories":"","description":"How to make them return the same result like python scipy","excerpt":"How to make them return the same result like python scipy","ref":"/altinity-kb-functions/kurt_skew_statistics/","tags":"","title":"kurt \u0026 skew statistical functions in ClickHouse"},{"body":"Q. I get errors:\nFile not found: /var/log/clickhouse-server/clickhouse-server.log.0. File not found: /var/log/clickhouse-server/clickhouse-server.log.8.gz. ... File not found: /var/log/clickhouse-server/clickhouse-server.err.log.0, Stack trace (when copying this message, always include the lines below): 0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c2b345 in /usr/bin/clickhouse 1. Poco::PurgeOneFileStrategy::purge(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c84618 in /usr/bin/clickhouse 2. Poco::FileChannel::log(Poco::Message const\u0026) @ 0x11c314cc in /usr/bin/clickhouse 3. DB::OwnFormattingChannel::logExtended(DB::ExtendedLogMessage const\u0026) @ 0x8681402 in /usr/bin/clickhouse 4. DB::OwnSplitChannel::logSplit(Poco::Message const\u0026) @ 0x8682fa8 in /usr/bin/clickhouse 5. DB::OwnSplitChannel::log(Poco::Message const\u0026) @ 0x8682e41 in /usr/bin/clickhouse A. Check if you have proper permission to a log files folder, and enough disk space (\u0026 inode numbers) on the block device used for logging.\nls -la /var/log/clickhouse-server/ df -Th df -Thi Q. How to configure logging in clickhouse?\nA. See https://github.com/ClickHouse/ClickHouse/blob/ceaf6d57b7f00e1925b85754298cf958a278289a/programs/server/config.xml#L9-L62\n","categories":"","description":"Logging configuration and issues","excerpt":"Logging configuration and issues","ref":"/altinity-kb-setup-and-maintenance/logging/","tags":"","title":"Logging"},{"body":"For a general overview of TTL, see the article Putting Things Where They Belong Using New TTL Moves.\nALTER TABLE tbl MODIFY (ADD) TTL: It’s 2 step process:\nALTER TABLE tbl MODIFY (ADD) TTL ... Update table metadata: schema .sql \u0026 metadata in ZK. It’s usually cheap and fast command. And any new INSERT after schema change will calculate TTL according to new rule.\nALTER TABLE tbl MATERIALIZE TTL Recalculate TTL for already exist parts. It can be heavy operation, because ClickHouse will read column data \u0026 recalculate TTL \u0026 apply TTL expression. You can disable this step completely by using materialize_ttl_after_modify user session setting (by default it’s 1, so materialization is enabled).\nSET materialize_ttl_after_modify=0; ALTER TABLE tbl MODIFY TTL If you will disable materialization of TTL, it does mean that all old parts will be transformed according OLD TTL rules. MATERIALIZE TTL:\nRecalculate TTL (Kinda cheap, it read only column participate in TTL) Apply TTL (Rewrite of table data for all columns) You also can disable apply TTL substep via materialize_ttl_recalculate_only merge_tree setting (by default it’s 0, so clickhouse will apply TTL expression)\nALTER TABLE tbl MODIFY SETTING materialize_ttl_recalculate_only=1; It does mean, that TTL rule will not be applied during ALTER TABLE tbl MODIFY (ADD) TTL ... query.\nMATERIALIZE TTL done via Mutation:\nClickHouse create new parts via hardlinks and write new ttl.txt file ClickHouse remove old(inactive) parts after remove time (default is 8 minutes) To stop materialization of TTL:\nSELECT * FROM system.mutations WHERE is_done=0 AND table = 'tbl'; KILL MUTATION WHERE command LIKE '%MATERIALIZE TTL%' AND table = 'tbl' MODIFY TTL MOVE today: 2022-06-02\nTable tbl\nDaily partitioning by toYYYYMMDD(timestamp) -\u003e 20220602\nIncrease of TTL TTL timestamp + INTERVAL 30 DAY MOVE TO DISK s3 -\u003e TTL timestamp + INTERVAL 60 DAY MOVE TO DISK s3\nIdea: ClickHouse need to move data from s3 to local disk BACK Actual: There is no rule that data eariler than 60 DAY should be on local disk Table parts:\n20220401 ttl: 20220501 disk: s3 20220416 ttl: 20220516 disk: s3 20220501 ttl: 20220531 disk: s3 20220502 ttl: 20220601 disk: local 20220516 ttl: 20220616 disk: local 20220601 ttl: 20220631 disk: local ALTER TABLE tbl MODIFY TTL timestamp + INTERVAL 60 DAY MOVE TO DISK s3; Table parts:\n20220401 ttl: 20220601 disk: s3 20220416 ttl: 20220616 disk: s3 20220501 ttl: 20220631 disk: s3 (ClickHouse will not move this part to local disk, because there is no TTL rule for that) 20220502 ttl: 20220701 disk: local 20220516 ttl: 20220716 disk: local 20220601 ttl: 20220731 disk: local Decrease of TTL TTL timestamp + INTERVAL 30 DAY MOVE TO DISK s3 -\u003e TTL timestamp + INTERVAL 14 DAY MOVE TO DISK s3\nTable parts:\n20220401 ttl: 20220401 disk: s3 20220416 ttl: 20220516 disk: s3 20220501 ttl: 20220531 disk: s3 20220502 ttl: 20220601 disk: local 20220516 ttl: 20220616 disk: local 20220601 ttl: 20220631 disk: local ALTER TABLE tbl MODIFY TTL timestamp + INTERVAL 14 DAY MOVE TO DISK s3; Table parts:\n20220401 ttl: 20220415 disk: s3 20220416 ttl: 20220501 disk: s3 20220501 ttl: 20220515 disk: s3 20220502 ttl: 20220517 disk: local (ClickHouse will move this part to disk s3 in background according to TTL rule) 20220516 ttl: 20220601 disk: local (ClickHouse will move this part to disk s3 in background according to TTL rule) 20220601 ttl: 20220616 disk: local Possible TTL Rules TTL:\nDELETE (With enabled `ttl_only_drop_parts`, it's cheap operation, ClickHouse will drop the whole part) MOVE GROUP BY WHERE RECOMPRESS Related settings:\nServer settings:\nbackground_move_processing_pool_thread_sleep_seconds | 10 | background_move_processing_pool_thread_sleep_seconds_random_part | 1.0 | background_move_processing_pool_thread_sleep_seconds_if_nothing_to_do | 0.1 | background_move_processing_pool_task_sleep_seconds_when_no_work_min | 10 | background_move_processing_pool_task_sleep_seconds_when_no_work_max | 600 | background_move_processing_pool_task_sleep_seconds_when_no_work_multiplier | 1.1 | background_move_processing_pool_task_sleep_seconds_when_no_work_random_part | 1.0 | MergeTree settings:\nmerge_with_ttl_timeout │ 14400 │ 0 │ Minimal time in seconds, when merge with delete TTL can be repeated. merge_with_recompression_ttl_timeout │ 14400 │ 0 │ Minimal time in seconds, when merge with recompression TTL can be repeated. max_replicated_merges_with_ttl_in_queue │ 1 │ 0 │ How many tasks of merging parts with TTL are allowed simultaneously in ReplicatedMergeTree queue. max_number_of_merges_with_ttl_in_pool │ 2 │ 0 │ When there is more than specified number of merges with TTL entries in pool, do not assign new merge with TTL. This is to leave free threads for regular merges and avoid \"Too many parts\" ttl_only_drop_parts │ 0 │ 0 │ Only drop altogether the expired parts and not partially prune them. Session settings:\nmaterialize_ttl_after_modify │ 1 │ 0 │ Apply TTL for old data, after ALTER MODIFY TTL query ","categories":"","description":"What happening during MODIFY or ADD TTL query. ","excerpt":"What happening during MODIFY or ADD TTL query. ","ref":"/altinity-kb-queries-and-syntax/ttl/modify-ttl/","tags":"","title":"MODIFY (ADD) TTL"},{"body":"Alternative to doing that by minmax skip index.\nCREATE TABLE part_key_multiple_dates ( `key` UInt32, `date` Date, `time` DateTime, `created_at` DateTime, `inserted_at` DateTime ) ENGINE = MergeTree PARTITION BY (toYYYYMM(date), ignore(created_at, inserted_at)) ORDER BY (key, time); INSERT INTO part_key_multiple_dates SELECT number, toDate(x), now() + intDiv(number, 10) AS x, x - (rand() % 100), x + (rand() % 100) FROM numbers(100000000); SELECT count() FROM part_key_multiple_dates WHERE date \u003e (now() + toIntervalDay(105)); ┌─count()─┐ │ 8434210 │ └─────────┘ 1 rows in set. Elapsed: 0.022 sec. Processed 11.03 million rows, 22.05 MB (501.94 million rows/s., 1.00 GB/s.) SELECT count() FROM part_key_multiple_dates WHERE inserted_at \u003e (now() + toIntervalDay(105)); ┌─count()─┐ │ 9279818 │ └─────────┘ 1 rows in set. Elapsed: 0.046 sec. Processed 11.03 million rows, 44.10 MB (237.64 million rows/s., 950.57 MB/s.) SELECT count() FROM part_key_multiple_dates WHERE created_at \u003e (now() + toIntervalDay(105)); ┌─count()─┐ │ 9279139 │ └─────────┘ 1 rows in set. Elapsed: 0.043 sec. Processed 11.03 million rows, 44.10 MB (258.22 million rows/s., 1.03 GB/s.) ","categories":"","description":"How to put multiple correlated date-like columns in partition key without generating a lot of partitions in case not exact match between them.","excerpt":"How to put multiple correlated date-like columns in partition key …","ref":"/altinity-kb-queries-and-syntax/multiple-date-column-in-partition-key/","tags":"","title":"Multiple aligned date columns in PARTITION BY expression"},{"body":"ODBC interface for ClickHouse RDBMS.\nLicensed under the Apache 2.0.\nInstallation and usage Windows Download the latest release. On 64bit system you usually need both 32 bit and 64 bit drivers. Install (usually you will need ANSI driver, but better to install both versions, see below). Configure ClickHouse DSN. Note: that install driver linked against MDAC (which is default for Windows), some non-windows native applications (cygwin / msys64 based) may require driver linked agains unixodbc. Build section below.\nMacOS Install homebrew. Install driver brew install https://raw.githubusercontent.com/proller/homebrew-core/chodbc/Formula/clickhouse-odbc.rb Add clickhouse DSN configuration into ~/.odbc.ini file. (sample) Note: that install driver linked against iodbc (which is default for Mac), some homebrew applications (like python) may require unixodbc driver to work properly. In that case see Build section below.\nLinux DEB/RPM packaging is not provided yet, please build \u0026 install the driver from sources. Add clickhouse DSN configuration into ~/.odbc.ini file. (sample) Configuration On Linux / Max you configure DSN by adding new desctions in ~/.odbc.ini (See sample file: https://github.com/ClickHouse/clickhouse-odbc/blob/fd74398b50201ab13b535cdfab57bca86e588b37/packaging/odbc.ini.sample )\nOn Windows you can create/edit DSN using GUI tool through Control Panel.\nThe list of DSN parameters recognized by the driver is as follows:\nParameter Default value Description Url empty URL that points to a running ClickHouse instance, may include username, password, port, database, etc. Proto deduced from Url, or from Port and SSLMode: https if 443 or 8443 or SSLMode is not empty, http otherwise Protocol, one of: http, https Server or Host deduced from Url IP or hostname of a server with a running ClickHouse instance on it Port deduced from Url, or from Proto: 8443 if https, 8123 otherwise Port on which the ClickHouse instance is listening Path /query Path portion of the URL UID or Username default User name PWD or Password empty Password Database default Database name to connect to Timeout 30 Connection timeout SSLMode empty Certificate verification method (used by TLS/SSL connections, ignored in Windows), one of: allow, prefer, require, use allow to enable \u003ccode\u003eSSL_VERIFY_PEER\u003c/code\u003e TLS/SSL certificate verification mode, \u003ccode\u003eSSL_VERIFY_PEER | SSL_VERIFY_FAIL_IF_NO_PEER_CERT\u003c/code\u003e is used otherwise PrivateKeyFile empty Path to private key file (used by TLS/SSL connections), can be empty if no private key file is used CertificateFile empty Path to certificate file (used by TLS/SSL connections, ignored in Windows), if the private key and the certificate are stored in the same file, this can be empty if PrivateKeyFile is specified CALocation empty Path to the file or directory containing the CA/root certificates (used by TLS/SSL connections, ignored in Windows) DriverLog on if CMAKE_BUILD_TYPE is Debug, off otherwise Enable or disable the extended driver logging DriverLogFile \\temp\\clickhouse-odbc-driver.log on Windows, /tmp/clickhouse-odbc-driver.log otherwise Path to the extended driver log file (used when DriverLog is on) Troubleshooting \u0026 bug reporting If some software doesn’t work properly with that driver, but works good with other drivers - we will be appritiate if you will be able to collect debug info.\nTo debug issues with the driver, first things that need to be done are:\nenabling driver manager tracing. Links may contain some irrelevant vendor-specific details. on Windows/MDAC: 1, 2, 3 on Mac/iODBC: 1, 2 on Linux/unixODBC: 1, 2 enabling driver logging, see DriverLog and DriverLogFile DSN parameters above making sure that the application is allowed to create and write these driver log and driver manager trace files follow the steps leading to the issue. Collected log files will help to diagnose \u0026 solve the issue.\nDriver Managers Note, that since ODBC drivers are not used directly by a user, but rather accessed through applications, which in their turn access the driver through ODBC driver manager, user have to install the driver for the same architecture (32- or 64-bit) as the application that is going to access the driver. Moreover, both the driver and the application must be compiled for (and actually use during run-time) the same ODBC driver manager implementation (we call them “ODBC providers” here). There are three supported ODBC providers:\nODBC driver manager associated with MDAC (Microsoft Data Access Components, sometimes referenced as WDAC, Windows Data Access Components) - the standard ODBC provider of Windows UnixODBC - the most common ODBC provider in Unix-like systems. Theoretically, could be used in Cygwin or MSYS/MinGW environments in Windows too. iODBC - less common ODBC provider, mainly used in Unix-like systems, however, it is the standard ODBC provider in macOS. Theoretically, could be used in Cygwin or MSYS/MinGW environments in Windows too. If you don’t see a package that matches your platforms, or the version of your system is significantly different than those of the available packages, or maybe you want to try a bleeding edge version of the code that hasn’t been released yet, you can always build the driver manually from sources.\nNote, that it is always a good idea to install the driver from the corresponding native package (.msi, etc., which you can also easily create if you are building from sources), than use the binaries that were manually copied to some folder.\nBuilding from sources The general requirements for building the driver from sources are as follows:\nCMake 3.12 and later C++17 and C11 capable compiler toolchain: Clang 4 and later GCC 7 and later Xcode 10 and later Microsoft Visual Studio 2017 and later ODBC Driver manager (MDAC / unixodbc / iODBC) SSL library (openssl) Generic build scenario:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git cd clickhouse-odbc mkdir build cd build cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo .. cmake --build . -C RelWithDebInfo Additional requirements exist for each platform, which also depend on whether packaging and/or testing is performed.\nLinux/macOS Execute the following in the terminal to install needed dependencies:\n# on Red Hat/CentOS (tested on CentOS 7) sudo yum groupinstall \"Development Tools\" sudo yum install centos-release-scl sudo yum install devtoolset-8 sudo yum install git cmake openssl-devel unixODBC-devel # You may use libiodbc-devel INSTEAD of unixODBC-devel scl enable devtoolset-8 -- bash # Enable Software collections for that terminal session, to use newer versions of complilers # on Ubuntu (tested on Ubuntu 18.10, for older versions you may need to install newer c++ compiler and cmake versions) sudo apt install build-essential git cmake libpoco-dev libssl-dev unixodbc-dev # You may use libiodbc-devel INSEAD of unixODBC-devel # MacOS: # You will need Xcode 10 or later and Command Line Tools to be installed, as well as [Homebrew](https://brew.sh/). brew install git cmake make poco openssl libiodbc # You may use unixodbc INSTEAD of libiodbc Note: usually on Linux you use unixODBC driver manager, and on Mac - iODBC. In some (rare) cases you may need use other driver manager, please do it only if you clearly understand the differencies. Driver should be used with the driver manager it was linked to.\nClone the repo with submodules:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git Enter the cloned source tree, create a temporary build folder, and generate a Makefile for the project in it:\ncd clickhouse-odbc mkdir build cd build # Configuration options for the project can be specified in the next command in a form of '-Dopt=val' # For MacOS: you may also add '-G Xcode' to the next command, in order to use Xcode as a build system or IDE, and generate the solution and project files instead of Makefile. cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo .. Build the generated solution in-place:\ncmake --build . -C RelWithDebInfo cmake --build . -C RelWithDebInfo --target package …and, optionally, run tests (note, that for non-unit tests, preconfigured driver and DSN entries must exist, that point to the binaries generated in this build folder):\ncmake --build . -C RelWithDebInfo --target test For MacOS: if you configured the project with ‘-G Xcode’ initially, open the IDE and build all, package, and test targets manually from there\ncmake --open . Windows CMake bundled with the recent versions of Visual Studio can be used.\nAn SDK required for building the ODBC driver is included in Windows SDK, which in its turn is also bundled with Visual Studio.\nYou will need to install WiX toolset to be able to generate .msi packages. You can download and install it from WiX toolset home page.\nAll of the following commands have to be issued in Visual Studio Command Prompt:\nuse x86 Native Tools Command Prompt for VS 2019 or equivalent for 32-bit builds use x64 Native Tools Command Prompt for VS 2019 or equivalent for 64-bit builds Clone the repo with submodules:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git Enter the cloned source tree, create a temporary build folder, and generate the solution and project files in it:\ncd clickhouse-odbc mkdir build cd build # Configuration options for the project can be specified in the next command in a form of '-Dopt=val' # Use the following command for 32-bit build only. cmake -A Win32 -DCMAKE_BUILD_TYPE=RelWithDebInfo .. # Use the following command for 64-bit build only. cmake -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo .. Build the generated solution in-place:\ncmake --build . -C RelWithDebInfo cmake --build . -C RelWithDebInfo --target package …and, optionally, run tests (note, that for non-unit tests, preconfigured driver and DSN entries must exist, that point to the binaries generated in this build folder):\ncmake --build . -C RelWithDebInfo --target test …or open the IDE and build all, package, and test targets manually from there:\ncmake --open . cmake options The list of configuration options recognized during the CMake generation step is as follows:\nOption Default value Description CMAKE_BUILD_TYPE RelWithDebInfo Build type, one of: Debug, Release, RelWithDebInfo CH_ODBC_ENABLE_SSL ON Enable TLS/SSL (required for utilizing https:// interface, etc.) CH_ODBC_ENABLE_INSTALL ON Enable install targets (required for packaging) CH_ODBC_ENABLE_TESTING inherits value of BUILD_TESTING Enable test targets CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES ON Prefer bundled over system variants of third party libraries CH_ODBC_PREFER_BUNDLED_POCO inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of Poco library CH_ODBC_PREFER_BUNDLED_SSL inherits value of CH_ODBC_PREFER_BUNDLED_POCO Prefer bundled over system variants of TLS/SSL library CH_ODBC_PREFER_BUNDLED_GOOGLETEST inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of Google Test library CH_ODBC_PREFER_BUNDLED_NANODBC inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of nanodbc library CH_ODBC_RUNTIME_LINK_STATIC OFF Link with compiler and language runtime statically CH_ODBC_THIRD_PARTY_LINK_STATIC ON Link with third party libraries statically CH_ODBC_DEFAULT_DSN_ANSI ClickHouse DSN (ANSI) Default ANSI DSN name CH_ODBC_DEFAULT_DSN_UNICODE ClickHouse DSN (Unicode) Default Unicode DSN name TEST_DSN inherits value of CH_ODBC_DEFAULT_DSN_ANSI ANSI DSN name to use in tests TEST_DSN_W inherits value of CH_ODBC_DEFAULT_DSN_UNICODE Unicode DSN name to use in tests Packaging / redistributing the driver You can just copy the library to another computer, in that case you need to\ninstall run-time dependencies on target computer Windows: MDAC driver manager (preinstalled on all modern Windows systems) C++ Redistributable for Visual Studio 2017 or same for 2019, etc. Linux # CentOS / RedHat sudo yum install openssl unixODBC # Debian/Ubuntu sudo apt install openssl unixodbc MacOS (assuming you have Homebrew installed): brew install poco openssl libiodbc register the driver so that the corresponding ODBC provider is able to locate it. All this involves modifying a dedicated registry keys in case of MDAC, or editing odbcinst.ini (for driver registration) and odbc.ini (for DSN definition) files for UnixODBC or iODBC, directly or indirectly.\nThis will be done automatically using some default values if you are installing the driver using native installers.\nOtherwise, if you are configuring manually, or need to modify the default configuration created by the installer, please see the exact locations of files (or registry keys) that need to be modified.\n","categories":"","description":"ODBC Driver for ClickHouse","excerpt":"ODBC Driver for ClickHouse","ref":"/altinity-kb-integrations/clickhouse-odbc/","tags":"","title":"ODBC Driver for ClickHouse"},{"body":"Part names \u0026 multiversion concurrency control Part name format is:\n\u003cpartitionid\u003e_\u003cmin_block_number\u003e_\u003cmax_block_number\u003e_\u003clevel\u003e_\u003cdata_version\u003e system.parts contains all the information parsed.\npartitionid is quite simple (it just comes from your partitioning key).\nWhat are block_numbers?\nDROP TABLE IF EXISTS part_names; create table part_names (date Date, n UInt8, m UInt8) engine=MergeTree PARTITION BY toYYYYMM(date) ORDER BY n; insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ │ 202203_2_2_0 │ 202203 │ 2 │ 2 │ 0 │ 2 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ │ 202203_2_2_0 │ 202203 │ 2 │ 2 │ 0 │ 2 │ │ 202203_3_3_0 │ 202203 │ 3 │ 3 │ 0 │ 3 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ As you can see every insert creates a new incremental block_number which is written in part names both as \u003cmin_block_number\u003e and \u003cmin_block_number\u003e (and the level is 0 meaning that the part was never merged).\nThose block numbering works in the scope of partition (for Replicated table) or globally across all partition (for plain MergeTree table).\nClickHouse always merge only continuous blocks . And new part names always refer to the minimum and maximum block numbers.\nOPTIMIZE TABLE part_names; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1 │ 202203 │ 1 │ 3 │ 1 │ 1 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ As you can see here - three parts (with block number 1,2,3) were merged and they formed the new part with name 1_3 as min/max block size. Level get incremented.\nNow even while previous (merged) parts still exists in filesystem for a while (as inactive) clickhouse is smart enough to understand that new part ‘covers’ same range of blocks as 3 parts of the prev ‘generation’\nThere might be a fifth section in the part name, data version.\nData version gets increased when a part mutates.\nEvery mutation takes one block number:\ninsert into part_names VALUES (now(), 0, 0); insert into part_names VALUES (now(), 0, 0); insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1 │ 202203 │ 1 │ 3 │ 1 │ 1 │ │ 202203_4_4_0 │ 202203 │ 4 │ 4 │ 0 │ 4 │ │ 202203_5_5_0 │ 202203 │ 5 │ 5 │ 0 │ 5 │ │ 202203_6_6_0 │ 202203 │ 6 │ 6 │ 0 │ 6 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); alter table part_names update m=n where 1; select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name───────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1_7 │ 202203 │ 1 │ 3 │ 1 │ 7 │ │ 202203_4_4_0_7 │ 202203 │ 4 │ 4 │ 0 │ 7 │ │ 202203_5_5_0_7 │ 202203 │ 5 │ 5 │ 0 │ 7 │ │ 202203_6_6_0_7 │ 202203 │ 6 │ 6 │ 0 │ 7 │ │ 202203_8_8_0 │ 202203 │ 8 │ 8 │ 0 │ 8 │ └────────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ OPTIMIZE TABLE part_names; select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name───────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_8_2_7 │ 202203 │ 1 │ 8 │ 2 │ 7 │ └────────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ ","categories":"","description":"Part names \u0026 multiversion concurrency control.","excerpt":"Part names \u0026 multiversion concurrency control.","ref":"/engines/mergetree-table-engine-family/part-naming-and-mvcc/","tags":"","title":"Part names \u0026 MVCC"},{"body":"Pre-Aggregation approaches: ETL vs Materialized Views vs Projections ETL MV Projections Realtime no yes yes How complex queries can be used to build the preaggregaton any complex very simple Impacts the insert speed no yes yes Are inconsistancies possible Depends on ETL. If it process the errors properly - no. yes (no transactions / atomicity) no Lifetime of aggregation any any Same as the raw data Requirements need external tools/scripting is a part of database schema is a part of table schema How complex to use in queries Depends on aggregation, usually simple, quering a separate table Depends on aggregation, sometimes quite complex, quering a separate table Very simple, quering the main table Can work correctly with ReplacingMergeTree as a source Yes No No Can work correctly with CollapsingMergeTree as a source Yes For simple aggregations For simple aggregations Can be chained Yes (Usually with DAGs / special scripts) Yes (but may be not straightforward, and often is a bad idea) No Resources needed to calculate the increment May be signigicant Usually tiny Usually tiny ","categories":"","description":"ETL vs Materialized Views vs Projections in ClickHouse.","excerpt":"ETL vs Materialized Views vs Projections in ClickHouse.","ref":"/altinity-kb-schema-design/preaggregations/","tags":"","title":"Pre-Aggregation approaches"},{"body":"Precreate parts using clickhouse-local the code below were testes on 23.3\n## 1. Imagine we want to process this file: cat \u003c\u003cEOF \u003e /tmp/data.csv 1,2020-01-01,\"String\" 2,2020-02-02,\"Another string\" 3,2020-03-03,\"One more string\" 4,2020-01-02,\"String for first partition\" EOF rm -rf /tmp/precreate_parts mkdir -p /tmp/precreate_parts cd /tmp/precreate_parts ## 2. that is the metadata for the table we want to fill ## schema should match the schema of the table from server ## (the easiest way is just to copy it from the server) ## I've added sleepEachRow(0.5) here just to mimic slow insert clickhouse-local --path=. --query=\"CREATE DATABASE local\" clickhouse-local --path=. --query=\"CREATE TABLE local.test (id UInt64, d Date, s String, x MATERIALIZED sleepEachRow(0.5)) Engine=MergeTree ORDER BY id PARTITION BY toYYYYMM(d);\" ## 3. we can insert the input file into that table in different manners: ## a) just plain insert cat /tmp/data.csv | clickhouse-local --path=. --query=\"INSERT INTO local.test FORMAT CSV\" ## b) use File on the top of stdin (allows to tune the types) clickhouse-local --path=. --query=\"CREATE TABLE local.stdin (id UInt64, d Date, s String) Engine=File(CSV, stdin)\" cat /tmp/data.csv | clickhouse-local --path=. --query=\"INSERT INTO local.test SELECT * FROM local.stdin\" ## c) Instead of stdin you can use file engine clickhouse-local --path=. --query \"CREATE TABLE local.data_csv (id UInt64, d Date, s String) Engine=File(CSV, '/tmp/data.csv')\" clickhouse-local --path=. --query \"INSERT INTO local.test SELECT * FROM local.data_csv\" # 4. now we have already parts created clickhouse-local --path=. --query \"SELECT _part,* FROM local.test ORDER BY id\" ls -la data/local/test/ # if needed we can even preprocess them more agressively - by doing OPTIMIZE ON that clickhouse-local --path=. --query \"OPTIMIZE TABLE local.test FINAL\" # that works, but clickhouse will keep inactive parts (those 'unmerged') in place. ls -la data/local/test/ # we can use a bit hacky way to force it to remove inactive parts them clickhouse-local --path=. --query \"ALTER TABLE local.test MODIFY SETTING old_parts_lifetime=0, cleanup_delay_period=0, cleanup_delay_period_random_add=0\" ## needed to give background threads time to clean inactive parts (max_block_size allows to stop that quickly if needed) clickhouse-local --path=. --query \"SELECT count() FROM numbers(100) WHERE sleepEachRow(0.1) SETTINGS max_block_size=1\" ls -la data/local/test/ clickhouse-local --path=. --query \"SELECT _part,* FROM local.test ORDER BY id\" ","categories":"","description":"Precreate parts using clickhouse-local.","excerpt":"Precreate parts using clickhouse-local.","ref":"/altinity-kb-setup-and-maintenance/precreate_parts_using_clickhouse_local.sh/","tags":"","title":"Precreate parts using clickhouse-local"},{"body":"Good order by usually have 3 to 5 columns, from lowest cardinal on the left (and the most important for filtering) to highest cardinal (and less important for filtering).\nPractical approach to create an good ORDER BY for a table:\nPick the columns you use in filtering always The most important for filtering and the lowest cardinal should be the left-most. Typically it’s something like tenant_id Next column is more cardinal, less important. It can be rounded time sometimes, or site_id, or source_id, or group_id or something similar. repeat p.3 once again (or few times) if you added already all columns important for filtering and you still not addressing a single row with you pk - you can add more columns which can help to put similar records close to each other (to improve the compression) if you have something like hierarchy / tree-like relations between the columns - put there the records from ‘root’ to ’leaves’ for example (continent, country, cityname). This way clickhouse can do lookup by country / city even if continent is not specified (it will just ‘check all continents’) special variants of MergeTree may require special ORDER BY to make the record unique etc. For timeseries it usually make sense to put timestamp as latest column in ORDER BY, it helps with putting the same data near by for better locality. There is only 2 major patterns for timestamps in ORDER BY: (…, toStartOf(Day|Hour|…)(timestamp), …, timestamp) and (…, timestamp). First one is useful when your often query small part of table partition. (table partitioned by months and your read only 1-4 days 90% of times) Some examples of good order by\nORDER BY (tenantid, site_id, utm_source, clientid, timestamp) ORDER BY (site_id, toStartOfHour(timestamp), sessionid, timestamp ) PRIMARY KEY (site_id, toStartOfHour(timestamp), sessionid) For Summing / Aggregating All dimensions go to ORDER BY, all metrics - outside of that.\nThe most important for filtering columns with the lowest cardinality should be the left most.\nIf number of dimensions is high it’s typically make sense to use a prefix of ORDER BY as a PRIMARY KEY to avoid polluting sparse index.\nExamples:\nORDER BY (tenant_id, hour, country_code, team_id, group_id, source_id) PRIMARY KEY (tenant_id, hour, country_code, team_id) For Replacing / Collapsing You need to keep all ‘mutable’ columns outside of ORDER BY, and have some unique id (a base to collapse duplicates) inside. Typically the right-most column is some row identifier. And it’s often not needed in sparse index (so PRIMARY KEY can be a prefix of ORDER BY) The rest consideration are the same.\nExamples:\nORDER BY (tenantid, site_id, eventid) -- utm_source is mutable, while tenantid, site_id is not PRIMARY KEY (tenantid, site_id) -- eventid is not used for filtering, needed only for collapsing duplicates ORDER BY example -- col1: high Cardinality -- col2: low cardinality CREATE TABLE tests.order_test ( `col1` DateTime, `col2` UInt8 ) ENGINE = MergeTree PARTITION BY toYYYYMM(col1) ORDER BY (col1, col2) -- SELECT count() ┌───count()─┐ │ 126371225 │ └───────────┘ So let’s put the highest cardinal column to the left and the least to the right in the ORDER BY definition. This will impact in queries like:\nSELECT * FROM order_test WHERE col1 \u003e toDateTime('2020-10-01') ORDER BY col1, col2 FORMAT `Null` Here for the filtering it will use the skipping index to select the parts WHERE col1 \u003e xxx and the result wont be need to be ordered because the ORDER BY in the query aligns with the ORDER BY in the table and the data is already ordered in disk.\nexecuteQuery: (from [::ffff:192.168.11.171]:39428, user: admin) SELECT * FROM order_test WHERE col1 \u003e toDateTime('2020-10-01') ORDER BY col1,col2 FORMAT Null; (stage: Complete) ContextAccess (admin): Access granted: SELECT(col1, col2) ON tests.order_test ContextAccess (admin): Access granted: SELECT(col1, col2) ON tests.order_test InterpreterSelectQuery: FetchColumns -\u003e Complete tests.order_test (SelectExecutor): Key condition: (column 0 in [1601503201, +Inf)) tests.order_test (SelectExecutor): MinMax index condition: (column 0 in [1601503201, +Inf)) tests.order_test (SelectExecutor): Running binary search on index range for part 202010_367_545_8 (7612 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202010_549_729_12 (37 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202011_689_719_2 (1403 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202012_550_730_12 (3 marks) tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 37 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 3 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 1403 tests.order_test (SelectExecutor): Found continuous range in 11 steps tests.order_test (SelectExecutor): Found continuous range in 3 steps tests.order_test (SelectExecutor): Running binary search on index range for part 202011_728_728_0 (84 marks) tests.order_test (SelectExecutor): Found continuous range in 21 steps tests.order_test (SelectExecutor): Running binary search on index range for part 202011_725_725_0 (128 marks) tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 84 tests.order_test (SelectExecutor): Running binary search on index range for part 202011_722_722_0 (128 marks) tests.order_test (SelectExecutor): Found continuous range in 13 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 128 tests.order_test (SelectExecutor): Found continuous range in 14 steps tests.order_test (SelectExecutor): Running binary search on index range for part 202011_370_686_19 (5993 marks) tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 5993 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found continuous range in 25 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 128 tests.order_test (SelectExecutor): Found continuous range in 14 steps tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 7612 tests.order_test (SelectExecutor): Found continuous range in 25 steps tests.order_test (SelectExecutor): Selected 8/9 parts by partition key, 8 parts by primary key, 15380/15380 marks by primary key, 15380 marks to read from 8 ranges Ok. 0 rows in set. Elapsed: 0.649 sec. Processed 125.97 million rows, 629.86 MB (194.17 million rows/s., 970.84 MB/s.) If we change the ORDER BY expression in the query, Clickhouse will need to retrieve the rows and reorder them:\nSELECT * FROM order_test WHERE col1 \u003e toDateTime('2020-10-01') ORDER BY col2, col1 FORMAT `Null` As seen In the MergingSortedTransform message, the ORDER BY in the table definition is not aligned with the ORDER BY in the query, so ClickHouse has to reorder the resultset.\nexecuteQuery: (from [::ffff:192.168.11.171]:39428, user: admin) SELECT * FROM order_test WHERE col1 \u003e toDateTime('2020-10-01') ORDER BY col2,col1 FORMAT Null; (stage: Complete) ContextAccess (admin): Access granted: SELECT(col1, col2) ON tests.order_test ContextAccess (admin): Access granted: SELECT(col1, col2) ON tests.order_test InterpreterSelectQuery: FetchColumns -\u003e Complete tests.order_test (SelectExecutor): Key condition: (column 0 in [1601503201, +Inf)) tests.order_test (SelectExecutor): MinMax index condition: (column 0 in [1601503201, +Inf)) tests.order_test (SelectExecutor): Running binary search on index range for part 202010_367_545_8 (7612 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202012_550_730_12 (3 marks) tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Running binary search on index range for part 202011_725_725_0 (128 marks) tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 3 tests.order_test (SelectExecutor): Running binary search on index range for part 202011_689_719_2 (1403 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202010_549_729_12 (37 marks) tests.order_test (SelectExecutor): Running binary search on index range for part 202011_728_728_0 (84 marks) tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found continuous range in 3 steps tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Running binary search on index range for part 202011_722_722_0 (128 marks) tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 7612 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 37 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found continuous range in 11 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 1403 tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 84 tests.order_test (SelectExecutor): Found continuous range in 25 steps tests.order_test (SelectExecutor): Running binary search on index range for part 202011_370_686_19 (5993 marks) tests.order_test (SelectExecutor): Found continuous range in 21 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 128 tests.order_test (SelectExecutor): Found continuous range in 13 steps tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found continuous range in 14 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 128 tests.order_test (SelectExecutor): Found (LEFT) boundary mark: 0 tests.order_test (SelectExecutor): Found continuous range in 14 steps tests.order_test (SelectExecutor): Found (RIGHT) boundary mark: 5993 tests.order_test (SelectExecutor): Found continuous range in 25 steps tests.order_test (SelectExecutor): Selected 8/9 parts by partition key, 8 parts by primary key, 15380/15380 marks by primary key, 15380 marks to read from 8 ranges tests.order_test (SelectExecutor): MergingSortedTransform: Merge sorted 1947 blocks, 125972070 rows in 1.423973879 sec., 88465155.05499662 rows/sec., 423.78 MiB/sec Ok. 0 rows in set. Elapsed: 1.424 sec. Processed 125.97 million rows, 629.86 MB (88.46 million rows/s., 442.28 MB/s.) PARTITION BY Good size for single partition is something like 1-300Gb. For Summing/Replacing a bit smaller (400Mb-40Gb) Better to avoid touching more that few dozens of partitions with typical SELECT query. Single insert should bring data to one or few partitions. The number of partitons in table - dozen or hundreds, not thousands. The size of partitions you can check in system.parts table.\nExamples:\n-- for time-series: PARTITION BY toYear(timestamp) -- long retention, not too much data PARTITION BY toYYYYMM(timestamp) -- PARTITION BY toMonday(timestamp) -- PARTITION BY toDate(timestamp) -- PARTITION BY toStartOfHour(timestamp) -- short retention, lot of data -- for table with some incremental (non time-bounded) counter PARTITION BY intDiv(transaction_id, 1000000) -- for some dimention tables (always requested with WHERE userid) PARTITION BY userid % 16 For the small tables (smaller than few gigabytes) partitioning is usually not needed at all (just skip PARTITION BY expresssion when you create the table).\nSee also How to change ORDER BY\nClickHouse Anti-Patterns. Learning from Users’ Mistakes A short talk by Mikhail Filimonov\nhttps://youtu.be/DP7l6Swkskw?t=3777\n","categories":"","description":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree table.","excerpt":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree …","ref":"/engines/mergetree-table-engine-family/pick-keys/","tags":"","title":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree-family table"},{"body":"Documentation https://clickhouse.com/docs/en/operations/access-rights/\nEnable RBAC and create admin user Create an admin user like (root in MySQL or postgres in PostgreSQL) to do the DBA/admin ops in the user.xml file and set the access management property for the admin user\n\u003cclickhouse\u003e \u003cusers\u003e \u003cdefault\u003e .... \u003c/default\u003e \u003cadmin\u003e \u003c!-- Password could be specified in plaintext or in SHA256 (in hex format). If you want to specify password in plaintext (not recommended), place it in 'password' element. Example: \u003cpassword\u003eqwerty\u003c/password\u003e. Password could be empty. If you want to specify SHA256, place it in 'password_sha256_hex' element. Example: \u003cpassword_sha256_hex\u003e65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5\u003c/password_sha256_hex\u003e Restrictions of SHA256: impossibility to connect to ClickHouse using MySQL JS client (as of July 2019). If you want to specify double SHA1, place it in 'password_double_sha1_hex' element. Example: \u003cpassword_double_sha1_hex\u003ee395796d6546b1b65db9d665cd43f0e858dd4303\u003c/password_double_sha1_hex\u003e --\u003e \u003cpassword\u003e\u003c/password\u003e \u003cnetworks\u003e \u003cip\u003e::/0\u003c/ip\u003e \u003c/networks\u003e \u003c!-- Settings profile for user. --\u003e \u003cprofile\u003edefault\u003c/profile\u003e \u003c!-- Quota for user. --\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003c!-- Set This parameter to Enable RBAC Admin user can create other users and grant rights to them. --\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/admin\u003e ... \u003c/clickhouse\u003e default user As default is used for many internal and background operations, so it is not convenient to set it up with a password, because you would have to change it in many configs/parts. Best way to secure the default user is only allow localhost or trusted network connections like this in users.xml:\n\u003cclickhouse\u003e \u003cusers\u003e \u003cdefault\u003e ...... \u003cnetworks\u003e \u003cip\u003e127.0.0.1/8\u003c/ip\u003e \u003cip\u003e10.10.10.0/24\u003c/ip\u003e \u003c/networks\u003e ...... \u003c/default\u003e \u003c/clickhouse\u003e replication user The replication user is defined by interserver_http_credential tag. It does not relate to a ClickHouse client credentials configuration. If this tag is ommited then authentication is not used during replication. Ports 9009 and 9010(tls) provide low-level data access between servers. This ports should not be accessible from untrusted networks. You can specify credentials for authenthication between replicas. This is required when interserver_https_port is accessible from untrusted networks. You can do so by defining user and password to the interserver credentials. Then replication protocol will use basic access authentication when connecting by HTTP/HTTPS to other replicas:\n\u003cinterserver_http_credentials\u003e \u003cuser\u003ereplication\u003c/user\u003e \u003cpassword\u003epassword\u003c/password\u003e \u003c/interserver_http_credentials\u003e Create users and roles Now we can setup users/roles using a generic best-practice approach for RBAC from other databases, like using roles, granting permissions to roles, creating users for different applications, etc…\nsee User Hardening article\nExample: 3 roles (dba, dashboard_ro, ingester_rw) create role dba on cluster '{cluster}'; grant all on *.* to dba on cluster '{cluster}'; create user `user1` identified by 'pass1234' on cluster '{cluster}'; grant dba to user1 on cluster '{cluster}'; create role dashboard_ro on cluster '{cluster}'; grant select on default.* to dashboard_ro on cluster '{cluster}'; grant dictGet on *.* to dashboard_ro on cluster '{cluster}'; create settings profile or replace profile_dashboard_ro on cluster '{cluster}' settings max_concurrent_queries_for_user = 10 READONLY, max_threads = 16 READONLY, max_memory_usage_for_user = '30G' READONLY, max_memory_usage = '30G' READONLY, max_execution_time = 60 READONLY, max_rows_to_read = 1000000000 READONLY, max_bytes_to_read = '5000G' READONLY TO dashboard_ro; create user `dash1` identified by 'pass1234' on cluster '{cluster}'; grant dashboard_ro to dash1 on cluster '{cluster}'; create role ingester_rw on cluster '{cluster}'; grant select,insert on default.* to ingester_rw on cluster '{cluster}'; create settings profile or replace profile_ingester_rw on cluster '{cluster}' settings max_concurrent_queries_for_user = 40 READONLY, -- user can run 40 queries (select, insert ...) simultaneously max_threads = 10 READONLY, -- each query can use up to 10 cpu (READONLY means user cannot override a value) max_memory_usage_for_user = '30G' READONLY, -- all queries of the user can use up to 30G RAM max_memory_usage = '25G' READONLY, -- each query can use up to 25G RAM max_execution_time = 200 READONLY, -- each query can executes no longer 200 seconds max_rows_to_read = 1000000000 READONLY, -- each query can read up to 1 billion rows max_bytes_to_read = '5000G' READONLY -- each query can read up to 5 TB from a MergeTree TO ingester_rw; create user `ingester_app1` identified by 'pass1234'　on cluster '{cluster}'; grant ingester_rw to ingester_app1 on cluster '{cluster}'; check $ clickhouse-client -u dash1 --password pass1234 create table test ( A Int64) Engine=Log; DB::Exception: dash1: Not enough privileges $ clickhouse-client -u user1 --password pass1234 create table test ( A Int64) Engine=Log; Ok. drop table test; Ok. $ clickhouse-client -u ingester_app1 --password pass1234 select count() from system.numbers limit 1000000000000; DB::Exception: Received from localhost:9000. DB::Exception: Limit for rows or bytes to read exceeded, max rows: 1.00 billion clean up show profiles; ┌─name─────────────────┐ │ default │ │ profile_dashboard_ro │ │ profile_ingester_rw │ │ readonly │ └──────────────────────┘ drop profile if exists readonly on cluster '{cluster}'; drop profile if exists profile_dashboard_ro on cluster '{cluster}'; drop profile if exists profile_ingester_rw on cluster '{cluster}'; show roles; ┌─name─────────┐ │ dashboard_ro │ │ dba │ │ ingester_rw │ └──────────────┘ drop role if exists dba on cluster '{cluster}'; drop role if exists dashboard_ro on cluster '{cluster}'; drop role if exists ingester_rw on cluster '{cluster}'; show users; ┌─name──────────┐ │ dash1 │ │ default │ │ ingester_app1 │ │ user1 │ └───────────────┘ drop user if exists ingester_app1 on cluster '{cluster}'; drop user if exists user1 on cluster '{cluster}'; drop user if exists dash1 on cluster '{cluster}'; ","categories":"","description":"Access Control and Account Management (RBAC).","excerpt":"Access Control and Account Management (RBAC).","ref":"/altinity-kb-setup-and-maintenance/rbac/","tags":"","title":"Access Control and Account Management (RBAC)"},{"body":"Atomic \u0026 Ordinary databases. srv1 – good replica\nsrv2 – lost replica / we will restore it from srv1\ntest data (3 tables (atomic \u0026 ordinary databases)) srv1\ncreate database testatomic on cluster '{cluster}' engine=Atomic; create table testatomic.test on cluster '{cluster}' (A Int64, D Date, s String) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}') partition by toYYYYMM(D) order by A; insert into testatomic.test select number, today(), '' from numbers(1000000); create database testordinary on cluster '{cluster}' engine=Ordinary; create table testordinary.test on cluster '{cluster}' (A Int64, D Date, s String) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}') partition by toYYYYMM(D) order by A; insert into testordinary.test select number, today(), '' from numbers(1000000); create table default.test on cluster '{cluster}' (A Int64, D Date, s String) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}') partition by toYYYYMM(D) order by A; insert into default.test select number, today(), '' from numbers(1000000); destroy srv2 srv2\n/etc/init.d/clickhouse-server stop rm -rf /var/lib/clickhouse/* generate script to re-create databases (create_database.sql). srv1\n$ cat /home/ubuntu/generate_schema.sql SELECT concat('CREATE DATABASE \"', name, '\" ENGINE = ', engine, ' COMMENT \\'', comment, '\\';') FROM system.databases WHERE name NOT IN ('INFORMATION_SCHEMA', 'information_schema', 'system', 'default'); clickhouse-client \u003c /home/denis.zhuravlev/generate_schema.sql \u003e create_database.sql check the result\n$ cat create_database.sql CREATE DATABASE \"testatomic\" ENGINE = Atomic COMMENT ''; CREATE DATABASE \"testordinary\" ENGINE = Ordinary COMMENT ''; transfer this create_database.sql to srv2 (scp / rsync)\nmake a copy of schema sql files (metadata_schema.tar) srv1\ncd /var/lib/clickhouse/ tar -cvhf /home/ubuntu/metadata_schema.tar metadata -h - is important! (-h, –dereference Follow symlinks; archive and dump the files they point to.)\ntransfer this metadata_schema.tar to srv2 (scp / rsync)\ncreate databases at srv2 srv2\n/etc/init.d/clickhouse-server start clickhouse-client \u003c create_database.sql /etc/init.d/clickhouse-server stop create tables at srv2 srv2\ncd /var/lib/clickhouse/ tar xkfv /home/ubuntu/metadata_schema.tar sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data /etc/init.d/clickhouse-server start tar xkfv -k is important! To save folders/symlinks created with create database ( -k, –keep-old-files Don’t replace existing files when extracting )\ncheck a recovery srv2\nSELECT count() FROM testatomic.test; ┌─count()─┐ │ 1000000 │ └─────────┘ SELECT count() FROM testordinary.test; ┌─count()─┐ │ 1000000 │ └─────────┘ SELECT count() FROM default.test; ┌─count()─┐ │ 1000000 │ └─────────┘ ","categories":"","description":"Recovery after complete data loss ","excerpt":"Recovery after complete data loss ","ref":"/altinity-kb-setup-and-maintenance/recovery-after-complete-data-loss/","tags":"","title":"recovery-after-complete-data-loss"},{"body":"Remove block numbers from zookeeper for removed partitions SELECT distinct concat('delete ', zk.block_numbers_path, zk.partition_id) FROM ( SELECT r.database, r.table, zk.block_numbers_path, zk.partition_id, p.partition_id FROM ( SELECT path as block_numbers_path, name as partition_id FROM system.zookeeper WHERE path IN ( SELECT concat(zookeeper_path, '/block_numbers/') as block_numbers_path FROM clusterAllReplicas('{cluster}',system.replicas) ) ) as zk LEFT JOIN ( SELECT database, table, concat(zookeeper_path, '/block_numbers/') as block_numbers_path FROM clusterAllReplicas('{cluster}',system.replicas) ) as r ON (r.block_numbers_path = zk.block_numbers_path) LEFT JOIN ( SELECT DISTINCT partition_id, database, table FROM clusterAllReplicas('{cluster}',system.parts) ) as p ON (p.partition_id = zk.partition_id AND p.database = r.database AND p.table = r.table) WHERE p.partition_id = '' AND zk.partition_id \u003c\u003e 'all' ORDER BY r.database, r.table, zk.block_numbers_path, zk.partition_id, p.partition_id ) t FORMAT TSVRaw; ","categories":"","description":"","excerpt":"Remove block numbers from zookeeper for removed partitions SELECT …","ref":"/altinity-kb-useful-queries/remove_unneeded_block_numbers/","tags":"","title":"Remove block numbers from zookeeper for removed partitions"},{"body":"Removing tasks in the replication queue related to empty partitions SELECT 'ALTER TABLE ' || database || '.' || table || ' DROP PARTITION ID \\''|| partition_id || '\\';' FROM (SELECT DISTINCT database, table, extract(new_part_name, '^[^_]+') as partition_id FROM clusterAllReplicas('{cluster}', system.replication_queue) ) as rq LEFT JOIN (SELECT database, table, partition_id, sum(rows) as rows_count, count() as part_count FROM clusterAllReplicas('{cluster}', system.parts) WHERE active GROUP BY database, table, partition_id ) as p USING (database, table, partition_id) WHERE p.rows_count = 0 AND p.part_count = 0 FORMAT TSVRaw; ","categories":"","description":"Removing tasks in the replication queue related to empty partitions","excerpt":"Removing tasks in the replication queue related to empty partitions","ref":"/altinity-kb-useful-queries/remove_empty_partitions_from_rq/","tags":"","title":"Removing tasks in the replication queue related to empty partitions"},{"body":"Symptom When configuring Replication the ClickHouse cluster nodes are experiencing communication issues, and an error message appears in the log that states that the ClickHouse host cannot be resolved.\n\u003cError\u003e DNSResolver: Cannot resolve host (xxxxx), error 0: DNS error. auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr \u0026) const: Code: 198. DB::Exception: Not found address of host: xxxx. (DNS_ERROR), Cause: The error message indicates that the host name of the one of the nodes of the cluster cannot be resolved by other cluster members, causing communication issues between the nodes.\nEach node in the replication setup pushes its Fully Qualified Domain Name (FQDN) to Zookeeper, and if other nodes cannot access it using its FQDN, this can cause issues.\nAction: There are two possible solutions to this problem:\nChange the FQDN to allow other nodes to access it. This solution can also help to keep the environment more organized. To do this, use the following command to edit the hostname file: sudo vim /etc/hostname Or use the following command to change the hostname:\nsudo hostnamectl set-hostname ... Use the configuration parameter \u003cinterserver_http_host\u003e to specify the IP address or hostname that the nodes can use to communicate with each other. This solution can have some issues, such as the one described in this link: https://github.com/ClickHouse/ClickHouse/issues/2154. To configure this parameter, refer to the documentation for more information: https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#interserver-http-host. ","categories":"","description":"","excerpt":"Symptom When configuring Replication the ClickHouse cluster nodes are …","ref":"/altinity-kb-setup-and-maintenance/change-me/","tags":"","title":"Replication: Can not resolve host of another clickhouse server"},{"body":"No row policy CREATE TABLE test_delete ( tenant Int64, key Int64, ts DateTime, value_a String ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (tenant, key, ts); INSERT INTO test_delete SELECT number%5, number, toDateTime('2020-01-01')+number/10, concat('some_looong_string', toString(number)), FROM numbers(1e8); INSERT INTO test_delete -- multiple small tenants SELECT number%5000, number, toDateTime('2020-01-01')+number/10, concat('some_looong_string', toString(number)), FROM numbers(1e8); Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 1 │ 20020000 │ │ 2 │ 20020000 │ │ 3 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.285 sec. Processed 200.00 million rows, 1.60 GB (702.60 million rows/s., 5.62 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.265 sec. Processed 20.23 million rows, 863.93 MB (76.33 million rows/s., 3.26 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.062 sec. Processed 20.23 million rows, 242.31 MB (324.83 million rows/s., 3.89 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.009 sec. Processed 212.99 thousand rows, 1.80 MB (24.39 million rows/s., 206.36 MB/s.) row policy using expression CREATE ROW POLICY pol1 ON test_delete USING tenant not in (1,2,3) TO all; Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ │ 6 │ 20000 │ │ 7 │ 20000 │ │ 8 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.333 sec. Processed 140.08 million rows, 1.12 GB (420.59 million rows/s., 3.36 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.287 sec. Processed 20.23 million rows, 863.93 MB (70.48 million rows/s., 3.01 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.080 sec. Processed 20.23 million rows, 242.31 MB (254.20 million rows/s., 3.05 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.011 sec. Processed 212.99 thousand rows, 3.44 MB (19.53 million rows/s., 315.46 MB/s.) Q5) SELECT uniq(value_a) FROM test_delete where tenant = 1; ┌─uniq(value_a)─┐ │ 0 │ └───────────────┘ 1 row in set. Elapsed: 0.008 sec. Processed 180.22 thousand rows, 1.44 MB (23.69 million rows/s., 189.54 MB/s.) DROP ROW POLICY pol1 ON test_delete; row policy using table subquery create table deleted_tenants(tenant Int64) ENGINE=MergeTree order by tenant; CREATE ROW POLICY pol1 ON test_delete USING tenant not in deleted_tenants TO all; SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 1 │ 20020000 │ │ 2 │ 20020000 │ │ 3 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.455 sec. Processed 200.00 million rows, 1.60 GB (439.11 million rows/s., 3.51 GB/s.) insert into deleted_tenants values(1),(2),(3); Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ │ 6 │ 20000 │ │ 7 │ 20000 │ │ 8 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.329 sec. Processed 140.08 million rows, 1.12 GB (426.34 million rows/s., 3.41 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.287 sec. Processed 20.23 million rows, 863.93 MB (70.56 million rows/s., 3.01 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.080 sec. Processed 20.23 million rows, 242.31 MB (251.39 million rows/s., 3.01 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.010 sec. Processed 213.00 thousand rows, 3.44 MB (20.33 million rows/s., 328.44 MB/s.) Q5) SELECT uniq(value_a) FROM test_delete where tenant = 1; ┌─uniq(value_a)─┐ │ 0 │ └───────────────┘ 1 row in set. Elapsed: 0.008 sec. Processed 180.23 thousand rows, 1.44 MB (22.11 million rows/s., 176.90 MB/s.) DROP ROW POLICY pol1 ON test_delete; DROP TABLE deleted_tenants; row policy using external dictionary (NOT dictHas) create table deleted_tenants(tenant Int64, deleted UInt8 default 1) ENGINE=MergeTree order by tenant; insert into deleted_tenants(tenant) values(1),(2),(3); CREATE DICTIONARY deleted_tenants_dict (tenant UInt64, deleted UInt8) PRIMARY KEY tenant SOURCE(CLICKHOUSE(TABLE deleted_tenants)) LIFETIME(600) LAYOUT(FLAT()); CREATE ROW POLICY pol1 ON test_delete USING NOT dictHas('deleted_tenants_dict', tenant) TO all; Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ │ 6 │ 20000 │ │ 7 │ 20000 │ │ 8 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.388 sec. Processed 200.00 million rows, 1.60 GB (515.79 million rows/s., 4.13 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.291 sec. Processed 20.23 million rows, 863.93 MB (69.47 million rows/s., 2.97 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.084 sec. Processed 20.23 million rows, 242.31 MB (240.07 million rows/s., 2.88 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.010 sec. Processed 212.99 thousand rows, 3.44 MB (21.45 million rows/s., 346.56 MB/s.) Q5) SELECT uniq(value_a) FROM test_delete where tenant = 1; ┌─uniq(value_a)─┐ │ 0 │ └───────────────┘ 1 row in set. Elapsed: 0.046 sec. Processed 20.22 million rows, 161.74 MB (440.26 million rows/s., 3.52 GB/s.) DROP ROW POLICY pol1 ON test_delete; DROP DICTIONARY deleted_tenants_dict; DROP TABLE deleted_tenants; row policy using external dictionary (dictHas) create table deleted_tenants(tenant Int64, deleted UInt8 default 1) ENGINE=MergeTree order by tenant; insert into deleted_tenants(tenant) select distinct tenant from test_delete where tenant not in (1,2,3); CREATE DICTIONARY deleted_tenants_dict (tenant UInt64, deleted UInt8) PRIMARY KEY tenant SOURCE(CLICKHOUSE(TABLE deleted_tenants)) LIFETIME(600) LAYOUT(FLAT()); CREATE ROW POLICY pol1 ON test_delete USING dictHas('deleted_tenants_dict', tenant) TO all; Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ │ 6 │ 20000 │ │ 7 │ 20000 │ │ 8 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.399 sec. Processed 200.00 million rows, 1.60 GB (501.18 million rows/s., 4.01 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.284 sec. Processed 20.23 million rows, 863.93 MB (71.30 million rows/s., 3.05 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.080 sec. Processed 20.23 million rows, 242.31 MB (251.88 million rows/s., 3.02 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.010 sec. Processed 212.99 thousand rows, 3.44 MB (22.01 million rows/s., 355.50 MB/s.) Q5) SELECT uniq(value_a) FROM test_delete where tenant = 1; ┌─uniq(value_a)─┐ │ 0 │ └───────────────┘ 1 row in set. Elapsed: 0.034 sec. Processed 20.22 million rows, 161.74 MB (589.90 million rows/s., 4.72 GB/s.) DROP ROW POLICY pol1 ON test_delete; DROP DICTIONARY deleted_tenants_dict; DROP TABLE deleted_tenants; row policy using engine=Set create table deleted_tenants(tenant Int64) ENGINE=Set; insert into deleted_tenants(tenant) values(1),(2),(3); CREATE ROW POLICY pol1 ON test_delete USING tenant not in deleted_tenants TO all; Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ │ 6 │ 20000 │ │ 7 │ 20000 │ │ 8 │ 20000 │ └────────┴──────────┘ 6 rows in set. Elapsed: 0.322 sec. Processed 200.00 million rows, 1.60 GB (621.38 million rows/s., 4.97 GB/s.) Q2) SELECT uniq(value_a) FROM test_delete where tenant = 4; ┌─uniq(value_a)─┐ │ 20016427 │ └───────────────┘ 1 row in set. Elapsed: 0.275 sec. Processed 20.23 million rows, 863.93 MB (73.56 million rows/s., 3.14 GB/s.) Q3) SELECT max(ts) FROM test_delete where tenant = 4; ┌─────────────max(ts)─┐ │ 2020-04-25 17:46:39 │ └─────────────────────┘ 1 row in set. Elapsed: 0.084 sec. Processed 20.23 million rows, 242.31 MB (240.07 million rows/s., 2.88 GB/s.) Q4) SELECT max(ts) FROM test_delete where tenant = 4 and key = 444; ┌─────────────max(ts)─┐ │ 2020-01-01 00:00:44 │ └─────────────────────┘ 1 row in set. Elapsed: 0.010 sec. Processed 212.99 thousand rows, 3.44 MB (20.69 million rows/s., 334.18 MB/s.) Q5) SELECT uniq(value_a) FROM test_delete where tenant = 1; ┌─uniq(value_a)─┐ │ 0 │ └───────────────┘ 1 row in set. Elapsed: 0.030 sec. Processed 20.22 million rows, 161.74 MB (667.06 million rows/s., 5.34 GB/s.) DROP ROW POLICY pol1 ON test_delete; DROP TABLE deleted_tenants; results expression: CREATE ROW POLICY pol1 ON test_delete USING tenant not in (1,2,3) TO all;\ntable subq: CREATE ROW POLICY pol1 ON test_delete USING tenant not in deleted_tenants TO all;\next. dict. NOT dictHas : CREATE ROW POLICY pol1 ON test_delete USING NOT dictHas('deleted_tenants_dict', tenant) TO all;\next. dict. dictHas :\nQ no policy expression table subq ext. dict. NOT ext. dict. engine=Set Q1 0.285 / 200.00m 0.333 / 140.08m 0.329 / 140.08m 0.388 / 200.00m 0.399 / 200.00m 0.322 / 200.00m Q2 0.265 / 20.23m 0.287 / 20.23m 0.287 / 20.23m 0.291 / 20.23m 0.284 / 20.23m 0.275 / 20.23m Q3 0.062 / 20.23m 0.080 / 20.23m 0.080 / 20.23m 0.084 / 20.23m 0.080 / 20.23m 0.084 / 20.23m Q4 0.009 / 212.99t 0.011 / 212.99t 0.010 / 213.00t 0.010 / 212.99t 0.010 / 212.99t 0.010 / 212.99t Q5 0.008 / 180.22t 0.008 / 180.23t 0.046 / 20.22m 0.034 / 20.22m 0.030 / 20.22m Expression in row policy seems to be fastest way (Q1, Q5).\n","categories":"","description":"one more approach how to hide (delete) rows in Clickhouse.\n","excerpt":"one more approach how to hide (delete) rows in Clickhouse.\n","ref":"/altinity-kb-queries-and-syntax/row_policy_using_dictionary/","tags":"","title":"Row policies overhead (hiding 'removed' tenants)"},{"body":"Configuration S3 disk with disabled merges\n\u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cs3disk\u003e \u003ctype\u003es3\u003c/type\u003e \u003cendpoint\u003ehttps://s3.us-east-1.amazonaws.com/mybucket/test/test/\u003c/endpoint\u003e \u003cuse_environment_credentials\u003e1\u003c/use_environment_credentials\u003e \u003c!-- use IAM AWS role --\u003e \u003c!--access_key_id\u003exxxx\u003c/access_key_id\u003e \u003csecret_access_key\u003exxx\u003c/secret_access_key--\u003e \u003c/s3disk\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cs3tiered\u003e \u003cvolumes\u003e \u003cdefault\u003e \u003cdisk\u003edefault\u003c/disk\u003e \u003c/default\u003e \u003cs3disk\u003e \u003cdisk\u003es3disk\u003c/disk\u003e \u003cprefer_not_to_merge\u003etrue\u003c/prefer_not_to_merge\u003e \u003c/s3disk\u003e \u003c/volumes\u003e \u003c/s3tiered\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e Let’s create a table and load some synthetic data.\nCREATE TABLE test_s3 ( `A` Int64, `S` String, `D` Date ) ENGINE = MergeTree PARTITION BY D ORDER BY A SETTINGS storage_policy = 's3tiered'; insert into test_s3 select number, number, today() - intDiv(number, 10000000) from numbers(7e8); 0 rows in set. Elapsed: 98.091 sec. Processed 700.36 million rows, 5.60 GB (7.14 million rows/s., 57.12 MB/s.) select disk_name, partition, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table= 'test_s3' and active group by disk_name, partition order by partition; ┌─disk_name─┬─partition──┬─sum(rows)─┬─size──────┬─part_count─┐ │ default │ 2023-05-06 │ 10000000 │ 78.23 MiB │ 5 │ │ default │ 2023-05-07 │ 10000000 │ 78.31 MiB │ 6 │ │ default │ 2023-05-08 │ 10000000 │ 78.16 MiB │ 5 │ .... │ default │ 2023-07-12 │ 10000000 │ 78.21 MiB │ 5 │ │ default │ 2023-07-13 │ 10000000 │ 78.23 MiB │ 6 │ │ default │ 2023-07-14 │ 10000000 │ 77.39 MiB │ 5 │ └───────────┴────────────┴───────────┴───────────┴────────────┘ 70 rows in set. Elapsed: 0.023 sec. Perfomance of mutations for a local EBS (throughput: 500 MB/s) select * from test_s3 where A=490000000; 1 row in set. Elapsed: 0.020 sec. Processed 8.19 thousand rows, 92.67 KB (419.17 thousand rows/s., 4.74 MB/s.) select * from test_s3 where S='490000000'; 1 row in set. Elapsed: 14.117 sec. Processed 700.00 million rows, 12.49 GB (49.59 million rows/s., 884.68 MB/s.) delete from test_s3 where S = '490000000'; 0 rows in set. Elapsed: 22.192 sec. delete from test_s3 where A = '490000001'; 0 rows in set. Elapsed: 2.243 sec. alter table test_s3 delete where S = 590000000 settings mutations_sync=2; 0 rows in set. Elapsed: 21.387 sec. alter table test_s3 delete where A = '590000001' settings mutations_sync=2; 0 rows in set. Elapsed: 3.372 sec. alter table test_s3 update S='' where S = '690000000' settings mutations_sync=2; 0 rows in set. Elapsed: 20.265 sec. alter table test_s3 update S='' where A = '690000001' settings mutations_sync=2; 0 rows in set. Elapsed: 1.979 sec. Let’s move data to S3 alter table test_s3 modify TTL D + interval 10 day to disk 's3disk'; -- 10 minutes later ┌─disk_name─┬─partition──┬─sum(rows)─┬─size──────┬─part_count─┐ │ s3disk │ 2023-05-06 │ 10000000 │ 78.23 MiB │ 5 │ │ s3disk │ 2023-05-07 │ 10000000 │ 78.31 MiB │ 6 │ │ s3disk │ 2023-05-08 │ 10000000 │ 78.16 MiB │ 5 │ │ s3disk │ 2023-05-09 │ 10000000 │ 78.21 MiB │ 6 │ │ s3disk │ 2023-05-10 │ 10000000 │ 78.21 MiB │ 6 │ ... │ s3disk │ 2023-07-02 │ 10000000 │ 78.22 MiB │ 5 │ ... │ default │ 2023-07-11 │ 10000000 │ 78.20 MiB │ 6 │ │ default │ 2023-07-12 │ 10000000 │ 78.21 MiB │ 5 │ │ default │ 2023-07-13 │ 10000000 │ 78.23 MiB │ 6 │ │ default │ 2023-07-14 │ 10000000 │ 77.40 MiB │ 5 │ └───────────┴────────────┴───────────┴───────────┴────────────┘ 70 rows in set. Elapsed: 0.007 sec. Sizes of a table on S3 and a size of each column select sum(rows), formatReadableSize(sum(bytes_on_disk)) size from system.parts where table= 'test_s3' and active and disk_name = 's3disk'; ┌─sum(rows)─┬─size─────┐ │ 600000000 │ 4.58 GiB │ └───────────┴──────────┘ SELECT database, table, column, formatReadableSize(sum(column_data_compressed_bytes) AS size) AS compressed FROM system.parts_columns WHERE (active = 1) AND (database LIKE '%') AND (table LIKE 'test_s3') AND (disk_name = 's3disk') GROUP BY database, table, column ORDER BY column ASC ┌─database─┬─table───┬─column─┬─compressed─┐ │ default │ test_s3 │ A │ 2.22 GiB │ │ default │ test_s3 │ D │ 5.09 MiB │ │ default │ test_s3 │ S │ 2.33 GiB │ └──────────┴─────────┴────────┴────────────┘ S3 Statistics of selects select *, _part from test_s3 where A=100000000; ┌─────────A─┬─S─────────┬──────────D─┬─_part──────────────────┐ │ 100000000 │ 100000000 │ 2023-07-08 │ 20230708_106_111_1_738 │ └───────────┴───────────┴────────────┴────────────────────────┘ 1 row in set. Elapsed: 0.104 sec. Processed 8.19 thousand rows, 65.56 KB (79.11 thousand rows/s., 633.07 KB/s.) ┌─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ 6 │ 0 │ 70.58 KiB │ 0.00 B │ └─────────────┴─────────────┴──────────────────┴───────────────────┘ Select by primary key read only 70.58 KiB from S3\nSize of this part\nSELECT database, table, column, formatReadableSize(sum(column_data_compressed_bytes) AS size) AS compressed FROM system.parts_columns WHERE (active = 1) AND (database LIKE '%') AND (table LIKE 'test_s3') AND (disk_name = 's3disk') and name = '20230708_106_111_1_738' GROUP BY database, table, column ORDER BY column ASC ┌─database─┬─table───┬─column─┬─compressed─┐ │ default │ test_s3 │ A │ 22.51 MiB │ │ default │ test_s3 │ D │ 51.47 KiB │ │ default │ test_s3 │ S │ 23.52 MiB │ └──────────┴─────────┴────────┴────────────┘ select * from test_s3 where S='100000000'; ┌─────────A─┬─S─────────┬──────────D─┐ │ 100000000 │ 100000000 │ 2023-07-08 │ └───────────┴───────────┴────────────┘ 1 row in set. Elapsed: 86.745 sec. Processed 700.00 million rows, 12.49 GB (8.07 million rows/s., 144.04 MB/s.) ┌─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ 947 │ 0 │ 2.36 GiB │ 0.00 B │ └─────────────┴─────────────┴──────────────────┴───────────────────┘ Select using fullscan of S column read only 2.36 GiB from S3, the whole S column (2.33 GiB) plus parts of A and D.\ndelete from test_s3 where A=100000000; 0 rows in set. Elapsed: 17.429 sec. ┌─q──┬─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ Q3 │ 2981 │ 6 │ 23.06 MiB │ 27.25 KiB │ └────┴─────────────┴─────────────┴──────────────────┴───────────────────┘ insert into test select 'Q3' q, event,value from system.events where event like '%S3%'; delete from test_s3 where S='100000001'; 0 rows in set. Elapsed: 31.417 sec. ┌─q──┬─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ Q4 │ 4209 │ 6 │ 2.39 GiB │ 27.25 KiB │ └────┴─────────────┴─────────────┴──────────────────┴───────────────────┘ insert into test select 'Q4' q, event,value from system.events where event like '%S3%'; alter table test_s3 delete where A=110000000 settings mutations_sync=2; 0 rows in set. Elapsed: 19.521 sec. ┌─q──┬─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ Q5 │ 2986 │ 15 │ 42.27 MiB │ 41.72 MiB │ └────┴─────────────┴─────────────┴──────────────────┴───────────────────┘ insert into test select 'Q5' q, event,value from system.events where event like '%S3%'; alter table test_s3 delete where S='110000001' settings mutations_sync=2; 0 rows in set. Elapsed: 29.650 sec. ┌─q──┬─S3GetObject─┬─S3PutObject─┬─ReadBufferFromS3─┬─WriteBufferFromS3─┐ │ Q6 │ 4212 │ 15 │ 2.42 GiB │ 41.72 MiB │ └────┴─────────────┴─────────────┴──────────────────┴───────────────────┘ insert into test select 'Q6' q, event,value from system.events where event like '%S3%'; ","categories":"","description":"Example of how much data Clickhouse reads and writes to s3 during mutations.","excerpt":"Example of how much data Clickhouse reads and writes to s3 during …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_and_mutations/","tags":"","title":"How much data are written to S3 during mutations"},{"body":"Storage configuration cat /etc/clickhouse-server/config.d/s3.xml \u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cs3disk\u003e \u003ctype\u003es3\u003c/type\u003e \u003cendpoint\u003ehttps://s3.us-east-1.amazonaws.com/mybucket/test/s3cached/\u003c/endpoint\u003e \u003cuse_environment_credentials\u003e1\u003c/use_environment_credentials\u003e \u003c!-- use IAM AWS role --\u003e \u003c!--access_key_id\u003exxxx\u003c/access_key_id\u003e \u003csecret_access_key\u003exxx\u003c/secret_access_key--\u003e \u003c/s3disk\u003e \u003ccache\u003e \u003ctype\u003ecache\u003c/type\u003e \u003cdisk\u003es3disk\u003c/disk\u003e \u003cpath\u003e/var/lib/clickhouse/disks/s3_cache/\u003c/path\u003e \u003cmax_size\u003e50Gi\u003c/max_size\u003e \u003c!-- 50GB local cache to cache remote data --\u003e \u003c/cache\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cs3tiered\u003e \u003cvolumes\u003e \u003cdefault\u003e \u003cdisk\u003edefault\u003c/disk\u003e \u003cmax_data_part_size_bytes\u003e50000000000\u003c/max_data_part_size_bytes\u003e \u003c!-- only for parts less than 50GB after they moved to s3 during merges --\u003e \u003c/default\u003e \u003cs3cached\u003e \u003cdisk\u003ecache\u003c/disk\u003e \u003c!-- sandwich cache plus s3disk --\u003e \u003c!-- prefer_not_to_merge\u003etrue\u003c/prefer_not_to_merge\u003e \u003cperform_ttl_move_on_insert\u003efalse\u003c/perform_ttl_move_on_insert--\u003e \u003c/s3cached\u003e \u003c/volumes\u003e \u003c/s3tiered\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e select * from system.disks ┌─name────┬─path──────────────────────────────┬───────────free_space─┬──────────total_space─┬ │ cache │ /var/lib/clickhouse/disks/s3disk/ │ 18446744073709551615 │ 18446744073709551615 │ │ default │ /var/lib/clickhouse/ │ 149113987072 │ 207907635200 │ │ s3disk │ /var/lib/clickhouse/disks/s3disk/ │ 18446744073709551615 │ 18446744073709551615 │ └─────────┴───────────────────────────────────┴──────────────────────┴──────────────────────┴ select * from system.storage_policies; ┌─policy_name─┬─volume_name─┬─volume_priority─┬─disks───────┬─volume_type─┬─max_data_part_size─┬─move_factor─┬─prefer_not_to_merge─┐ │ default │ default │ 1 │ ['default'] │ JBOD │ 0 │ 0 │ 0 │ │ s3tiered │ default │ 1 │ ['default'] │ JBOD │ 50000000000 │ 0.1 │ 0 │ │ s3tiered │ s3cached │ 2 │ ['s3disk'] │ JBOD │ 0 │ 0.1 │ 0 │ └─────────────┴─────────────┴─────────────────┴─────────────┴─────────────┴────────────────────┴─────────────┴─────────────────────┘ example with a new table CREATE TABLE test_s3 ( `A` Int64, `S` String, `D` Date ) ENGINE = MergeTree PARTITION BY D ORDER BY A SETTINGS storage_policy = 's3tiered'; insert into test_s3 select number, number, '2023-01-01' from numbers(1e9); 0 rows in set. Elapsed: 270.285 sec. Processed 1.00 billion rows, 8.00 GB (3.70 million rows/s., 29.60 MB/s.) Table size is 7.65 GiB and it at the default disk (EBS):\nselect disk_name, partition, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table= 'test_s3' and active group by disk_name, partition; ┌─disk_name─┬─partition──┬──sum(rows)─┬─size─────┬─part_count─┐ │ default │ 2023-01-01 │ 1000000000 │ 7.65 GiB │ 8 │ └───────────┴────────────┴────────────┴──────────┴────────────┘ It seems my EBS write speed is slower than S3 write speed:\nalter table test_s3 move partition '2023-01-01' to volume 's3cached'; 0 rows in set. Elapsed: 98.979 sec. alter table test_s3 move partition '2023-01-01' to volume 'default'; 0 rows in set. Elapsed: 127.741 sec. Queries performance against EBS:\nselect * from test_s3 where A = 443; 1 row in set. Elapsed: 0.002 sec. Processed 8.19 thousand rows, 71.64 KB (3.36 million rows/s., 29.40 MB/s.) select uniq(A) from test_s3; 1 row in set. Elapsed: 11.439 sec. Processed 1.00 billion rows, 8.00 GB (87.42 million rows/s., 699.33 MB/s.) select count() from test_s3 where S like '%4422%' 1 row in set. Elapsed: 17.484 sec. Processed 1.00 billion rows, 17.89 GB (57.20 million rows/s., 1.02 GB/s.) Let’s move data to S3\nalter table test_s3 move partition '2023-01-01' to volume 's3cached'; 0 rows in set. Elapsed: 81.068 sec. select disk_name, partition, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table= 'test_s3' and active group by disk_name, partition; ┌─disk_name─┬─partition──┬──sum(rows)─┬─size─────┬─part_count─┐ │ s3disk │ 2023-01-01 │ 1000000000 │ 7.65 GiB │ 8 │ └───────────┴────────────┴────────────┴──────────┴────────────┘ The first query execution against S3, the second against the cache (local EBS):\nselect * from test_s3 where A = 443; 1 row in set. Elapsed: 0.458 sec. Processed 8.19 thousand rows, 71.64 KB (17.88 thousand rows/s., 156.35 KB/s.) 1 row in set. Elapsed: 0.003 sec. Processed 8.19 thousand rows, 71.64 KB (3.24 million rows/s., 28.32 MB/s.) select uniq(A) from test_s3; 1 row in set. Elapsed: 26.601 sec. Processed 1.00 billion rows, 8.00 GB (37.59 million rows/s., 300.74 MB/s.) 1 row in set. Elapsed: 8.675 sec. Processed 1.00 billion rows, 8.00 GB (115.27 million rows/s., 922.15 MB/s.) select count() from test_s3 where S like '%4422%' 1 row in set. Elapsed: 33.586 sec. Processed 1.00 billion rows, 17.89 GB (29.77 million rows/s., 532.63 MB/s.) 1 row in set. Elapsed: 16.551 sec. Processed 1.00 billion rows, 17.89 GB (60.42 million rows/s., 1.08 GB/s.) Cache introspection\nselect cache_base_path, formatReadableSize(sum(size)) from system.filesystem_cache group by 1; ┌─cache_base_path─────────────────────┬─formatReadableSize(sum(size))─┐ │ /var/lib/clickhouse/disks/s3_cache/ │ 7.64 GiB │ └─────────────────────────────────────┴───────────────────────────────┘ system drop FILESYSTEM cache; select cache_base_path, formatReadableSize(sum(size)) from system.filesystem_cache group by 1; 0 rows in set. Elapsed: 0.005 sec. select * from test_s3 where A = 443; 1 row in set. Elapsed: 0.221 sec. Processed 8.19 thousand rows, 71.64 KB (37.10 thousand rows/s., 324.47 KB/s.) select cache_base_path, formatReadableSize(sum(size)) from system.filesystem_cache group by 1; ┌─cache_base_path─────────────────────┬─formatReadableSize(sum(size))─┐ │ /var/lib/clickhouse/disks/s3_cache/ │ 105.95 KiB │ └─────────────────────────────────────┴───────────────────────────────┘ No data is stored locally (except system log tables).\nselect name, formatReadableSize(free_space) free_space, formatReadableSize(total_space) total_space from system.disks; ┌─name────┬─free_space─┬─total_space─┐ │ cache │ 16.00 EiB │ 16.00 EiB │ │ default │ 48.97 GiB │ 49.09 GiB │ │ s3disk │ 16.00 EiB │ 16.00 EiB │ └─────────┴────────────┴─────────────┘ example with an existing table The mydata table is created without the explicitly defined storage_policy, it means that implicitly storage_policy=default / volume=default / disk=default.\nselect disk_name, partition, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table='mydata' and active group by disk_name, partition order by partition; ┌─disk_name─┬─partition─┬─sum(rows)─┬─size───────┬─part_count─┐ │ default │ 202201 │ 516666677 │ 4.01 GiB │ 13 │ │ default │ 202202 │ 466666657 │ 3.64 GiB │ 13 │ │ default │ 202203 │ 16666666 │ 138.36 MiB │ 10 │ │ default │ 202301 │ 516666677 │ 4.01 GiB │ 10 │ │ default │ 202302 │ 466666657 │ 3.64 GiB │ 10 │ │ default │ 202303 │ 16666666 │ 138.36 MiB │ 10 │ └───────────┴───────────┴───────────┴────────────┴────────────┘ -- Let's change the storage policy, this command instant and changes only metadata of the table, and possible because the new storage policy and the old has the volume `default`. alter table mydata modify setting storage_policy = 's3tiered'; 0 rows in set. Elapsed: 0.057 sec. straightforward (heavy) approach -- Let's add TTL, it's a heavy command and takes a lot time and creates the performance impact, because it reads `D` column and moves parts to s3. alter table mydata modify TTL D + interval 1 year to volume 's3cached'; 0 rows in set. Elapsed: 140.661 sec. ┌─disk_name─┬─partition─┬─sum(rows)─┬─size───────┬─part_count─┐ │ s3disk │ 202201 │ 516666677 │ 4.01 GiB │ 13 │ │ s3disk │ 202202 │ 466666657 │ 3.64 GiB │ 13 │ │ s3disk │ 202203 │ 16666666 │ 138.36 MiB │ 10 │ │ default │ 202301 │ 516666677 │ 4.01 GiB │ 10 │ │ default │ 202302 │ 466666657 │ 3.64 GiB │ 10 │ │ default │ 202303 │ 16666666 │ 138.36 MiB │ 10 │ └───────────┴───────────┴───────────┴────────────┴────────────┘ gentle (manual) approach -- alter modify TTL changes only metadata of the table and applied to only newly insterted data. set materialize_ttl_after_modify=0; alter table mydata modify TTL D + interval 1 year to volume 's3cached'; 0 rows in set. Elapsed: 0.049 sec. -- move data slowly partition by partition alter table mydata move partition id '202201' to volume 's3cached'; 0 rows in set. Elapsed: 49.410 sec. alter table mydata move partition id '202202' to volume 's3cached'; 0 rows in set. Elapsed: 36.952 sec. alter table mydata move partition id '202203' to volume 's3cached'; 0 rows in set. Elapsed: 4.808 sec. -- data can be optimized to reduce number of parts before moving it to s3 optimize table mydata partition id '202301' final; 0 rows in set. Elapsed: 66.551 sec. alter table mydata move partition id '202301' to volume 's3cached'; 0 rows in set. Elapsed: 33.332 sec. ┌─disk_name─┬─partition─┬─sum(rows)─┬─size───────┬─part_count─┐ │ s3disk │ 202201 │ 516666677 │ 4.01 GiB │ 13 │ │ s3disk │ 202202 │ 466666657 │ 3.64 GiB │ 13 │ │ s3disk │ 202203 │ 16666666 │ 138.36 MiB │ 10 │ │ s3disk │ 202301 │ 516666677 │ 4.01 GiB │ 1 │ -- optimized partition │ default │ 202302 │ 466666657 │ 3.64 GiB │ 13 │ │ default │ 202303 │ 16666666 │ 138.36 MiB │ 10 │ └───────────┴───────────┴───────────┴────────────┴────────────┘ S3 and Clickhouse start time Let’s create a table with 1000 parts and move them to s3.\nCREATE TABLE test_s3( A Int64, S String, D Date) ENGINE = MergeTree PARTITION BY D ORDER BY A SETTINGS storage_policy = 's3tiered'; insert into test_s3 select number, number, toDate('2000-01-01') + intDiv(number,1e6) from numbers(1e9); optimize table test_s3 final settings optimize_skip_merged_partitions = 1; select disk_name, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table= 'test_s3' and active group by disk_name; ┌─disk_name─┬──sum(rows)─┬─size─────┬─part_count─┐ │ default │ 1000000000 │ 7.64 GiB │ 1000 │ └───────────┴────────────┴──────────┴────────────┘ alter table test_s3 modify ttl D + interval 1 year to disk 's3disk'; select disk_name, sum(rows), formatReadableSize(sum(bytes_on_disk)) size, count() part_count from system.parts where table= 'test_s3' and active group by disk_name; ┌─disk_name─┬─sum(rows)─┬─size─────┬─part_count─┐ │ default │ 755000000 │ 5.77 GiB │ 755 │ │ s3disk │ 245000000 │ 1.87 GiB │ 245 │ └───────────┴───────────┴──────────┴────────────┘ ---- several minutes later ---- ┌─disk_name─┬──sum(rows)─┬─size─────┬─part_count─┐ │ s3disk │ 1000000000 │ 7.64 GiB │ 1000 │ └───────────┴────────────┴──────────┴────────────┘ start time :) select name, value from system.merge_tree_settings where name = 'max_part_loading_threads'; ┌─name─────────────────────┬─value─────┐ │ max_part_loading_threads │ 'auto(4)' │ └──────────────────────────┴───────────┘ # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t4m26.766s # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t4m24.263s # cat /etc/clickhouse-server/config.d/max_part_loading_threads.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cmerge_tree\u003e \u003cmax_part_loading_threads\u003e128\u003c/max_part_loading_threads\u003e \u003c/merge_tree\u003e \u003c/clickhouse\u003e # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t0m11.225s # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t0m10.797s \u003cmax_part_loading_threads\u003e256\u003c/max_part_loading_threads\u003e # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t0m8.474s # systemctl stop clickhouse-server # time systemctl start clickhouse-server / real\t0m8.130s ","categories":"","description":"s3 disk and s3 cache.","excerpt":"s3 disk and s3 cache.","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_cache_example/","tags":"","title":"Example of the table at s3 with cache"},{"body":"Settings \u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cs3\u003e \u003ctype\u003es3\u003c/type\u003e \u003cendpoint\u003ehttp://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/\u003c/endpoint\u003e \u003caccess_key_id\u003eACCESS_KEY_ID\u003c/access_key_id\u003e \u003csecret_access_key\u003eSECRET_ACCESS_KEY\u003c/secret_access_key\u003e \u003cskip_access_check\u003etrue\u003c/skip_access_check\u003e \u003csend_metadata\u003etrue\u003c/send_metadata\u003e \u003c/s3\u003e \u003c/disks\u003e \u003c/storage_configuration\u003e \u003c/clickhouse\u003e skip_access_check — if true, it’s possible to use read only credentials with regular MergeTree table. But you would need to disable merges (prefer_not_to_merge setting) on s3 volume as well.\nsend_metadata — if true, ClickHouse will populate s3 object with initial part \u0026 file path, which allow you to recover metadata from s3 and make debug easier.\nRestore metadata from S3 Default Limitations:\nClickHouse need RW access to this bucket In order to restore metadata, you would need to create restore file in metadata_path/_s3_disk_name_ directory:\ntouch /var/lib/clickhouse/disks/_s3_disk_name_/restore In that case ClickHouse would restore to the same bucket and path and update only metadata files in s3 bucket.\nCustom Limitations:\nClickHouse needs RO access to the old bucket and RW to the new. ClickHouse will copy objects in case of restoring to a different bucket or path. If you would like to change bucket or path, you need to populate restore file with settings in key=value format:\ncat /var/lib/clickhouse/disks/_s3_disk_name_/restore source_bucket=s3disk source_path=vol1/ Links https://altinity.com/blog/integrating-clickhouse-with-minio https://altinity.com/blog/clickhouse-object-storage-performance-minio-vs-aws-s3 https://altinity.com/blog/tips-for-high-performance-clickhouse-clusters-with-s3-object-storage ","categories":"","description":"","excerpt":"Settings \u003cclickhouse\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003cs3\u003e …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3disk/","tags":"","title":"S3Disk"},{"body":"Why is simple SELECT count() Slow in ClickHouse? ClickHouse is a columnar database that provides excellent performance for analytical queries. However, in some cases, a simple count query can be slow. In this article, we’ll explore the reasons why this can happen and how to optimize the query.\nThree Strategies for Counting Rows in ClickHouse There are three ways to count rows in a table in ClickHouse:\noptimize_trivial_count_query: This strategy extracts the number of rows from the table metadata. It’s the fastest and most efficient way to count rows, but it only works for simple count queries.\nallow_experimental_projection_optimization: This strategy uses a virtual projection called _minmax_count_projection to count rows. It’s faster than scanning the table but slower than the trivial count query.\nScanning the smallest column in the table and reading rows from that. This is the slowest strategy and is only used when the other two strategies can’t be used.\nWhy Does ClickHouse Sometimes Choose the Slowest Counting Strategy? In some cases, ClickHouse may choose the slowest counting strategy even when there are faster options available. Here are some possible reasons why this can happen:\nRow policies are used on the table: If row policies are used, ClickHouse needs to filter rows to give the proper count. You can check if row policies are used by selecting from system.row_policies.\nExperimental light-weight delete feature was used on the table: If the experimental light-weight delete feature was used, ClickHouse may use the slowest counting strategy. You can check this by looking into parts_columns for the column named _row_exists. To do this, run the following query:\nSELECT DISTINCT database, table FROM system.parts_columns WHERE column = '_row_exists'; You can also refer to this issue on GitHub for more information: https://github.com/ClickHouse/ClickHouse/issues/47930.\nSELECT FINAL or final=1 setting is used.\nmax_parallel_replicas \u003e 1 is used.\nSampling is used.\nSome other features like allow_experimental_query_deduplication or empty_result_for_aggregation_by_empty_set is used.\n","categories":"","description":"","excerpt":"Why is simple SELECT count() Slow in ClickHouse? ClickHouse is a …","ref":"/altinity-kb-queries-and-syntax/slow_select_count/","tags":"","title":"Why is simple `SELECT count()` Slow in ClickHouse?"},{"body":"Symptom I see messages like: source parts size (...) is greater than the current maximum (...) in the logs and/or inside system.replication_queue\nCause Usually that means that there are already few big merges running. You can see the running merges using the query:\nSELECT * FROM system.merges That logic is needed to prevent picking a log of huge merges simultaneously (otherwise they will take all available slots and clickhouse will not be able to do smaller merges, which usally are important for keeping the number of parts stable).\nAction It is normal to see those messages on some stale replicas. And it should be resolved automatically after some time. So just wait \u0026 monitor system.merges \u0026 system.replication_queue tables, it should be resolved by it’s own.\nIf it happens often or don’t resolves by it’s own during some longer period of time, it could be caused by:\nincreased insert pressure disk issues / high load (it works slow, not enought space etc.) high CPU load (not enough CPU power to catch up with merges) issue with table schemas leading to high merges pressure (high / increased number of tables / partitions / etc.) Start from checking dmesg / system journals / clickhouse monitoring to find the anomalies.\n","categories":"","description":"source parts size (...) is greater than the current maximum (...)","excerpt":"source parts size (...) is greater than the current maximum (...)","ref":"/altinity-kb-setup-and-maintenance/source-pars-size-is-greater-than-maximum/","tags":"","title":"source parts size is greater than the current maximum"},{"body":"ClickHouse + Spark jdbc The trivial \u0026 natural way to talk to ClickHouse from Spark is using jdbc. There are 2 jdbc drivers:\nhttps://github.com/ClickHouse/clickhouse-jdbc/ https://github.com/housepower/ClickHouse-Native-JDBC#integration-with-spark ClickHouse-Native-JDBC has some hints about integration with Spark even in the main README file.\n‘Official’ driver does support some conversion of complex data types (Roarring bitmaps) for Spark-Clickhouse integration: https://github.com/ClickHouse/clickhouse-jdbc/pull/596\nBut proper partitioning of the data (to spark partitions) may be tricky with jdbc.\nSome example snippets:\nhttps://markelic.de/how-to-access-your-clickhouse-database-with-spark-in-python/ https://stackoverflow.com/questions/60448877/how-can-i-write-spark-dataframe-to-clickhouse Connectors https://github.com/DmitryBe/spark-clickhouse (looks dead) https://github.com/VaBezruchko/spark-clickhouse-connector (is not actively maintained). https://github.com/housepower/spark-clickhouse-connector (actively developing connector from housepower - same guys as authors of ClickHouse-Native-JDBC) via Kafka ClickHouse can produce / consume data from/to Kafka to exchange data with Spark.\nvia hdfs You can load data into hadoop/hdfs using sequence of statements like INSERT INTO FUNCTION hdfs(...) SELECT ... FROM clickhouse_table later process the data from hdfs by spark and do the same in reverse direction.\nvia s3 Similar to above but using s3.\nvia shell calls You can call other commands from Spark. Those commands can be clickhouse-client and/or clickhouse-local.\ndo you really need Spark? :) In many cases you can do everything inside ClickHouse without Spark help :) Arrays, Higher-order functions, machine learning, integration with lot of different things including the possibility to run some external code using executable dictionaries or UDF.\nMore info + some unordered links (mostly in Chinese / Russian) Spark + ClickHouse: not a fight, but a symbiosis (Russian) https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/spark_and_clickhouse.pdf (russian) Using a bunch of ClickHouse and Spark in MFI Soft (Russian) https://www.youtube.com/watch?v=ID8eTnmag0s (russian) Spark read and write ClickHouse (Chinese: Spark读写ClickHouse) https://yerias.github.io/2020/12/08/clickhouse/9/#Jdbc%E6%93%8D%E4%BD%9Cclickhouse Spark JDBC write clickhouse operation summary (Chinese: Spark JDBC 写 clickhouse 操作总结) https://www.jianshu.com/p/43f78c8a025b?hmsr=toutiao.io\u0026utm_campaign=toutiao.io\u0026utm_medium=toutiao.io\u0026utm_source=toutiao.io Spark-sql is based on Clickhouse’s DataSourceV2 data source extension (Chinese: spark-sql基于Clickhouse的DataSourceV2数据源扩展) https://www.cnblogs.com/mengyao/p/4689866.html Alibaba integration instructions (English) https://www.alibabacloud.com/help/doc-detail/191192.htm Tencent integration instructions (English) https://intl.cloud.tencent.com/document/product/1026/35884 Yandex DataProc demo: loading files from S3 to ClickHouse with Spark (Russian) https://www.youtube.com/watch?v=N3bZW0_rRzI Clickhouse official documentation_Spark JDBC writes some pits of ClickHouse (Chinese: clickhouse官方文档_Spark JDBC写ClickHouse的一些坑) https://blog.csdn.net/weixin_39615984/article/details/111206050 ClickHouse data import: Flink, Spark, Kafka, MySQL, Hive (Chinese: 篇五|ClickHouse数据导入 Flink、Spark、Kafka、MySQL、Hive) https://zhuanlan.zhihu.com/p/299094269 Baifendian Big Data Technical Team: Practice of ClickHouse data synchronization solutionbased on multiple Spark tasks (Chinese: 百分点大数据技术团队：基于多 Spark 任务的 ClickHouse 数据同步方案实践) https://www.6aiq.com/article/1635461873075 SPARK-CLICKHOUSE-ES REAL-TIME PROJECT EIGHTH DAY-PRECISE ONE-TIME CONSUMPTION SAVE OFFSET. (Chinese: SPARK-CLICKHOUSE-ES实时项目第八天-精确一次性消费保存偏移量) https://www.freesion.com/article/71421322524/ HDFS+ClickHouse+Spark: A lightweight big data analysis system from 0 to 1. (Chinese: HDFS+ClickHouse+Spark：从0到1实现一款轻量级大数据分析系统) https://juejin.cn/post/6850418114962653198 ClickHouse Clustering for Spark Developer (English) http://blog.madhukaraphatak.com/clickouse-clustering-spark-developer/ «Иногда приходится заглядывать в код Spark»: Александр Морозов (SEMrush) об использовании Scala, Spark и ClickHouse. (Russian) https://habr.com/ru/company/jugru/blog/341288/ ","categories":"","description":"Spark","excerpt":"Spark","ref":"/altinity-kb-integrations/spark/","tags":"","title":"ClickHouse + Spark"},{"body":"Successful ClickHouse deployment plan Stage 0. Build POC Install single node clickhouse https://clickhouse.com/docs/en/getting-started/tutorial/ https://clickhouse.com/docs/en/getting-started/install/ https://docs.altinity.com/altinitystablebuilds/stablequickstartguide/ Start with creating a single table (the biggest one), use MergeTree engine. Create ‘some’ schema (most probably it will be far from optimal). Prefer denormalized approach for all immutable dimensions, for mutable dimensions - consider dictionaries. Load some amount of data (at least 5 Gb, and 10 mln rows) - preferable the real one, or as close to real as possible. Usully the simplest options are either through CSV / TSV files (or insert into clickhouse_table select * FROM mysql(...) where ...) Create several representative queries. Check the columns cardinality, and appropriate types, use minimal needed type Review the partition by and order by. https://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/ Create the schema(s) with better/promising order by / partitioning, load data in. Pick the best schema. consider different improvements of particular columns (codecs / better data types etc.) https://kb.altinity.com/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/ If the performance of certain queries is not enough - consider using PREWHERE / skipping indexes Repeat 2-9 for next big table(s). Avoid scenarios when you need to join big tables. Pick the clients library for you programming language (the most mature are python / golang / java / c++), build some pipeline - for inserts (low QPS, lot of rows in singe insert, check acknowledgements \u0026 retry the same block on failures), ETLs if needed, some reporting layer (https://kb.altinity.com/altinity-kb-integrations/bi-tools/) Stage 1. Planning the production setup Collect more data / estimate insert speed, estimate the column sizes per day / month. Measure the speed of queries Consider improvement using materialized views / projections / dictionaries. Collect requirements (ha / number of simultaneous queries / insert pressure / ’exactly once’ etc) Do a cluster sizing estimation, plan the hardware https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/ https://blog.cloudflare.com/clickhouse-capacity-estimation-framework/ plan the network, if needed - consider using LoadBalancers etc. https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/ If you need sharding - consider different sharding approaches. Stage 2. Preprod setup \u0026 developement Install clickhouse in cluster - several nodes / VMs + zookeeper https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/ https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/ https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/ Create good config \u0026 automate config / os / restarts (ansible / puppet etc) https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/ for docker: https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/ for k8, use clickhouse-operator OR https://kb.altinity.com/altinity-kb-kubernetes/altinity-kb-possible-issues-with-running-clickhouse-in-k8s/ Set up monitoring / log processing / alerts etc. https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/#build-your-own-monitoring Set up users. https://kb.altinity.com/altinity-kb-setup-and-maintenance/rbac/ Think of schema management. Deploy the schema. https://kb.altinity.com/altinity-kb-setup-and-maintenance/schema-migration-tools/ Design backup / failover strategies: https://clickhouse.com/docs/en/operations/backup/ https://github.com/AlexAkulov/clickhouse-backup Develop pipelines / queries, create test suite, CI/CD Do benchmark / stress tests Test configuration changes / server restarts / failovers / version upgrades Review the security topics (tls, limits / restrictions, network, passwords) Document the solution for operations Stage 3. Production setup Deploy the production setup (consider also canary / blue-greed deployments etc) Schedule ClickHouse upgrades every 6 to 12 months (if possible) ","categories":"","description":"Successful ClickHouse deployment plan.","excerpt":"Successful ClickHouse deployment plan.","ref":"/altinity-kb-setup-and-maintenance/clickhouse-deployment-plan/","tags":"","title":"Successful ClickHouse deployment plan"},{"body":"Requirements The idea is that you have a macros cluster with cluster name.\nFor example you have a cluster named production and this cluster includes all ClickHouse nodes.\n$ cat /etc/clickhouse-server/config.d/clusters.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cremote_servers\u003e \u003cproduction\u003e \u003cshard\u003e ... And you need to have a macro cluster set to production:\ncat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cmacros\u003e \u003ccluster\u003eproduction\u003c/cluster\u003e \u003creplica\u003e....\u003c/replica\u003e .... \u003c/macros\u003e \u003c/yandex\u003e Now you should be able to query all nodes using clusterAllReplicas:\nSELECT hostName(), FQDN(), materialize(uptime()) AS uptime FROM clusterAllReplicas('{cluster}', system.one) SETTINGS skip_unavailable_shards = 1 ┌─hostName()─┬─FQDN()──────────────┬──uptime─┐ │ chhost1 │ chhost1.localdomain │ 1071574 │ │ chhost2 │ chhost2.localdomain │ 1071517 │ └────────────┴─────────────────────┴─────────┘ skip_unavailable_shards is necessary to query a system with some nodes are down.\nScript to create DB objects clickhouse-client -q 'show tables from system'\u003e list for i in `cat list`; do echo \"CREATE OR REPLACE VIEW sysall.\"$i\" as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.\"$i\") SETTINGS skip_unavailable_shards = 1;\"; done; CREATE DATABASE sysall; CREATE OR REPLACE VIEW sysall.cluster_state AS SELECT shard_num, replica_num, host_name, host_address, port, errors_count, uptime, if(uptime \u003e 0, 'UP', 'DOWN') AS node_state FROM system.clusters LEFT JOIN ( SELECT hostName() AS host_name, FQDN() AS fqdn, materialize(uptime()) AS uptime FROM clusterAllReplicas('{cluster}', system.one) ) as hosts_info USING (host_name) WHERE cluster = getMacro('cluster') SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.asynchronous_inserts as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.asynchronous_inserts) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.asynchronous_metrics as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.asynchronous_metrics) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.backups as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.backups) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.clusters as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.clusters) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.columns as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.columns) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.current_roles as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.current_roles) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.data_skipping_indices as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.data_skipping_indices) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.databases as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.databases) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.detached_parts as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.detached_parts) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.dictionaries as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.dictionaries) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.disks as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.disks) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.distributed_ddl_queue as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.distributed_ddl_queue) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.distribution_queue as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.distribution_queue) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.dropped_tables as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.dropped_tables) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.enabled_roles as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.enabled_roles) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.errors as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.errors) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.events as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.events) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.filesystem_cache as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.filesystem_cache) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.grants as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.grants) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.jemalloc_bins as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.jemalloc_bins) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.macros as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.macros) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.merge_tree_settings as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.merge_tree_settings) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.merges as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.merges) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.metrics as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.metrics) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.moves as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.moves) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.mutations as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.mutations) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.named_collections as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.named_collections) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.parts as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.parts) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.parts_columns as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.parts_columns) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.privileges as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.privileges) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.processes as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.processes) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.projection_parts as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.projection_parts) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.projection_parts_columns as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.projection_parts_columns) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.query_cache as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.query_cache) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.query_log as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.query_log) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.quota_limits as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.quota_limits) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.quota_usage as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.quota_usage) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.quotas as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.quotas) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.quotas_usage as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.quotas_usage) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.replicas as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.replicas) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.replicated_fetches as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.replicated_fetches) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.replicated_merge_tree_settings as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.replicated_merge_tree_settings) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.replication_queue as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.replication_queue) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.role_grants as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.role_grants) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.roles as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.roles) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.row_policies as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.row_policies) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.server_settings as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.server_settings) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.settings as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.settings) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.settings_profile_elements as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.settings_profile_elements) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.settings_profiles as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.settings_profiles) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.storage_policies as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.storage_policies) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.tables as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.tables) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.user_directories as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.user_directories) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.user_processes as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.user_processes) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.users as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.users) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.warnings as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.warnings) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.zookeeper as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.zookeeper) SETTINGS skip_unavailable_shards = 1; CREATE OR REPLACE VIEW sysall.zookeeper_connection as select hostName() nodeHost, FQDN() nodeFQDN, * from clusterAllReplicas('{cluster}', system.zookeeper_connection) SETTINGS skip_unavailable_shards = 1; Some examples select * from sysall.cluster_state; ┌─shard_num─┬─replica_num─┬─host_name───────────┬─host_address─┬─port─┬─errors_count─┬──uptime─┬─node_state─┐ │ 1 │ 1 │ chhost1.localdomain │ 10.253.86.2 │ 9000 │ 0 │ 1071788 │ UP │ │ 2 │ 1 │ chhost2.localdomain │ 10.253.215.2 │ 9000 │ 0 │ 1071731 │ UP │ │ 3 │ 1 │ chhost3.localdomain │ 10.252.83.8 │ 9999 │ 0 │ 0 │ DOWN │ └───────────┴─────────────┴─────────────────────┴──────────────┴──────┴──────────────┴─────────┴────────────┘ SELECT nodeFQDN, path, formatReadableSize(free_space) AS free, formatReadableSize(total_space) AS total FROM sysall.disks ┌─nodeFQDN────────────┬─path─────────────────┬─free───────┬─total──────┐ │ chhost1.localdomain │ /var/lib/clickhouse/ │ 511.04 GiB │ 937.54 GiB │ │ chhost2.localdomain │ /var/lib/clickhouse/ │ 495.77 GiB │ 937.54 GiB │ └─────────────────────┴──────────────────────┴────────────┴────────────┘ ","categories":"","description":"sysall database (system tables on a cluster level)","excerpt":"sysall database (system tables on a cluster level)","ref":"/altinity-kb-setup-and-maintenance/sysall/","tags":"","title":"sysall database (system tables on a cluster level)"},{"body":"Timeout exceeded ... or executing longer than distributed_ddl_task_timeout during OPTIMIZE FINAL Timeout may occur\ndue to the fact that the client reach timeout interval.\nin case of TCP / native clients - you can change send_timeout / recieve_timeout + tcp_keep_alive_timeout + driver timeout settings in case of HTTP clients - you can change http_send_timeout / http_receive_timeout + tcp_keep_alive_timeout + driver timeout settings (in the case of ON CLUSTER queries) due to the fact that the timeout for query execution by shards ends\nsee setting distributed_ddl_task_timeout In the first case you additionally may get the misleading messages: Cancelling query. ... Query was cancelled.\nIn both cases, this does NOT stop the execution of the OPTIMIZE command. It continues to work even after the client is disconnected. You can see the progress of that in system.processes / show processlist / system.merges / system.query_log.\nThe same applies to queries like:\nINSERT ... SELECT CREATE TABLE ... AS SELECT CREATE MATERIALIZED VIEW ... POPULATE ... It is possible to run a query with some special query_id and then poll the status from the processlist (in the case of a cluster, it can be a bit more complicated).\nSee also\nhttps://github.com/ClickHouse/ClickHouse/issues/6093 https://github.com/ClickHouse/ClickHouse/issues/7794 https://github.com/ClickHouse/ClickHouse/issues/28896 https://github.com/ClickHouse/ClickHouse/issues/19319 ","categories":"","description":"`Timeout exceeded ...` or `executing longer than distributed_ddl_task_timeout`  during `OPTIMIZE FINAL`.","excerpt":"`Timeout exceeded ...` or `executing longer than …","ref":"/altinity-kb-setup-and-maintenance/timeouts-during-optimize-final/","tags":"","title":"Timeouts during OPTIMIZE FINAL"},{"body":"Collecting query execution flamegraph using system.trace_log ClickHouse has embedded functionality to analyze the details of query performance.\nIt’s system.trace_log table.\nBy default it collects information only about queries when runs longer than 1 sec (and collects stacktraces every second).\nYou can adjust that per query using settings query_profiler_real_time_period_ns \u0026 query_profiler_cpu_time_period_ns.\nBoth works very similar (with desired interval dump the stacktraces of all the threads which execute the query). real timer - allows to ‘see’ the situtions when cpu was not working much, but time was spend for example on IO. cpu timer - allows to see the ‘hot’ points in calculations more accurately (skip the io time).\nTrying to collect stacktraces with a frequency higher than few KHz is usually not possible.\nTo check where most of the RAM is used you can collect stacktraces during memory allocations / deallocation, by using the setting memory_profiler_sample_probability.\nclickhouse-speedscope # install wget https://github.com/laplab/clickhouse-speedscope/archive/refs/heads/master.tar.gz -O clickhouse-speedscope.tar.gz tar -xvzf clickhouse-speedscope.tar.gz cd clickhouse-speedscope-master/ pip3 install -r requirements.txt For debugging particular query:\nclickhouse-client SET query_profiler_cpu_time_period_ns=1000000; -- 1000 times per 'cpu' sec -- or SET query_profiler_real_time_period_ns=2000000; -- 500 times per 'real' sec. -- or SET memory_profiler_sample_probability=0.1; -- to debug the memory allocations SELECT ... \u003cyour select\u003e SYSTEM FLUSH LOGS; -- get the query_id from the clickhouse-client output or from system.query_log (also pay attention on query_id vs initial_query_id for distributed queries). Now let’s process that:\npython3 main.py \u0026 # start the proxy in background python3 main.py --query-id 908952ee-71a8-48a4-84d5-f4db92d45a5d # process the stacktraces fg # get the proxy from background Ctrl + C # stop it. To access ClickHouse with other username / password etc. - see the sources of https://github.com/laplab/clickhouse-speedscope/blob/master/main.py\nclickhouse-flamegraph Installation \u0026 usage instructions: https://github.com/Slach/clickhouse-flamegraph\npure flamegraph.pl examples git clone https://github.com/brendangregg/FlameGraph /opt/flamegraph clickhouse-client -q \"SELECT arrayStringConcat(arrayReverse(arrayMap(x -\u003e concat( addressToLine(x), '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, count() AS samples FROM system.trace_log WHERE event_time \u003e= subtractMinutes(now(),10) GROUP BY trace FORMAT TabSeparated\" | /opt/flamegraph/flamegraph.pl \u003e flamegraph.svg clickhouse-client -q \"SELECT arrayStringConcat((arrayMap(x -\u003e concat(splitByChar('/', addressToLine(x))[-1], '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, sum(abs(size)) AS samples FROM system.trace_log where trace_type = 'Memory' and event_date = today() group by trace order by samples desc FORMAT TabSeparated\" | /opt/flamegraph/flamegraph.pl \u003e allocs.svg clickhouse-client -q \"SELECT arrayStringConcat(arrayReverse(arrayMap(x -\u003e concat(splitByChar('/', addressToLine(x))[-1], '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, count() AS samples FROM system.trace_log where trace_type = 'Memory' group by trace FORMAT TabSeparated SETTINGS allow_introspection_functions=1\" | /opt/flamegraph/flamegraph.pl \u003e ~/mem1.svg similar using perf apt-get update -y apt-get install -y linux-tools-common linux-tools-generic linux-tools-`uname -r`git apt-get install -y clickhouse-common-static-dbg clickhouse-common-dbg mkdir -p /opt/flamegraph git clone https://github.com/brendangregg/FlameGraph /opt/flamegraph perf record -F 99 -p $(pidof clickhouse) -G perf script \u003e /tmp/out.perf /opt/flamegraph/stackcollapse-perf.pl /tmp/out.perf | /opt/flamegraph/flamegraph.pl \u003e /tmp/flamegraph.svg also https://kb.altinity.com/altinity-kb-queries-and-syntax/troubleshooting/#flamegraph\nhttps://github.com/samber/grafana-flamegraph-panel/pull/2\n","categories":"","description":"Collecting query execution flamegraph using trace_log","excerpt":"Collecting query execution flamegraph using trace_log","ref":"/altinity-kb-queries-and-syntax/trace_log/","tags":"","title":"Collecting query execution flamegraphs using system.trace_log"},{"body":"Suppose we have telecom CDR data in which A party calls B party. Each data row consists of A party details : event_timestamp, A msisdn , A imei, A Imsi , A start location, A end location , B msisdn, B imei, B imsi , B start location, B end location and some other meta data.\nSearches will be using one of A or B fields , for example A imsi within start and end time window.\nA msisdn, A imsi, A imei values are tightly coupled as users rarely change their phones.\nThe queries will be:\nselect * from X where A = '0123456789' and ts between ...; select * from X where B = '0123456789' and ts between ...; and both A \u0026 B are high-cardinality values\nClickhouse primary skip index (ORDER BY/PRIMARY KEY) work great when you always include leading ORDER BY columns in WHERE filter. There is an exceptions for low-cardinality columns and high-correlated values, but here is another case. A \u0026 B both high cardinality and seems that their correlation is at medium level.\nVarious solutions exist, and their effectiveness largely depends on the correlation of different column data. It is necessary to test all solutions on actual data to select the best one.\nORDER BY + additional Skip Index create table X ( A UInt32, B UInt32, ts DateTime, .... INDEX ix_B (B) type minmax GRANULARITY 3 ) engine = MergeTree partition by toYYYYMM(ts) order by (toStartOfDay(ts),A,B); bloom_filter index type instead of min_max could work fine in some situations.\nReverse index as a projection create table X ( A UInt32, B UInt32, ts DateTime, .... PROJECTION ix_B ( select A,B,ts ORDER BY B,ts ) ) engine = MergeTree partition by toYYYYMM(ts) order by (toStartOfDay(ts),A,B); select * from X where A in (select A from X where B='....' and ts between ...) and B='...' and ts between ... ; Separate table with Materialized View also can be used the same way.\nmortonEncode (available from 23.10) Not give the priority neither A nor B, but distribute indexing efficiancy between all of them.\nhttps://github.com/ClickHouse/ClickHouse/issues/41195 https://www.youtube.com/watch?v=5GR1J4T4_d8 https://clickhouse.com/docs/en/operations/settings/settings#analyze_index_with_space_filling_curves create table X ( A UInt32, B UInt32, ts DateTime, .... ) engine = MergeTree partition by toYYYYMM(ts) order by (toStartOfDay(ts),mortonEncode(A,B)); select * from X where A = '0123456789' and ts between ...; select * from X where B = '0123456789' and ts between ...; mortonEncode with non UInt columns mortonEncode function requires UInt columns, but sometimes diffent column types are needed (like String or ipv6). In such a case cityHash64 function can be used both for inserting and quering:\ncreate table X ( A IPv6, B IPv6, AA alias cityHash64(A), BB alias cityHash64(B), ts DateTime materialized now() ) engine = MergeTree partition by toYYYYMM(ts) order by (toStartOfDay(ts),mortonEncode(cityHash64(A),cityHash64(B))) ; insert into X values ('fd7a:115c:a1e0:ab12:4843:cd96:624c:9a17','fd7a:115c:a1e0:ab12:4843:cd96:624c:9a17') select * from X where cityHash64(toIPv6('fd7a:115c:a1e0:ab12:4843:cd96:624c:9a17')) = AA; ","categories":"","description":"How to create ORDER BY suitable for filtering over two different columns in two different queries","excerpt":"How to create ORDER BY suitable for filtering over two different …","ref":"/altinity-kb-schema-design/two-columns-indexing/","tags":"","title":"Two columns indexing"},{"body":"Useful settings to turn on/Defaults that should be reconsidered Some setting that are not enabled by default.\nttl_only_drop_parts Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.\nWhen ttl_only_drop_parts is disabled (by default), the ClickHouse server only deletes expired rows according to their TTL.\nWhen ttl_only_drop_parts is enabled, the ClickHouse server drops a whole part when all rows in it are expired.\nDropping whole parts instead of partial cleaning TTL-d rows allows having shorter merge_with_ttl_timeout times and lower impact on system performance.\njoin_use_nulls Might be you not expect that join will be filled with default values for missing columns (instead of classic NULLs) during JOIN.\nSets the type of JOIN behaviour. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting.\nPossible values:\n0 — The empty cells are filled with the default value of the corresponding field type. 1 — JOIN behaves the same way as in standard SQL. The type of the corresponding field is converted to Nullable, and empty cells are filled with NULL.\naggregate_functions_null_for_empty Default behaviour is not compatible with ANSI SQL (ClickHouse avoids Nullable types by perfomance reasons)\nselect sum(x), avg(x) from (select 1 x where 0); ┌─sum(x)─┬─avg(x)─┐ │ 0 │ nan │ └────────┴────────┘ set aggregate_functions_null_for_empty=1; select sum(x), avg(x) from (select 1 x where 0); ┌─sumOrNull(x)─┬─avgOrNull(x)─┐ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └──────────────┴──────────────┘ ","categories":"","description":"Useful settings to turn on.","excerpt":"Useful settings to turn on.","ref":"/altinity-kb-setup-and-maintenance/useful-setting-to-turn-on/","tags":"","title":"Useful settings to turn on/Defaults that should be reconsidered"},{"body":"Using array functions to mimic window functions alike behavior There are some usecases when you may want to mimic window functions using Arrays - as an optimization step, or to contol the memory better / use on-disk spiling, or just if you have old ClickHouse version.\nRunning difference sample DROP TABLE IS EXISTS test_running_difference CREATE TABLE test_running_difference ENGINE = Log AS SELECT number % 20 AS id, toDateTime('2010-01-01 00:00:00') + (intDiv(number, 20) * 15) AS ts, (number * round(xxHash32(number % 20) / 1000000)) - round(rand() / 1000000) AS val FROM numbers(100) SELECT * FROM test_running_difference ┌─id─┬──────────────────ts─┬────val─┐ │ 0 │ 2010-01-01 00:00:00 │ -1209 │ │ 1 │ 2010-01-01 00:00:00 │ 43 │ │ 2 │ 2010-01-01 00:00:00 │ 4322 │ │ 3 │ 2010-01-01 00:00:00 │ -25 │ │ 4 │ 2010-01-01 00:00:00 │ 13720 │ │ 5 │ 2010-01-01 00:00:00 │ 903 │ │ 6 │ 2010-01-01 00:00:00 │ 18062 │ │ 7 │ 2010-01-01 00:00:00 │ -2873 │ │ 8 │ 2010-01-01 00:00:00 │ 6286 │ │ 9 │ 2010-01-01 00:00:00 │ 13399 │ │ 10 │ 2010-01-01 00:00:00 │ 18320 │ │ 11 │ 2010-01-01 00:00:00 │ 11731 │ │ 12 │ 2010-01-01 00:00:00 │ 857 │ │ 13 │ 2010-01-01 00:00:00 │ 8752 │ │ 14 │ 2010-01-01 00:00:00 │ 23060 │ │ 15 │ 2010-01-01 00:00:00 │ 41902 │ │ 16 │ 2010-01-01 00:00:00 │ 39406 │ │ 17 │ 2010-01-01 00:00:00 │ 50010 │ │ 18 │ 2010-01-01 00:00:00 │ 57673 │ │ 19 │ 2010-01-01 00:00:00 │ 51389 │ │ 0 │ 2010-01-01 00:00:15 │ 66839 │ │ 1 │ 2010-01-01 00:00:15 │ 19440 │ │ 2 │ 2010-01-01 00:00:15 │ 74513 │ │ 3 │ 2010-01-01 00:00:15 │ 10542 │ │ 4 │ 2010-01-01 00:00:15 │ 94245 │ │ 5 │ 2010-01-01 00:00:15 │ 8230 │ │ 6 │ 2010-01-01 00:00:15 │ 87823 │ │ 7 │ 2010-01-01 00:00:15 │ -128 │ │ 8 │ 2010-01-01 00:00:15 │ 30101 │ │ 9 │ 2010-01-01 00:00:15 │ 54321 │ │ 10 │ 2010-01-01 00:00:15 │ 64078 │ │ 11 │ 2010-01-01 00:00:15 │ 31886 │ │ 12 │ 2010-01-01 00:00:15 │ 8749 │ │ 13 │ 2010-01-01 00:00:15 │ 28982 │ │ 14 │ 2010-01-01 00:00:15 │ 61299 │ │ 15 │ 2010-01-01 00:00:15 │ 95867 │ │ 16 │ 2010-01-01 00:00:15 │ 93667 │ │ 17 │ 2010-01-01 00:00:15 │ 114072 │ │ 18 │ 2010-01-01 00:00:15 │ 124279 │ │ 19 │ 2010-01-01 00:00:15 │ 109605 │ │ 0 │ 2010-01-01 00:00:30 │ 135082 │ │ 1 │ 2010-01-01 00:00:30 │ 37345 │ │ 2 │ 2010-01-01 00:00:30 │ 148744 │ │ 3 │ 2010-01-01 00:00:30 │ 21607 │ │ 4 │ 2010-01-01 00:00:30 │ 171744 │ │ 5 │ 2010-01-01 00:00:30 │ 14736 │ │ 6 │ 2010-01-01 00:00:30 │ 155349 │ │ 7 │ 2010-01-01 00:00:30 │ -3901 │ │ 8 │ 2010-01-01 00:00:30 │ 54303 │ │ 9 │ 2010-01-01 00:00:30 │ 89629 │ │ 10 │ 2010-01-01 00:00:30 │ 106595 │ │ 11 │ 2010-01-01 00:00:30 │ 54545 │ │ 12 │ 2010-01-01 00:00:30 │ 18903 │ │ 13 │ 2010-01-01 00:00:30 │ 48023 │ │ 14 │ 2010-01-01 00:00:30 │ 97930 │ │ 15 │ 2010-01-01 00:00:30 │ 152165 │ │ 16 │ 2010-01-01 00:00:30 │ 146130 │ │ 17 │ 2010-01-01 00:00:30 │ 174854 │ │ 18 │ 2010-01-01 00:00:30 │ 189194 │ │ 19 │ 2010-01-01 00:00:30 │ 170134 │ │ 0 │ 2010-01-01 00:00:45 │ 207471 │ │ 1 │ 2010-01-01 00:00:45 │ 54323 │ │ 2 │ 2010-01-01 00:00:45 │ 217984 │ │ 3 │ 2010-01-01 00:00:45 │ 31835 │ │ 4 │ 2010-01-01 00:00:45 │ 252709 │ │ 5 │ 2010-01-01 00:00:45 │ 21493 │ │ 6 │ 2010-01-01 00:00:45 │ 221271 │ │ 7 │ 2010-01-01 00:00:45 │ -488 │ │ 8 │ 2010-01-01 00:00:45 │ 76827 │ │ 9 │ 2010-01-01 00:00:45 │ 131066 │ │ 10 │ 2010-01-01 00:00:45 │ 149087 │ │ 11 │ 2010-01-01 00:00:45 │ 71934 │ │ 12 │ 2010-01-01 00:00:45 │ 25125 │ │ 13 │ 2010-01-01 00:00:45 │ 65274 │ │ 14 │ 2010-01-01 00:00:45 │ 135980 │ │ 15 │ 2010-01-01 00:00:45 │ 210910 │ │ 16 │ 2010-01-01 00:00:45 │ 200007 │ │ 17 │ 2010-01-01 00:00:45 │ 235872 │ │ 18 │ 2010-01-01 00:00:45 │ 256112 │ │ 19 │ 2010-01-01 00:00:45 │ 229371 │ │ 0 │ 2010-01-01 00:01:00 │ 275331 │ │ 1 │ 2010-01-01 00:01:00 │ 72668 │ │ 2 │ 2010-01-01 00:01:00 │ 290366 │ │ 3 │ 2010-01-01 00:01:00 │ 46074 │ │ 4 │ 2010-01-01 00:01:00 │ 329207 │ │ 5 │ 2010-01-01 00:01:00 │ 26770 │ │ 6 │ 2010-01-01 00:01:00 │ 287619 │ │ 7 │ 2010-01-01 00:01:00 │ -2207 │ │ 8 │ 2010-01-01 00:01:00 │ 100456 │ │ 9 │ 2010-01-01 00:01:00 │ 165688 │ │ 10 │ 2010-01-01 00:01:00 │ 194136 │ │ 11 │ 2010-01-01 00:01:00 │ 94113 │ │ 12 │ 2010-01-01 00:01:00 │ 35810 │ │ 13 │ 2010-01-01 00:01:00 │ 85081 │ │ 14 │ 2010-01-01 00:01:00 │ 170256 │ │ 15 │ 2010-01-01 00:01:00 │ 265445 │ │ 16 │ 2010-01-01 00:01:00 │ 254828 │ │ 17 │ 2010-01-01 00:01:00 │ 297238 │ │ 18 │ 2010-01-01 00:01:00 │ 323494 │ │ 19 │ 2010-01-01 00:01:00 │ 286252 │ └────┴─────────────────────┴────────┘ 100 rows in set. Elapsed: 0.003 sec. runningDifference works only in blocks \u0026 require ordered data \u0026 problematic when group changes\nselect id, val, runningDifference(val) from (select * from test_running_difference order by id, ts); ┌─id─┬────val─┬─runningDifference(val)─┐ │ 0 │ -1209 │ 0 │ │ 0 │ 66839 │ 68048 │ │ 0 │ 135082 │ 68243 │ │ 0 │ 207471 │ 72389 │ │ 0 │ 275331 │ 67860 │ │ 1 │ 43 │ -275288 │ │ 1 │ 19440 │ 19397 │ │ 1 │ 37345 │ 17905 │ │ 1 │ 54323 │ 16978 │ │ 1 │ 72668 │ 18345 │ │ 2 │ 4322 │ -68346 │ │ 2 │ 74513 │ 70191 │ │ 2 │ 148744 │ 74231 │ │ 2 │ 217984 │ 69240 │ │ 2 │ 290366 │ 72382 │ │ 3 │ -25 │ -290391 │ │ 3 │ 10542 │ 10567 │ │ 3 │ 21607 │ 11065 │ │ 3 │ 31835 │ 10228 │ │ 3 │ 46074 │ 14239 │ │ 4 │ 13720 │ -32354 │ │ 4 │ 94245 │ 80525 │ │ 4 │ 171744 │ 77499 │ │ 4 │ 252709 │ 80965 │ │ 4 │ 329207 │ 76498 │ │ 5 │ 903 │ -328304 │ │ 5 │ 8230 │ 7327 │ │ 5 │ 14736 │ 6506 │ │ 5 │ 21493 │ 6757 │ │ 5 │ 26770 │ 5277 │ │ 6 │ 18062 │ -8708 │ │ 6 │ 87823 │ 69761 │ │ 6 │ 155349 │ 67526 │ │ 6 │ 221271 │ 65922 │ │ 6 │ 287619 │ 66348 │ │ 7 │ -2873 │ -290492 │ │ 7 │ -128 │ 2745 │ │ 7 │ -3901 │ -3773 │ │ 7 │ -488 │ 3413 │ │ 7 │ -2207 │ -1719 │ │ 8 │ 6286 │ 8493 │ │ 8 │ 30101 │ 23815 │ │ 8 │ 54303 │ 24202 │ │ 8 │ 76827 │ 22524 │ │ 8 │ 100456 │ 23629 │ │ 9 │ 13399 │ -87057 │ │ 9 │ 54321 │ 40922 │ │ 9 │ 89629 │ 35308 │ │ 9 │ 131066 │ 41437 │ │ 9 │ 165688 │ 34622 │ │ 10 │ 18320 │ -147368 │ │ 10 │ 64078 │ 45758 │ │ 10 │ 106595 │ 42517 │ │ 10 │ 149087 │ 42492 │ │ 10 │ 194136 │ 45049 │ │ 11 │ 11731 │ -182405 │ │ 11 │ 31886 │ 20155 │ │ 11 │ 54545 │ 22659 │ │ 11 │ 71934 │ 17389 │ │ 11 │ 94113 │ 22179 │ │ 12 │ 857 │ -93256 │ │ 12 │ 8749 │ 7892 │ │ 12 │ 18903 │ 10154 │ │ 12 │ 25125 │ 6222 │ │ 12 │ 35810 │ 10685 │ │ 13 │ 8752 │ -27058 │ │ 13 │ 28982 │ 20230 │ │ 13 │ 48023 │ 19041 │ │ 13 │ 65274 │ 17251 │ │ 13 │ 85081 │ 19807 │ │ 14 │ 23060 │ -62021 │ │ 14 │ 61299 │ 38239 │ │ 14 │ 97930 │ 36631 │ │ 14 │ 135980 │ 38050 │ │ 14 │ 170256 │ 34276 │ │ 15 │ 41902 │ -128354 │ │ 15 │ 95867 │ 53965 │ │ 15 │ 152165 │ 56298 │ │ 15 │ 210910 │ 58745 │ │ 15 │ 265445 │ 54535 │ │ 16 │ 39406 │ -226039 │ │ 16 │ 93667 │ 54261 │ │ 16 │ 146130 │ 52463 │ │ 16 │ 200007 │ 53877 │ │ 16 │ 254828 │ 54821 │ │ 17 │ 50010 │ -204818 │ │ 17 │ 114072 │ 64062 │ │ 17 │ 174854 │ 60782 │ │ 17 │ 235872 │ 61018 │ │ 17 │ 297238 │ 61366 │ │ 18 │ 57673 │ -239565 │ │ 18 │ 124279 │ 66606 │ │ 18 │ 189194 │ 64915 │ │ 18 │ 256112 │ 66918 │ │ 18 │ 323494 │ 67382 │ │ 19 │ 51389 │ -272105 │ │ 19 │ 109605 │ 58216 │ │ 19 │ 170134 │ 60529 │ │ 19 │ 229371 │ 59237 │ │ 19 │ 286252 │ 56881 │ └────┴────────┴────────────────────────┘ 100 rows in set. Elapsed: 0.005 sec. Arrays ! 1. Group \u0026 Collect the data into array you can collect several column by builing array of tuples:\nSELECT id, groupArray(tuple(ts, val)) FROM test_running_difference GROUP BY id ┌─id─┬─groupArray(tuple(ts, val))──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 0 │ [('2010-01-01 00:00:00',-1209),('2010-01-01 00:00:15',66839),('2010-01-01 00:00:30',135082),('2010-01-01 00:00:45',207471),('2010-01-01 00:01:00',275331)] │ │ 1 │ [('2010-01-01 00:00:00',43),('2010-01-01 00:00:15',19440),('2010-01-01 00:00:30',37345),('2010-01-01 00:00:45',54323),('2010-01-01 00:01:00',72668)] │ │ 2 │ [('2010-01-01 00:00:00',4322),('2010-01-01 00:00:15',74513),('2010-01-01 00:00:30',148744),('2010-01-01 00:00:45',217984),('2010-01-01 00:01:00',290366)] │ │ 3 │ [('2010-01-01 00:00:00',-25),('2010-01-01 00:00:15',10542),('2010-01-01 00:00:30',21607),('2010-01-01 00:00:45',31835),('2010-01-01 00:01:00',46074)] │ │ 4 │ [('2010-01-01 00:00:00',13720),('2010-01-01 00:00:15',94245),('2010-01-01 00:00:30',171744),('2010-01-01 00:00:45',252709),('2010-01-01 00:01:00',329207)] │ │ 5 │ [('2010-01-01 00:00:00',903),('2010-01-01 00:00:15',8230),('2010-01-01 00:00:30',14736),('2010-01-01 00:00:45',21493),('2010-01-01 00:01:00',26770)] │ │ 6 │ [('2010-01-01 00:00:00',18062),('2010-01-01 00:00:15',87823),('2010-01-01 00:00:30',155349),('2010-01-01 00:00:45',221271),('2010-01-01 00:01:00',287619)] │ │ 7 │ [('2010-01-01 00:00:00',-2873),('2010-01-01 00:00:15',-128),('2010-01-01 00:00:30',-3901),('2010-01-01 00:00:45',-488),('2010-01-01 00:01:00',-2207)] │ │ 8 │ [('2010-01-01 00:00:00',6286),('2010-01-01 00:00:15',30101),('2010-01-01 00:00:30',54303),('2010-01-01 00:00:45',76827),('2010-01-01 00:01:00',100456)] │ │ 9 │ [('2010-01-01 00:00:00',13399),('2010-01-01 00:00:15',54321),('2010-01-01 00:00:30',89629),('2010-01-01 00:00:45',131066),('2010-01-01 00:01:00',165688)] │ │ 10 │ [('2010-01-01 00:00:00',18320),('2010-01-01 00:00:15',64078),('2010-01-01 00:00:30',106595),('2010-01-01 00:00:45',149087),('2010-01-01 00:01:00',194136)] │ │ 11 │ [('2010-01-01 00:00:00',11731),('2010-01-01 00:00:15',31886),('2010-01-01 00:00:30',54545),('2010-01-01 00:00:45',71934),('2010-01-01 00:01:00',94113)] │ │ 12 │ [('2010-01-01 00:00:00',857),('2010-01-01 00:00:15',8749),('2010-01-01 00:00:30',18903),('2010-01-01 00:00:45',25125),('2010-01-01 00:01:00',35810)] │ │ 13 │ [('2010-01-01 00:00:00',8752),('2010-01-01 00:00:15',28982),('2010-01-01 00:00:30',48023),('2010-01-01 00:00:45',65274),('2010-01-01 00:01:00',85081)] │ │ 14 │ [('2010-01-01 00:00:00',23060),('2010-01-01 00:00:15',61299),('2010-01-01 00:00:30',97930),('2010-01-01 00:00:45',135980),('2010-01-01 00:01:00',170256)] │ │ 15 │ [('2010-01-01 00:00:00',41902),('2010-01-01 00:00:15',95867),('2010-01-01 00:00:30',152165),('2010-01-01 00:00:45',210910),('2010-01-01 00:01:00',265445)] │ │ 16 │ [('2010-01-01 00:00:00',39406),('2010-01-01 00:00:15',93667),('2010-01-01 00:00:30',146130),('2010-01-01 00:00:45',200007),('2010-01-01 00:01:00',254828)] │ │ 17 │ [('2010-01-01 00:00:00',50010),('2010-01-01 00:00:15',114072),('2010-01-01 00:00:30',174854),('2010-01-01 00:00:45',235872),('2010-01-01 00:01:00',297238)] │ │ 18 │ [('2010-01-01 00:00:00',57673),('2010-01-01 00:00:15',124279),('2010-01-01 00:00:30',189194),('2010-01-01 00:00:45',256112),('2010-01-01 00:01:00',323494)] │ │ 19 │ [('2010-01-01 00:00:00',51389),('2010-01-01 00:00:15',109605),('2010-01-01 00:00:30',170134),('2010-01-01 00:00:45',229371),('2010-01-01 00:01:00',286252)] │ └────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ Do needed ordering in each array For example - by second element of tuple:\nSELECT id, arraySort(x -\u003e (x.2), groupArray((ts, val))) FROM test_running_difference GROUP BY id ┌─id─┬─arraySort(lambda(tuple(x), tupleElement(x, 2)), groupArray(tuple(ts, val)))─────────────────────────────────────────────────────────────────────────────────┐ │ 0 │ [('2010-01-01 00:00:00',-1209),('2010-01-01 00:00:15',66839),('2010-01-01 00:00:30',135082),('2010-01-01 00:00:45',207471),('2010-01-01 00:01:00',275331)] │ │ 1 │ [('2010-01-01 00:00:00',43),('2010-01-01 00:00:15',19440),('2010-01-01 00:00:30',37345),('2010-01-01 00:00:45',54323),('2010-01-01 00:01:00',72668)] │ │ 2 │ [('2010-01-01 00:00:00',4322),('2010-01-01 00:00:15',74513),('2010-01-01 00:00:30',148744),('2010-01-01 00:00:45',217984),('2010-01-01 00:01:00',290366)] │ │ 3 │ [('2010-01-01 00:00:00',-25),('2010-01-01 00:00:15',10542),('2010-01-01 00:00:30',21607),('2010-01-01 00:00:45',31835),('2010-01-01 00:01:00',46074)] │ │ 4 │ [('2010-01-01 00:00:00',13720),('2010-01-01 00:00:15',94245),('2010-01-01 00:00:30',171744),('2010-01-01 00:00:45',252709),('2010-01-01 00:01:00',329207)] │ │ 5 │ [('2010-01-01 00:00:00',903),('2010-01-01 00:00:15',8230),('2010-01-01 00:00:30',14736),('2010-01-01 00:00:45',21493),('2010-01-01 00:01:00',26770)] │ │ 6 │ [('2010-01-01 00:00:00',18062),('2010-01-01 00:00:15',87823),('2010-01-01 00:00:30',155349),('2010-01-01 00:00:45',221271),('2010-01-01 00:01:00',287619)] │ │ 7 │ [('2010-01-01 00:00:30',-3901),('2010-01-01 00:00:00',-2873),('2010-01-01 00:01:00',-2207),('2010-01-01 00:00:45',-488),('2010-01-01 00:00:15',-128)] │ │ 8 │ [('2010-01-01 00:00:00',6286),('2010-01-01 00:00:15',30101),('2010-01-01 00:00:30',54303),('2010-01-01 00:00:45',76827),('2010-01-01 00:01:00',100456)] │ │ 9 │ [('2010-01-01 00:00:00',13399),('2010-01-01 00:00:15',54321),('2010-01-01 00:00:30',89629),('2010-01-01 00:00:45',131066),('2010-01-01 00:01:00',165688)] │ │ 10 │ [('2010-01-01 00:00:00',18320),('2010-01-01 00:00:15',64078),('2010-01-01 00:00:30',106595),('2010-01-01 00:00:45',149087),('2010-01-01 00:01:00',194136)] │ │ 11 │ [('2010-01-01 00:00:00',11731),('2010-01-01 00:00:15',31886),('2010-01-01 00:00:30',54545),('2010-01-01 00:00:45',71934),('2010-01-01 00:01:00',94113)] │ │ 12 │ [('2010-01-01 00:00:00',857),('2010-01-01 00:00:15',8749),('2010-01-01 00:00:30',18903),('2010-01-01 00:00:45',25125),('2010-01-01 00:01:00',35810)] │ │ 13 │ [('2010-01-01 00:00:00',8752),('2010-01-01 00:00:15',28982),('2010-01-01 00:00:30',48023),('2010-01-01 00:00:45',65274),('2010-01-01 00:01:00',85081)] │ │ 14 │ [('2010-01-01 00:00:00',23060),('2010-01-01 00:00:15',61299),('2010-01-01 00:00:30',97930),('2010-01-01 00:00:45',135980),('2010-01-01 00:01:00',170256)] │ │ 15 │ [('2010-01-01 00:00:00',41902),('2010-01-01 00:00:15',95867),('2010-01-01 00:00:30',152165),('2010-01-01 00:00:45',210910),('2010-01-01 00:01:00',265445)] │ │ 16 │ [('2010-01-01 00:00:00',39406),('2010-01-01 00:00:15',93667),('2010-01-01 00:00:30',146130),('2010-01-01 00:00:45',200007),('2010-01-01 00:01:00',254828)] │ │ 17 │ [('2010-01-01 00:00:00',50010),('2010-01-01 00:00:15',114072),('2010-01-01 00:00:30',174854),('2010-01-01 00:00:45',235872),('2010-01-01 00:01:00',297238)] │ │ 18 │ [('2010-01-01 00:00:00',57673),('2010-01-01 00:00:15',124279),('2010-01-01 00:00:30',189194),('2010-01-01 00:00:45',256112),('2010-01-01 00:01:00',323494)] │ │ 19 │ [('2010-01-01 00:00:00',51389),('2010-01-01 00:00:15',109605),('2010-01-01 00:00:30',170134),('2010-01-01 00:00:45',229371),('2010-01-01 00:01:00',286252)] │ └────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 20 rows in set. Elapsed: 0.004 sec. That can be rewritten like this:\nWITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows SELECT id, sorted_window_rows FROM test_running_difference GROUP BY id Apply needed logic arrayMap / arrayDifference etc WITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows, arrayMap(x -\u003e x.2, sorted_window_rows) as sorted_window_rows_val_column, arrayDifference(sorted_window_rows_val_column) as sorted_window_rows_val_column_diff SELECT id, sorted_window_rows_val_column_diff FROM test_running_difference GROUP BY id ┌─id─┬─sorted_window_rows_val_column_diff─┐ │ 0 │ [0,68048,68243,72389,67860] │ │ 1 │ [0,19397,17905,16978,18345] │ │ 2 │ [0,70191,74231,69240,72382] │ │ 3 │ [0,10567,11065,10228,14239] │ │ 4 │ [0,80525,77499,80965,76498] │ │ 5 │ [0,7327,6506,6757,5277] │ │ 6 │ [0,69761,67526,65922,66348] │ │ 7 │ [0,2745,-3773,3413,-1719] │ │ 8 │ [0,23815,24202,22524,23629] │ │ 9 │ [0,40922,35308,41437,34622] │ │ 10 │ [0,45758,42517,42492,45049] │ │ 11 │ [0,20155,22659,17389,22179] │ │ 12 │ [0,7892,10154,6222,10685] │ │ 13 │ [0,20230,19041,17251,19807] │ │ 14 │ [0,38239,36631,38050,34276] │ │ 15 │ [0,53965,56298,58745,54535] │ │ 16 │ [0,54261,52463,53877,54821] │ │ 17 │ [0,64062,60782,61018,61366] │ │ 18 │ [0,66606,64915,66918,67382] │ │ 19 │ [0,58216,60529,59237,56881] │ └────┴────────────────────────────────────┘ 20 rows in set. Elapsed: 0.005 sec. You can do also a lot of magic with arrayEnumerate and accessing different values by their ids.\nNow you can return you arrays back to rows use arrayJoin\nWITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows, arrayMap(x -\u003e x.2, sorted_window_rows) as sorted_window_rows_val_column, arrayDifference(sorted_window_rows_val_column) as sorted_window_rows_val_column_diff, arrayJoin(sorted_window_rows_val_column_diff) as diff SELECT id, diff FROM test_running_difference GROUP BY id or ARRAY JOIN\nSELECT id, diff, ts FROM ( WITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows, arrayMap(x -\u003e x.2, sorted_window_rows) as sorted_window_rows_val_column SELECT id, arrayDifference(sorted_window_rows_val_column) as sorted_window_rows_val_column_diff, arrayMap(x -\u003e x.1, sorted_window_rows) as sorted_window_rows_ts_column FROM test_running_difference GROUP BY id ) as t1 ARRAY JOIN sorted_window_rows_val_column_diff as diff, sorted_window_rows_ts_column as ts etc.\n","categories":"","description":"Using array functions to mimic window-functions alike behavior.","excerpt":"Using array functions to mimic window-functions alike behavior.","ref":"/altinity-kb-queries-and-syntax/array-functions-as-window/","tags":"","title":"Using array functions to mimic window-functions alike behavior"},{"body":"2022-03-15: 7 vulnerabulities in ClickHouse were published. See the details https://jfrog.com/blog/7-rce-and-dos-vulnerabilities-found-in-clickhouse-dbms/\nThose vulnerabilities were fixed by 2 PRs:\nhttps://github.com/ClickHouse/ClickHouse/pull/27136 https://github.com/ClickHouse/ClickHouse/pull/27743 All releases starting from v21.10.2.15 have that problem fixed.\nAlso, the fix was backported to 21.3 and 21.8 branches - versions v21.8.11.4-lts and v21.3.19.1-lts accordingly have the problem fixed (and all newer releases in those branches).\nThe latest Altinity stable releases also contain the bugfix.\n21.8.13 21.3.20 If you use some older version we recommend upgrading.\nBefore the upgrade - please ensure that ports 9000 and 8123 are not exposed to the internet, so external clients who can try to exploit those vulnerabilities can not access your clickhouse node.\n","categories":"","description":"Vulnerabilities","excerpt":"Vulnerabilities","ref":"/upgrade/vulnerabilities/","tags":"","title":"Vulnerabilities"},{"body":"Using SHOW CREATE TABLE If you just want to see the current TTL settings on a table, you can look at the schema definition.\nSHOW CREATE TABLE events2_local FORMAT Vertical Query id: eba671e5-6b8c-4a81-a4d8-3e21e39fb76b Row 1: ────── statement: CREATE TABLE default.events2_local ( `EventDate` DateTime, `EventID` UInt32, `Value` String ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/events2_local', '{replica}') PARTITION BY toYYYYMM(EventDate) ORDER BY (EventID, EventDate) TTL EventDate + toIntervalMonth(1) SETTINGS index_granularity = 8192 This works even when there’s no data in the table. It does not tell you when the TTLs expire or anything specific to data in one or more of the table parts.\nUsing system.parts If you want to see the actually TTL values for specific data, run a query on system.parts. There are columns listing all currently applicable TTL limits for each part. (It does not work if the table is empty because there aren’t any parts yet.)\nSELECT * FROM system.parts WHERE (database = 'default') AND (table = 'events2_local') FORMAT Vertical Query id: 59106476-210f-4397-b843-9920745b6200 Row 1: ────── partition: 202203 name: 202203_0_0_0 ... database: default table: events2_local ... delete_ttl_info_min: 2022-04-27 21:26:30 delete_ttl_info_max: 2022-04-27 21:26:30 move_ttl_info.expression: [] move_ttl_info.min: [] move_ttl_info.max: [] default_compression_codec: LZ4 recompression_ttl_info.expression: [] recompression_ttl_info.min: [] recompression_ttl_info.max: [] group_by_ttl_info.expression: [] group_by_ttl_info.min: [] group_by_ttl_info.max: [] rows_where_ttl_info.expression: [] rows_where_ttl_info.min: [] rows_where_ttl_info.max: [] ","categories":"","description":"What are my TTL settings?","excerpt":"What are my TTL settings?","ref":"/altinity-kb-queries-and-syntax/ttl/what-are-my-ttls/","tags":"","title":"What are my TTL settings?"},{"body":"Merges SELECT table, round((elapsed * (1 / progress)) - elapsed, 2) AS estimate, elapsed, progress, is_mutation, formatReadableSize(total_size_bytes_compressed) AS size, formatReadableSize(memory_usage) AS mem FROM system.merges ORDER BY elapsed DESC Mutations SELECT database, table, substr(command, 1, 30) AS command, sum(parts_to_do) AS parts_to_do, anyIf(latest_fail_reason, latest_fail_reason != '') FROM system.mutations WHERE NOT is_done GROUP BY database, table, command Current Processes select elapsed, query from system.processes where is_initial_query and elapsed \u003e 2 Processes retrospectively SELECT normalizedQueryHash(query), current_database, sum(`ProfileEvents.Values`[indexOf(`ProfileEvents.Names`, 'UserTimeMicroseconds')])/1000 AS userCPUms, count(), sum(query_duration_ms) query_duration_ms, userCPUms/query_duration_ms cpu_per_sec, any(query) FROM system.query_log WHERE (type = 2) AND (event_date \u003e= today()) GROUP BY current_database, normalizedQueryHash(query) ORDER BY userCPUms DESC LIMIT 10 FORMAT Vertical; ","categories":"","description":"Queries to find which subsytem of Clickhouse is using the most of CPU.","excerpt":"Queries to find which subsytem of Clickhouse is using the most of CPU.","ref":"/altinity-kb-setup-and-maintenance/who-ate-my-cpu/","tags":"","title":"Who ate my CPU"},{"body":"Reference script to install standalone Zookeeper for Ubuntu / Debian Tested on Ubuntu 20.\n# install java runtime environment sudo apt-get update sudo apt install default-jre # prepare folders, logs folder should be on the low-latency disk. sudo mkdir -p /var/lib/zookeeper/data /var/lib/zookeeper/logs /etc/zookeeper /var/log/zookeeper /opt # download and install files export ZOOKEEPER_VERSION=3.6.3 wget https://dlcdn.apache.org/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -O /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz sudo tar -xvf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -C /opt rm -rf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz # create the user sudo groupadd -r zookeeper sudo useradd -r -g zookeeper --home-dir=/var/lib/zookeeper --shell=/bin/false zookeeper # symlink pointing to the used version of zookeeper distibution sudo ln -s /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin /opt/zookeeper sudo chown -R zookeeper:zookeeper /var/lib/zookeeper /var/log/zookeeper /etc/zookeeper /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin sudo chown -h zookeeper:zookeeper /opt/zookeeper # shortcuts in /usr/local/bin/ echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkCli.sh \"$@\"' | sudo tee /usr/local/bin/zkCli echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkServer.sh \"$@\"' | sudo tee /usr/local/bin/zkServer echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkCleanup.sh \"$@\"' | sudo tee /usr/local/bin/zkCleanup echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkSnapShotToolkit.sh \"$@\"' | sudo tee /usr/local/bin/zkSnapShotToolkit echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkTxnLogToolkit.sh \"$@\"' | sudo tee /usr/local/bin/zkTxnLogToolkit sudo chmod +x /usr/local/bin/zkCli /usr/local/bin/zkServer /usr/local/bin/zkCleanup /usr/local/bin/zkSnapShotToolkit /usr/local/bin/zkTxnLogToolkit # put in the config sudo cp opt/zookeeper/conf/* /etc/zookeeper cat \u003c\u003cEOF | sudo tee /etc/zookeeper/zoo.cfg initLimit=20 syncLimit=10 maxSessionTimeout=60000000 maxClientCnxns=2000 preAllocSize=131072 snapCount=3000000 dataDir=/var/lib/zookeeper/data dataLogDir=/var/lib/zookeeper/logs # use low-latency disk! clientPort=2181 #clientPortAddress=nthk-zoo1.localdomain autopurge.snapRetainCount=10 autopurge.purgeInterval=1 4lw.commands.whitelist=* EOF sudo chown -R zookeeper:zookeeper /etc/zookeeper # create systemd service file cat \u003c\u003cEOF | sudo tee /etc/systemd/system/zookeeper.service [Unit] Description=Zookeeper Daemon Documentation=http://zookeeper.apache.org Requires=network.target After=network.target [Service] Type=forking WorkingDirectory=/var/lib/zookeeper User=zookeeper Group=zookeeper Environment=ZK_SERVER_HEAP=1536 # in megabytes, adjust to ~ 80-90% of avaliable RAM (more than 8Gb is rather overkill) Environment=SERVER_JVMFLAGS=\"-Xms256m -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" Environment=ZOO_LOG_DIR=/var/log/zookeeper ExecStart=/opt/zookeeper/bin/zkServer.sh start /etc/zookeeper/zoo.cfg ExecStop=/opt/zookeeper/bin/zkServer.sh stop /etc/zookeeper/zoo.cfg ExecReload=/opt/zookeeper/bin/zkServer.sh restart /etc/zookeeper/zoo.cfg TimeoutSec=30 Restart=on-failure [Install] WantedBy=default.target EOF # start zookeeper sudo systemctl daemon-reload sudo systemctl start zookeeper.service # check status etc. echo stat | nc localhost 2181 echo ruok | nc localhost 2181 echo mntr | nc localhost 2181 ","categories":"","description":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian.","excerpt":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian.","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/","tags":"","title":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian"},{"body":" Q. I get “ZooKeeper session has expired” once. What should i do? Should I worry?\nGetting exceptions or lack of acknowledgement in distributed system from time to time is a normal situation. Your client should do the retry. If that happened once and your client do retries correctly - nothing to worry about.\nIt it happens often, or with every retry - it may be a sign of some misconfiguration / issue in cluster (see below).\nQ. we see a lot of these: ZooKeeper session has expired. Switching to a new session\nA. There is a single zookeeper session per server. But there are many threads that can use zookeeper simultaneously. So the same event (we lose the single zookeeper session we had), will be reported by all the threads/queries which were using that zookeeper session.\nUsually after loosing the zookeeper session that exception is printed by all the thread which watch zookeeper replication queues, and all the threads which had some in-flight zookeeper operations (for example inserts, ON CLUSTER commands etc).\nIf you see a lot of those simultaneously - that just means you have a lot of threads talking to zookeeper simultaneously (or may be you have many replicated tables?).\nBTW: every Replicated table comes with its own cost, so you can\u0026rsquo;t scale the number of replicated tables indefinitely.\nTypically after several hundreds (sometimes thousands) of replicated tables, the clickhouse server becomes unusable: it can’t do any other work, but only keeping replication housekeeping tasks. ‘ClickHouse-way’ is to have a few (maybe dozens) of very huge tables instead of having thousands of tiny tables. (Side note: the number of not-replicated tables can be scaled much better).\nSo again if during short period of time you see lot of those exceptions and that don’t happen anymore for a while - nothing to worry about. Just ensure your client is doing retries properly.\nQ. We are wondering what is causing that session to “timeout” as the default looks like 30 seconds, and there’s certainly stuff happening much more frequently than every 30 seconds.\nTypically that has nothing with an expiration/timeout - even if you do nothing there are heartbeat events in the zookeeper protocol.\nSo internally inside clickhouse:\nwe have a ‘zookeeper client’ which in practice is a single zookeeper connection (TCP socket), with 2 threads - one serving reads, the seconds serving writes, and some API around. while everything is ok zookeeper client keeps a single logical ‘zookeeper session’ (also by sending heartbeats etc). we may have hundreds of ‘users’ of that zookeeper client - those are threads that do some housekeeping, serve queries etc. zookeeper client normally have dozen ‘in-flight’ requests (asked by different threads). And if something bad happens with that (disconnect, some issue with zookeeper server, some other failure), zookeeper client needs to re-establish the connection and switch to the new session so all those ‘in-flight’ requests will be terminated with a ‘session expired’ exception. Q. That problem happens very often (all the time, every X minutes / hours / days).\nSometimes the real issue can be visible somewhere close to the first ‘session expired’ exception in the log. (i.e. zookeeper client thread can know \u0026 print to logs the real reason, while all ‘user’ threads just get ‘session expired’).\nAlso zookeeper logs may ofter have a clue to that was the real problem.\nKnown issues which can lead to session termination by zookeeper:\nconnectivity / network issues. jute.maxbuffer overrun. If you need to pass too much data in a single zookeeper transaction. (often happens if you need to do ALTER table UPDATE or other mutation on the table with big number of parts). The fix is adjusting JVM setting: -Djute.maxbuffer=8388608. See https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/ XID overflow. XID is a transaction counter in zookeeper, if you do too many transactions the counter reaches maxint32, and to restart the counter zookeeper closes all the connections. Usually, that happens rarely, and is not avoidable in zookeeper (well in clickhouse-keeper that problem solved). There are some corner cases / some schemas which may end up with that XID overflow happening quite often. (a worst case we saw was once per 3 weeks). Q. “ZooKeeper session has expired” happens every time I try to start the mutation / do other ALTER on Replicated table.\nDuring ALTERing replicated table ClickHouse need to create a record in zookeeper listing all the parts which should be mutated (that usually means = list names of all parts of the table). If the size of list of parts exceeds maximum buffer size - zookeeper drops the connection.\nParts name length can be different for different tables. In average with default jute.maxbuffer (1Mb) mutations start to fail for tables which have more than 5000 parts.\nSolutions:\nrethink partitioning, high number of parts in table is usually not recommended increase jute.maxbuffer on zookeeper side to values about 8M use IN PARTITION clause for mutations (where applicable) - since 20.12 switch to clickhouse-keeper Q. “ZooKeeper session has expired and also Operation timeout” happens when reading blocks from Zookeeper:\n2024.02.22 07:20:39.222171 [ 1047 ] {} \u003cError\u003e ZooKeeperClient: Code: 999. Coordination::Exception: Operation timeout (no response) for request List for path: /clickhouse/tables/github_events/block_numbers/20240205105000 (Operation timeout). (KEEPER_EXCEPTION), 2024.02.22 07:20:39.223293 [ 246 ] {} \u003cError\u003e default.github_events : void DB::StorageReplicatedMergeTree::mergeSelectingTask(): Code: 999. Coordination::Exception: /clickhouse/tables/github_events/block_numbers/20240205105000 (Connection loss). Sometimes these Session expired and operation timeout are common, because of merges that read all the blocks in ZooKeeper for a table and if there are many blocks (and partitions) read time can be longer than the 10 secs default operation timeout. When dropping a partition, ClickHouse never drops old block numbers from ZooKeeper, so the list grows indefinitely. It is done as a precaution against race between DROP PARTITION and INSERT. It is safe to clean those old blocks manually\nThis is being addressed in #59507 Add \u003ccode\u003eFORGET PARTITION\u003c/code\u003e query to remove old partition nodes from\nSolutions: Manually remove old/forgotten blocks https://kb.altinity.com/altinity-kb-useful-queries/remove_unneeded_block_numbers/\nRelated issues:\nhttps://github.com/ClickHouse/ClickHouse/issues/16307 https://github.com/ClickHouse/ClickHouse/issues/11933 https://github.com/ClickHouse/ClickHouse/issues/32646 https://github.com/ClickHouse/ClickHouse/issues/15882 ","categories":"","description":"ZooKeeper session has expired.","excerpt":"ZooKeeper session has expired.","ref":"/altinity-kb-setup-and-maintenance/zookeeper-session-expired/","tags":"","title":"ZooKeeper session has expired"},{"body":"","categories":"","description":"","excerpt":"","ref":"/join_slack/","tags":"","title":"Join Slack"},{"body":"","categories":"","description":"","excerpt":"","ref":"/upgrade_ebook/","tags":"","title":"Upgrade eBook"},{"body":"","categories":"","description":"","excerpt":"","ref":"/clickhouse_training/","tags":"","title":"ClickHouse Training"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":""},{"body":"5 categories SELECT sumResample(0, 5, 1)(number, number % 5) AS sum FROM numbers_mt(1000000000) ┌─sum───────────────────────────────────────────────────────────────────────────────────────────┐ │ [99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000] │ └───────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 1.010 sec. Processed 1.00 billion rows, 8.00 GB (990.20 million rows/s., 7.92 GB/s.) SELECT sumMap([number % 5], [number]) AS sum FROM numbers_mt(1000000000) ┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ ([0,1,2,3,4],[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000]) │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 5.730 sec. Processed 1.00 billion rows, 8.00 GB (174.51 million rows/s., 1.40 GB/s.) SELECT sumMap(map(number % 5, number)) AS sum FROM numbers_mt(1000000000) ┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ {0:99999999500000000,1:99999999700000000,2:99999999900000000,3:100000000100000000,4:100000000300000000} │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 4.169 sec. Processed 1.00 billion rows, 8.00 GB (239.89 million rows/s., 1.92 GB/s.) SELECT sumIf(number, (number % 5) = 0) AS sum_0, sumIf(number, (number % 5) = 1) AS sum_1, sumIf(number, (number % 5) = 2) AS sum_2, sumIf(number, (number % 5) = 3) AS sum_3, sumIf(number, (number % 5) = 4) AS sum_4 FROM numbers_mt(1000000000) ┌─────────────sum_0─┬─────────────sum_1─┬─────────────sum_2─┬──────────────sum_3─┬──────────────sum_4─┐ │ 99999999500000000 │ 99999999700000000 │ 99999999900000000 │ 100000000100000000 │ 100000000300000000 │ └───────────────────┴───────────────────┴───────────────────┴────────────────────┴────────────────────┘ 1 rows in set. Elapsed: 0.762 sec. Processed 1.00 billion rows, 8.00 GB (1.31 billion rows/s., 10.50 GB/s.) SELECT sumMap([id], [sum]) AS sum FROM ( SELECT number % 5 AS id, sum(number) AS sum FROM numbers_mt(1000000000) GROUP BY id ) ┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ ([0,1,2,3,4],[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000]) │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 0.331 sec. Processed 1.00 billion rows, 8.00 GB (3.02 billion rows/s., 24.15 GB/s.) 20 categories SELECT sumResample(0, 20, 1)(number, number % 20) AS sum FROM numbers_mt(1000000000) 1 rows in set. Elapsed: 1.056 sec. Processed 1.00 billion rows, 8.00 GB (947.28 million rows/s., 7.58 GB/s.) SELECT sumMap([number % 20], [number]) AS sum FROM numbers_mt(1000000000) 1 rows in set. Elapsed: 6.410 sec. Processed 1.00 billion rows, 8.00 GB (156.00 million rows/s., 1.25 GB/s.) SELECT sumMap(map(number % 20, number)) AS sum FROM numbers_mt(1000000000) ┌─sum────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ {0:24999999500000000,1:24999999550000000,2:24999999600000000,3:24999999650000000,4:24999999700000000,5:24999999750000000,6:24999999800000000,7:24999999850000000,8:24999999900000000,9:24999999950000000,10:25000000000000000,11:25000000050000000,12:25000000100000000,13:25000000150000000,14:25000000200000000,15:25000000250000000,16:25000000300000000,17:25000000350000000,18:25000000400000000,19:25000000450000000} │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 4.629 sec. Processed 1.00 billion rows, 8.00 GB (216.04 million rows/s., 1.73 GB/s.) SELECT sumIf(number, (number % 5) = 0) AS sum_0, sumIf(number, (number % 5) = 1) AS sum_1, sumIf(number, (number % 5) = 2) AS sum_2, sumIf(number, (number % 5) = 3) AS sum_3, sumIf(number, (number % 5) = 4) AS sum_4, sumIf(number, (number % 5) = 5) AS sum_5, sumIf(number, (number % 5) = 6) AS sum_6, sumIf(number, (number % 5) = 7) AS sum_7, sumIf(number, (number % 5) = 8) AS sum_8, sumIf(number, (number % 5) = 9) AS sum_9, sumIf(number, (number % 5) = 10) AS sum_10, sumIf(number, (number % 5) = 11) AS sum_11, sumIf(number, (number % 5) = 12) AS sum_12, sumIf(number, (number % 5) = 13) AS sum_13, sumIf(number, (number % 5) = 14) AS sum_14, sumIf(number, (number % 5) = 15) AS sum_15, sumIf(number, (number % 5) = 16) AS sum_16, sumIf(number, (number % 5) = 17) AS sum_17, sumIf(number, (number % 5) = 18) AS sum_18, sumIf(number, (number % 5) = 19) AS sum_19 FROM numbers_mt(1000000000) 1 rows in set. Elapsed: 5.282 sec. Processed 1.00 billion rows, 8.00 GB (189.30 million rows/s., 1.51 GB/s.) SELECT sumMap([id], [sum]) AS sum FROM ( SELECT number % 20 AS id, sum(number) AS sum FROM numbers_mt(1000000000) GROUP BY id ) 1 rows in set. Elapsed: 0.362 sec. Processed 1.00 billion rows, 8.00 GB (2.76 billion rows/s., 22.10 GB/s.) SELECT sumMap(map(id, sum)) AS sum FROM ( SELECT number % 20 AS id, sum(number) AS sum FROM numbers_mt(1000000000) GROUP BY id ) sumMapResample It’s also possible to combine them.\nSELECT day, category_id, sales FROM ( SELECT sumMapResample(1, 31, 1)([category_id], [sales], day) AS res FROM ( SELECT number % 31 AS day, 100 * (number % 11) AS category_id, number AS sales FROM numbers(10000) ) ) ARRAY JOIN res.1 AS category_id, res.2 AS sales, arrayEnumerate(res.1) AS day ┌─day─┬─category_id──────────────────────────────────┬─sales──────────────────────────────────────────────────────────────────────────┐ │ 1 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [143869,148365,142970,147465,142071,146566,151155,145667,150225,144768,149295] │ │ 2 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [149325,143898,148395,142999,147494,142100,146595,151185,145696,150255,144797] │ │ 3 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [144826,149355,143927,148425,143028,147523,142129,146624,151215,145725,150285] │ │ 4 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [150315,144855,149385,143956,148455,143057,147552,142158,146653,151245,145754] │ │ 5 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [145783,150345,144884,149415,143985,148485,143086,147581,142187,146682,151275] │ │ 6 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [151305,145812,150375,144913,149445,144014,148515,143115,147610,142216,146711] │ │ 7 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [146740,151335,145841,150405,144942,149475,144043,148545,143144,147639,142245] │ │ 8 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [142274,146769,151365,145870,150435,144971,149505,144072,148575,143173,147668] │ │ 9 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [147697,142303,146798,151395,145899,150465,145000,149535,144101,148605,143202] │ │ 10 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [143231,147726,142332,146827,151425,145928,150495,145029,149565,144130,148635] │ │ 11 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [148665,143260,147755,142361,146856,151455,145957,150525,145058,149595,144159] │ │ 12 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [144188,148695,143289,147784,142390,146885,151485,145986,150555,145087,149625] │ │ 13 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [149655,144217,148725,143318,147813,142419,146914,151515,146015,150585,145116] │ │ 14 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [145145,149685,144246,148755,143347,147842,142448,146943,151545,146044,150615] │ │ 15 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [150645,145174,149715,144275,148785,143376,147871,142477,146972,151575,146073] │ │ 16 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [146102,150675,145203,149745,144304,148815,143405,147900,142506,147001,151605] │ │ 17 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [151635,146131,150705,145232,149775,144333,148845,143434,147929,142535,147030] │ │ 18 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [147059,141665,146160,150735,145261,149805,144362,148875,143463,147958,142564] │ │ 19 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [142593,147088,141694,146189,150765,145290,149835,144391,148905,143492,147987] │ │ 20 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [148016,142622,147117,141723,146218,150795,145319,149865,144420,148935,143521] │ │ 21 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [143550,148045,142651,147146,141752,146247,150825,145348,149895,144449,148965] │ │ 22 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [148995,143579,148074,142680,147175,141781,146276,150855,145377,149925,144478] │ │ 23 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [144507,149025,143608,148103,142709,147204,141810,146305,150885,145406,149955] │ │ 24 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [149985,144536,149055,143637,148132,142738,147233,141839,146334,150915,145435] │ │ 25 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [145464,150015,144565,149085,143666,148161,142767,147262,141868,146363,150945] │ │ 26 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [150975,145493,150045,144594,149115,143695,148190,142796,147291,141897,146392] │ │ 27 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [146421,151005,145522,150075,144623,149145,143724,148219,142825,147320,141926] │ │ 28 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [141955,146450,151035,145551,150105,144652,149175,143753,148248,142854,147349] │ │ 29 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [147378,141984,146479,151065,145580,150135,144681,149205,143782,148277,142883] │ │ 30 │ [0,100,200,300,400,500,600,700,800,900,1000] │ [142912,147407,142013,146508,151095,145609,150165,144710,149235,143811,148306] │ └─────┴──────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────┘ ","categories":"","description":"","excerpt":"5 categories SELECT sumResample(0, 5, 1)(number, number % 5) AS sum …","ref":"/altinity-kb-functions/resample-vs-if-vs-map-vs-subquery/","tags":"","title":"-Resample vs -If vs -Map vs Subquery"},{"body":"-State combinator doesn’t actually store information about -If combinator, so aggregate functions with -If and without have the same serialized data.\n$ clickhouse-local --query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(10) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 9 9 $ clickhouse-local --query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(11) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 9 10 -State combinator have the same serialized data footprint regardless of parameters used in definition of aggregate function. That’s true for quantile* and sequenceMatch/sequenceCount functions.\n$ clickhouse-local --query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantileTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantileTDigestWeightedMerge(0.4)(x) FROM table\" 400000 $ clickhouse-local --query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantilesTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantilesTDigestWeightedMerge(0.4,0.8)(x) FROM table\" [400000,800000] SELECT quantileMerge(0.9)(x) FROM ( SELECT quantileState(0.1)(number) AS x FROM numbers(1000) ) ┌─quantileMerge(0.9)(x)─┐ │ 899.1 │ └───────────────────────┘ SELECT sequenceMatchMerge('(?2)(?3)')(x) AS `2_3`, sequenceMatchMerge('(?1)(?4)')(x) AS `1_4`, sequenceMatchMerge('(?1)(?2)(?3)')(x) AS `1_2_3` FROM ( SELECT sequenceMatchState('(?1)(?2)(?3)')(number, number = 8, number = 5, number = 6, number = 9) AS x FROM numbers(10) ) ┌─2_3─┬─1_4─┬─1_2_3─┐ │ 1 │ 1 │ 0 │ └─────┴─────┴───────┘ SELECT sequenceCountMerge('(?1)(?2)')(x) AS `2_3`, sequenceCountMerge('(?1)(?4)')(x) AS `1_4`, sequenceCountMerge('(?1)(?2)(?3)')(x) AS `1_2_3` FROM ( WITH number % 4 AS cond SELECT sequenceCountState('(?1)(?2)(?3)')(number, cond = 1, cond = 2, cond = 3, cond = 5) AS x FROM numbers(11) ) ┌─2_3─┬─1_4─┬─1_2_3─┐ │ 3 │ 0 │ 2 │ └─────┴─────┴───────┘ ","categories":"","description":"-State \u0026 -Merge combinators\n","excerpt":"-State \u0026 -Merge combinators\n","ref":"/altinity-kb-queries-and-syntax/state-and-merge-combinators/","tags":"","title":"-State \u0026 -Merge combinators"},{"body":"ADD nodes/Replicas to a Cluster To add some replicas to an existing cluster if -30TB then better to use replication:\ndon’t add the remote_servers.xml until replication is done. Add these files and restart to limit bandwidth and avoid saturation (70% total bandwidth): Core Settings | ClickHouse Docs\n💡 Do the Gbps to Bps math correctly. For 10G —\u003e 1250MB/s —\u003e 1250000000 B/s and change max_replicated_* settings accordingly:\nNodes replicating from: \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_replicated_sends_network_bandwidth_for_server\u003e50000\u003c/max_replicated_sends_network_bandwidth_for_server\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e Nodes replicating to: \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_replicated_fetches_network_bandwidth_for_server\u003e50000\u003c/max_replicated_sends_network_bandwidth_for_server\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e Manual method (DDL) Create tables manually and be sure macros in all replicas are aligned with the ZK path. If zk path uses {cluster} then this method won’t work. ZK path should use {shard} and {replica} or {uuid} (if databases are Atomic) only. -- DDL for Databases SELECT concat('CREATE DATABASE \"', name, '\" ENGINE = ', engine_full, ';') FROM system.databases INTO OUTFILE 'databases.sql' FORMAT TSVRaw; -- DDL for tables and views SELECT replaceRegexpOne(replaceOne(concat(create_table_query, ';'), '(', 'ON CLUSTER \\'{cluster}\\' ('), 'CREATE (TABLE|DICTIONARY|VIEW|LIVE VIEW|WINDOW VIEW)', 'CREATE \\\\1 IF NOT EXISTS') FROM system.tables WHERE database NOT IN ('system', 'information_schema', 'INFORMATION_SCHEMA') AND create_table_query != '' AND name NOT LIKE '.inner.%%' AND name NOT LIKE '.inner_id.%%' INTO OUTFILE '/tmp/schema.sql' AND STDOUT FORMAT TSVRaw SETTINGS show_table_uuid_in_table_create_query_if_not_nil=1; --- DDL only for materialized views SELECT replaceRegexpOne(replaceOne(concat(create_table_query, ';'), 'TO', 'ON CLUSTER \\'{cluster}\\' TO'), '(CREATE MATERIALIZED VIEW)', '\\\\1 IF NOT EXISTS') FROM system.tables WHERE database NOT IN ('system', 'information_schema', 'INFORMATION_SCHEMA') AND create_table_query != '' AND name NOT LIKE '.inner.%%' AND name NOT LIKE '.inner_id.%%' AND as_select != '' INTO OUTFILE '/tmp/schema.sql' APPEND AND STDOUT FORMAT TSVRaw SETTINGS show_table_uuid_in_table_create_query_if_not_nil=1; This will generate the UUIDs in the CREATE TABLE definition, something like this:\nCREATE TABLE IF NOT EXISTS default.insert_test UUID '51b41170-5192-4947-b13b-d4094c511f06' ON CLUSTER '{cluster}' (`id_order` UInt16, `id_plat` UInt32, `id_warehouse` UInt64, `id_product` UInt16, `order_type` UInt16, `order_status` String, `datetime_order` DateTime, `units` Int16, `total` Float32) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}', '{replica}') PARTITION BY tuple() ORDER BY (id_order, id_plat, id_warehouse) SETTINGS index_granularity = 8192; Copy both SQL to destination replica and execute clickhouse-client --host localhost --port 9000 -mn \u003c databases.sql clickhouse-client --host localhost --port 9000 -mn \u003c schema.sql Using clickhouse-backup Using clickhouse-backup to copy the schema of a replica to another is also convenient and moreover if using Atomic database with {uuid} macros in ReplicatedMergeTree engines: sudo -u clickhouse clickhouse-backup --schema --rbac create_remote full-replica # From the destination replica sudo -u clickhouse clickhouse-backup --schema --rbac restore_remote full-replica Check that schema migration was successful and node is replicating To check that the schema migration has been successful query system.replicas: SELECT DISTINCT database,table,replica_is_active FROM system.replicas FORMAT Vertical Check how the replication process is performing using https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/\nIf there are many postponed tasks with message: Not executing fetch of part 7_22719661_22719661_0 because 16 fetches already executing, max 16. │ 2023-09-25 17:03:06 │ │ then it is ok, the maximum replication slots are being used. Exceptions are not OK and should be investigated\nIf migration was sucessful and replication is working then wait until the replication is finished. It may take some days depending on how much data is being replicated. After this edit the cluster configuration xml file for all replicas (remote_servers.xml) and add the new replica to the cluster.\nPossible problems Exception REPLICA_ALREADY_EXISTS Code: 253. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [dl-ny2-vm-09.internal.io:9000]: Code: 253. DB::Exception: Replica /clickhouse/tables/3c3503c3-ed3c-443b-9cb3-ef41b3aed0a8/1/replicas/dl-ny2-vm-09.internal.io already exists. (REPLICA_ALREADY_EXISTS) (version 23.5.3.24 (official build)). (REPLICA_ALREADY_EXISTS) (query: CREATE TABLE IF NOT EXISTS xxxx.yyyy UUID '3c3503c3-ed3c-443b-9cb3-ef41b3aed0a8' The DDLs have been executed and some tables have been created and after that dropped but some left overs are left in ZK:\nIf databases can be dropped then use DROP DATABASE xxxxx SYNC If databases cannot be dropped use SYSTEM DROP REPLICA ‘replica_name’ FROM db.table Exception TABLE_ALREADY_EXISTS Code: 57. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [dl-ny2-vm-09.internal.io:9000]: Code: 57. DB::Exception: Directory for table data store/3c3/3c3503c3-ed3c-443b-9cb3-ef41b3aed0a8/ already exists. (TABLE_ALREADY_EXISTS) (version 23.5.3.24 (official build)). (TABLE_ALREADY_EXISTS) (query: CREATE TABLE IF NOT EXISTS xxxx.yyyy UUID '3c3503c3-ed3c-443b-9cb3-ef41b3aed0a8' ON CLUSTER '{cluster}' Tables have not been dropped correctly:\nIf databases can be dropped then use DROP DATABASE xxxxx SYNC If databases cannot be dropped use: SELECT concat('DROP TABLE ', database, '.', name, ' SYNC;') FROM system.tables WHERE database NOT IN ('system', 'information_schema', 'INFORMATION_SCHEMA') INTO OUTFILE '/tmp/drop_tables.sql' FORMAT TSVRaw; Tuning Sometimes replication goes very fast and if you have a tiered storage hot/cold you could run out of space, so for that it is interesting to: reduce fetches from 8 to 4 increase moves from 8 to 16 \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_replicated_fetches_network_bandwidth_for_server\u003e625000000\u003c/max_replicated_fetches_network_bandwidth_for_server\u003e \u003cbackground_fetches_pool_size\u003e4\u003c/background_fetches_pool_size\u003e \u003cbackground_move_pool_size\u003e16\u003c/background_move_pool_size\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e Also to monitor this with: SELECT * FROM system.metrics WHERE metric LIKE '%Move%' Query id: 5050155b-af4a-474f-a07a-f2f7e95fb395 ┌─metric─────────────────┬─value─┬─description──────────────────────────────────────────────────┐ │ BackgroundMovePoolTask │ 0 │ Number of active tasks in BackgroundProcessingPool for moves │ └────────────────────────┴───────┴──────────────────────────────────────────────────────────────┘ 1 row in set. Elapsed: 0.164 sec. dnieto-test :) SELECT * FROM system.metrics WHERE metric LIKE '%Fetch%'; SELECT * FROM system.metrics WHERE metric LIKE '%Fetch%' Query id: 992cae2a-fb58-4150-a088-83273805d0c4 ┌─metric────────────────────┬─value─┬─description───────────────────────────────────────────────┐ │ ReplicatedFetch │ 0 │ Number of data parts being fetched from replica │ │ BackgroundFetchesPoolTask │ 0 │ Number of active fetches in an associated background pool │ └───────────────────────────┴───────┴───────────────────────────────────────────────────────────┘ 2 rows in set. Elapsed: 0.163 sec. There are new tables in v23 system.replicated_fetches and system.moves check it out for more info. if needed just stop replication using SYSTEM STOP FETCHES from the replicating nodes REMOVE nodes/Replicas from a Cluster It is important to know which replica/node you want to remove to avoid problems. To check it you need to connect to the replica/node you want to remove and: SELECT DISTINCT replica_name FROM system.replicas ┌─replica_name─┐ │ arg_t01 │ │ arg_t02 │ │ arg_t03 │ │ arg_t04 │ └──────────────┘ After that we need connect to a replica different from the one that we want to remove (arg_tg01) and execute: SYSTEM DROP REPLICA 'arg_t01' This cannot be executed on the replica we want to remove (drop local replica), please use DROP TABLE/DATABASE for that. DROP REPLICA does not drop any tables and does not remove any data or metadata from disk: -- What happens if executing system drop replica in the local replica to remove. SYSTEM DROP REPLICA 'arg_t01' Elapsed: 0.017 sec. Received exception from server (version 23.8.6): Code: 305. DB::Exception: Received from dnieto-zenbook.lan:9440. DB::Exception: We can't drop local replica, please use `DROP TABLE` if you want to clean the data and drop this replica. (TABLE_WAS_NOT_DROPPED) After DROP REPLCA, we need to check that the replica is gone from the list or replicas. Connect to a node and execute: SELECT DISTINCT replica_name FROM system.replicas ┌─replica_name─┐ │ arg_t02 │ │ arg_t03 │ │ arg_t04 │ └──────────────┘ -- We should see there is no replica arg_t01 Delete the replica in the cluster configuration: remote_servers.xml and shutdown the node/replica removed. ","categories":"","description":"How to add/remove a new replica manually and using clickhouse-backup\n","excerpt":"How to add/remove a new replica manually and using clickhouse-backup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/add_remove_replica/","tags":"","title":"Add/Remove a new replica to a ClickHouse cluster"},{"body":" To set rdkafka options - add to \u003ckafka\u003e section in config.xml or preferably use a separate file in config.d/: https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md Some random example:\n\u003cyandex\u003e \u003ckafka\u003e \u003cmax_poll_interval_ms\u003e60000\u003c/max_poll_interval_ms\u003e \u003csession_timeout_ms\u003e60000\u003c/session_timeout_ms\u003e \u003cheartbeat_interval_ms\u003e10000\u003c/heartbeat_interval_ms\u003e \u003creconnect_backoff_ms\u003e5000\u003c/reconnect_backoff_ms\u003e \u003creconnect_backoff_max_ms\u003e60000\u003c/reconnect_backoff_max_ms\u003e \u003crequest_timeout_ms\u003e20000\u003c/request_timeout_ms\u003e \u003cretry_backoff_ms\u003e500\u003c/retry_backoff_ms\u003e \u003cmessage_max_bytes\u003e20971520\u003c/message_max_bytes\u003e \u003cdebug\u003eall\u003c/debug\u003e\u003c!-- only to get the errors --\u003e \u003csecurity_protocol\u003eSSL\u003c/security_protocol\u003e \u003cssl_ca_location\u003e/etc/clickhouse-server/ssl/kafka-ca-qa.crt\u003c/ssl_ca_location\u003e \u003cssl_certificate_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.pem\u003c/ssl_certificate_location\u003e \u003cssl_key_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.key\u003c/ssl_key_location\u003e \u003cssl_key_password\u003epass\u003c/ssl_key_password\u003e \u003c/kafka\u003e \u003c/yandex\u003e Authentication / connectivity Amazon MSK \u003cyandex\u003e \u003ckafka\u003e \u003csecurity_protocol\u003esasl_ssl\u003c/security_protocol\u003e \u003csasl_username\u003eroot\u003c/sasl_username\u003e \u003csasl_password\u003etoor\u003c/sasl_password\u003e \u003c/kafka\u003e \u003c/yandex\u003e SASL/SCRAM \u003cyandex\u003e \u003ckafka\u003e \u003csecurity_protocol\u003esasl_ssl\u003c/security_protocol\u003e \u003csasl_mechanism\u003eSCRAM-SHA-512\u003c/sasl_mechanism\u003e \u003csasl_username\u003eroot\u003c/sasl_username\u003e \u003csasl_password\u003etoor\u003c/sasl_password\u003e \u003c/kafka\u003e \u003c/yandex\u003e https://leftjoin.ru/all/clickhouse-as-a-consumer-to-amazon-msk/\nInline Kafka certs To connect to some Kafka cloud services you may need to use certificates.\nIf needed they can be converted to pem format and inlined into ClickHouse config.xml Example:\n\u003ckafka\u003e \u003cssl_key_pem\u003e\u003c![CDATA[ RSA Private-Key: (3072 bit, 2 primes) .... -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- ]]\u003e\u003c/ssl_key_pem\u003e \u003cssl_certificate_pem\u003e\u003c![CDATA[ -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ]]\u003e\u003c/ssl_certificate_pem\u003e \u003c/kafka\u003e See xml\nhttps://help.aiven.io/en/articles/489572-getting-started-with-aiven-kafka\nhttps://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files\nAzure Event Hub See https://github.com/ClickHouse/ClickHouse/issues/12609\nKerberos https://clickhouse.tech/docs/en/engines/table-engines/integrations/kafka/#kafka-kerberos-support https://github.com/ClickHouse/ClickHouse/blob/master/tests/integration/test_storage_kerberized_kafka/configs/kafka.xml \u003c!-- Kerberos-aware Kafka --\u003e \u003ckafka\u003e \u003csecurity_protocol\u003eSASL_PLAINTEXT\u003c/security_protocol\u003e \u003csasl_kerberos_keytab\u003e/home/kafkauser/kafkauser.keytab\u003c/sasl_kerberos_keytab\u003e \u003csasl_kerberos_principal\u003ekafkauser/kafkahost@EXAMPLE.COM\u003c/sasl_kerberos_principal\u003e \u003c/kafka\u003e confluent cloud \u003cyandex\u003e \u003ckafka\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003csecurity_protocol\u003eSASL_SSL\u003c/security_protocol\u003e \u003cssl_endpoint_identification_algorithm\u003ehttps\u003c/ssl_endpoint_identification_algorithm\u003e \u003csasl_mechanism\u003ePLAIN\u003c/sasl_mechanism\u003e \u003csasl_username\u003eusername\u003c/sasl_username\u003e \u003csasl_password\u003epassword\u003c/sasl_password\u003e \u003cssl_ca_location\u003eprobe\u003c/ssl_ca_location\u003e \u003c!-- \u003cssl_ca_location\u003e/path/to/cert.pem\u003c/ssl_ca_location\u003e --\u003e \u003c/kafka\u003e \u003c/yandex\u003e https://docs.confluent.io/cloud/current/client-apps/config-client.html\nHow to test connection settings Use kafkacat utility - it internally uses same library to access Kafla as clickhouse itself and allows easily to test different settings.\nkafkacat -b my_broker:9092 -C -o -10 -t my_topic \\ -X security.protocol=SASL_SSL \\ -X sasl.mechanisms=PLAIN \\ -X sasl.username=uerName \\ -X sasl.password=Password Different configurations for different tables? Is there some more documentation how to use this multiconfiguration for Kafka ?\nThe whole logic is here: https://github.com/ClickHouse/ClickHouse/blob/da4856a2be035260708fe2ba3ffb9e437d9b7fef/src/Storages/Kafka/StorageKafka.cpp#L466-L475\nSo it load the main config first, after that it load (with overwrites) the configs for all topics, listed in kafka_topic_list of the table.\nAlso since v21.12 it’s possible to use more straght-forward way using named_collections: https://github.com/ClickHouse/ClickHouse/pull/31691\nSo you can say something like\nCREATE TABLE test.kafka (key UInt64, value UInt64) ENGINE = Kafka(kafka1, kafka_format='CSV'); And after that in configuration:\n\u003cclickhouse\u003e \u003cnamed_collections\u003e \u003ckafka1\u003e \u003ckafka_broker_list\u003ekafka1:19092\u003c/kafka_broker_list\u003e \u003ckafka_topic_list\u003econf\u003c/kafka_topic_list\u003e \u003ckafka_group_name\u003econf\u003c/kafka_group_name\u003e \u003c/kafka1\u003e \u003c/named_collections\u003e \u003c/clickhouse\u003e \u003cyandex\u003e \u003cnamed_collections\u003e \u003ckafka_preset1\u003e \u003ckafka_broker_list\u003e...\u003c/kafka_broker_list\u003e \u003ckafka_topic_list\u003efoo.bar\u003c/kafka_topic_list\u003e \u003ckafka_group_name\u003efoo.bar.group\u003c/kafka_group_name\u003e \u003ckafka\u003e \u003csecurity_protocol\u003e...\u003c/security_protocol\u003e \u003csasl_mechanism\u003e...\u003c/sasl_mechanism\u003e \u003csasl_username\u003e...\u003c/sasl_username\u003e \u003csasl_password\u003e...\u003c/sasl_password\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003cssl_endpoint_identification_algorithm\u003ehttps\u003c/ssl_endpoint_identification_algorithm\u003e \u003cssl_ca_location\u003eprobe\u003c/ssl_ca_location\u003e \u003c/kafka\u003e \u003c/kafka_preset1\u003e \u003c/named_collections\u003e \u003c/yandex\u003e The same fragment of code in newer versions: https://github.com/ClickHouse/ClickHouse/blob/d19e24f530c30f002488bc136da78f5fb55aedab/src/Storages/Kafka/StorageKafka.cpp#L474-L496\n","categories":"","description":"Adjusting librdkafka settings\n","excerpt":"Adjusting librdkafka settings\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/","tags":"","title":"Adjusting librdkafka settings"},{"body":"Q. What happens with columns which are nor the part of ORDER BY key, nor have the AggregateFunction type?\nA. it picks the first value met, (similar to any)\nCREATE TABLE agg_test ( `a` String, `b` UInt8, `c` SimpleAggregateFunction(max, UInt8) ) ENGINE = AggregatingMergeTree ORDER BY a; INSERT INTO agg_test VALUES ('a', 1, 1); INSERT INTO agg_test VALUES ('a', 2, 2); SELECT * FROM agg_test FINAL; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 2 │ └───┴───┴───┘ INSERT INTO agg_test VALUES ('a', 3, 3); SELECT * FROM agg_test; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 2 │ └───┴───┴───┘ ┌─a─┬─b─┬─c─┐ │ a │ 3 │ 3 │ └───┴───┴───┘ OPTIMIZE TABLE agg_test FINAL; SELECT * FROM agg_test; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 3 │ └───┴───┴───┘ Last non-null value for each column CREATE TABLE test_last ( `col1` Int32, `col2` SimpleAggregateFunction(anyLast, Nullable(DateTime)), `col3` SimpleAggregateFunction(anyLast, Nullable(DateTime)) ) ENGINE = AggregatingMergeTree ORDER BY col1 Ok. 0 rows in set. Elapsed: 0.003 sec. INSERT INTO test_last (col1, col2) VALUES (1, now()); Ok. 1 rows in set. Elapsed: 0.014 sec. INSERT INTO test_last (col1, col3) VALUES (1, now()) Ok. 1 rows in set. Elapsed: 0.006 sec. SELECT col1, anyLast(col2), anyLast(col3) FROM test_last GROUP BY col1 ┌─col1─┬───────anyLast(col2)─┬───────anyLast(col3)─┐ │ 1 │ 2020-01-16 20:57:46 │ 2020-01-16 20:57:51 │ └──────┴─────────────────────┴─────────────────────┘ 1 rows in set. Elapsed: 0.005 sec. SELECT * FROM test_last FINAL ┌─col1─┬────────────────col2─┬────────────────col3─┐ │ 1 │ 2020-01-16 20:57:46 │ 2020-01-16 20:57:51 │ └──────┴─────────────────────┴─────────────────────┘ 1 rows in set. Elapsed: 0.003 sec. Merge two data streams Q. I have 2 Kafka topics from which I am storing events into 2 different tables (A and B) having the same unique ID. I want to create a single table that combines the data in tables A and B into one table C. The problem is that data received asynchronously and not all the data is available when a row arrives in Table A or vice-versa.\nA. You can use AggregatingMergeTree with Nullable columns and any aggregation function or Non-Nullable column and max aggregation function if it aceptable for your data.\nCREATE TABLE table_C ( id Int64, colA SimpleAggregatingFunction(any,Nullable(UInt32)), colB SimpleAggregatingFunction(max, String) ) ENGINE = AggregatingMergeTree() ORDER BY id; CREATE MATERIALIZED VIEW mv_A TO table_C AS SELECT id,colA FROM Kafka_A; CREATE MATERIALIZED VIEW mv_B TO table_C AS SELECT id,colB FROM Kafka_B; ","categories":"","description":"AggregatingMergeTree\n","excerpt":"AggregatingMergeTree\n","ref":"/engines/mergetree-table-engine-family/aggregatingmergetree/","tags":"","title":"AggregatingMergeTree"},{"body":"Q: Is there any way I can dedicate more resources to the merging process when running Clickhouse on pretty beefy machines (like 36 cores, 1TB of RAM, and large NVMe disks)?\nMostly such things doing by changing the level of parallelism:\n1. background_pool_size - how many threads will be actually doing the merge (if you can push all the server resources to do the merges, i.e. no selects will be running - you can give all the cores to that, so try increasing to 36). If you use replicated table - use the same value for max_replicated_merges_in_queue.\n2. background_merges_mutations_concurrency_ratio - how many merges will be assigned (multiplier of background_pool_size), sometimes the default (2) may work against you since it will assign smaller merges, which is nice if you need to deal with real-time inserts, but is not important it you do bulk inserts and later start a lot of merges. So I would try 1.\nnumber_of_free_entries_in_pool_to_lower_max_size_of_merge (merge_tree setting) should be changed together with background_pool_size (50-90% of that). “When there is less than a specified number of free entries in the pool (or replicated queue), start to lower the maximum size of the merge to process (or to put in the queue). This is to allow small merges to process - not filling the pool with long-running merges.” To make it really aggressive try 90-95% of background_pool_size, for ex. 34 (so you will have 34 huge merges and 2 small ones). Additionally, you can:\ncontrol how big target parts will be created by the merges (max_bytes_to_merge_at_max_space_in_pool) disable direct io for big merges (min_merge_bytes_to_use_direct_io) - direct io is often slower (it bypasses the page cache, and it is used there to prevent pushing out the often used data from the cache by the running merge). on a replicated system with slow merges and a fast network you can use execute_merges_on_single_replica_time_threshold analyze if the Vertical or Horizontal merge is better / faster for your case/schema. (Vertical first merges the columns from the table ORDER BY and then other columns one by another - that normally requires less ram, and keep fewer files opened, but requires more complex computations compared to horizontal when all columns are merged simultaneously). if you have a lot of tables - you can give also give more resources to the scheduler (the component which assigns the merges, and do some housekeeping) - background_schedule_pool_size \u0026 background_common_pool_size review the schema, especially codes/compression used (they allow to reduce the size, but often can impact the merge speed significantly). try to form bigger parts when doing inserts (min_insert_block_size_bytes / min_insert_block_size_rows / max_insert_block_size) check if wide (every column in a separate file) or compact (columns are mixed in one file) parts are used (system.parts). By default min_bytes_for_wide_part=10 mln rows (so if the part is bigger that that the wide format will be used, compact otherwise). Sometimes it can be beneficial to use a compact format even for bigger parts (a lot of relatively small columns) or oppositely - use a wide format even for small parts (few fat columns in the table). consider using recent clickhouse releases - they use compressed marks by default, which can be beneficial for reducing the i/o All the adjustments/performance optimizations should be controlled by some reproducible ‘benchmark’ so you can control/prove that the change gives the expected result (sometimes it’s quite hard to predict the impact of some change on the real system). Please also monitors how system resources (especially CPU, IO + for replicated tables: network \u0026 zookeeper) are used/saturated during the test. Also monitor/plot the usage of the pools:\nselect * from system.metrics where metric like '%PoolTask' Those recommendations are NOT generic. For systems with real-time insert \u0026 select pressure happening together with merges - those adjustments can be ’too aggressive’. So if you have different setups with different usage patterns - avoid using the same ‘aggressive’ settings template for all of them.\nTL/DR version:\ncat /etc/clickhouse-server/config.d/aggresive_merges.xml \u003cclickhouse\u003e \u003cbackground_pool_size\u003e36\u003c/background_pool_size\u003e \u003cbackground_schedule_pool_size\u003e128\u003c/background_schedule_pool_size\u003e \u003cbackground_common_pool_size\u003e8\u003c/background_common_pool_size\u003e \u003cbackground_merges_mutations_concurrency_ratio\u003e1\u003c/background_merges_mutations_concurrency_ratio\u003e \u003cmerge_tree\u003e \u003cnumber_of_free_entries_in_pool_to_lower_max_size_of_merge\u003e32\u003c/number_of_free_entries_in_pool_to_lower_max_size_of_merge\u003e \u003cmax_replicated_merges_in_queue\u003e36\u003c/max_replicated_merges_in_queue\u003e \u003cmax_bytes_to_merge_at_max_space_in_pool\u003e161061273600\u003c/max_bytes_to_merge_at_max_space_in_pool\u003e \u003cmin_merge_bytes_to_use_direct_io\u003e10737418240\u003c/min_merge_bytes_to_use_direct_io\u003e \u003c!-- 0 to disable --\u003e \u003c/merge_tree\u003e \u003c/clickhouse\u003e cat /etc/clickhouse-server/users.d/aggresive_merges.xml \u003cclickhouse\u003e \u003c!-- on 22.8 that should be adjusted in both places - default profile and main config --\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cbackground_pool_size\u003e36\u003c/background_pool_size\u003e \u003cbackground_schedule_pool_size\u003e128\u003c/background_schedule_pool_size\u003e \u003cbackground_common_pool_size\u003e8\u003c/background_common_pool_size\u003e \u003cbackground_merges_mutations_concurrency_ratio\u003e1\u003c/background_merges_mutations_concurrency_ratio\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e ","categories":"","description":"Aggressive merges\n","excerpt":"Aggressive merges\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-aggressive_merges/","tags":"","title":"Aggressive merges"},{"body":"Problem You have table:\nCREATE TABLE modify_column(column_n String) ENGINE=MergeTree() ORDER BY tuple(); Populate it with data:\nINSERT INTO modify_column VALUES ('key_a'); INSERT INTO modify_column VALUES ('key_b'); INSERT INTO modify_column VALUES ('key_c'); Tried to apply alter table query with changing column type:\nALTER TABLE modify_column MODIFY COLUMN column_n Enum8('key_a'=1, 'key_b'=2); But it didn’t succeed and you see an error in system.mutations table:\nSELECT * FROM system.mutations WHERE (table = 'modify_column') AND (is_done = 0) FORMAT Vertical Row 1: ────── database: default table: modify_column mutation_id: mutation_4.txt command: MODIFY COLUMN `column_n` Enum8('key_a' = 1, 'key_b' = 2) create_time: 2021-03-03 18:38:09 block_numbers.partition_id: [''] block_numbers.number: [4] parts_to_do_names: ['all_3_3_0'] parts_to_do: 1 is_done: 0 latest_failed_part: all_3_3_0 latest_fail_time: 2021-03-03 18:38:59 latest_fail_reason: Code: 36, e.displayText() = DB::Exception: Unknown element 'key_c' for type Enum8('key_a' = 1, 'key_b' = 2): while executing 'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\' = 1, \\'key_b\\' = 2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\' = 1, \\'key_b\\' = 2)') Enum8('key_a' = 1, 'key_b' = 2) : 2': (while reading from part /var/lib/clickhouse/data/default/modify_column/all_3_3_0/): While executing MergeTree (version 21.3.1.6041) And you can’t query that column anymore:\nSELECT column_n FROM modify_column ┌─column_n─┐ │ key_a │ └──────────┘ ┌─column_n─┐ │ key_b │ └──────────┘ ↓ Progress: 2.00 rows, 2.00 B (19.48 rows/s., 19.48 B/s.) 2 rows in set. Elapsed: 0.104 sec. Received exception from server (version 21.3.1): Code: 36. DB::Exception: Received from localhost:9000. DB::Exception: Unknown element 'key_c' for type Enum8('key_a' = 1, 'key_b' = 2): while executing 'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\' = 1, \\'key_b\\' = 2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\' = 1, \\'key_b\\' = 2)') Enum8('key_a' = 1, 'key_b' = 2) : 2': (while reading from part /var/lib/clickhouse/data/default/modify_column/all_3_3_0/): While executing MergeTreeThread. Solution You should do the following:\nCheck which mutation is stuck and kill it:\nSELECT * FROM system.mutations WHERE table = 'modify_column' AND is_done=0 FORMAT Vertical; KILL MUTATION WHERE table = 'modify_column' AND mutation_id = 'id_of_stuck_mutation'; Apply reverting modify column query to convert table to previous column type:\nALTER TABLE modify_column MODIFY COLUMN column_n String; Check if column is accessible now:\nSELECT column_n, count() FROM modify_column GROUP BY column_n; Run fixed ALTER MODIFY COLUMN query.\nALTER TABLE modify_column MODIFY COLUMN column_n Enum8('key_a'=1, 'key_b'=2, 'key_c'=3); You can monitor progress of column type change with system.mutations or system.parts_columns tables:\nSELECT command, parts_to_do, is_done FROM system.mutations WHERE table = 'modify_column' SELECT column, type, count() AS parts, sum(rows) AS rows, sum(bytes_on_disk) AS bytes FROM system.parts_columns WHERE (table = 'modify_column') AND (column = 'column_n') AND active GROUP BY column, type ","categories":"","description":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","excerpt":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-alter-modify-column-is-stuck-the-column-is-inaccessible/","tags":"","title":"ALTER MODIFY COLUMN is stuck, the column is inaccessible."},{"body":"Welcome to the Altinity ClickHouse Knowledge Base (KB) This knowledge base is supported by Altinity engineers to provide quick answers to common questions and issues involving ClickHouse.\nThe Altinity Knowledge Base is licensed under Apache 2.0, and available to all ClickHouse users. The information and code samples are available freely and distributed under the Apache 2.0 license.\nFor more detailed information about Altinity services support, see the following:\nAltinity: Providers of Altinity.Cloud, providing SOC-2 certified support for ClickHouse. Altinity ClickHouse Documentation: Detailed guides on installing and connecting ClickHouse to other services. ClickHouse Blog: Blog posts about ClickHouse and Altinity services. The following sites are also useful references regarding ClickHouse:\nClickHouse.com documentation: From Yandex, the creators of ClickHouse ClickHouse at Stackoverflow: Community driven responses to questions regarding ClickHouse Google groups (Usenet) yes we remember it: The grandparent of all modern discussion boards. ","categories":"","description":"Up-to-date ClickHouse knowledge base for every ClickHouse user.","excerpt":"Up-to-date ClickHouse knowledge base for every ClickHouse user.","ref":"/","tags":"","title":"Altinity Knowledge Base"},{"body":"Working with Altinity \u0026 Yandex packaging together Since version 21.1 Altinity switches to the same packaging as used by Yandex. That is needed for syncing things and introduces several improvements (like adding systemd service file).\nUnfortunately, that change leads to compatibility issues - automatic dependencies resolution gets confused by the conflicting package names: both when you update ClickHouse to the new version (the one which uses older packaging) and when you want to install older altinity packages (20.8 and older).\nInstalling old clickhouse version (with old packaging schema) When you try to install versions 20.8 or older from Altinity repo -\nversion=20.8.12.2-1.el7 yum install clickhouse-client-${version} clickhouse-server-${version} yum outputs smth like\nyum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: centos.hitme.net.pl * extras: centos1.hti.pl * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 --\u003e Running transaction check ---\u003e Package clickhouse-common-static.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) Requires: clickhouse-server-common = 20.8.12.2-1.el7 Available: clickhouse-server-common-1.1.54370-2.x86_64 (clickhouse-stable) clickhouse-server-common = 1.1.54370-2 Available: clickhouse-server-common-1.1.54378-2.x86_64 (clickhouse-stable) clickhouse-server-common = 1.1.54378-2 ... Available: clickhouse-server-common-20.8.11.17-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.11.17-1.el7 Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.12.2-1.el7 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest As you can see yum has an issue with resolving clickhouse-server-common dependency, which marked as obsoleted by newer packages.\nSolution with Old Packaging Scheme add --setopt=obsoletes=0 flag to the yum call.\nversion=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} --- title: \"installation succeeded\" linkTitle: \"installation succeeded\" description: \u003e installation succeeded --- Alternatively, you can add obsoletes=0 into /etc/yum.conf.\nTo update to new ClickHouse version (from old packaging schema to new packaging schema) version=21.1.7.1-2 yum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: centos.hitme.net.pl * extras: centos1.hti.pl * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Nothing to do It is caused by wrong dependencies resolution.\nSolution with New Package Scheme To update to the latest available version - just add clickhouse-server-common:\nyum install clickhouse-client clickhouse-server clickhouse-server-common This way the latest available version will be installed (even if you will request some other version explicitly).\nTo install some specific version remove old packages first, then install new ones.\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=21.1.7.1 yum install clickhouse-client-${version} clickhouse-server-${version} Downgrade from new version to old one version=20.8.12.2-1.el7 yum downgrade clickhouse-client-${version} clickhouse-server-${version} will not work:\nLoaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: ftp.agh.edu.pl * extras: ftp.agh.edu.pl * updates: centos.wielun.net Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be a downgrade ---\u003e Package clickhouse-client.noarch 0:21.1.7.1-2 will be erased ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be a downgrade --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common-20.8.12.2-1.el7.x86_64 is obsoleted by clickhouse-server-21.1.7.1-2.noarch which is already installed --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 ---\u003e Package clickhouse-server.noarch 0:21.1.7.1-2 will be erased --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) Requires: clickhouse-common-static = 20.8.12.2-1.el7 Installed: clickhouse-common-static-21.1.7.1-2.x86_64 (@clickhouse-stable) clickhouse-common-static = 21.1.7.1-2 Available: clickhouse-common-static-1.1.54378-2.x86_64 (clickhouse-stable) clickhouse-common-static = 1.1.54378-2 Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) ... Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.12.2-1.el7 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest Solution With Downgrading Remove packages first, then install older versions:\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} ","categories":"","description":"Altinity packaging compatibility \u003e21.x and earlier\n","excerpt":"Altinity packaging compatibility \u003e21.x and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-packaging-compatibility-greater-than-21.x-and-earlier/","tags":"","title":"Altinity packaging compatibility \u003e21.x and earlier"},{"body":"It’s possible to tune some settings which would make ClickHouse more ANSI SQL compatible(and slower):\nSET join_use_nulls=1; -- introduced long ago SET cast_keep_nullable=1; -- introduced in 20.5 SET union_default_mode='DISTINCT'; -- introduced in 21.1 SET allow_experimental_window_functions=1; -- introduced in 21.3 SET prefer_column_name_to_alias=1; -- introduced in 21.4; SET group_by_use_nulls=1; -- introduced in 22.7; ","categories":"","description":"ANSI SQL mode\n","excerpt":"ANSI SQL mode\n","ref":"/altinity-kb-queries-and-syntax/ansi-sql-mode/","tags":"","title":"ANSI SQL mode"},{"body":"EWMA example WITH [40, 45, 43, 31, 20] AS data, 0.3 AS alpha SELECT arrayFold((acc, x) -\u003e arrayPushBack(acc, (alpha * x) + ((1 - alpha) * (acc[-1]))), arrayPopFront(data), [CAST(data[1], 'Float64')]) as ewma ┌─ewma─────────────────────────────────────────────────────────────┐ │ [40,41.5,41.949999999999996,38.66499999999999,33.06549999999999] │ └──────────────────────────────────────────────────────────────────┘ ","categories":"","description":"","excerpt":"EWMA example WITH [40, 45, 43, 31, 20] AS data, 0.3 AS alpha SELECT …","ref":"/altinity-kb-functions/arrayfold/","tags":"","title":"arrayFold"},{"body":"arrayMap-like functions memory usage calculation. In order to calculate arrayMap or similar array* functions ClickHouse temporarily does arrayJoin-like operation, which in certain conditions can lead to huge memory usage for big arrays.\nSo for example, you have 2 columns:\nSELECT * FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ┌─array_1─────┬─array_2─────┐ │ [1,2,3,4,5] │ [1,2,3,4,5] │ └─────────────┴─────────────┘ Let’s say we want to multiply array elements at corresponding positions.\nSELECT arrayMap(x -\u003e ((array_1[x]) * (array_2[x])), arrayEnumerate(array_1)) AS multi FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ┌─multi─────────┐ │ [1,4,9,16,25] │ └───────────────┘ ClickHouse create temporary structure in memory like this:\nSELECT array_1, array_2, x FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ARRAY JOIN arrayEnumerate(array_1) AS x ┌─array_1─────┬─array_2─────┬─x─┐ │ [1,2,3,4,5] │ [1,2,3,4,5] │ 1 │ │ [1,2,3,4,5] │ [1,2,3,4,5] │ 2 │ │ [1,2,3,4,5] │ [1,2,3,4,5] │ 3 │ │ [1,2,3,4,5] │ [1,2,3,4,5] │ 4 │ │ [1,2,3,4,5] │ [1,2,3,4,5] │ 5 │ └─────────────┴─────────────┴───┘ We can roughly estimate memory usage by multiplying the size of columns participating in the lambda function by the size of the unnested array.\nAnd total memory usage will be 55 values (5(array size)*2(array count)*5(row count) + 5(unnested array size)), which is 5.5 times more than initial array size.\nSELECT groupArray((array_1[x]) * (array_2[x])) AS multi FROM ( SELECT array_1, array_2, x FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ARRAY JOIN arrayEnumerate(array_1) AS x ) ┌─multi─────────┐ │ [1,4,9,16,25] │ └───────────────┘ But what if we write this function in a more logical way, so we wouldn’t use any unnested arrays in lambda.\nSELECT arrayMap((x, y) -\u003e (x * y), array_1, array_2) AS multi FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ┌─multi─────────┐ │ [1,4,9,16,25] │ └───────────────┘ ClickHouse create temporary structure in memory like this:\nSELECT x, y FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ARRAY JOIN array_1 AS x, array_2 AS y ┌─x─┬─y─┐ │ 1 │ 1 │ │ 2 │ 2 │ │ 3 │ 3 │ │ 4 │ 4 │ │ 5 │ 5 │ └───┴───┘ We have only 10 values, which is no more than what we have in initial arrays.\nSELECT groupArray(x * y) AS multi FROM ( SELECT x, y FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ARRAY JOIN array_1 AS x, array_2 AS y ) ┌─multi─────────┐ │ [1,4,9,16,25] │ └───────────────┘ The same approach can be applied to other array* function with arrayMap-like capabilities to use lambda functions and ARRAY JOIN (arrayJoin).\nExamples with bigger arrays: SET max_threads=1; SET send_logs_level='trace'; SELECT arrayMap(x -\u003e ((array_1[x]) * (array_2[x])), arrayEnumerate(array_1)) AS multi FROM ( WITH 100 AS size SELECT materialize(CAST(range(size), 'Array(UInt32)')) AS array_1, materialize(CAST(range(size), 'Array(UInt32)')) AS array_2 FROM numbers(100000000) ) FORMAT `Null` \u003cDebug\u003e MemoryTracker: Current memory usage (for query): 8.13 GiB. size=100, (2*size)*size = 2*(size^2) Elapsed: 24.879 sec. Processed 524.04 thousand rows, 4.19 MB (21.06 thousand rows/s., 168.51 KB/s.) SELECT arrayMap(x -\u003e ((array_1[x]) * (array_2[x])), arrayEnumerate(array_1)) AS multi FROM ( WITH 100 AS size SELECT materialize(CAST(range(2*size), 'Array(UInt32)')) AS array_1, materialize(CAST(range(size), 'Array(UInt32)')) AS array_2 FROM numbers(100000000) ) FORMAT `Null` \u003cDebug\u003e MemoryTracker: Current memory usage (for query): 24.28 GiB. size=100, (3*size)*2*size = 6*(size^2) Elapsed: 71.547 sec. Processed 524.04 thousand rows, 4.19 MB (7.32 thousand rows/s., 58.60 KB/s.) SELECT arrayMap(x -\u003e ((array_1[x]) * (array_2[x])), arrayEnumerate(array_1)) AS multi FROM ( WITH 100 AS size SELECT materialize(CAST(range(size), 'Array(UInt32)')) AS array_1, materialize(CAST(range(2*size), 'Array(UInt32)')) AS array_2 FROM numbers(100000000) ) FORMAT `Null` \u003cDebug\u003e MemoryTracker: Current memory usage (for query): 12.19 GiB. size=100, (3*size)*size = 3*(size^2) Elapsed: 36.777 sec. Processed 524.04 thousand rows, 4.19 MB (14.25 thousand rows/s., 113.99 KB/s.) Which data types we have in those arrays?\nWITH 100 AS size SELECT toTypeName(materialize(CAST(range(size), 'Array(UInt32)'))) AS array_1, toTypeName(materialize(CAST(range(2 * size), 'Array(UInt32)'))) AS array_2, toTypeName(arrayEnumerate(materialize(CAST(range(size), 'Array(UInt32)')))) AS x ┌─array_1───────┬─array_2───────┬─x─────────────┐ │ Array(UInt32) │ Array(UInt32) │ Array(UInt32) │ └───────────────┴───────────────┴───────────────┘ So each value use 4 bytes.\nBy default ClickHouse execute query by blocks of 65515 rows (max_block_size setting value)\nLets estimate query total memory usage given previous calculations.\nWITH 100 AS size, 4 AS value_size, 65515 AS max_block_size SELECT array_1_multiplier, array_2_multiplier, formatReadableSize(((value_size * max_block_size) * ((array_1_multiplier * size) + (array_2_multiplier * size))) * (array_1_multiplier * size) AS estimated_memory_usage_bytes) AS estimated_memory_usage, real_memory_usage, round(estimated_memory_usage_bytes / (real_memory_usage * 1073741824), 2) AS ratio FROM ( WITH arrayJoin([(1, 1, 8.13), (2, 1, 24.28), (1, 2, 12.19)]) AS tpl SELECT tpl.1 AS array_1_multiplier, tpl.2 AS array_2_multiplier, tpl.3 AS real_memory_usage ) ┌─array_1_multiplier─┬─array_2_multiplier─┬─estimated_memory_usage─┬─real_memory_usage─┬─ratio─┐ │ 1 │ 1 │ 4.88 GiB │ 8.13 │ 0.6 │ │ 2 │ 1 │ 14.64 GiB │ 24.28 │ 0.6 │ │ 1 │ 2 │ 7.32 GiB │ 12.19 │ 0.6 │ └────────────────────┴────────────────────┴────────────────────────┴───────────────────┴───────┘ Correlation is pretty clear.\nWhat if we will reduce size of blocks used for query execution?\nSET max_block_size = '16k'; SELECT arrayMap(x -\u003e ((array_1[x]) * (array_2[x])), arrayEnumerate(array_1)) AS multi FROM ( WITH 100 AS size SELECT materialize(CAST(range(size), 'Array(UInt32)')) AS array_1, materialize(CAST(range(2 * size), 'Array(UInt32)')) AS array_2 FROM numbers(100000000) ) FORMAT `Null` \u003cDebug\u003e MemoryTracker: Current memory usage (for query): 3.05 GiB. Elapsed: 35.935 sec. Processed 512.00 thousand rows, 4.10 MB (14.25 thousand rows/s., 113.98 KB/s.) Memory usage down in 4 times, which has strong correlation with our change: 65k -\u003e 16k ~ 4 times.\nSELECT arrayMap((x, y) -\u003e (x * y), array_1, array_2) AS multi FROM ( WITH 100 AS size SELECT materialize(CAST(range(size), 'Array(UInt32)')) AS array_1, materialize(CAST(range(size), 'Array(UInt32)')) AS array_2 FROM numbers(100000000) ) FORMAT `Null` \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 226.04 MiB. Elapsed: 5.700 sec. Processed 11.53 million rows, 92.23 MB (2.02 million rows/s., 16.18 MB/s.) Almost 100 times faster than first query!\n","categories":"","description":"Why arrayMap, arrayFilter, arrayJoin use so much memory?\n","excerpt":"Why arrayMap, arrayFilter, arrayJoin use so much memory?\n","ref":"/altinity-kb-functions/array-like-memory-usage/","tags":"","title":"arrayMap, arrayJoin or ARRAY JOIN memory usage"},{"body":"assumeNotNull result is implementation specific:\nWITH CAST(NULL, 'Nullable(UInt8)') AS column SELECT column, assumeNotNull(column + 999) AS x; ┌─column─┬─x─┐ │ null │ 0 │ └────────┴───┘ WITH CAST(NULL, 'Nullable(UInt8)') AS column SELECT column, assumeNotNull(materialize(column) + 999) AS x; ┌─column─┬───x─┐ │ null │ 999 │ └────────┴─────┘ CREATE TABLE test_null ( `key` UInt32, `value` Nullable(String) ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_null SELECT number, concat('value ', toString(number)) FROM numbers(4); SELECT * FROM test_null; ┌─key─┬─value───┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ └─────┴─────────┘ ALTER TABLE test_null UPDATE value = NULL WHERE key = 3; SELECT * FROM test_null; ┌─key─┬─value───┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ null │ └─────┴─────────┘ SELECT key, assumeNotNull(value) FROM test_null; ┌─key─┬─assumeNotNull(value)─┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ └─────┴──────────────────────┘ WITH CAST(NULL, 'Nullable(Enum8(\\'a\\' = 1, \\'b\\' = 0))') AS test SELECT assumeNotNull(test) ┌─assumeNotNull(test)─┐ │ b │ └─────────────────────┘ WITH CAST(NULL, 'Nullable(Enum8(\\'a\\' = 1))') AS test SELECT assumeNotNull(test) Error on processing query 'with CAST(null, 'Nullable(Enum8(\\'a\\' = 1))') as test select assumeNotNull(test); ;': Code: 36, e.displayText() = DB::Exception: Unexpected value 0 in enum, Stack trace (when copying this message, always include the lines below): Info Null values in ClickHouse are stored in a separate dictionary: is this value Null. And for faster dispatch of functions there is no check on Null value while function execution, so functions like plus can modify internal column value (which has default value). In normal conditions it’s not a problem because on read attempt, ClickHouse first would check the Null dictionary and return value from column itself for non-Nulls only. And assumeNotNull function just ignores this Null dictionary. So it would return only column values, and in certain cases it’s possible to have unexpected results. If it’s possible to have Null values, it’s better to use ifNull function instead.\nSELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(ifNull(toNullable(number), 0)) ┌────count()─┐ │ 1000000000 │ └────────────┘ 1 rows in set. Elapsed: 0.705 sec. Processed 1.00 billion rows, 8.00 GB (1.42 billion rows/s., 11.35 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(coalesce(toNullable(number), 0)) ┌────count()─┐ │ 1000000000 │ └────────────┘ 1 rows in set. Elapsed: 2.383 sec. Processed 1.00 billion rows, 8.00 GB (419.56 million rows/s., 3.36 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(assumeNotNull(toNullable(number))) ┌────count()─┐ │ 1000000000 │ └────────────┘ 1 rows in set. Elapsed: 0.051 sec. Processed 1.00 billion rows, 8.00 GB (19.62 billion rows/s., 156.98 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(toNullable(number)) ┌────count()─┐ │ 1000000000 │ └────────────┘ 1 rows in set. Elapsed: 0.050 sec. Processed 1.00 billion rows, 8.00 GB (20.19 billion rows/s., 161.56 GB/s.) Info There is no overhead for assumeNotNull at all. ","categories":"","description":"assumeNotNull and friends\n","excerpt":"assumeNotNull and friends\n","ref":"/altinity-kb-functions/assumenotnull-and-friends/","tags":"","title":"assumeNotNull and friends"},{"body":"Async INSERTs is a ClickHouse feature tha enables batching data automatically and transparently on the server-side. Although async inserts work, they still have issues, but have been improved in latest versions. We recommend to batch at app/ingestor level because you will have more control and you decouple this responsibility from ClickHouse. Being said that here some insights about Async inserts you should now:\nAsync inserts give acknowledgment immediately after the data got inserted into the buffer (wait_for_async_insert = 0) or by default, after the data got written to a part after flushing from buffer (wait_for_async_insert = 1). INSERT .. SELECT is NOT async insert. (You can use matView + Null table OR ephemeral columns instead of INPUT function, then ASYNC insert work) Async inserts will do (idempotent) retries. Async inserts can collect data for some offline remote clusters: Yandex self-driving cars were collecting the metrics data during the ride into ClickHouse installed on the car computer to a distributed table with Async inserts enabled, which were flushed to the cluster once the car was plugged to the network. Async inserts can do batching, so multiple inserts can be squashed as a single insert (but in that case, retries are not idempotent anymore). Async inserts can loose your data in case of sudden restart (no fsyncs by default). Async inserted data becomes available for selects not immediately after acknowledgment. Async inserts generally have more moving parts there are some background threads monitoring new data to be sent and pushing it out. Async inserts require extra monitoring from different system.tables (see system.part_log, system.query_log and system.asynchronous_inserts for 22.8). Previously such queries didn’t appear in the query log. Check: #33239. Important to use wait_for_async_insert = 1 because with any error you will loose data without knowing it. For example your table is read only -\u003e losing data, out of disk space -\u003e losing data, too many parts -\u003e losing data. 22.10+ bugfixes/features Fixed bug which could lead to deadlock while using asynchronous inserts. See #43233. Async insert dedup: Support block deduplication for asynchronous inserts. Before this change, async inserts did not support deduplication, because multiple small inserts coexisted in one inserted batch. See #38075 and #43304. Added system table asynchronous_insert_log. It contains information about asynchronous inserts (including results of queries in fire-and-forget mode. (with wait_for_async_insert=0)) for better introspection. See #42040. Support async inserts in clickhouse-client for queries with inlined data (Native protocol). Check: #34267 #54098. Also new feature opened by @alexey-milovidov to use async inserts with prepared blocks like a normal insert #54381 Async insert backpressure: #47623 Back pressure for asynchronous inserts In order to limit the deduplication overhead when using async_insert_deduplicate, clickhouse writes lots of keys to keeper, and it’s easy to exceed the txn limitation. So the setting async_insert_max_query_number is added to limit the number of async inserts in a block. This will impact on the throughput of async inserts, so this setting should not considered when duplication is disabled: async_insert_deduplicate = 0 #46549 enable async-insert-max-query-number only if async_insert_deduplicate SYSTEM FLUSH ASYNC INSERTS #49160 Allow to flush asynchronous insert queue Fix crash when async inserts with deduplication are used for ReplicatedMergeTree tables using a nondefault merging algorithm Fix async insert with deduplication for ReplicatedMergeTree using merging algorithms #51676 Async inserts not working with log_comment setting: Async inserts dont work if people is using log_comment setting with different values Fix misbehaviour with async inserts Correctly disable async insert with deduplication when its not needed #50663 To improve observability / introspection In 22.x versions, it is not possible to relate part_log/query_id column with asynchronous_insert_log/query_id column. We need to use query_log/query_id:\nasynchronous_insert_log shows up the query_id and flush_query_id of each async insert. The query_id from asynchronous_insert_log shows up in the system.query_log as type = 'QueryStart' but the same query_id does not show up in the query_id column of the system.part_log. Because the query_id column in the part_log is the identifier of the INSERT query that created a data part, and it seems it is for sync INSERTS but not for async inserts.\nSo in asynchronous_inserts table you can check the current batch that still has not been flushed. In the asynchronous_insert_log you can find a log of all the async inserts executed.\nBut in ClickHouse 23.7 Flush queries for async inserts (the queries that do the final push of data) are now logged in the system.query_log where they appear as query_kind = 'AsyncInsertFlush'.\nLog async insert flush queries into to system.query_log and system.processes #51160 Metrics SELECT name FROM system.columns WHERE (table = 'metric_log') AND (name ILIKE '%Async%') Query id: 3d0b7cbc-7990-4498-9c18-1c988796c487 ┌─name────────────────────────────────────────────────┐ │ ProfileEvent_AsyncInsertQuery │ │ ProfileEvent_AsyncInsertBytes │ │ ProfileEvent_AsyncInsertCacheHits │ │ ProfileEvent_FailedAsyncInsertQuery │ │ ProfileEvent_AsynchronousReadWaitMicroseconds │ │ ProfileEvent_AsynchronousRemoteReadWaitMicroseconds │ │ CurrentMetric_DiskObjectStorageAsyncThreads │ │ CurrentMetric_DiskObjectStorageAsyncThreadsActive │ │ CurrentMetric_AsynchronousInsertThreads │ │ CurrentMetric_AsynchronousInsertThreadsActive │part │ CurrentMetric_AsynchronousReadWait │ │ CurrentMetric_PendingAsyncInsert │ │ CurrentMetric_AsyncInsertCacheSize │ └─────────────────────────────────────────────────────┘ SELECT * FROM system.metrics WHERE metric ILIKE '%async%' ┌─metric──────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────┐ │ AsynchronousInsertThreads │ 0 │ Number of threads in the AsynchronousInsert thread pool. │ │ AsynchronousInsertThreadsActive │ 0 │ Number of threads in the AsynchronousInsert thread pool running a task. │ │ AsynchronousReadWait │ 0 │ Number of threads waiting for asynchronous read. │ │ PendingAsyncInsert │ 0 │ Number of asynchronous inserts that are waiting for flush. │ │ AsyncInsertCacheSize │ 0 │ Number of async insert hash id in cache │ └─────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────┘ ","categories":"","description":"Async INSERTs\n","excerpt":"Async INSERTs\n","ref":"/altinity-kb-queries-and-syntax/async-inserts/","tags":"","title":"Async INSERTs"},{"body":"In version 20.5 ClickHouse first introduced database engine=Atomic.\nSince version 20.10 it is a default database engine (before engine=Ordinary was used).\nThose 2 database engine differs in a way how they store data on a filesystem, and engine Atomic allows to resolve some of the issues existed in engine=Ordinary.\nengine=Atomic supports\nnon-blocking drop table / rename table tables delete (\u0026detach) async (wait for selects finish but invisible for new selects) atomic drop table (all files / folders removed) atomic table swap (table swap by “EXCHANGE TABLES t1 AND t2;”) rename dictionary / rename database unique automatic UUID paths in FS and ZK for Replicated FAQ Q. Data is not removed immediately A. UseDROP TABLE t SYNC;\nOr use parameter (user level) database_atomic_wait_for_drop_and_detach_synchronously:\nSET database_atomic_wait_for_drop_and_detach_synchronously = 1; Also, you can decrease the delay used by Atomic for real table drop (it’s 8 minutes by default)\ncat /etc/clickhouse-server/config.d/database_atomic_delay_before_drop_table.xml \u003cclickhouse\u003e \u003cdatabase_atomic_delay_before_drop_table_sec\u003e1\u003c/database_atomic_delay_before_drop_table_sec\u003e \u003c/clickhouse\u003e Q. I cannot reuse zookeeper path after dropping the table. A. This happens because real table deletion occurs with a controlled delay. See the previous question to remove the table immediately.\nWith engine=Atomic it’s possible (and is a good practice if you do it correctly) to include UUID into zookeeper path, i.e. :\nCREATE ... ON CLUSTER ... ENGINE=ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}/', '{replica}') See also: https://github.com/ClickHouse/ClickHouse/issues/12135#issuecomment-653932557\nIt’s very important that the table will have the same UUID cluster-wide.\nWhen the table is created using ON CLUSTER - all tables will get the same UUID automatically. When it needs to be done manually (for example - you need to add one more replica), pick CREATE TABLE statement with UUID from one of the existing replicas.\nset show_table_uuid_in_table_create_qquery_if_not_nil=1　; SHOW CREATE TABLE xxx; /* or SELECT create_table_query FROM system.tables WHERE ... */ Q. Should I use Atomic or Ordinary for new setups? All things inside clickhouse itself should work smoothly with Atomic.\nBut some external tools - backup tools, things involving other kinds of direct manipulations with clickhouse files \u0026 folders may have issues with Atomic.\nOrdinary layout on the filesystem is simpler. And the issues which address Atomic (lock-free renames, drops, atomic exchange of table) are not so critical in most cases.\nOrdinary Atomic filesystem layout very simple more complicated external tool support (like clickhouse-backup) good / mature limited / beta some DDL queries (DROP / RENAME) may\nhang for a long time (waiting for some other things)\nyes 👎 no 👍 Possibility to swap 2 tables rename a to a_old, b to a,\na_old to b;\nOperation is not atomic, and can break in the middle (while chances are low).\nEXCHANGE TABLES t1 AND t2\nAtomic, have no intermediate states.\nuuid in zookeeper path Not possible to use.\nThe typical pattern is to add version suffix to zookeeper path when you need to create the new version of the same table.\nYou can use uuid in zookeeper paths. That requires some extra care when you expand the cluster, and makes zookeeper paths harder to map to real table.\nBut allows to to do any kind of manipulations on tables (rename, recreate with same name etc).\nMaterialized view without TO syntax\n(!we recommend using TO syntax always!)\n.inner.mv_name\nThe name is predictable, easy to match with MV.\n.inner_id.{uuid}\nThe name is unpredictable, hard to match with MV (maybe problematic for MV chains, and similar scenarios)\nUsing Ordinary by default instead of Atomic --- title: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" linkTitle: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" description: \u003e cat /etc/clickhouse-server/users.d/disable_atomic_database.xml --- \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cdefault_database_engine\u003eOrdinary\u003c/default_database_engine\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e Other sources Presentation https://youtu.be/1LVJ_WcLgF8?t=2744\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup46/database_engines.pdf\n","categories":"","description":"Atomic Database Engine\n","excerpt":"Atomic Database Engine\n","ref":"/engines/altinity-kb-atomic-database-engine/","tags":"","title":"Atomic Database Engine"},{"body":"An insert is atomic if it creates only one part.\nAn insert will create one part if:\nData is inserted directly into a MergeTree table Data is inserted into a single partition. For INSERT FORMAT: Number of rows is less than max_insert_block_size (default is 1048545) Parallel formatting is disabled (For TSV, TSKV, CSV, and JSONEachRow formats setting input_format_parallel_parsing=0 is set). For INSERT SELECT: Number of rows is less than max_block_size Smaller blocks are properly squashed up to the configured block size (min_insert_block_size_rows and min_insert_block_size_bytes) The MergeTree table doesn’t have Materialized Views (there is no atomicity Table \u003c\u003e MV) https://github.com/ClickHouse/ClickHouse/issues/9195#issuecomment-587500824 https://github.com/ClickHouse/ClickHouse/issues/5148#issuecomment-487757235\nExample how to make a large insert atomically Generate test data in Native and TSV format ( 100 millions rows ) Text formats and Native format require different set of settings, here I want to find / demonstrate mandatory minumum of settings for any case.\nclickhouse-client -q \\ 'SELECT toInt64(number) A, toString(number) S FROM numbers(100000000) FORMAT Native' \u003e t.native clickhouse-client -q \\ 'SELECT toInt64(number) A, toString(number) S FROM numbers(100000000) FORMAT TSV' \u003e t.tsv Insert with default settings (not atomic) DROP TABLE IF EXISTS trg; CREATE TABLE trg(A Int64, S String) Engine=MergeTree ORDER BY A; -- Load data in Native format clickhouse-client -q 'INSERT INTO trg FORMAT Native' \u003ct.native -- Check how many parts is created SELECT count(), min(rows), max(rows), sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 90 │ 890935 │ 1113585 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘ --- 90 parts! was created - not atomic DROP TABLE IF EXISTS trg; CREATE TABLE trg(A Int64, S String) Engine=MergeTree ORDER BY A; -- Load data in TSV format clickhouse-client -q 'INSERT INTO trg FORMAT TSV' \u003ct.tsv -- Check how many parts is created SELECT count(), min(rows), max(rows), sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 85 │ 898207 │ 1449610 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘ --- 85 parts! was created - not atomic Insert with adjusted settings (atomic) Atomic insert use more memory because it needs 100 millions rows in memory.\nDROP TABLE IF EXISTS trg; CREATE TABLE trg(A Int64, S String) Engine=MergeTree ORDER BY A; clickhouse-client --input_format_parallel_parsing=0 \\ --min_insert_block_size_bytes=0 \\ --min_insert_block_size_rows=1000000000 \\ -q 'INSERT INTO trg FORMAT Native' \u003ct.native -- Check that only one part is created SELECT count(), min(rows), max(rows), sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 1 │ 100000000 │ 100000000 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘ -- 1 part, success. DROP TABLE IF EXISTS trg; CREATE TABLE trg(A Int64, S String) Engine=MergeTree ORDER BY A; -- Load data in TSV format clickhouse-client --input_format_parallel_parsing=0 \\ --min_insert_block_size_bytes=0 \\ --min_insert_block_size_rows=1000000000 \\ -q 'INSERT INTO trg FORMAT TSV' \u003ct.tsv -- Check that only one part is created SELECT count(), min(rows), max(rows), sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 1 │ 100000000 │ 100000000 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘ -- 1 part, success. ","categories":"","description":"Atomic insert\n","excerpt":"Atomic insert\n","ref":"/altinity-kb-queries-and-syntax/atomic-insert/","tags":"","title":"Atomic insert"},{"body":"EBS Most native choose for ClickHouse as fast storage, because it usually guarantees best throughput, IOPS, latency for reasonable price.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nGeneral Purpose SSD volumes In usual conditions ClickHouse being limited by throughput of volumes and amount of provided IOPS doesn’t make any big difference for performance starting from a certain number. So the most native choice for clickhouse is gp3 and gp2 volumes.\n‌EC2 instances also have an EBS throughput limit, it depends on the size of the EC2 instance. That means if you would attach multiple volumes which would have high potential throughput, you would be limited by your EC2 instance, so usually there is no reason to have more than 1-3 GP3 volume or 4-5 GP2 volume per node.\nIt’s pretty straightforward to set up a ClickHouse for using multiple EBS volumes with jbod storage_policies.\ngeneral purpose\nVolume type gp3 gp2 Max throughput per volume 1000 MiB/s 250 MiB/s Price $0.08/GB-month\n3,000 IOPS free and\n$0.005/provisioned IOPS-month over 3,000;\n125 MB/s free and\n$0.04/provisioned MB/s-month over 125\n$0.10/GB-month GP3 It’s recommended option, as it allow you to have only one volume, for instances which have less than 10 Gbps EBS Bandwidth (nodes =\u003c32 VCPU usually) and still have maximum performance. For bigger instances, it make sense to look into option of having several GP3 volumes.\nIt’s a new type of volume, which is 20% cheaper than gp2 per GB-month and has lower free throughput: only 125 MiB/s vs 250 MiB/s. But you can buy additional throughput and IOPS for volume. It also works better if most of your queries read only one or several parts, because in that case you are not being limited by performance of a single EBS disk, as parts can be located only on one disk at once.\nBecause, you need to have less GP3 volumes compared to GP2 option, it’s suggested approach for now.\nFor best performance, it’s suggested to buy:\n7000 IOPS Throughput up to the limit of your EC2 instance (1000 MiB/s is safe option) GP2 ‌GP2 volumes have a hard limit of 250 MiB/s per volume (for volumes bigger than 334 GB), it usually makes sense to split one big volume in multiple smaller volumes larger than 334GB in order to have maximum possible throughput.\nThroughput Optimized HDD volumes ST1 Looks like a good candidate for cheap cold storage for old data with decent maximum throughput 500 MiB/s. But it achieved only for big volumes \u003e5 TiB.\nThroughput credits and burst performance\nProvisioned IOPS SSD volumes IO2 Block Express, IO2, IO1 In 99.99% cases doesn’t give any benefit for ClickHouse compared to GP3 option and perform worse because maximum throughput is limited to 500 MiB/s per volume if you buy less than 32 000 IOPS, which is really expensive (compared to other options) and unneded for ClickHouse. And if you have spare money, it’s better to spend them on better EC2 instance.\nS3 Best option for cold data, it can give considerably good throughput and really good price, but latencies and IOPS much worse than EBS option. Another intresting point is, for EC2 instance throughput limit for EBS and S3 calculated separately, so if you access your data both from EBS and S3, you can get double throughput.\nIt’s stated in AWS documentation, that S3 can fully utilize network capacity of EC2 instance. (up to 100 Gb/s) Latencies or (first-byte-out) estimated to be 100-200 milliseconds withing single region.\nIt also recommended to enable gateway endpoint for s3, it can push throughput even futher (up to 800 Gb/s)\nS3 best practices\nEFS Works over NFSv4.1 version. We have clients, which run their ClickHouse installations over NFS. It works considerabely well as cold storage, so it’s recommended to have EBS disks for hot data. A fast network is required.\nClickHouse doesn’t have any native option to reuse the same data on durable network disk via several replicas. You either need to store the same data twice or build custom tooling around ClickHouse and use it without Replicated*MergeTree tables.\nFSx Lustre We have several clients, who use Lustre (some of them use AWS FSx Lustre, another is self managed Lustre) without any big issue. Fast network is requered. There were known problems with data damage on older versions caused by issues with O_DIRECT or async IO support on Lustre.\nClickHouse doesn’t have any native option to reuse the same data on durable network disk via several replicas. You either need to store the same data twice or build custom tooling around ClickHouse and use it without Replicated*MergeTree tables.\nhttps://altinity.com/blog/2019/11/27/amplifying-clickhouse-capacity-with-multi-volume-storage-part-1\nhttps://altinity.com/blog/2019/11/29/amplifying-clickhouse-capacity-with-multi-volume-storage-part-2\nhttps://calculator.aws/#/createCalculator/EBS?nc2=h_ql_pr_calc\n","categories":"","description":"AWS EBS, EFS, FSx, Lustre\n","excerpt":"AWS EBS, EFS, FSx, Lustre\n","ref":"/altinity-kb-setup-and-maintenance/aws-ec2-storage/","tags":"","title":"AWS EC2 Storage"},{"body":"Q. How to populate MV create with TO syntax? INSERT INTO mv SELECT * FROM huge_table? Will it work if the source table has billions of rows?\nA. single huge insert ... select ... actually will work, but it will take A LOT of time, and during that time lot of bad things can happen (lack of disk space, hard restart etc). Because of that, it’s better to do such backfill in a more controlled manner and in smaller pieces.\nOne of the best options is to fill one partition at a time, and if it breaks you can drop the partition and refill it.\nIf you need to construct a single partition from several sources - then the following approach may be the best.\nCREATE TABLE mv_import AS mv; INSERT INTO mv_import SELECT * FROM huge_table WHERE toYYYYMM(ts) = 202105; /* or other partition expression*/ /* that insert select may take a lot of time, if something bad will happen during that - just truncate mv_import and restart the process */ /* after successful loading of mv_import do*/ ALTER TABLE mv ATTACH PARTITION ID '202105' FROM mv_import; See also https://clickhouse.tech/docs/en/sql-reference/statements/alter/partition/#alter_attach-partition-from.\nQ. I still do not have enough RAM to GROUP BY the whole partition.\nA. Push aggregating to the background during MERGES\nThere is a modified version of MergeTree Engine, called AggregatingMergeTree. That engine has additional logic that is applied to rows with the same set of values in columns that are specified in the table’s ORDER BY expression. All such rows are aggregated to only one rows using the aggregating functions defined in the column definitions. There are two “special” column types, designed specifically for that purpose:\nAggregatingFunction SimpleAggregatingFunction INSERT … SELECT operating over the very large partition will create data parts by 1M rows (min_insert_block_size_rows), those parts will be aggregated during the merge process the same way as GROUP BY do it, but the number of rows will be much less than the total rows in the partition and RAM usage too. Merge combined with GROUP BY will create a new part with a much less number of rows. That data part possibly will be merged again with other data, but the number of rows will be not too big.\nCREATE TABLE mv_import ( id UInt64, ts SimpleAggregatingFunction(max,DateTime), -- most fresh v1 SimpleAggregatingFunction(sum,UInt64), -- just sum v2 SimpleAggregatingFunction(max,String), -- some not empty string v3 AggregatingFunction(argMax,String,ts) -- last value ) ENGINE = AggregatingMergeTree() ORDER BY id; INSERT INTO mv_import SELECT id, -- ORDER BY column ts,v1,v2, -- state for SimpleAggregatingFunction the same as value initializeAggregation('argMaxState',v3,ts) -- we need to convert from values to States for columns with AggregatingFunction type FROM huge_table WHERE toYYYYMM(ts) = 202105; Actually, the first GROUP BY run will happen just before 1M rows will be stored on disk as a data part. You may disable that behavior by switching off optimize_on_insert setting if you have heavy calculations during aggregation.\nYou may attach such a table (with AggregatingFunction columns) to the main table as in the example above, but if you don’t like having States in the Materialized Table, data should be finalized and converted back to normal values. In that case, you have to move data by INSERT … SELECT again:\nINSERT INTO MV SELECT id,ts,v1,v2, -- nothing special for SimpleAggregatingFunction columns finalizeAggregation(v3) from mv_import FINAL The last run of GROUP BY will happen during FINAL execution and AggregatingFunction types converted back to normal values. To simplify retries after failures an additional temporary table and the same trick with ATTACH could be applied.\n","categories":"","description":"Backfill/populate MV in a controlled manner\n","excerpt":"Backfill/populate MV in a controlled manner\n","ref":"/altinity-kb-schema-design/materialized-views/backfill-populate-mv-in-a-controlled-manner/","tags":"","title":"Backfill/populate MV in a controlled manner"},{"body":"ClickHouse is currently at the design stage of creating some universal backup solution. Some custom backup strategies are:\nEach shard is backed up separately. FREEZE the table/partition. For more information, see Alter Freeze Partition. This creates hard links in shadow subdirectory. rsync that directory to a backup location, then remove that subfolder from shadow. Cloud users are recommended to use Rclone. Always add the full contents of the metadata subfolder that contains the current DB schema and clickhouse configs to your backup. For a second replica, it’s enough to copy metadata and configuration. Data in clickhouse is already compressed with lz4, backup can be compressed bit better, but avoid using cpu-heavy compression algorythms like gzip, use something like zstd instead. The tool automating that process clickhouse-backup.\n","categories":"","description":"Backups\n","excerpt":"Backups\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/","tags":"","title":"Backups"},{"body":"Picking the best schema for storing many metrics registered from single source is quite a common problem.\n1 One row per metric i.e.: timestamp, sourceid, metric_name, metric_value\nPros and cons:\nPros: simple well normalized schema easy to extend that is quite typical pattern for timeseries databases Cons different metrics values stored in same columns (worse compression) to use values of different datatype you need to cast everything to string or introduce few columns for values of different types. not always nice as you need to repeat all ‘common’ fields for each row if you need to select all data for one time point you need to scan several ranges of data. 2 Each measurement (with lot of metrics) in it’s own row In that way you need to put all the metrics in one row (i.e.: timestamp, sourceid, ….) That approach is usually a source of debates about how to put all the metrics in one row.\n2a Every metric in it’s own column i.e.: timestamp, sourceid, metric1_value, … , metricN_value\nPros and cons:\nPros simple really easy to access / scan for rows with particular metric specialized and well adjusted datatypes for every metric. good for dense recording (each time point can have almost 100% of all the possible metrics) Cons adding new metric = changing the schema (adding new column). not suitable when set of metric changes dynamically not applicable when there are too many metrics (when you have more than 100-200) when each timepoint have only small subset of metrics recorded - if will create a lot of sparse filled columns. you need to store ’lack of value’ somehow (NULLs or default values) to read full row - you need to read a lot of column files. 2b Using arrays / Nested / Map i.e.: timestamp, sourceid, array_of_metric_names, array_of_metric_values\nPros and cons:\nPros easy to extend, you can have very dynamic / huge number of metrics. you can use Array(LowCardinality(String)) for storing metric names efficiently good for sparse recording (each time point can have only 1% of all the possible metrics) Cons you need to extract all metrics for row to reach a single metric not very handy / complicated non-standard syntax different metrics values stored in single array (bad compression) to use values of different datatype you need to cast everything to string or introduce few arrays for values of different types. 2c Using JSON i.e.: timestamp, sourceid, metrics_data_json\nPros and cons:\nPros easy to extend, you can have very dynamic / huge number of metrics. the only option to store hierarchical / complicated data structures, also with arrays etc. inside. good for sparse recording (each time point can have only 1% of all the possible metrics) ClickHouse has efficient API to work with JSON nice if your data originally came in JSON (don’t need to reformat) Cons uses storage non efficiently different metrics values stored in single array (bad compression) you need to extract whole JSON field to reach single metric slower than arrays 2d Using querystring-format URLs i.e.: timestamp, sourceid, metrics_querystring Same pros/cons as raw JSON, but usually bit more compact than JSON\nPros and cons:\nPros clickhouse has efficient API to work with URLs (extractURLParameter etc) can have sense if you data came in such format (i.e. you can store GET / POST request data directly w/o reprocessing) Cons slower than arrays 2e Several ‘baskets’ of arrays i.e.: timestamp, sourceid, metric_names_basket1, metric_values_basker1, …, metric_names_basketN, metric_values_basketN The same as 2b, but there are several key-value arrays (‘basket’), and metric go to one particular basket depending on metric name (and optionally by metric type)\nPros and cons:\nPros address some disadvantages of 2b (you need to read only single, smaller basket for reaching a value, better compression - less unrelated metrics are mixed together) Cons complex 2f Combined approach In real life Pareto principle is correct for many fields.\nFor that particular case: usually you need only about 20% of metrics 80% of the time. So you can pick the metrics which are used intensively, and which have a high density, and extract them into separate columns (like in option 2a), leaving the rest in a common ’trash bin’ (like in variants 2b-2e).\nWith that approach you can have as many metrics as you need and they can be very dynamic. At the same time the most used metrics are stored in special, fine-tuned columns.\nAt any time you can decide to move one more metric to a separate column ALTER TABLE ... ADD COLUMN metricX Float64 MATERIALIZED metrics.value[indexOf(metrics.names,'metricX')];\n2e Subcolumns [future] https://github.com/ClickHouse/ClickHouse/issues/23516\nWIP currently, ETA of first beta = autumn 2021\nRelated links:\nThere is one article on our blog on this subject with some benchmarks.\nSlides from Percona Live\n","categories":"","description":"Best schema for storing many metrics registered from the single source\n","excerpt":"Best schema for storing many metrics registered from the single source …","ref":"/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/","tags":"","title":"Best schema for storing many metrics registered from the single source"},{"body":" Superset: https://superset.apache.org/docs/databases/clickhouse Metabase: https://github.com/enqueue/metabase-clickhouse-driver Querybook: https://www.querybook.org/docs/setup_guide/connect_to_query_engines/#all-query-engines Tableau: Altinity Tableau Connector for ClickHouse support both JDBC \u0026 ODBC drivers Looker: https://docs.looker.com/setup-and-management/database-config/clickhouse Apache Zeppelin SeekTable ReDash Mondrian: https://altinity.com/blog/accessing-clickhouse-from-excel-using-mondrian-rolap-engine Grafana: Integrating Grafana with ClickHouse Cumul.io Tablum: https://tablum.io ","categories":"","description":"Business Intelligence Tools\n","excerpt":"Business Intelligence Tools\n","ref":"/altinity-kb-integrations/bi-tools/","tags":"","title":"BI Tools"},{"body":"Here is what different statuses mean:\nParts are renamed to ‘ignored’ if they were found during ATTACH together with other, bigger parts that cover the same blocks of data, i.e. they were already merged into something else. parts are renamed to ‘broken’ if ClickHouse was not able to load data from the parts. There could be different reasons: some files are lost, checksums are not correct, etc. parts are renamed to ‘unexpected’ if they are present locally, but are not found in ZooKeeper, in case when an insert was not completed properly. The part is detached only if it’s old enough (5 minutes), otherwise CH registers this part in ZooKeeper as a new part. parts are renamed to ‘cloned’ if ClickHouse have had some parts on local disk while repairing lost replica so already existed parts being renamed and put in detached directory. Controlled by setting detach_old_local_parts_when_cloning_replica. ‘Ignored’ parts are safe to delete. ‘Unexpected’ and ‘broken’ should be investigated, but it might not be an easy thing to do, especially for older parts. If the system.part_log table is enabled you can find some information there. Otherwise you will need to look in clickhouse-server.log for what happened when the parts were detached. If there is another way you could confirm that there is no data loss in the affected tables, you could simply delete all detached parts.\nHere is a query that can help with investigations. It looks for active parts containing the same data blocks that the detached parts:\nSELECT *, concat('alter table ',database,'.',table,' drop detached part ''',a.name,''' settings allow_drop_detached=1;') as drop FROM system.detached_parts a ALL LEFT JOIN (SELECT database, table, partition_id, name, active, min_block_number, max_block_number FROM system.parts WHERE active ) b USING (database, table, partition_id) WHERE a.min_block_number \u003e= b.min_block_number AND a.max_block_number \u003c= b.max_block_number ORDER BY table, min_block_number, max_block_number Other reasons # rg forgetPartAndMoveToDetached --type cpp # rg renameToDetached --type cpp # rg makeCloneInDetached --type cpp broken unexpected ignored noquorum merge-not-byte-identical mutate-not-byte-identical - broken-on-start clone covered-by-broken - that means that clickhouse during initialization of replicated table detected that some part is not ok, and decided to refetch it from healthy replicas. So the part itself will be detached as 'broken' and if that part was a result of merge / mutation all the previuos generations of that will be marked as covered-by-broken. If clickhouse was able to download the final part you don't need those covered-by-broken. ","categories":"","description":"Can detached parts be dropped?\n","excerpt":"Can detached parts be dropped?\n","ref":"/altinity-kb-useful-queries/detached-parts/","tags":"","title":"Can detached parts be dropped?"},{"body":" Info Article is based on feedback provided by one of Altinity clients. CatBoost:\nIt uses gradient boosting - a hard to use technique which can outperform neural networks. Gradient boosting is powerful but it’s easy to shoot yourself in the foot using it. The documentation on how to use it is quite lacking. The only good source of information on how to properly configure a model to yield good results is this video: https://www.youtube.com/watch?v=usdEWSDisS0 . We had to dig around GitHub issues to find out how to make it work with ClickHouse. CatBoost is fast. Other libraries will take ~5X to ~10X as long to do what CatBoost does. CatBoost will do preprocessing out of the box (fills nulls, apply standard scaling, encodes strings as numbers). CatBoost has all functions you’d need (metrics, plotters, feature importance) It makes sense to split what CatBoost does into 2 parts:\npreprocessing (fills nulls, apply standard scaling, encodes strings as numbers) number crunching (convert preprocessed numbers to another number - ex: revenue of impression) Compared to Fast.ai, CatBoost pre-processing is as simple to use and produces results that can be as good as Fast.ai.\nThe number crunching part of Fast.ai is no-config. For CatBoost you need to configure it, a lot.\nCatBoost won’t simplify or hide any complexity of the process. So you need to know data science terms and what it does (ex: if your model is underfitting you can use a smaller l2_reg parameter in the model constructor).\nIn the end both Fast.ai and CatBoost can yield comparable results.\nRegarding deploying models, CatBoost is really good. The model runs fast, it has a simple binary format which can be loaded in ClickHouse, C, or Python and it will encapsulate pre-processing with the binary file. Deploying Fast.ai models at scale/speed is impossible out of the box (we have our custom solution to do it which is not simple).\nTLDR: CatBoost is fast, produces awesome models, is super easy to deploy and it’s easy to use/train (after becoming familiar with it despite the bad documentation \u0026 if you know data science terms).\nRegarding MindsDB The project seems to be a good idea but it’s too young. I was using the GUI version and I’ve encountered some bugs, and none of those bugs have a good error message.\nIt won’t show data in preview.\nThe “download” button won’t work.\nIt’s trying to create and drop tables in ClickHouse without me asking it to.\nOther than bugs:\nIt will only use 1 core to do everything (training, analysis, download). Analysis will only run with a very small subset of data, if I use something like 1M rows it never finishes. Training a model on 100k rows took 25 minutes - (CatBoost takes 90s to train with 1M rows)\nThe model trained on MindsDB is way worse. It had r-squared of 0.46 (CatBoost=0.58)\nTo me it seems that they are a plugin which connects ClickHouse to MySQL to run the model in Pytorch.\nIt’s too complex and hard to debug and understand. The resulting model is not good enough.\nTLDR: Easy to use (if bugs are ignored), too slow to train \u0026 produces a bad model.\n","categories":"","description":"CatBoost / MindsDB /  Fast.ai\n","excerpt":"CatBoost / MindsDB /  Fast.ai\n","ref":"/altinity-kb-integrations/catboost-mindsdb-fast.ai/","tags":"","title":"CatBoost / MindsDB /  Fast.ai"},{"body":"Follow this link for a complete report on ClickHouse features with their availability: https://github.com/anselmodadams/ChMisc/blob/main/report/report.md. It is frequently updated (at least once a month).\n","categories":"","description":"Report on ClickHouse functions, table functions, table engines, system and MergeTree settings, with availability information. \n","excerpt":"Report on ClickHouse functions, table functions, table engines, system …","ref":"/upgrade/clickhouse-feature-report/","tags":"","title":"Clickhouse Function/Engines/Settings Report"},{"body":"Do you have documentation on Docker deployments? Check\nhttps://hub.docker.com/r/yandex/clickhouse-server/ https://docs.altinity.com/clickhouseonkubernetes/ sources of entry point - https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh Important things:\nuse concrete version tag (avoid using latest) if possible use --network=host (due to performance reasons) you need to mount the folder /var/lib/clickhouse to have persistency. you MAY also mount the folder /var/log/clickhouse-server to have logs accessible outside of the container. Also, you may mount in some files or folders in the configuration folder: /etc/clickhouse-server/config.d/listen_ports.xml --ulimit nofile=262144:262144 You can also set on some linux capabilities to enable some of extra features of ClickHouse (not obligatory): SYS_PTRACE NET_ADMIN IPC_LOCK SYS_NICE you may also mount in the folder /docker-entrypoint-initdb.d/ - all SQL or bash scripts there will be executed during container startup. if you use cgroup limits - it may misbehave https://github.com/ClickHouse/ClickHouse/issues/2261 (set up \u003cmax_server_memory_usage\u003e manually) there are several ENV switches, see: https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh TLDR version: use it as a starting point:\ndocker run -d \\ --name some-clickhouse-server \\ --ulimit nofile=262144:262144 \\ --volume=$(pwd)/data:/var/lib/clickhouse \\ --volume=$(pwd)/logs:/var/log/clickhouse-server \\ --volume=$(pwd)/configs/memory_adjustment.xml:/etc/clickhouse-server/config.d/memory_adjustment.xml \\ --cap-add=SYS_NICE \\ --cap-add=NET_ADMIN \\ --cap-add=IPC_LOCK \\ --cap-add=SYS_PTRACE \\ --network=host \\ yandex/clickhouse-server:21.1.7 docker exec -it some-clickhouse-server clickhouse-client docker exec -it some-clickhouse-server bash ","categories":"","description":"ClickHouse in Docker\n","excerpt":"ClickHouse in Docker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/","tags":"","title":"ClickHouse in Docker"},{"body":"ClickHouse Monitoring Monitoring helps to track potential issues in your cluster before they cause a critical error.\nWhat to read / watch on subject:\nAltinity webinar “ClickHouse Monitoring 101: What to monitor and how”. recording, slides docs https://clickhouse.com/docs/en/operations/monitoring/ What should be monitored The following metrics should be collected / monitored\nFor Host Machine:\nCPU Memory Network (bytes/packets) Storage (iops) Disk Space (free / used) For ClickHouse:\nConnections (count) RWLocks Read / Write / Return (bytes) Read / Write / Return (rows) Zookeeper operations (count) Absolute delay Query duration (optional) Replication parts and queue (count) For Zookeeper:\nSee separate article Monitoring tools Prometheus (embedded exporter) + Grafana Enable embedded exporter Grafana dashboards https://grafana.com/grafana/dashboards/14192 or https://grafana.com/grafana/dashboards/13500 Prometheus (embedded http handler with clickhouse-operator style metrics) + Grafana Enable http handler Useful, if you want to use clickhouse-operator dashboard, but do not run ClickHouse in k8s. Prometheus (clickhouse-operator embedded exporter) + Grafana exporter is included in clickhouse-operator, and enabled automatically see instructions of Prometheus and Grafana installation (if you don’t have one) Grafana dashboard https://github.com/Altinity/clickhouse-operator/tree/master/grafana-dashboard Prometheus alerts https://github.com/Altinity/clickhouse-operator/blob/master/deploy/prometheus/prometheus-alert-rules-clickhouse.yaml Prometheus (clickhouse external exporter) + Grafana clickhouse-exporter Dashboard: https://grafana.com/grafana/dashboards/882 (unmaintained)\nDashboards quering clickhouse directly via vertamedia / Altinity plugin Overview: https://grafana.com/grafana/dashboards/13606 Queries dashboard (analyzing system.query_log) https://grafana.com/grafana/dashboards/2515 Dashboard quering clickhouse directly via Grafana plugin https://grafana.com/blog/2022/05/05/introducing-the-official-clickhouse-plugin-for-grafana/ Zabbix https://www.zabbix.com/integrations/clickhouse https://github.com/Altinity/clickhouse-zabbix-template Graphite Use the embedded exporter. See docs and config.xml InfluxDB You can use embedded exporter, plus Telegraf. For more information, see Graphite protocol support in InfluxDB. Nagios/Icinga https://github.com/exogroup/check_clickhouse/ Commercial solution Datadog https://docs.datadoghq.com/integrations/clickhouse/?tab=host Sematext https://sematext.com/docs/integration/clickhouse/ Instana https://www.instana.com/supported-technologies/clickhouse-monitoring/ site24x7 https://www.site24x7.com/plugins/clickhouse-monitoring.html Acceldata Pulse https://www.acceldata.io/blog/acceldata-pulse-for-clickhouse-monitoring “Build your own” monitoring ClickHouse allow to access lot of internals using system tables. The main tables to access monitoring data are:\nsystem.metrics system.asynchronous_metrics system.events Minimum neccessary set of checks\nCheck Name Shell or SQL command Severity ClickHouse status $ curl 'http://localhost:8123/' Ok. Critical Too many simultaneous queries. Maximum: 100 (by default) select value from system.metrics where metric='Query' Critical Replication status $ curl 'http://localhost:8123/replicas_status' Ok. High Read only replicas (reflected by replicas_status as well) select value from system.metrics where metric='ReadonlyReplica' High Some replication tasks are stuck select count() from system.replication_queue where num_tries \u003e 100 or num_postponed \u003e 1000 High ZooKeeper is available select count() from system.zookeeper where path='/' Critical for writes ZooKeeper exceptions select value from system.events where event='ZooKeeperHardwareExceptions' Medium Other CH nodes are available $ for node in `echo \"select distinct host_address from system.clusters where host_name !='localhost'\" | curl 'http://localhost:8123/' --silent --data-binary @-`; do curl \"http://$node:8123/\" --silent ; done | sort -u Ok. High All CH clusters are available (i.e. every configured cluster has enough replicas to serve queries) for cluster in `echo \"select distinct cluster from system.clusters where host_name !='localhost'\" | curl 'http://localhost:8123/' --silent --data-binary @-` ; do clickhouse-client --query=\"select '$cluster', 'OK' from cluster('$cluster', system, one)\" ; done Critical There are files in 'detached' folders $ find /var/lib/clickhouse/data/*/*/detached/* -type d | wc -l; \\ 19.8+ select count() from system.detached_parts Medium Too many parts: \\ Number of parts is growing; \\ Inserts are being delayed; \\ Inserts are being rejected select value from system.asynchronous_metrics where metric='MaxPartCountForPartition'; select value from system.events/system.metrics where event/metric='DelayedInserts'; \\ select value from system.events where event='RejectedInserts' Critical Dictionaries: exception select concat(name,': ',last_exception) from system.dictionaries where last_exception != '' Medium ClickHouse has been restarted select uptime(); select value from system.asynchronous_metrics where metric='Uptime' DistributedFilesToInsert should not be always increasing select value from system.metrics where metric='DistributedFilesToInsert' Medium A data part was lost select value from system.events where event='ReplicatedDataLoss' High Data parts are not the same on different replicas select value from system.events where event='DataAfterMergeDiffersFromReplica'; \\ select value from system.events where event='DataAfterMutationDiffersFromReplica' Medium The following queries are recommended to be included in monitoring:\nSELECT * FROM system.replicas For more information, see the ClickHouse guide on System Tables SELECT * FROM system.merges Checks on the speed and progress of currently executed merges. SELECT * FROM system.mutations This is the source of information on the speed and progress of currently executed merges. Logs monitoring ClickHouse logs can be another important source of information. There are 2 logs enabled by default\n/var/log/clickhouse-server/clickhouse-server.err.log (error \u0026 warning, you may want to keep an eye on that or send it to some monitoring system) /var/log/clickhouse-server/clickhouse-server.log (trace logs, very detailed, useful for debugging, usually too verbose to monitor). You can additionally enable system.text_log table to have an access to the logs from clickhouse sql queries (ensure that you will not expose some information to the users which should not see it).\n$ cat /etc/clickhouse-server/config.d/text_log.xml \u003cyandex\u003e \u003ctext_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003etext_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003clevel\u003ewarning\u003c/level\u003e \u003c/text_log\u003e \u003c/yandex\u003e OpenTelemetry support See https://clickhouse.com/docs/en/operations/opentelemetry/\nOther sources https://tech.marksblogg.com/clickhouse-prometheus-grafana.html Key Metrics for Monitoring ClickHouse Monitor ClickHouse with Datadog Unsorted notes on monitor and Alerts https://intl.cloud.tencent.com/document/product/1026/36887 ","categories":"","description":"ClickHouse Monitoring\n","excerpt":"ClickHouse Monitoring\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/","tags":"","title":"ClickHouse Monitoring"},{"body":"ClickHouse versioning schema Example:\n21.3.10.1-lts\n21 is the year of release. 3 indicates a Feature Release. This is an increment where features are delivered. 10 is the bugfix / maintenance version. When that version is incremented it means that some bugs was fixed comparing to 21.3.9. 1 - build number, means nothing for end users. lts - type of release. (long time support). What is Altinity Stable version? It is one of general / public version of ClickHouse which has passed some extra testings, the upgrade path and changelog was analyzed, known issues are documented, and at least few big companies use it on production. All those things take some time, so usually that means that Altinity Stable is always a ‘behind’ the main releases.\nAltinity version - is an option for conservative users, who prefer bit older but better known things.\nUsually there is no reason to use version older than Altinity Stable. If you see that new Altinity Version arrived and you still use some older version - you should for sure consider an upgrade.\nAdditionally for Altinity client we provide extra support for those version for a longer time (and we also support newer versions).\nWhich version should I use? We recommend the following approach:\nWhen you start using ClickHouse and before you go on production - pick the latest stable version. If you already have ClickHouse running on production: Check all the new queries / schemas on the staging first, especially if some new ClickHouse features are used. Do minor (bugfix) upgrades regularly: monitor new maintenance releases of the feature release you use. When considering upgrade - check Altinity Stable release docs, if you want to use newer release - analyze changelog and known issues. Check latest stable or test versions of ClickHouse on your staging environment regularly and pass the feedback to us or on the official ClickHouse github. Consider blue/green or canary upgrades. See also: https://clickhouse.tech/docs/en/faq/operations/production/\nHow do I upgrade? Warning Check upgrade / downgrade scenario on staging first. check if you need to adjust some settings / to opt-out some new features you don’t need (maybe needed to to make the downgrade path possible, or to make it possible for 2 versions to work together). upgrade packages on odd replicas (if needed / depends on use case) stop ingestion into odd replicas / remove them for load-balancer etc. restart clickhouse-server service on odd replicas. once odd replicas will go back online - repeat the same procedure on the even replicas. In some upgrade scenarios (depending from which version to which you do upgrate) when differerent replicas use different clickhouse versions you may see following issues:\nthe replication don’t work at all and delays grow. errors about ‘checksum mismatch’ and traffic between replicase increase (they need to resync merge results). Both problems will go away once all replicas will be upgraded.\nBugs? ClickHouse development process goes in a very high pace and has already thousands of features. CI system doing tens of thousands of tests (including tests with different sanitizers) against every commit.\nAll core features are well-tested, and very stable, and code is high-quality. But as with any other software bad things may happen. Usually the most of bugs happens in the new, freshly added functionality, and in some complex combination of several features (of course all possible combinations of features just physically can’t be tested). Usually new features are adopted by the community and stabilize quickly.\nWhat should I do if I found a bug in clickhouse? First of all: try to upgrade to the latest bugfix release Example: if you use v21.3.5.42-lts but you know that v21.3.10.1-lts already exists - start with upgrade to that. Upgrades to latest maintenance releases are smooth and safe. Look for similar issues in github. Maybe the fix is on the way. If you can reproduce the bug: try to isolate it - remove some pieces of query one-by-one / simplify the scenario until the issue still reproduces. This way you can figure out which part is responsible for that bug, and you can try to create minimal reproducible example Once you have minimal reproducible example: report it to github (or to Altinity Support) check if it reproduces on newer clickhouse versions ","categories":"","description":"ClickHouse versions\n","excerpt":"ClickHouse versions\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-versions/","tags":"","title":"ClickHouse versions"},{"body":"Installation and configuration Download the latest clickhouse-backup.tar.gz from assets from https://github.com/AlexAkulov/clickhouse-backup/releases\nThis tar.gz contains a single binary of clickhouse-backup and an example of config file.\nBackblaze has s3 compatible API but requires empty acl parameter acl: \"\".\nhttps://www.backblaze.com/ has 15 days and free 10Gb S3 trial.\n$ mkdir clickhouse-backup $ cd clickhouse-backup $ wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/2.2.0/clickhouse-backup.tar.gz $ tar zxf clickhouse-backup.tar.gz $ rm clickhouse-backup.tar.gz $ cat config.yml general: remote_storage: s3 max_file_size: 1099511627776 disable_progress_bar: false backups_to_keep_local: 0 backups_to_keep_remote: 0 log_level: info allow_empty_backups: false clickhouse: username: default password: \"\" host: localhost port: 9000 disk_mapping: {} skip_tables: - system.* timeout: 5m freeze_by_part: false secure: false skip_verify: false sync_replicated_tables: true log_sql_queries: false s3: access_key: 0****1 secret_key: K****1 bucket: \"mybucket\" endpoint: https://s3.us-west-000.backblazeb2.com region: us-west-000 acl: \"\" force_path_style: false path: clickhouse-backup disable_ssl: false part_size: 536870912 compression_level: 1 compression_format: tar sse: \"\" disable_cert_verification: false storage_class: STANDARD I have a database test with table test\nselect count() from test.test; ┌─count()─┐ │ 400000 │ └─────────┘ clickhouse-backup list should work without errors (it scans local and remote (s3) folders):\n$ sudo ./clickhouse-backup list -c config.yml $ Backup create a local backup of database test upload this backup to remote remove the local backup drop the source database $ sudo ./clickhouse-backup create --tables='test.*' bkp01 -c config.yml 2021/05/31 23:11:13 info done backup=bkp01 operation=create table=test.test 2021/05/31 23:11:13 info done backup=bkp01 operation=create $ sudo ./clickhouse-backup upload bkp01 -c config.yml 1.44 MiB / 1.44 MiB [=====================] 100.00% 2s 2021/05/31 23:12:13 info done backup=bkp01 operation=upload table=test.test 2021/05/31 23:12:17 info done backup=bkp01 operation=upload $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 local bkp01 1.44MiB 31/05/2021 23:11:13 remote tar $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:13:29 info delete 'bkp01' DROP DATABASE test; Restore download the remote backup restore database $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 remote tar $ sudo ./clickhouse-backup download bkp01 -c config.yml 2021/05/31 23:14:41 info done backup=bkp01 operation=download table=test.test 1.47 MiB / 1.47 MiB [=====================] 100.00% 0s 2021/05/31 23:14:43 info done backup=bkp01 operation=download table=test.test 2021/05/31 23:14:43 info done backup=bkp01 operation=download $ sudo ./clickhouse-backup restore bkp01 -c config.yml 2021/05/31 23:16:04 info done backup=bkp01 operation=restore table=test.test 2021/05/31 23:16:04 info done backup=bkp01 operation=restore SELECT count() FROM test.test; ┌─count()─┐ │ 400000 │ └─────────┘ Delete backups $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:17:05 info delete 'bkp01' $ sudo ./clickhouse-backup delete remote bkp01 -c config.yml ","categories":"","description":"clickhouse-backup + backblaze\n","excerpt":"clickhouse-backup + backblaze\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-backup/","tags":"","title":"clickhouse-backup"},{"body":"Q. How can I input multi-line SQL code? can you guys give me an example?\nA. Just run clickhouse-client with -m switch, and it starts executing only after you finish the line with a semicolon.\nQ. How can i use pager with clickhouse-client\nA. Here is an example: clickhouse-client --pager 'less -RS'\nQ. Data is returned in chunks / several tables.\nA. Data get streamed from the server in blocks, every block is formatted individually when the default PrettyCompact format is used. You can use PrettyCompactMonoBlock format instead, using one of the options:\nstart clickhouse-client with an extra flag: clickhouse-client --format=PrettyCompactMonoBlock add FORMAT PrettyCompactMonoBlock at the end of your query. change clickhouse-client default format in the config. See https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42 Q. Сustomize client config\nA. you can change it globally (for all users of the workstation)\nnano /etc/clickhouse-client/conf.d/user.xml \u003cconfig\u003e \u003cuser\u003edefault1\u003c/user\u003e \u003cpassword\u003edefault1\u003c/password\u003e \u003chost\u003e\u003c/host\u003e \u003cmultiline\u003etrue\u003c/multiline\u003e \u003cmultiquery\u003etrue\u003c/multiquery\u003e \u003c/config\u003e See also https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42 or for particular users - by adjusting one of.\n./clickhouse-client.xml ~/.clickhouse-client/config.xml Also, it’s possible to have several client config files and pass one of them to clickhouse-client command explicitly\nReferences:\nhttps://clickhouse.tech/docs/en/interfaces/cli/ ","categories":"","description":"ClickHouse client\n","excerpt":"ClickHouse client\n","ref":"/altinity-kb-interfaces/altinity-kb-clickhouse-client/","tags":"","title":"clickhouse-client"},{"body":"The description of the utility and its parameters, as well as examples of the config files that you need to create for the copier are in the official doc ClickHouse copier utility\nThe steps to run a task:\nCreate a config file for clickhouse-copier (zookeeper.xml)\nZooKeeper config format\nCreate a config file for the task (task1.xml)\nCopy task configuration\nCreate the task in ZooKeeper and start an instance of clickhouse-copier\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config=/opt/clickhouse-copier/zookeeper.xml --task-path=/clickhouse/copier/task1 --task-file=/opt/clickhouse-copier/task1.xml\nIf the node in ZooKeeper already exists and you want to change it, you need to add the task-upload-force parameter:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config=/opt/clickhouse-copier/zookeeper.xml --task-path=/clickhouse/copier/task1 --task-file=/opt/clickhouse-copier/task1.xml --task-upload-force=1\nIf you want to run another instance of clickhouse-copier for the same task, you need to copy the config file (zookeeper.xml) to another server, and run this command:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config=/opt/clickhouse-copier/zookeeper.xml --task-path=/clickhouse/copier/task1\nThe number of simultaneously running instances is controlled be the max_workers parameter in your task configuration file. If you run more workers superfluous workers will sleep and log messages like this:\n\u003cDebug\u003e ClusterCopier: Too many workers (1, maximum 1). Postpone processing\nSee also https://clickhouse.com/docs/en/operations/utilities/clickhouse-copier/ Никита Михайлов. Кластер ClickHouse ctrl-с ctrl-v. HighLoad++ Весна 2021 slides 21.7 have a huge bulk of fixes / improvements. https://github.com/ClickHouse/ClickHouse/pull/23518 https://altinity.com/blog/2018/8/22/clickhouse-copier-in-practice https://github.com/getsentry/snuba/blob/master/docs/clickhouse-copier.md https://hughsite.com/post/clickhouse-copier-usage.html https://www.jianshu.com/p/c058edd664a6 ","categories":"","description":"clickhouse-copier\n","excerpt":"clickhouse-copier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/","tags":"","title":"clickhouse-copier"},{"body":"Clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe process is as follows Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one. Drop the partition from the target table if it’s not empty Copy data from source shards one by one. Check if there is data for the partition on a source shard. Check the status of the task in ZooKeeper. Create target tables on all shards of the target cluster. Insert the partition of data into the target table. Mark the partition as completed in ZooKeeper. If there are several workers running simultaneously, they will assign themselves to different source shards. If a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table Clickhouse-copier uses the engine from the task configuration file for these purposes:\nto create target tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to DROP existing partitions from target tables. Clickhouse-copier does not support the old MergeTree format. However, you can create the target tables manually and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for its SELECT queries.\nHow to monitor the status of running tasks Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers. Here is a list of queries that you can use to see what’s happening.\n--task-path /clickhouse/copier/task1 -- The task config select * from system.zookeeper where path='\u003ctask-path\u003e' name | ctime | mtime ----------------------------+---------------------+-------------------- description | 2019-10-18 15:40:00 | 2020-09-11 16:01:14 task_active_workers_version | 2019-10-18 16:00:09 | 2020-09-11 16:07:08 tables | 2019-10-18 16:00:25 | 2019-10-18 16:00:25 task_active_workers | 2019-10-18 16:00:09 | 2019-10-18 16:00:09 -- Running workers select * from system.zookeeper where path='\u003ctask-path\u003e/task_active_workers' -- The list of processed tables select * from system.zookeeper where path='\u003ctask-path\u003e/tables' -- The list of processed partitions select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e' name | ctime -------+-------------------- 201909 | 2019-10-18 18:24:18 -- The status of a partition select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e' name | ctime -------------------------+-------------------- shards | 2019-10-18 18:24:18 partition_active_workers | 2019-10-18 18:24:18 -- The status of source shards select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/shards' name | ctime | mtime -----+---------------------+-------------------- 1 | 2019-10-18 22:37:48 | 2019-10-18 22:49:29 ","categories":"","description":"clickhouse-copier 20.3 and earlier\n","excerpt":"clickhouse-copier 20.3 and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/","tags":"","title":"clickhouse-copier 20.3 and earlier"},{"body":"Clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe behavior of clickhouse-copier was changed in 20.4:\nNow clickhouse-copier inserts data into intermediate tables, and after the insert finishes successfully clickhouse-copier attaches the completed partition into the target table. This allows for incremental data copying, because the data in the target table is intact during the process. Important note: ATTACH PARTITION respects the max_partition_size_to_drop limit. Make sure the max_partition_size_to_drop limit is big enough (or set to zero) in the destination cluster. If clickhouse-copier is unable to attach a partition because of the limit, it will proceed to the next partition, and it will drop the intermediate table when the task is finished (if the intermediate table is less than the max_table_size_to_drop limit). Another important note: ATTACH PARTITION is replicated. The attached partition will need to be downloaded by the other replicas. This can create significant network traffic between ClickHouse nodes. If an attach takes a long time, clickhouse-copier will log a timeout and will proceed to the next step. Now clickhouse-copier splits the source data into chunks and copies them one by one. This is useful for big source tables, when inserting one partition of data can take hours. If there is an error during the insert clickhouse-copier has to drop the whole partition and start again. The number_of_splits parameter lets you split your data into chunks so that in case of an exception clickhouse-copier has to re-insert only one chunk of the data. Now clickhouse-copier runs OPTIMIZE target_table PARTITION ... DEDUPLICATE for non-Replicated MergeTree tables. Important note: This is a very strange feature that can do more harm than good. We recommend to disable it by configuring the engine of the target table as Replicated in the task configuration file, and create the target tables manually if they are not supposed to be replicated. Intermediate tables are always created as plain MergeTree. The process is as follows Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one ** The metadata in ZooKeeper suggests the order described here.** Copy chunks of data one by one. Copy data from source shards one by one. Create intermediate tables on all shards of the target cluster. Check the status of the chunk in ZooKeeper. Drop the partition from the intermediate table if the previous attempt was interrupted. Insert the chunk of data into the intermediate tables. Mark the shard as completed in ZooKeeper Attach the chunks of the completed partition into the target table one by one Attach a chunk into the target table. non-Replicated: Run OPTIMIZE target_table DEDUPLICATE for the partition on the target table. Drop intermediate tables (may not succeed if the tables are bigger than max_table_size_to_drop). If there are several workers running simultaneously, they will assign themselves to different source shards. If a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table Clickhouse-copier uses the engine from the task configuration file for these purposes:\nto create target and intermediate tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to ATTACH partitions into target tables, to DROP incomplete partitions from intermediate tables, to OPTIMIZE partitions after they are attached to the target. ORDER BY: to SELECT a chunk of data from the source table. Here is an example of SELECT that clickhouse-copier runs to get the sixth of ten chunks of data:\nWHERE (\u003cthe PARTITION BY clause\u003e = (\u003ca value of the PARTITION BY expression\u003e AS partition_key)) AND (cityHash64(\u003cthe ORDER BY clause\u003e) % 10 = 6 ) Clickhouse-copier does not support the old MergeTree format. However, you can create the intermediate tables manually with the same engine as the target tables (otherwise ATTACH will not work), and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for SELECT, ATTACH PARTITION and DROP PARTITION queries.\nImportant note: always configure engine as Replicated to disable OPTIMIZE … DEDUPLICATE (unless you know why you need clickhouse-copier to run OPTIMIZE … DEDUPLICATE).\nHow to configure the number of chunks The default value for number_of_splits is 10. You can change this parameter in the table section of the task configuration file. We recommend setting it to 1 for smaller tables.\n\u003ccluster_push\u003etarget_cluster\u003c/cluster_push\u003e \u003cdatabase_push\u003etarget_database\u003c/database_push\u003e \u003ctable_push\u003etarget_table\u003c/table_push\u003e \u003cnumber_of_splits\u003e1\u003c/number_of_splits\u003e \u003cengine\u003eEngine=Replicated...\u003cengine\u003e How to monitor the status of running tasks Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers. Here is a list of queries that you can use to see what’s happening.\n--task-path=/clickhouse/copier/task1 -- The task config select * from system.zookeeper where path='\u003ctask-path\u003e' name | ctime | mtime ----------------------------+---------------------+-------------------- description | 2021-03-22 13:15:48 | 2021-03-22 13:25:28 status | 2021-03-22 13:15:48 | 2021-03-22 13:25:28 task_active_workers_version | 2021-03-22 13:15:48 | 2021-03-22 20:32:09 tables | 2021-03-22 13:16:47 | 2021-03-22 13:16:47 task_active_workers | 2021-03-22 13:15:48 | 2021-03-22 13:15:48 -- Status select * from system.zookeeper where path='\u003ctask-path\u003e/status' -- Running workers select * from system.zookeeper where path='\u003ctask-path\u003e/task_active_workers' -- The list of processed tables select * from system.zookeeper where path='\u003ctask-path\u003e/tables' -- The list of processed partitions select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e' name | ctime -------+-------------------- 202103 | 2021-03-22 13:16:47 202102 | 2021-03-22 13:18:31 202101 | 2021-03-22 13:27:36 202012 | 2021-03-22 14:05:08 -- The status of a partition select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e' name | ctime ---------------+-------------------- piece_0 | 2021-03-22 13:18:31 attach_is_done | 2021-03-22 14:05:05 -- The status of a piece select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N' name | ctime -------------------------------+-------------------- shards | 2021-03-22 13:18:31 is_dirty | 2021-03-22 13:26:51 partition_piece_active_workers | 2021-03-22 13:26:54 clean_start | 2021-03-22 13:26:54 -- The status of source shards select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N/shards' name | ctime | mtime -----+---------------------+-------------------- 1 | 2021-03-22 13:26:54 | 2021-03-22 14:05:05 ","categories":"","description":"clickhouse-copier 20.4 - 21.6\n","excerpt":"clickhouse-copier 20.4 - 21.6\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4_21.6/","tags":"","title":"clickhouse-copier 20.4 - 21.6"},{"body":"clickhouse-keeper Since 2021 the developement of built-in alternative for Zookeeper is happening, which goal is to address several design pitfalls, and get rid of extra dependency.\nSee slides: https://presentations.clickhouse.com/meetup54/keeper.pdf and video https://youtu.be/IfgtdU1Mrm0?t=2682\nCurrent status (last updated: July 2023) Since version 23.3 we recommend using clickhouse-keeper for new installations.\nEven better if you will use the latest version of clickhouse-keeper (currently it’s 23.7), and it’s not necessary to use the same version of clickhouse-keeper as clickhouse itself.\nFor existing systems that currently use Apache Zookeeper, you can consider upgrading to clickhouse-keeper especially if you will upgrade clickhouse also.\nBut please remember that on very loaded systems the change can give no performance benefits or can sometimes lead to a worse perfomance.\nThe development pace of keeper code is still high so every new version should bring improvements / cover the issues, and stability/maturity grows from version to version, so if you want to play with clickhouse-keeper in some environment - please use the most recent ClickHouse releases! And of course: share your feedback :)\nHow does it work Official docs: https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper/\nClickhouse-keeper still need to be started additionally on few nodes (similar to ’normal’ zookeeper) and speaks normal zookeeper protocol - needed to simplify A/B tests with real zookeeper.\nTo test that you need to run 3 instances of clickhouse-server (which will mimic zookeeper) with an extra config like that:\nhttps://github.com/ClickHouse/ClickHouse/blob/master/tests/integration/test_keeper_multinode_simple/configs/enable_keeper1.xml\nhttps://github.com/ClickHouse/ClickHouse/blob/master/tests/integration/test_keeper_snapshots/configs/enable_keeper.xml\nor event single instance with config like that: https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/keeper_port.xml https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/zookeeper.xml\nAnd point all the clickhouses (zookeeper config secton) to those nodes / ports.\nLatests version is recommended (even testing / master builds). We will be thankful for any feedback.\nsystemd service file See https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-service/\ninit.d script See https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-initd/\nExample of a simple cluster with 2 nodes of Clickhouse using built-in keeper For example you can start two Clikhouse nodes (hostname1, hostname2)\nhostname1 $ cat /etc/clickhouse-server/config.d/keeper.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003ckeeper_server\u003e \u003ctcp_port\u003e2181\u003c/tcp_port\u003e \u003cserver_id\u003e1\u003c/server_id\u003e \u003clog_storage_path\u003e/var/lib/clickhouse/coordination/log\u003c/log_storage_path\u003e \u003csnapshot_storage_path\u003e/var/lib/clickhouse/coordination/snapshots\u003c/snapshot_storage_path\u003e \u003ccoordination_settings\u003e \u003coperation_timeout_ms\u003e10000\u003c/operation_timeout_ms\u003e \u003csession_timeout_ms\u003e30000\u003c/session_timeout_ms\u003e \u003craft_logs_level\u003etrace\u003c/raft_logs_level\u003e \u003crotate_log_storage_interval\u003e10000\u003c/rotate_log_storage_interval\u003e \u003c/coordination_settings\u003e \u003craft_configuration\u003e \u003cserver\u003e \u003cid\u003e1\u003c/id\u003e \u003chostname\u003ehostname1\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003cserver\u003e \u003cid\u003e2\u003c/id\u003e \u003chostname\u003ehostname2\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003c/raft_configuration\u003e \u003c/keeper_server\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/testcluster/task_queue/ddl\u003c/path\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e $ cat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cmacros\u003e \u003ccluster\u003etestcluster\u003c/cluster\u003e \u003creplica\u003ereplica1\u003c/replica\u003e \u003cshard\u003e1\u003c/shard\u003e \u003c/macros\u003e \u003c/yandex\u003e hostname2 $ cat /etc/clickhouse-server/config.d/keeper.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003ckeeper_server\u003e \u003ctcp_port\u003e2181\u003c/tcp_port\u003e \u003cserver_id\u003e2\u003c/server_id\u003e \u003clog_storage_path\u003e/var/lib/clickhouse/coordination/log\u003c/log_storage_path\u003e \u003csnapshot_storage_path\u003e/var/lib/clickhouse/coordination/snapshots\u003c/snapshot_storage_path\u003e \u003ccoordination_settings\u003e \u003coperation_timeout_ms\u003e10000\u003c/operation_timeout_ms\u003e \u003csession_timeout_ms\u003e30000\u003c/session_timeout_ms\u003e \u003craft_logs_level\u003etrace\u003c/raft_logs_level\u003e \u003crotate_log_storage_interval\u003e10000\u003c/rotate_log_storage_interval\u003e \u003c/coordination_settings\u003e \u003craft_configuration\u003e \u003cserver\u003e \u003cid\u003e1\u003c/id\u003e \u003chostname\u003ehostname1\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003cserver\u003e \u003cid\u003e2\u003c/id\u003e \u003chostname\u003ehostname2\u003c/hostname\u003e \u003cport\u003e9444\u003c/port\u003e \u003c/server\u003e \u003c/raft_configuration\u003e \u003c/keeper_server\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/testcluster/task_queue/ddl\u003c/path\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e $ cat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cmacros\u003e \u003ccluster\u003etestcluster\u003c/cluster\u003e \u003creplica\u003ereplica2\u003c/replica\u003e \u003cshard\u003e1\u003c/shard\u003e \u003c/macros\u003e \u003c/yandex\u003e on both $ cat /etc/clickhouse-server/config.d/clusters.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cremote_servers\u003e \u003ctestcluster\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003ehostname1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003ehostname2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/testcluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e Then create a table\ncreate table test on cluster '{cluster}' ( A Int64, S String) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}') Order by A; insert into test select number, '' from numbers(100000000); -- on both nodes: select count() from test; ","categories":"","description":"clickhouse-keeper\n","excerpt":"clickhouse-keeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper/","tags":"","title":"clickhouse-keeper"},{"body":"ClickHouse does not start, some other unexpected behavior happening Check clickhouse logs, they are your friends:\ntail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log | less tail -n 10000 /var/log/clickhouse-server/clickhouse-server.log | less\nHow Do I Restrict Memory Usage? See our knowledge base article and official documentation for more information.\nClickHouse died during big query execution Misconfigured clickhouse can try to allocate more RAM than is available on the system.\nIn that case an OS component called oomkiller can kill the clickhouse process.\nThat event leaves traces inside system logs (can be checked by running dmesg command).\nHow Do I make huge ‘Group By’ queries use less RAM? Enable on disk GROUP BY (it is slower, so is disabled by default)\nSet max_bytes_before_external_group_by to a value about 70-80% of your max_memory_usage value.\nData returned in chunks by clickhouse-client See altinity-kb-clickhouse-client\nI Can’t Connect From Other Hosts. What do I do? Check the settings in config.xml. Verify that the connection can connect on both IPV4 and IPV6.\n","categories":"","description":"Cluster Configuration FAQ\n","excerpt":"Cluster Configuration FAQ\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/","tags":"","title":"Cluster Configuration FAQ"},{"body":"So you set up 3 nodes with zookeeper (zookeeper1, zookeeper2, zookeeper3 - How to install zookeer?), and and 4 nodes with ClickHouse (clickhouse-sh1r1,clickhouse-sh1r2,clickhouse-sh2r1,clickhouse-sh2r2 - how to install ClickHouse?). Now we need to make them work together.\nUse ansible/puppet/salt or other systems to control the servers’ configurations.\nConfigure ClickHouse access to Zookeeper by adding the file zookeeper.xml in /etc/clickhouse-server/config.d/ folder. This file must be placed on all ClickHouse servers. \u003cyandex\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003ezookeeper1\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper2\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper3\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003c/yandex\u003e On each server put the file macros.xml in /etc/clickhouse-server/config.d/ folder. \u003cyandex\u003e \u003c!-- That macros are defined per server, and they can be used in DDL, to make the DB schema cluster/server neutral --\u003e \u003cmacros\u003e \u003ccluster\u003eprod_cluster\u003c/cluster\u003e \u003cshard\u003e01\u003c/shard\u003e \u003creplica\u003eclickhouse-sh1r1\u003c/replica\u003e \u003c!-- better - use the same as hostname --\u003e \u003c/macros\u003e \u003c/yandex\u003e On each server place the file cluster.xml in /etc/clickhouse-server/config.d/ folder. Before 20.10 ClickHouse will use default user to connect to other nodes (configurable, other users can be used), since 20.10 we recommend to use passwordless intercluster authentication based on common secret (HMAC auth) \u003cyandex\u003e \u003cremote_servers\u003e \u003cprod_cluster\u003e \u003c!-- you need to give a some name for a cluster --\u003e \u003c!-- \u003csecret\u003esome_random_string, same on all cluster nodes, keep it safe\u003c/secret\u003e --\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/prod_cluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e A good practice is to create 2 additional cluster configurations similar to prod_cluster above with the following distinction: but listing all nodes of single shard (all are replicas) and as nodes of 6 different shards (no replicas) all-replicated: All nodes are listed as replicas in a single shard. all-sharded: All nodes are listed as separate shards with no replicas. Once this is complete, other queries that span nodes can be performed. For example:\nCREATE TABLE test_table_local ON CLUSTER '{cluster}' ( id UInt8 ) Engine=ReplicatedMergeTree('/clickhouse/tables/{database}/{table}/{shard}', '{replica}') ORDER BY (id); That will create a table on all servers in the cluster. You can insert data into this table and it will be replicated automatically to the other shards.To store the data or read the data from all shards at the same time, create a Distributed table that links to the replicatedMergeTree table.\nCREATE TABLE test_table ON CLUSTER '{cluster}' Engine=Distributed('{cluster}', 'default', ' Hardening ClickHouse Security See https://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\nAdditional Settings See altinity-kb-settings-to-adjust\nUsers Disable or add password for the default users default and readonly if your server is accessible from non-trusted networks.\nIf you add password to the default user, you will need to adjust cluster configuration, since the other servers need to know the default user’s should know the default user’s to connect to each other.\nIf you’re inside a trusted network, you can leave default user set to nothing to allow the ClickHouse nodes to communicate with each other.\nEngines \u0026 ClickHouse building blocks For general explanations of roles of different engines - check the post Distributed vs Shard vs Replicated ahhh, help me!!!.\nZookeeper Paths Use conventions for zookeeper paths. For example, use:\nReplicatedMergeTree(’/clickhouse/{cluster}/tables/{shard}/table_name’, ‘{replica}’)\nfor:\nSELECT * FROM system.zookeeper WHERE path=’/ …';\nConfiguration Best Practices Attribution\nModified by a post [on GitHub by Mikhail Filimonov](https://github.com/ClickHouse/ClickHouse/issues/3607#issuecomment-440235298).\nThe following are recommended Best Practices when it comes to setting up a ClickHouse Cluster with Zookeeper:\nDon’t edit/overwrite default configuration files. Sometimes a newer version of ClickHouse introduces some new settings or changes the defaults in config.xml and users.xml. Set configurations via the extra files in conf.d directory. For example, to overwrite the interface save the file config.d/listen.xml, with the following: \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003clisten_host replace=\"replace\"\u003e::\u003c/listen_host\u003e \u003c/yandex\u003e The same is true for users. For example, change the default profile by putting the file in users.d/profile_default.xml: \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault replace=\"replace\"\u003e \u003cmax_memory_usage\u003e15000000000\u003c/max_memory_usage\u003e \u003cmax_bytes_before_external_group_by\u003e12000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e12000000000\u003c/max_bytes_before_external_sort\u003e \u003cdistributed_aggregation_memory_efficient\u003e1\u003c/distributed_aggregation_memory_efficient\u003e \u003cuse_uncompressed_cache\u003e0\u003c/use_uncompressed_cache\u003e \u003cload_balancing\u003erandom\u003c/load_balancing\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003cmax_execution_time\u003e600\u003c/max_execution_time\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e Or you can create a user by putting a file users.d/user_xxx.xml (since 20.5 you can also use CREATE USER) \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cusers\u003e \u003cxxx\u003e \u003c!-- PASSWORD=$(base64 \u003c /dev/urandom | head -c8); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha256sum | tr -d '-' --\u003e \u003cpassword_sha256_hex\u003e...\u003c/password_sha256_hex\u003e \u003cnetworks incl=\"networks\" /\u003e \u003cprofile\u003ereadonly\u003c/profile\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003callow_databases incl=\"allowed_databases\" /\u003e \u003c/xxx\u003e \u003c/users\u003e \u003c/yandex\u003e Some parts of configuration will contain repeated elements (like allowed ips for all the users). To avoid repeating that - use substitutions file. By default its /etc/metrika.xml, but you can change it for example to /etc/clickhouse-server/substitutions.xml with the \u003cinclude_from\u003e section of the main config. Put the repeated parts into substitutions file, like this: \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cnetworks\u003e \u003cip\u003e::1\u003c/ip\u003e \u003cip\u003e127.0.0.1\u003c/ip\u003e \u003cip\u003e10.42.0.0/16\u003c/ip\u003e \u003cip\u003e192.168.0.0/24\u003c/ip\u003e \u003c/networks\u003e \u003c/yandex\u003e These files can be common for all the servers inside the cluster or can be individualized per server. If you choose to use one substitutions file per cluster, not per node, you will also need to generate the file with macros, if macros are used.\nThis way you have full flexibility; you’re not limited to the settings described in the template. You can change any settings per server or data center just by assigning files with some settings to that server or server group. It becomes easy to navigate, edit, and assign files.\nOther Configuration Recommendations Other configurations that should be evaluated:\nin config.xml: Determines which IP addresses and ports the ClickHouse servers listen for incoming communications. \u003cmax_memory_..\u003e and \u003cmax_bytes_before_external_…\u003e in users.xml. These are part of the profile . \u003cmax_execution_time\u003e \u003clog_queries\u003e The following extra debug logs should be considered:\npart_log text_log Understanding The Configuration ClickHouse configuration stores most of its information in two files:\nconfig.xml: Stores Server configuration parameters. They are server wide, some are hierarchical , and most of them can’t be changed in runtime. The list of settings to apply without a restart changes from version to version. Some settings can be verified using system tables, for example: macros (system.macros) remote_servers (system.clusters) users.xml: Configure users, and user level / session level settings. Each user can change these during their session by: Using parameter in http query By using parameter for clickhouse-client Sending query like set allow_experimental_data_skipping_indices=1. Those settings and their current values are visible in system.settings. You can make some settings global by editing default profile in users.xml, which does not need restart. You can forbid users to change their settings by using readonly=2 for that user, or using setting constraints. Changes in users.xml are applied w/o restart. For both config.xml and users.xml, it’s preferable to put adjustments in the config.d and users.d subfolders instead of editing config.xml and users.xml directly.\nYou can check if the config file was reread by checking /var/lib/clickhouse/preprocessed_configs/ folder.\n","categories":"","description":"Cluster Configuration Process\n","excerpt":"Cluster Configuration Process\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/","tags":"","title":"Cluster Configuration Process"},{"body":" Codec Name Recommended Data Types Performance Notes LZ4 Any Used by default. Extremely fast; good compression; balanced speed and efficiency ZSTD(level) Any Good compression; pretty fast; best for high compression needs. Don’t use levels highter than 3. LZ4HC(level) Any LZ4 High Compression algorithm with configurable level; slower but better compression than LZ4, but decmpression is still fast. Delta Integer Types, Time Series Data, Timestamps Preprocessor (should be followed by some compression codec). Stores difference between neighboring values; good for monotonically increasing data. DoubleDelta Integer Types, Time Series Data Stores difference between neighboring delta values; suitable for time series data Gorilla Floating Point Types Calculates XOR between current and previous value; suitable for slowly changing numbers T64 Integer, Time Series Data, Timestamps Preprocessor (should be followed by some compression codec). Crops unused high bits; puts them into a 64x64 bit matrix; optimized for 64-bit data types GCD Integer Numbers Preprocessor (should be followed by some compression codec). Greatest common divisor compression; divides values by a common divisor; effective for divisible integer sequences FPC Floating Point Numbers Designed for Float64; Algorithm detailed in FPC paper, ClickHouse PR #37553 ZSTD_QAT Any Requires hardware support for QuickAssist Technology (QAT) hardware; provides accelerated compression tasks DEFLATE_QPL Any Requires hardware support for Intel’s QuickAssist Technology for DEFLATE compression; enhanced performance for specific hardware LowCardinality String It’s not a codec, but a datatype modifier. Reduces representation size; effective for columns with low cardinality NONE Non-compressable data with very high entropy, like some random string, or some AggregateFunction states No compression at all. Can be used on the columns that can not be compressed anyway. See\nHow to test different compression codecs\nhttps://altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse\nhttps://www.percona.com/sites/default/files/ple19-slides/day1-pm/clickhouse-for-timeseries.pdf\n","categories":"","description":"Codecs\n","excerpt":"Codecs\n","ref":"/altinity-kb-schema-design/codecs/","tags":"","title":"Codecs"},{"body":" Info Supported since 20.10 (PR #15089). On older versions you will get exception: DB::Exception: Codec Delta is not applicable for Array(UInt64) because the data type is not of fixed size. DROP TABLE IF EXISTS array_codec_test SYNC create table array_codec_test( number UInt64, arr Array(UInt64) ) Engine=MergeTree ORDER BY number; INSERT INTO array_codec_test SELECT number, arrayMap(i -\u003e number + i, range(100)) from numbers(10000000); /**** Default LZ4 *****/ OPTIMIZE TABLE array_codec_test FINAL; --- Elapsed: 3.386 sec. SELECT * FROM system.columns WHERE (table = 'array_codec_test') AND (name = 'arr') /* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 173866750 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: */ /****** Delta, LZ4 ******/ ALTER TABLE array_codec_test MODIFY COLUMN arr Array(UInt64) CODEC (Delta, LZ4); OPTIMIZE TABLE array_codec_test FINAL --0 rows in set. Elapsed: 4.577 sec. SELECT * FROM system.columns WHERE (table = 'array_codec_test') AND (name = 'arr') /* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 32458310 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: CODEC(Delta(8), LZ4) */ ","categories":"","description":"Codecs on array columns\n","excerpt":"Codecs on array columns\n","ref":"/altinity-kb-schema-design/codecs/codecs-on-array-columns/","tags":"","title":"Codecs on array columns"},{"body":"create table test_codec_speed engine=MergeTree ORDER BY tuple() as select cast(now() + rand()%2000 + number, 'DateTime') as x from numbers(1000000000); option 1: CODEC(LZ4) (same as default) option 2: CODEC(DoubleDelta) (`alter table test_codec_speed modify column x DateTime CODEC(DoubleDelta)`); option 3: CODEC(T64, LZ4) (`alter table test_codec_speed modify column x DateTime CODEC(T64, LZ4)`) option 4: CODEC(Delta, LZ4) (`alter table test_codec_speed modify column x DateTime CODEC(Delta, LZ4)`) option 5: CODEC(ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(ZSTD(1))`) option 6: CODEC(T64, ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(T64, ZSTD(1))`) option 7: CODEC(Delta, ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(Delta, ZSTD(1))`) option 8: CODEC(T64, LZ4HC(1)) (`alter table test_codec_speed modify column x DateTime CODEC(T64, LZ4HC(1))`) option 9: CODEC(Gorilla) (`alter table test_codec_speed modify column x DateTime CODEC(Gorilla)`) Result may be not 100% reliable (checked on my laptop, need to be repeated in lab environment) OPTIMIZE TABLE test_codec_speed FINAL (second run - i.e. read + write the same data) 1) 17 sec. 2) 30 sec. 3) 16 sec 4) 17 sec 5) 29 sec 6) 24 sec 7) 31 sec 8) 35 sec 9) 19 sec compressed size 1) 3181376881 2) 2333793699 3) 1862660307 4) 3408502757 5) 2393078266 6) 1765556173 7) 2176080497 8) 1810471247 9) 2109640716 select max(x) from test_codec_speed 1) 0.597 2) 2.756 :( 3) 1.168 4) 0.752 5) 1.362 6) 1.364 7) 1.752 8) 1.270 9) 1.607 ","categories":"","description":"Codecs speed\n","excerpt":"Codecs speed\n","ref":"/altinity-kb-schema-design/codecs/codecs-speed/","tags":"","title":"Codecs speed"},{"body":"Options here are:\nUseINSERT INTO foo_replicated SELECT * FROM foo. (suitable for small tables) Create table aside and attach all partition from the existing table then drop original table (uses hard links don’t require extra disk space). ALTER TABLE foo_replicated ATTACH PARTITION ID 'bar' FROM 'foo' You can easily auto generate those commands using a query like: SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;' from system.parts WHERE table = 'foo'; Do it ‘in place’ using some file manipulation. see the procedure described here: https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replication/#converting-from-mergetree-to-replicatedmergetree Do a backup of MergeTree and recover as ReplicatedMergeTree. https://github.com/AlexAkulov/clickhouse-backup/blob/master/Examples.md#how-to-convert-mergetree-to-replicatedmegretree Embedded command for that should be added in future. example for option 2 Note: ATTACH PARTITION ID ‘bar’ FROM ‘foo’` is practically free from compute and disk space perspective. This feature utilizes filesystem hard-links and the fact that files are immutable in Clickhouse ( it’s the core of the Clickhouse design, filesystem hard-links and such file manipulations are widely used ).\ncreate table foo( A Int64, D Date, S String ) Engine MergeTree partition by toYYYYMM(D) order by A; insert into foo select number, today(), '' from numbers(1e8); insert into foo select number, today()-60, '' from numbers(1e8); select count() from foo; ┌───count()─┐ │ 200000000 │ └───────────┘ create table foo_replicated as foo Engine ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}/{shard}','{replica}') partition by toYYYYMM(D) order by A; SYSTEM STOP MERGES; SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;' from system.parts WHERE table = 'foo' AND active; ┌─concat('ALTER TABLE foo_replicated ATTACH PARTITION ID \\'', partition_id, '\\' FROM foo;')─┐ │ ALTER TABLE foo_replicated ATTACH PARTITION ID '202111' FROM foo; │ │ ALTER TABLE foo_replicated ATTACH PARTITION ID '202201' FROM foo; │ └───────────────────────────────────────────────────────────────────────────────────────────┘ clickhouse-client -q \"SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;' from system.parts WHERE table = 'foo' format TabSeparatedRaw\" |clickhouse-client -mn SYSTEM START MERGES; SELECT count() FROM foo_replicated; ┌───count()─┐ │ 200000000 │ └───────────┘ rename table foo to foo_old, foo_replicated to foo; -- you can drop foo_old any time later, it's kinda a cheap backup, -- it cost nothing until you insert a lot of additional data into foo_replicated ","categories":"","description":"Converting MergeTree to Replicated\n","excerpt":"Converting MergeTree to Replicated\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-converting-mergetree-to-replicated/","tags":"","title":"Converting MergeTree to Replicated"},{"body":"Sample data CREATE TABLE events ( `ts` DateTime, `user_id` UInt32 ) ENGINE = Memory; INSERT INTO events SELECT toDateTime('2021-04-29 10:10:10') + toIntervalHour(7 * number) AS ts, toDayOfWeek(ts) + (number % 2) AS user_id FROM numbers(15); Using window functions (starting from Clickhouse 21.3) SELECT toStartOfDay(ts) AS ts, uniqExactMerge(uniqExactState(user_id)) OVER (ORDER BY ts ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS uniq FROM events GROUP BY ts ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ SELECT ts, uniqExactMerge(state) OVER (ORDER BY ts ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS uniq FROM ( SELECT toStartOfDay(ts) AS ts, uniqExactState(user_id) AS state FROM events GROUP BY ts ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ Using arrays WITH groupArray(_ts) AS ts_arr, groupArray(state) AS state_arr SELECT arrayJoin(ts_arr) AS ts, arrayReduce('uniqExactMerge', arrayFilter((x, y) -\u003e (y \u003c= ts), state_arr, ts_arr)) AS uniq FROM ( SELECT toStartOfDay(ts) AS _ts, uniqExactState(user_id) AS state FROM events GROUP BY _ts ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ WITH arrayJoin(range(toUInt32(_ts) AS int, least(int + toUInt32((3600 * 24) * 5), toUInt32(toDateTime('2021-05-04 00:00:00'))), 3600 * 24)) AS ts_expanded SELECT toDateTime(ts_expanded) AS ts, uniqExactMerge(state) AS uniq FROM ( SELECT toStartOfDay(ts) AS _ts, uniqExactState(user_id) AS state FROM events GROUP BY _ts ) GROUP BY ts ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ Using runningAccumulate (incorrect result over blocks) SELECT ts, runningAccumulate(state) AS uniq FROM ( SELECT toStartOfDay(ts) AS ts, uniqExactState(user_id) AS state FROM events GROUP BY ts ORDER BY ts ASC ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ ","categories":"","description":"Cumulative Anything\n","excerpt":"Cumulative Anything\n","ref":"/altinity-kb-queries-and-syntax/cumulative-unique/","tags":"","title":"Cumulative Anything"},{"body":"Export \u0026 Import into common data formats Pros:\nData can be inserted into any DBMS. Cons:\nDecoding \u0026 encoding of common data formats may be slower / require more CPU The data size is usually bigger than ClickHouse formats. Some of the common data formats have limitations. Info The best approach to do that is using clickhouse-client, in that case, encoding/decoding of format happens client-side, while client and server speak clickhouse Native format (columnar \u0026 compressed).\nIn contrast: when you use HTTP protocol, the server do encoding/decoding and more data is passed between client and server.\nremote/remoteSecure or cluster/Distributed table Pros:\nSimple to run. It’s possible to change the schema and distribution of data between shards. It’s possible to copy only some subset of data. Needs only access to ClickHouse TCP port. Cons:\nUses CPU / RAM (mostly on the receiver side) See details of both approaches in:\nremote-table-function.md\ndistributed-table-cluster.md\nclickhouse-copier Pros:\nPossible to do some changes in schema. Needs only access to ClickHouse TCP port. It’s possible to change the distribution of data between shards. Suitable for large clusters: many clickhouse-copier can execute the same task together. Cons:\nMay create an inconsistent result if source cluster data is changing during the process. Hard to setup. Requires zookeeper. Uses CPU / RAM (mostly on the clickhouse-copier and receiver side) Info Internally it works like smart INSERT INTO cluster(…) SELECT * FROM ... with some consistency checks. Info Run clickhouse copier on the same nodes as receiver clickhouse, to avoid doubling the network load. See details in:\naltinity-kb-clickhouse-copier\nManual parts moving: freeze / rsync / attach Pros:\nLow CPU / RAM usage. Cons:\nTable schema should be the same. A lot of manual operations/scripting. Info With some additional care and scripting, it’s possible to do cheap re-sharding on parts level. See details in:\nrsync.md\nclickhouse-backup Pros:\nLow CPU / RAM usage. Suitable to recover both schema \u0026 data for all tables at once. Cons:\nTable schema should be the same. Just create the backup on server 1, upload it to server 2, and restore the backup.\nSee https://github.com/AlexAkulov/clickhouse-backup\nhttps://altinity.com/blog/introduction-to-clickhouse-backups-and-clickhouse-backup\nFetch from zookeeper path Pros:\nLow CPU / RAM usage. Cons:\nTable schema should be the same. Works only when the source and the destination clickhouse servers share the same zookeeper (without chroot) Needs to access zookeeper and ClickHouse replication ports: (interserver_http_port or interserver_https_port) ALTER TABLE table_name FETCH PARTITION partition_expr FROM 'path-in-zookeeper' alter table fetch detail\nUsing the replication protocol by adding a new replica Just make one more replica in another place.\nPros:\nSimple to setup Data is consistent all the time automatically. Low CPU and network usage should be tuned. Cons:\nNeeds to reach both zookeeper client (2181) and ClickHouse replication ports: (interserver_http_port or interserver_https_port) In case of cluster migration, zookeeper need’s to be migrated too. Replication works both ways so new replica should be outside the main cluster. Check the details in:\nAdd a replica to a Cluster\nSee also Github issues https://github.com/ClickHouse/ClickHouse/issues/10943 https://github.com/ClickHouse/ClickHouse/issues/20219 https://github.com/ClickHouse/ClickHouse/pull/17871\nOther links https://habr.com/ru/company/avito/blog/500678/\n","categories":"","description":"Data Migration\n","excerpt":"Data Migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/","tags":"","title":"Data Migration"},{"body":" DataType RAM size (=byteSize) Disk Size String string byte length + 9 string length: 64 bit integer\nzero-byte terminator: 1 byte.\nstring length prefix (varint) + string itself:\nstring shorter than 128 - string byte length + 1 string shorter than 16384 - string byte length + 2 string shorter than 2097152 - string byte length + 2 string shorter than 268435456 - string byte length + 4\nAggregateFunction(count, ...) varint See also https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/data_processing.pdf (slide 17-22)\n","categories":"","description":"Data types on disk and in RAM\n","excerpt":"Data types on disk and in RAM\n","ref":"/altinity-kb-queries-and-syntax/data-types-on-disk-and-in-ram/","tags":"","title":"Data types on disk and in RAM"},{"body":"Tables Table size SELECT database, table, formatReadableSize(sum(data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_rate, sum(rows) AS rows, count() AS part_count FROM system.parts WHERE (active = 1) AND (database LIKE '%') AND (table LIKE '%') GROUP BY database, table ORDER BY size DESC; Table size + inner MatView (Atomic) SELECT p.database, if(t.name = '', p.table, p.table||' ('||t.name||')') tbl, formatReadableSize(sum(p.data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(p.data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_rate, sum(p.rows) AS rows, count() AS part_count FROM system.parts p left join system.tables t on p.database = t.database and p.table = '.inner_id.'||toString(t.uuid) WHERE (active = 1) AND (tbl LIKE '%') AND (database LIKE '%') GROUP BY p.database, tbl ORDER BY size DESC; Column size SELECT database, table, column, formatReadableSize(sum(column_data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(column_data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_ratio, sum(rows) rows_cnt, round(usize / rows_cnt, 2) avg_row_size FROM system.parts_columns WHERE (active = 1) AND (database LIKE '%') AND (table LIKE '%') GROUP BY database, table, column ORDER BY size DESC; Projections Projection size SELECT database, table, name, formatReadableSize(sum(data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_rate, sum(rows) AS rows, count() AS part_count FROM system.projection_parts WHERE (table = 'ptest') AND active GROUP BY database, table, name ORDER BY size DESC; Projection column size SELECT database, table, column, formatReadableSize(sum(column_data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(column_data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_rate FROM system.projection_parts_columns WHERE (active = 1) AND (table LIKE 'ptest') GROUP BY database, table, column ORDER BY size DESC; Understanding the columns data properties: SELECT count(), * APPLY (uniq), * APPLY (max), * APPLY (min), * APPLY(topK(5)) FROM table_name FORMAT Vertical; -- also you can add * APPLY (entropy) to show entropy (i.e. 'randomness' of the column). -- if the table is huge add some WHERE condition to slice some 'representative' data range, for example single month / week / day of data. Understanding the ingest pattern: SELECT database, table, median(rows), median(bytes_on_disk), sum(rows), max(bytes_on_disk), min(bytes_on_disk), round(quantile(0.95)(bytes_on_disk), 0), sum(bytes_on_disk), count(), countIf(NOT active), uniqExact(partition) FROM system.parts WHERE (modification_time \u003e (now() - 480)) AND (level = 0) GROUP BY database, table ORDER BY count() DESC part_log WITH 30 * 60 AS frame_size SELECT toStartOfInterval(event_time, toIntervalSecond(frame_size)) AS m, database, table, ROUND(countIf(event_type = 'NewPart') / frame_size, 2) AS new, ROUND(countIf(event_type = 'MergeParts') / frame_size, 2) AS merge, ROUND(countIf(event_type = 'DownloadPart') / frame_size, 2) AS dl, ROUND(countIf(event_type = 'RemovePart') / frame_size, 2) AS rm, ROUND(countIf(event_type = 'MutatePart') / frame_size, 2) AS mut, ROUND(countIf(event_type = 'MovePart') / frame_size, 2) AS mv FROM system.part_log WHERE event_time \u003e (now() - toIntervalHour(24)) GROUP BY m, database, table ORDER BY database ASC, table ASC, m ASC WITH 30 * 60 AS frame_size SELECT toStartOfInterval(event_time, toIntervalSecond(frame_size)) AS m, database, table, ROUND(countIf(event_type = 'NewPart') / frame_size, 2) AS inserts_per_sec, ROUND(sumIf(rows, event_type = 'NewPart') / frame_size, 2) AS rows_per_sec, ROUND(sumIf(size_in_bytes, event_type = 'NewPart') / frame_size, 2) AS bytes_per_sec FROM system.part_log WHERE event_time \u003e (now() - toIntervalHour(24)) GROUP BY m, database, table ORDER BY database ASC, table ASC, m ASC Understanding the partitioning SELECT database, table, count(), topK(5)(partition), COLUMNS('metric.*') APPLY(quantiles(0.005, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.995)) FROM ( SELECT database, table, partition, sum(bytes_on_disk) AS metric_bytes, sum(data_uncompressed_bytes) AS metric_uncompressed_bytes, sum(rows) AS metric_rows, sum(primary_key_bytes_in_memory) AS metric_pk_size, count() AS metric_count, countIf(part_type = 'Wide') AS metric_wide_count, countIf(part_type = 'Compact') AS metric_compact_count, countIf(part_type = 'Memory') AS metric_memory_count FROM system.parts GROUP BY database, table, partition ) GROUP BY database, table FORMAT Vertical ","categories":"","description":"Database Size - Table - Column size\n","excerpt":"Database Size - Table - Column size\n","ref":"/altinity-kb-useful-queries/altinity-kb-database-size-table-column-size/","tags":"","title":"Database Size - Table - Column size"},{"body":"DDLWorker is a subprocess (thread) of clickhouse-server that executes ON CLUSTER tasks at the node.\nWhen you execute a DDL query with ON CLUSTER mycluster section the query executor at the current node reads the cluster mycluster definition (remote_servers / system.clusters) and places tasks into Zookeeper znode task_queue/ddl/... for members of the cluster mycluster.\nDDLWorker at all ClickHouse nodes constantly check this task_queue for their tasks and executes them locally and reports about a result back into task_queue.\nThe common issue is the different hostnames/IPAddresses in the cluster definition and locally.\nSo a node initiator puts tasks for a host named Host1. But the Host1 thinks about own name as localhost or xdgt634678d (internal docker hostname) and never sees tasks for the Host1 because is looking tasks for xdgt634678d. The same with internal VS external IP addresses.\nAnother issue that sometimes DDLWorker thread can crash then ClickHouse node stops to execute ON CLUSTER tasks.\nCheck that DDLWorker is alive:\nps -eL|grep DDL 18829 18876 ? 00:00:00 DDLWorkerClnr 18829 18879 ? 00:00:00 DDLWorker ps -ef|grep 18829|grep -v grep clickho+ 18829 18828 1 Feb09 ? 00:55:00 /usr/bin/clickhouse-server --con... As you can see there are two threads: DDLWorker and DDLWorkerClnr.\nThe second thread – DDLWorkerCleaner cleans old tasks from task_queue. You can configure how many recent tasks to store:\nconfig.xml \u003cyandex\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/task_queue/ddl\u003c/path\u003e \u003cmax_tasks_in_queue\u003e1000\u003c/max_tasks_in_queue\u003e \u003ctask_max_lifetime\u003e604800\u003c/task_max_lifetime\u003e \u003ccleanup_delay_period\u003e60\u003c/cleanup_delay_period\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e Default values:\ncleanup_delay_period = 60 seconds – Sets how often to start cleanup to remove outdated data.\ntask_max_lifetime = 7 * 24 * 60 * 60 (in seconds = week) – Delete task if its age is greater than that.\nmax_tasks_in_queue = 1000 – How many tasks could be in the queue.\n","categories":"","description":"DDLWorker\n","excerpt":"DDLWorker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/","tags":"","title":"DDLWorker"},{"body":"CREATE TABLE test_delete ( `key` UInt32, `ts` UInt32, `value_a` String, `value_b` String, `value_c` String, `is_active` UInt8 DEFAULT 1 ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_delete (key, ts, value_a, value_b, value_c) SELECT number, 1, concat('some_looong_string', toString(number)), concat('another_long_str', toString(number)), concat('string', toString(number)) FROM numbers(10000000); INSERT INTO test_delete (key, ts, value_a, value_b, value_c) VALUES (400000, 2, 'totally different string', 'another totally different string', 'last string'); SELECT * FROM test_delete WHERE key = 400000; ┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐ │ 400000 │ 2 │ totally different string │ another totally different string │ last string │ 1 │ └────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ ┌────key─┬─ts─┬─value_a──────────────────┬─value_b────────────────┬─value_c──────┬─is_active─┐ │ 400000 │ 1 │ some_looong_string400000 │ another_long_str400000 │ string400000 │ 1 │ └────────┴────┴──────────────────────────┴────────────────────────┴──────────────┴───────────┘ SET mutations_sync = 2; ALTER TABLE test_delete UPDATE is_active = 0 WHERE (key = 400000) AND (ts = 1); Ok. 0 rows in set. Elapsed: 0.058 sec. SELECT * FROM test_delete WHERE (key = 400000) AND is_active; ┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐ │ 400000 │ 2 │ totally different string │ another totally different string │ last string │ 1 │ └────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ ALTER TABLE test_delete DELETE WHERE (key = 400000) AND (ts = 1); Ok. 0 rows in set. Elapsed: 1.101 sec. -- 20 times slower!!! SELECT * FROM test_delete WHERE key = 400000; ┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐ │ 400000 │ 2 │ totally different string │ another totally different string │ last string │ 1 │ └────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ -- For ReplacingMergeTree OPTIMIZE TABLE test_delete FINAL; Ok. 0 rows in set. Elapsed: 2.230 sec. -- 40 times slower!!! SELECT * FROM test_delete WHERE key = 400000 ┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐ │ 400000 │ 2 │ totally different string │ another totally different string │ last string │ 1 │ └────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ DELETE \u0026 S3 CREATE TABLE test_delete ( `key` UInt32, `value_a` String, `value_b` String, `value_c` String, `is_deleted` UInt8 DEFAULT 0 ) ENGINE = MergeTree ORDER BY key SETTINGS storage_policy = 's3tiered'; INSERT INTO test_delete (key, value_a, value_b, value_c) SELECT number, concat('some_looong_string', toString(number)), concat('another_long_str', toString(number)), concat('really long string', toString(arrayMap(i -\u003e cityHash64(i*number), range(50)))) FROM numbers(10000000); OPTIMIZE TABLE test_delete FINAL; ALTER TABLE test_delete MOVE PARTITION tuple() TO DISK 's3disk'; SELECT count() FROM test_delete; ┌──count()─┐ │ 10000000 │ └──────────┘ 1 row in set. Elapsed: 0.002 sec. DELETE USING ALTER UPDATE \u0026 ROW POLICY CREATE ROW POLICY pol1 ON test_delete USING is_deleted=0 TO all; SELECT count() FROM test_delete; -- select count() became much slower, it reads data now, not metadata ┌──count()─┐ │ 10000000 │ └──────────┘ 1 row in set. Elapsed: 0.314 sec. Processed 10.00 million rows, 10.00 MB (31.84 million rows/s., 31.84 MB/s.) ALTER TABLE test_delete UPDATE is_deleted = 1 WHERE (key = 400000) settings mutations_sync = 2; 0 rows in set. Elapsed: 1.256 sec. SELECT count() FROM test_delete; ┌─count()─┐ │ 9999999 │ └─────────┘ DELETE USING ALTER DELETE ALTER TABLE test_delete DELETE WHERE (key = 400001) settings mutations_sync = 2; 0 rows in set. Elapsed: 955.672 sec. SELECT count() FROM test_delete; ┌─count()─┐ │ 9999998 │ └─────────┘ DELETE USING DELETE DELETE FROM test_delete WHERE (key = 400002); 0 rows in set. Elapsed: 1.281 sec. SELECT count() FROM test_delete; ┌─count()─┐ │ 9999997 │ └─────────┘ ","categories":"","description":"DELETE via tombstone column\n","excerpt":"DELETE via tombstone column\n","ref":"/altinity-kb-queries-and-syntax/delete-via-tombstone-column/","tags":"","title":"DELETE via tombstone column"},{"body":"Dictionary with Clickhouse table as a source Test data DROP TABLE IF EXISTS arr_src; CREATE TABLE arr_src ( key UInt64, array_int Array(Int64), array_str Array(String) ) ENGINE = MergeTree order by key; INSERT INTO arr_src SELECT number, arrayMap(i -\u003e (number * i), range(5)), arrayMap(i -\u003e concat('str', toString(number * i)), range(5)) FROM numbers(1000); Dictionary DROP DICTIONARY IF EXISTS arr_dict; CREATE DICTIONARY arr_dict ( key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT ['1','2','3'] ) PRIMARY KEY key SOURCE(CLICKHOUSE(DATABASE 'default' TABLE 'arr_src')) LIFETIME(120) LAYOUT(HASHED()); SELECT dictGet('arr_dict', 'array_int', toUInt64(42)) AS res_int, dictGetOrDefault('arr_dict', 'array_str', toUInt64(424242), ['none']) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ ['none'] │ └───────────────────┴──────────┘ Dictionary with PostgreSQL as a source Test data in PG create user ch; create database ch; GRANT ALL PRIVILEGES ON DATABASE ch TO ch; ALTER USER ch WITH PASSWORD 'chch'; CREATE TABLE arr_src ( key int, array_int integer[], array_str text[] ); INSERT INTO arr_src VALUES (42, '{0,42,84,126,168}','{\"str0\",\"str42\",\"str84\",\"str126\",\"str168\"}'), (66, '{0,66,132,198,264}','{\"str0\",\"str66\",\"str132\",\"str198\",\"str264\"}'); Dictionary Example CREATE DICTIONARY pg_arr_dict ( key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT ['1','2','3'] ) PRIMARY KEY key SOURCE(POSTGRESQL(PORT 5432 HOST 'pg-host' user 'ch' password 'chch' DATABASE 'ch' TABLE 'arr_src')) LIFETIME(120) LAYOUT(HASHED()); select * from pg_arr_dict; ┌─key─┬─array_int──────────┬─array_str───────────────────────────────────┐ │ 66 │ [0,66,132,198,264] │ ['str0','str66','str132','str198','str264'] │ │ 42 │ [0,42,84,126,168] │ ['str0','str42','str84','str126','str168'] │ └─────┴────────────────────┴─────────────────────────────────────────────┘ SELECT dictGet('pg_arr_dict', 'array_int', toUInt64(42)) AS res_int, dictGetOrDefault('pg_arr_dict', 'array_str', toUInt64(424242), ['none']) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ ['none'] │ └───────────────────┴──────────┘ Dictionary with MySQL as a source Test data in MySQL -- casted into CH Arrays create table arr_src( _key bigint(20) NOT NULL, _array_int text, _array_str text, PRIMARY KEY(_key) ); INSERT INTO arr_src VALUES (42, '[0,42,84,126,168]','[''str0'',''str42'',''str84'',''str126'',''str168'']'), (66, '[0,66,132,198,264]','[''str0'',''str66'',''str132'',''str198'',''str264'']'); Dictionary in MySQL -- supporting table to cast data CREATE TABLE arr_src ( `_key` UInt8, `_array_int` String, `array_int` Array(Int32) ALIAS cast(_array_int, 'Array(Int32)'), `_array_str` String, `array_str` Array(String) ALIAS cast(_array_str, 'Array(String)') ) ENGINE = MySQL('mysql_host', 'ch', 'arr_src', 'ch', 'pass'); -- dictionary fetches data from the supporting table CREATE DICTIONARY mysql_arr_dict ( _key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT ['1','2','3'] ) PRIMARY KEY _key SOURCE(CLICKHOUSE(DATABASE 'default' TABLE 'arr_src')) LIFETIME(120) LAYOUT(HASHED()); select * from mysql_arr_dict; ┌─_key─┬─array_int──────────┬─array_str───────────────────────────────────┐ │ 66 │ [0,66,132,198,264] │ ['str0','str66','str132','str198','str264'] │ │ 42 │ [0,42,84,126,168] │ ['str0','str42','str84','str126','str168'] │ └──────┴────────────────────┴─────────────────────────────────────────────┘ SELECT dictGet('mysql_arr_dict', 'array_int', toUInt64(42)) AS res_int, dictGetOrDefault('mysql_arr_dict', 'array_str', toUInt64(424242), ['none']) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ ['none'] │ └───────────────────┴──────────┘ SELECT dictGet('mysql_arr_dict', 'array_int', toUInt64(66)) AS res_int, dictGetOrDefault('mysql_arr_dict', 'array_str', toUInt64(66), ['none']) AS res_str ┌─res_int────────────┬─res_str─────────────────────────────────────┐ │ [0,66,132,198,264] │ ['str0','str66','str132','str198','str264'] │ └────────────────────┴─────────────────────────────────────────────┘ ","categories":"","description":"Dictionaries \u0026 arrays\n","excerpt":"Dictionaries \u0026 arrays\n","ref":"/altinity-kb-dictionaries/dictionaries-and-arrays/","tags":"","title":"Dictionaries \u0026 arrays"},{"body":"Q. I think I’m still trying to understand how de-normalized is okay - with my relational mindset, I want to move repeated string fields into their own table, but I’m not sure to what extent this is necessary\nI will look at LowCardinality in more detail - I think it may work well here\nA. If it’s a simple repetition, which you don’t need to manipulate/change in future - LowCardinality works great, and you usually don’t need to increase the system complexity by introducing dicts.\nFor example: name of team ‘Manchester United’ will rather not be changed, and even if it will you can keep the historical records with historical name. So normalization here (with some dicts) is very optional, and de-normalized approach with LowCardinality is good \u0026 simpler alternative.\nFrom the other hand: if data can be changed in future, and that change should impact the reports, then normalization may be a big advantage.\nFor example if you need to change the used currency rare every day- it would be quite stupid to update all historical records to apply the newest exchange rate. And putting it to dict will allow to do calculations with latest exchange rate at select time.\nFor dictionary it’s possible to mark some of the attributes as injective. An attribute is called injective if different attribute values correspond to different keys. It would allow ClickHouse to replace dictGet call in GROUP BY with cheap dict key.\n","categories":"","description":"Dictionaries vs LowCardinality\n","excerpt":"Dictionaries vs LowCardinality\n","ref":"/altinity-kb-schema-design/altinity-kb-dictionaries-vs-lowcardinality/","tags":"","title":"Dictionaries vs LowCardinality"},{"body":" DROP TABLE IF EXISTS dictionary_source_en; DROP TABLE IF EXISTS dictionary_source_ru; DROP TABLE IF EXISTS dictionary_source_view; DROP DICTIONARY IF EXISTS flat_dictionary; CREATE TABLE dictionary_source_en ( id UInt64, value String ) ENGINE = TinyLog; INSERT INTO dictionary_source_en VALUES (1, 'One'), (2,'Two'), (3, 'Three'); CREATE TABLE dictionary_source_ru ( id UInt64, value String ) ENGINE = TinyLog; INSERT INTO dictionary_source_ru VALUES (1, 'Один'), (2,'Два'), (3, 'Три'); CREATE VIEW dictionary_source_view AS SELECT id, dictionary_source_en.value as value_en, dictionary_source_ru.value as value_ru FROM dictionary_source_en LEFT JOIN dictionary_source_ru USING (id); select * from dictionary_source_view; CREATE DICTIONARY flat_dictionary ( id UInt64, value_en String, value_ru String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' PASSWORD '' TABLE 'dictionary_source_view')) LIFETIME(MIN 1 MAX 1000) LAYOUT(FLAT()); SELECT dictGet(concat(currentDatabase(), '.flat_dictionary'), 'value_en', number + 1), dictGet(concat(currentDatabase(), '.flat_dictionary'), 'value_ru', number + 1) FROM numbers(3); ","categories":"","description":"Dictionary on the top of several tables using VIEW\n","excerpt":"Dictionary on the top of several tables using VIEW\n","ref":"/altinity-kb-dictionaries/dictionary-on-top-tables/","tags":"","title":"Dictionary on the top of several tables using VIEW"},{"body":"differential backups using clickhouse-backup Download the latest clickhouse-backup for your platform https://github.com/AlexAkulov/clickhouse-backup/releases # ubuntu / debian wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup_1.0.0_amd64.deb sudo dpkg -i clickhouse-backup_1.0.0_amd64.deb # centos / redhat / fedora sudo yum install https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup-1.0.0-1.x86_64.rpm # other platforms wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup.tar.gz sudo mkdir /etc/clickhouse-backup/ sudo mv clickhouse-backup/config.yml /etc/clickhouse-backup/config.yml.example sudo mv clickhouse-backup/clickhouse-backup /usr/bin/ rm -rf clickhouse-backup clickhouse-backup.tar.gz Create a runner script for the crontab mkdir /opt/clickhouse-backup-diff/ cat \u003c\u003c 'END' \u003e /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh #!/bin/bash set +x command_line_argument=$1 backup_name=$(date +%Y-%M-%d-%H-%M-%S) echo \"Creating local backup '${backup_name}' (full, using hardlinks)...\" clickhouse-backup create \"${backup_name}\" if [[ \"run_diff\" == \"${command_line_argument}\" \u0026\u0026 \"2\" -le \"$(clickhouse-backup list local | wc -l)\" ]]; then prev_backup_name=\"$(clickhouse-backup list local | tail -n 2 | head -n 1 | cut -d \" \" -f 1)\" echo \"Uploading the backup '${backup_name}' as diff from the previous backup ('${prev_backup_name}')\" clickhouse-backup upload --diff-from \"${prev_backup_name}\" \"${backup_name}\" elif [[ \"\" == \"${command_line_argument}\" ]]; then echo \"Uploading the backup '${backup_name}, and removing old unneeded backups\" KEEP_BACKUPS_LOCAL=1 KEEP_BACKUPS_REMOTE=1 clickhouse-backup upload \"${backup_name}\" fi END chmod +x /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh Create confuguration for clickhouse-backup # Check the example: /etc/clickhouse-backup/config.yml.example vim /etc/clickhouse-backup/config.yml Edit the crontab crontab -e # full backup at 0:00 Monday 0 0 * * 1 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh # differential backup every hour (except of 00:00) Monday 0 1-23 * * 1 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh run_diff # differential backup every hour Sunday, Tuesday-Saturday 0 */1 * * 0,2-6 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh run_diff Recover the last backup: last_remote_backup=\"$(clickhouse-backup list remote | tail -n 1 | cut -d \" \" -f 1)\" clickhouse-backup download \"${last_remote_backup}\" clickhouse-backup restore --rm \"${last_remote_backup}\" ","categories":"","description":"differential backups using clickhouse-backup\n","excerpt":"differential backups using clickhouse-backup\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-backup-diff/","tags":"","title":"differential backups using clickhouse-backup"},{"body":"In order to shift INSERTS to a standby cluster (for example increase zone availability or disaster recovery) some ClickHouse features can be used.\nBasically we need to create a distributed table, a MV, rewrite the remote_servers.xml config file and tune some parameters.\nDistributed engine information and parameters: https://clickhouse.com/docs/en/engines/table-engines/special/distributed/\nSteps Create a Distributed table in the source cluster For example, we should have a ReplicatedMergeTree table in which all inserts are falling. This table is the first step in our pipeline:\nCREATE TABLE db.inserts_source ON CLUSTER 'source' ( column1 String column2 DateTime ..... ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/inserts_source', '{replica}') PARTITION BY toYYYYMM(column2) ORDER BY (column1, column2) This table lives in the source cluster and all INSERTS go there. In order to shift all INSERTS in the source cluster to destination cluster we can create a Distributed table that points to another ReplicatedMergeTree in the destination cluster:\nCREATE TABLE db.inserts_source_dist ON CLUSTER 'source' ( column1 String column2 DateTime ..... ) ENGINE = Distributed('destination', db, inserts_destination) Create a Materialized View to shift INSERTS to destination cluster: CREATE MATERIALIZED VIEW shift_inserts ON CLUSTER 'source' TO db.inserts_source_dist AS SELECT * FROM db.inserts_source Create a ReplicatedMergeTree table in the destination cluster: This is the table in the destination cluster that is pointed by the distributed table in the source cluster\nCREATE TABLE db.inserts_destination ON CLUSTER 'destination' ( column1 String column2 DateTime ..... ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/inserts_destination', '{replica}') PARTITION BY toYYYYMM(column2) ORDER BY (column1, column2) Rewrite remote_servers.xml: All the hostnames/FQDN from each replica/node must be accessible from both clusters. Also the remote_servers.xml from the source cluster should read like this:\n\u003cclickhouse\u003e \u003cremote_servers\u003e \u003csource\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003ehost03\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003ehost04\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/source\u003e \u003cdestination\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003ehost01\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003ehost02\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/destination\u003e \u003c!-- If using a LB to shift inserts you need to use user and password and create MT destination table in an all-replicated cluster config --\u003e \u003cdestination_with_lb\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003eload_balancer.xxxx.com\u003c/host\u003e \u003cport\u003e9440\u003c/port\u003e \u003csecure\u003e1\u003c/secure\u003e \u003cusername\u003euser\u003c/username\u003e \u003cpassword\u003epass\u003c/password\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/destination_with_lb\u003e \u003c/remote_servers\u003e \u003c/clickhouse\u003e Configuration settings Depending on your use case you can set the the distributed INSERTs to sync or async mode. This example is for async mode: Put this config settings on the default profile. Check for more info about the possible modes:\nhttps://clickhouse.com/docs/en/operations/settings/settings#insert_distributed_sync\n\u003cclickhouse\u003e .... \u003cprofiles\u003e \u003cdefault\u003e \u003c!-- StorageDistributed DirectoryMonitors try to batch individual inserts into bigger ones to increase performance --\u003e \u003cdistributed_directory_monitor_batch_inserts\u003e1\u003c/distributed_directory_monitor_batch_inserts\u003e \u003c!-- StorageDistributed DirectoryMonitors try to split batch into smaller in case of failures --\u003e \u003cdistributed_directory_monitor_split_batch_on_failure\u003e1\u003c/distributed_directory_monitor_split_batch_on_failure\u003e \u003c/default\u003e ..... \u003c/profiles\u003e \u003c/clickhouse\u003e ","categories":"","description":"Distributed table to cluster\n","excerpt":"Distributed table to cluster\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/distributed-table-cluster/","tags":"","title":"Distributed table to Cluster"},{"body":"RocksDB is faster than MergeTree on Key/Value queries because MergeTree primary key index is sparse. Probably it’s possible to speedup MergeTree by reducing index_granularity.\nNVMe disk is used for the tests.\nThe main feature of RocksDB is instant updates. You can update a row instantly (microseconds):\nselect * from rocksDB where A=15645646; ┌────────A─┬─B────────────────────┐ │ 15645646 │ 12517841379565221195 │ └──────────┴──────────────────────┘ 1 rows in set. Elapsed: 0.001 sec. insert into rocksDB values (15645646, 'xxxx'); 1 rows in set. Elapsed: 0.001 sec. select * from rocksDB where A=15645646; ┌────────A─┬─B────┐ │ 15645646 │ xxxx │ └──────────┴──────┘ 1 rows in set. Elapsed: 0.001 sec. Let’s load 100 millions rows:\ncreate table rocksDB(A UInt64, B String, primary key A) Engine=EmbeddedRocksDB(); insert into rocksDB select number, toString(cityHash64(number)) from numbers(100000000); -- 0 rows in set. Elapsed: 154.559 sec. Processed 100.66 million rows, 805.28 MB (651.27 thousand rows/s., 5.21 MB/s.) -- Size on disk: 1.5GB create table mergeTreeDB(A UInt64, B String) Engine=MergeTree() order by A; insert into mergeTreeDB select number, toString(cityHash64(number)) from numbers(100000000); Size on disk: 973MB CREATE DICTIONARY test_rocksDB(A UInt64,B String) PRIMARY KEY A SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE rocksDB DB 'default' USER 'default')) LAYOUT(DIRECT()); CREATE DICTIONARY test_mergeTreeDB(A UInt64,B String) PRIMARY KEY A SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE mergeTreeDB DB 'default' USER 'default')) LAYOUT(DIRECT()); Direct queries to tables to request 10000 rows by a random key select count() from ( select * from rocksDB where A in (select toUInt64(rand64()%100000000) from numbers(10000))) Elapsed: 0.076 sec. Processed 10.00 thousand rows select count() from ( select * from mergeTreeDB where A in (select toUInt64(rand64()%100000000) from numbers(10000))) Elapsed: 0.202 sec. Processed 55.95 million rows RocksDB as expected is much faster: 0.076 sec. VS 0.202 sec.\nRocksDB processes less rows: 10.00 thousand rows VS 55.95 million rows\ndictGet – 100.00 thousand random rows select count() from ( select dictGet( 'default.test_rocksDB', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(100000)) Elapsed: 0.786 sec. Processed 100.00 thousand rows select count() from ( select dictGet( 'default.test_mergeTreeDB', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(100000)) Elapsed: 3.160 sec. Processed 100.00 thousand rows dictGet – 1million random rows select count() from ( select dictGet( 'default.test_rocksDB', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(1000000)) Elapsed: 5.643 sec. Processed 1.00 million rows select count() from ( select dictGet( 'default.test_mergeTreeDB', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(1000000)) Elapsed: 31.111 sec. Processed 1.00 million rows dictGet – 1million random rows from Hashed CREATE DICTIONARY test_mergeTreeDBHashed(A UInt64,B String) PRIMARY KEY A SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE mergeTreeDB DB 'default' USER 'default')) LAYOUT(Hashed()) LIFETIME(0); 0 rows in set. Elapsed: 46.564 sec. ┌─name───────────────────┬─type───┬─status─┬─element_count─┬─RAM──────┐ │ test_mergeTreeDBHashed │ Hashed │ LOADED │ 100000000 │ 7.87 GiB │ └────────────────────────┴────────┴────────┴───────────────┴──────────┘ select count() from ( select dictGet( 'default.test_mergeTreeDBHashed', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(1000000)) Elapsed: 0.079 sec. Processed 1.00 million rows dictGet – 1million random rows from SparseHashed CREATE DICTIONARY test_mergeTreeDBSparseHashed(A UInt64,B String) PRIMARY KEY A SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE mergeTreeDB DB 'default' USER 'default')) LAYOUT(SPARSE_HASHED()) LIFETIME(0); 0 rows in set. Elapsed: 81.404 sec. ┌─name─────────────────────────┬─type─────────┬─status─┬─element_count─┬─RAM──────┐ │ test_mergeTreeDBSparseHashed │ SparseHashed │ LOADED │ 100000000 │ 4.24 GiB │ └──────────────────────────────┴──────────────┴────────┴───────────────┴──────────┘ select count() from ( select dictGet( 'default.test_mergeTreeDBSparseHashed', 'B', toUInt64(rand64()%100000000) ) from numbers_mt(1000000)) Elapsed: 0.065 sec. Processed 1.00 million rows ","categories":"","description":"EmbeddedRocksDB \u0026 dictionary\n","excerpt":"EmbeddedRocksDB \u0026 dictionary\n","ref":"/engines/altinity-kb-embeddedrocksdb-and-dictionary/","tags":"","title":"EmbeddedRocksDB \u0026 dictionary"},{"body":"WHERE over encrypted column CREATE TABLE encrypt ( `key` UInt32, `value` FixedString(4) ) ENGINE = MergeTree ORDER BY key; INSERT INTO encrypt SELECT number, encrypt('aes-256-ctr', reinterpretAsString(number + 0.3), 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'xxxxxxxxxxxxxxxx') FROM numbers(100000000); SET max_threads = 1; SELECT count() FROM encrypt WHERE value IN encrypt('aes-256-ctr', reinterpretAsString(toFloat32(1.3)), 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'xxxxxxxxxxxxxxxx') ┌─count()─┐ │ 1 │ └─────────┘ 1 rows in set. Elapsed: 0.666 sec. Processed 100.00 million rows, 400.01 MB (150.23 million rows/s., 600.93 MB/s.) SELECT count() FROM encrypt WHERE reinterpretAsFloat32(encrypt('aes-256-ctr', value, 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'xxxxxxxxxxxxxxxx')) IN toFloat32(1.3) ┌─count()─┐ │ 1 │ └─────────┘ 1 rows in set. Elapsed: 8.395 sec. Processed 100.00 million rows, 400.01 MB (11.91 million rows/s., 47.65 MB/s.) Info Because encryption and decryption can be expensive due re-initialization of keys and iv, usually it make sense to use those functions over literal values instead of table column. ","categories":"","description":"","excerpt":"WHERE over encrypted column CREATE TABLE encrypt ( `key` UInt32, …","ref":"/altinity-kb-functions/encrypt/","tags":"","title":"Encrypt"},{"body":"Pre 21.6 There are couple options:\nCertain formats which has schema in built in them (like JSONEachRow) could silently skip any unexpected fields after enabling setting input_format_skip_unknown_fields\nIt’s also possible to skip up to N malformed messages for each block, with used setting kafka_skip_broken_messages but it’s also does not support all possible formats.\nAfter 21.6 It’s possible to stream messages which could not be parsed, this behavior could be enabled via setting: kafka_handle_error_mode='stream' and clickhouse wil write error and message from Kafka itself to two new virtual columns: _error, _raw_message.\nSo you can create another Materialized View which would collect to a separate table all errors happening while parsing with all important information like offset and content of message.\nCREATE TABLE default.kafka_engine ( `i` Int64, `s` String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'kafka:9092' kafka_topic_list = 'topic', kafka_group_name = 'clickhouse', kafka_format = 'JSONEachRow', kafka_handle_error_mode='stream'; CREATE MATERIALIZED VIEW default.kafka_errors ( `topic` String, `partition` Int64, `offset` Int64, `raw` String, `error` String ) ENGINE = MergeTree ORDER BY (topic, partition, offset) SETTINGS index_granularity = 8192 AS SELECT _topic AS topic, _partition AS partition, _offset AS offset, _raw_message AS raw, _error AS error FROM default.kafka_engine WHERE length(_error) \u003e 0 https://github.com/ClickHouse/ClickHouse/pull/20249\nhttps://github.com/ClickHouse/ClickHouse/pull/21850\nhttps://altinity.com/blog/clickhouse-kafka-engine-faq\n","categories":"","description":"Error handling\n","excerpt":"Error handling\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/error-handling/","tags":"","title":"Error handling"},{"body":"EOS consumer (isolation.level=read_committed) is enabled by default since librdkafka 1.2.0, so for ClickHouse - since 20.2\nSee:\nedenhill/librdkafka@6b2a155 9de5dff BUT: while EOS semantics will guarantee you that no duplicates will happen on the Kafka side (i.e. even if you produce the same messages few times it will be consumed once), but ClickHouse as a Kafka client can currently guarantee only at-least-once. And in some corner cases (connection lost etc) you can get duplicates.\nWe need to have something like transactions on ClickHouse side to be able to avoid that. Adding something like simple transactions is in plans for Y2022.\nblock-aggregator by eBay Block Aggregator is a data loader that subscribes to Kafka topics, aggregates the Kafka messages into blocks that follow the Clickhouse’s table schemas, and then inserts the blocks into ClickHouse. Block Aggregator provides exactly-once delivery guarantee to load data from Kafka to ClickHouse. Block Aggregator utilizes Kafka’s metadata to keep track of blocks that are intended to send to ClickHouse, and later uses this metadata information to deterministically re-produce ClickHouse blocks for re-tries in case of failures. The identical blocks are guaranteed to be deduplicated by ClickHouse.\neBay/block-aggregator\n","categories":"","description":"Exactly once semantics\n","excerpt":"Exactly once semantics\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/","tags":"","title":"Exactly once semantics"},{"body":"CREATE DICTIONARY postgres_dict ( id UInt32, value String ) PRIMARY KEY id SOURCE( POSTGRESQL( port 5432 host 'postgres1' user 'postgres' password 'mysecretpassword' db 'clickhouse' table 'test_schema.test_table' ) ) LIFETIME(MIN 300 MAX 600) LAYOUT(HASHED()); and later do\nSELECT dictGetString(postgres_dict, 'value', toUInt64(1)) ","categories":"","description":"Example of PostgreSQL dictionary\n","excerpt":"Example of PostgreSQL dictionary\n","ref":"/altinity-kb-dictionaries/example-of-postgresql-dictionary/","tags":"","title":"Example of PostgreSQL dictionary"},{"body":"Use cases Strong correlation between column from table ORDER BY / PARTITION BY key and other column which is regularly being used in WHERE condition Good example is incremental ID which increasing with time.\nCREATE TABLE skip_idx_corr ( `key` UInt32, `id` UInt32, `ts` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, id); INSERT INTO skip_idx_corr SELECT rand(), number, now() + intDiv(number, 10) FROM numbers(100000000); SELECT count() FROM skip_idx_corr WHERE id = 6000000 1 rows in set. Elapsed: 0.167 sec. Processed 100.00 million rows, 400.00 MB (599.96 million rows/s., 2.40 GB/s.) ALTER TABLE skip_idx_corr ADD INDEX id_idx id TYPE minmax GRANULARITY 10; ALTER TABLE skip_idx_corr MATERIALIZE INDEX id_idx; SELECT count() FROM skip_idx_corr WHERE id = 6000000 1 rows in set. Elapsed: 0.017 sec. Processed 6.29 million rows, 25.17 MB (359.78 million rows/s., 1.44 GB/s.) Multiple Date/DateTime columns can be used in WHERE conditions Usually it could happen if you have separate Date and DateTime columns and different column being used in PARTITION BY expression and in WHERE condition. Another possible scenario when you have multiple DateTime columns which have pretty the same date or even time.\nCREATE TABLE skip_idx_multiple ( `key` UInt32, `date` Date, `time` DateTime, `created_at` DateTime, `inserted_at` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY (key, time); INSERT INTO skip_idx_multiple SELECT number, toDate(x), now() + intDiv(number, 10) AS x, x - (rand() % 100), x + (rand() % 100) FROM numbers(100000000); SELECT count() FROM skip_idx_multiple WHERE date \u003e (now() + toIntervalDay(105)); 1 rows in set. Elapsed: 0.048 sec. Processed 14.02 million rows, 28.04 MB (290.96 million rows/s., 581.92 MB/s.) SELECT count() FROM skip_idx_multiple WHERE time \u003e (now() + toIntervalDay(105)); 1 rows in set. Elapsed: 0.188 sec. Processed 100.00 million rows, 400.00 MB (530.58 million rows/s., 2.12 GB/s.) SELECT count() FROM skip_idx_multiple WHERE created_at \u003e (now() + toIntervalDay(105)); 1 rows in set. Elapsed: 0.400 sec. Processed 100.00 million rows, 400.00 MB (250.28 million rows/s., 1.00 GB/s.) ALTER TABLE skip_idx_multiple ADD INDEX time_idx time TYPE minmax GRANULARITY 1000; ALTER TABLE skip_idx_multiple MATERIALIZE INDEX time_idx; SELECT count() FROM skip_idx_multiple WHERE time \u003e (now() + toIntervalDay(105)); 1 rows in set. Elapsed: 0.036 sec. Processed 14.02 million rows, 56.08 MB (391.99 million rows/s., 1.57 GB/s.) ALTER TABLE skip_idx_multiple ADD INDEX created_at_idx created_at TYPE minmax GRANULARITY 1000; ALTER TABLE skip_idx_multiple MATERIALIZE INDEX created_at_idx; SELECT count() FROM skip_idx_multiple WHERE created_at \u003e (now() + toIntervalDay(105)); 1 rows in set. Elapsed: 0.076 sec. Processed 14.02 million rows, 56.08 MB (184.90 million rows/s., 739.62 MB/s.) Condition in query trying to filter outlier value CREATE TABLE skip_idx_outlier ( `key` UInt32, `ts` DateTime, `value` UInt32 ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, ts); INSERT INTO skip_idx_outlier SELECT number, now(), rand() % 10 FROM numbers(10000000); INSERT INTO skip_idx_outlier SELECT number, now(), 20 FROM numbers(10); SELECT count() FROM skip_idx_outlier WHERE value \u003e 15; 1 rows in set. Elapsed: 0.059 sec. Processed 10.00 million rows, 40.00 MB (170.64 million rows/s., 682.57 MB/s.) ALTER TABLE skip_idx_outlier ADD INDEX value_idx value TYPE minmax GRANULARITY 10; ALTER TABLE skip_idx_outlier MATERIALIZE INDEX value_idx; SELECT count() FROM skip_idx_outlier WHERE value \u003e 15; 1 rows in set. Elapsed: 0.004 sec. ","categories":"","description":"Example: minmax\n","excerpt":"Example: minmax\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/minmax/","tags":"","title":"Example: minmax"},{"body":"EXPLAIN types EXPLAIN AST SYNTAX PLAN indexes = 0, header = 0, description = 1, actions = 0, optimize = 1 json = 0 PIPELINE header = 0, graph = 0, compact = 1 ESTIMATE SELECT ... AST - abstract syntax tree SYNTAX - query text after AST-level optimizations PLAN - query execution plan PIPELINE - query execution pipeline ESTIMATE - https://github.com/ClickHouse/ClickHouse/pull/26131 (since 21.9) indexes=1 supported starting from 21.6 (https://github.com/ClickHouse/ClickHouse/pull/22352 ) json=1 supported starting from 21.6 (https://github.com/ClickHouse/ClickHouse/pull/23082) References\nhttps://clickhouse.com/docs/en/sql-reference/statements/explain/ Nikolai Kochetov from Yandeх. EXPLAIN query in ClickHouse. slides, video https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/query-profiling.pdf https://github.com/ClickHouse/ClickHouse/issues/28847 ","categories":"","description":"EXPLAIN query\n","excerpt":"EXPLAIN query\n","ref":"/altinity-kb-queries-and-syntax/explain-query/","tags":"","title":"EXPLAIN query"},{"body":"FETCH Parts from Zookeeper This is a detailed explanation on how to move data by fetching partitions or parts between replicas\nGet partitions by database and table: SELECT hostName() AS host, database, table partition_id, name as part_id FROM cluster('{cluster}', system.parts) WHERE database IN ('db1','db2' ... 'dbn') AND active This query will return all the partitions and parts stored in this node for the databases and their tables.\nFetch the partitions: Prior starting with the fetching process it is recommended to check the system.detached_parts table of the destination node. There is a chance that detached folders already contain some old parts, and you will have to remove them all before starting moving data. Otherwise you will attach those old parts together with the fetched parts. Also you could run into issues if there are detached folders with the same names as the ones you are fetching (not very probable, put possible). Simply delete the detached parts and continue with the process.\nTo fetch a partition:\nALTER TABLE \u003ctablename\u003e FETCH PARTITION \u003cpartition_id\u003e FROM '/clickhouse/{cluster}/tables/{shard}/{table}' The FROM path is from the zookeeper node and you have to specify the shard from you’re fetching the partition. Next executing the DDL query:\nALTER TABLE \u003ctablename\u003e ATTACH PARTITION \u003cpartition_id\u003e will attach the partitions to a table. Again and because the process is manual, it is recommended to check that the fetched partitions are attached correctly and that there are no detached parts left. Check both system.parts and system.detached_parts tables.\nDetach tables and delete replicas: If needed, after moving the data and checking that everything is sound, you can detach the tables and delete the replicas.\n-- Required for DROP REPLICA DETACH TABLE \u003ctable_name\u003e; -- This will remove everything from /table_path_in_z/replicas/replica_name -- but not the data. You could reattach the table again and -- restore the replica if needed. Get the zookeeper_path and replica_name from system.replicas SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/table_path_in_zk/'; Query to generate all the DDL: With this query you can generate the DDL script that will do the fetch and attach operations for each table and partition.\nSELECT DISTINCT 'alter table '||database||'.'||table||' FETCH PARTITION '''||partition_id||''' FROM '''||zookeeper_path||'''; ' ||'alter table '||database||'.'||table||' ATTACH PARTITION '''||partition_id||''';' FROM system.parts INNER JOIN system.replicas USING (database, table) WHERE database IN ('db1','db2' ... 'dbn') AND active You could add an ORDER BY to manually make the list in the order you need, or use ORDER BY rand() to randomize it. You will then need to split the commands between the shards.\n","categories":"","description":"Fetch Alter Table\n","excerpt":"Fetch Alter Table\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/fetch_alter_table/","tags":"","title":"Fetch Alter Table"},{"body":"CREATE TABLE event_table ( `key` UInt32, `created_at` DateTime, `value_a` UInt32, `value_b` String ) ENGINE = MergeTree ORDER BY (key, created_at) INSERT INTO event_table SELECT 1 AS key, toDateTime('2020-10-11 10:10:10') + number AS created_at, if((number = 0) OR ((number % 5) = 1), number + 1, 0) AS value_a, if((number = 0) OR ((number % 3) = 1), toString(number), '') AS value_b FROM numbers(10) SELECT main.key, main.created_at, a.value_a, b.value_b FROM event_table AS main ASOF INNER JOIN ( SELECT key, created_at, value_a FROM event_table WHERE value_a != 0 ) AS a ON (main.key = a.key) AND (main.created_at \u003e= a.created_at) ASOF INNER JOIN ( SELECT key, created_at, value_b FROM event_table WHERE value_b != '' ) AS b ON (main.key = b.key) AND (main.created_at \u003e= b.created_at) ┌─main.key─┬─────main.created_at─┬─a.value_a─┬─b.value_b─┐ │ 1 │ 2020-10-11 10:10:10 │ 1 │ 0 │ │ 1 │ 2020-10-11 10:10:11 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:12 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:13 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:14 │ 2 │ 4 │ │ 1 │ 2020-10-11 10:10:15 │ 2 │ 4 │ │ 1 │ 2020-10-11 10:10:16 │ 7 │ 4 │ │ 1 │ 2020-10-11 10:10:17 │ 7 │ 7 │ │ 1 │ 2020-10-11 10:10:18 │ 7 │ 7 │ │ 1 │ 2020-10-11 10:10:19 │ 7 │ 7 │ └──────────┴─────────────────────┴───────────┴───────────┘ SELECT key, created_at, value_a, value_b FROM ( SELECT key, groupArray(created_at) AS created_arr, arrayFill(x -\u003e (x != 0), groupArray(value_a)) AS a_arr, arrayFill(x -\u003e (x != ''), groupArray(value_b)) AS b_arr FROM ( SELECT * FROM event_table ORDER BY key ASC, created_at ASC ) GROUP BY key ) ARRAY JOIN created_arr AS created_at, a_arr AS value_a, b_arr AS value_b ┌─key─┬──────────created_at─┬─value_a─┬─value_b─┐ │ 1 │ 2020-10-11 10:10:10 │ 1 │ 0 │ │ 1 │ 2020-10-11 10:10:11 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:12 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:13 │ 2 │ 1 │ │ 1 │ 2020-10-11 10:10:14 │ 2 │ 4 │ │ 1 │ 2020-10-11 10:10:15 │ 2 │ 4 │ │ 1 │ 2020-10-11 10:10:16 │ 7 │ 4 │ │ 1 │ 2020-10-11 10:10:17 │ 7 │ 7 │ │ 1 │ 2020-10-11 10:10:18 │ 7 │ 7 │ │ 1 │ 2020-10-11 10:10:19 │ 7 │ 7 │ └─────┴─────────────────────┴─────────┴─────────┘ ","categories":"","description":"Fill missing values at query time\n","excerpt":"Fill missing values at query time\n","ref":"/altinity-kb-queries-and-syntax/fill-missing-values-at-query-time/","tags":"","title":"Fill missing values at query time"},{"body":"SELECT * FROM table FINAL\nBefore 20.5 - always executed in a single thread and slow. Since 20.5 - final can be parallel, see https://github.com/ClickHouse/ClickHouse/pull/10463 Since 20.10 - you can use do_not_merge_across_partitions_select_final setting. Since 22.6 - final even more parallel, see https://github.com/ClickHouse/ClickHouse/pull/36396 Since 22.8 - final doesn’t read excessive data, see https://github.com/ClickHouse/ClickHouse/pull/47801 Since 23.5 - final use less memory, see https://github.com/ClickHouse/ClickHouse/pull/50429 Since 23.9 - final doesn’t read PK columns if unneeded ie only one part in partition, see https://github.com/ClickHouse/ClickHouse/pull/53919 Since 23.12 - final applied only for interesecting ranges of parts, see https://github.com/ClickHouse/ClickHouse/pull/58120 Since 24.1 - final doesn’t compare rows from the same part with level \u003e 0, see https://github.com/ClickHouse/ClickHouse/pull/58142 Since 24.1 - final use vertical algorithm, (more cache friendly), see https://github.com/ClickHouse/ClickHouse/pull/54366 See https://github.com/ClickHouse/ClickHouse/pull/15938 and https://github.com/ClickHouse/ClickHouse/issues/11722\nSo it can work in the following way:\nDaily partitioning After day end + some time interval during which you can get some updates - for example at 3am / 6am you do OPTIMIZE TABLE xxx PARTITION 'prev_day' FINAL In that case using that FINAL with do_not_merge_across_partitions_select_final will be cheap. DROP TABLE IF EXISTS repl_tbl; CREATE TABLE repl_tbl ( `key` UInt32, `val_1` UInt32, `val_2` String, `val_3` String, `val_4` String, `val_5` UUID, `ts` DateTime ) ENGINE = ReplacingMergeTree(ts) PARTITION BY toDate(ts) ORDER BY key; ​ INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, '2020-01-01 00:00:00' as ts FROM numbers(10000000); OPTIMIZE TABLE repl_tbl PARTITION ID '20200101' FINAL; INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, '2020-01-02 00:00:00' as ts FROM numbers(10000000); OPTIMIZE TABLE repl_tbl PARTITION ID '20200102' FINAL; INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, '2020-01-03 00:00:00' as ts FROM numbers(10000000); OPTIMIZE TABLE repl_tbl PARTITION ID '20200103' FINAL; INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, '2020-01-04 00:00:00' as ts FROM numbers(10000000); OPTIMIZE TABLE repl_tbl PARTITION ID '20200104' FINAL; SYSTEM STOP MERGES repl_tbl; INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, '2020-01-05 00:00:00' as ts FROM numbers(10000000); ​ SELECT count() FROM repl_tbl WHERE NOT ignore(*) ┌──count()─┐ │ 50000000 │ └──────────┘ 1 rows in set. Elapsed: 1.504 sec. Processed 50.00 million rows, 6.40 GB (33.24 million rows/s., 4.26 GB/s.) SELECT count() FROM repl_tbl FINAL WHERE NOT ignore(*) ┌──count()─┐ │ 10000000 │ └──────────┘ 1 rows in set. Elapsed: 3.314 sec. Processed 50.00 million rows, 6.40 GB (15.09 million rows/s., 1.93 GB/s.) /* more that 2 time slower, and will get worse once you will have more data */ set do_not_merge_across_partitions_select_final=1; SELECT count() FROM repl_tbl FINAL WHERE NOT ignore(*) ┌──count()─┐ │ 50000000 │ └──────────┘ 1 rows in set. Elapsed: 1.850 sec. Processed 50.00 million rows, 6.40 GB (27.03 million rows/s., 3.46 GB/s.) /* only 0.35 sec slower, and while partitions have about the same size that extra cost will be about constant */ ","categories":"","description":"FINAL clause speed\n","excerpt":"FINAL clause speed\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/","tags":"","title":"FINAL clause speed"},{"body":"It’s possible to use dictionaries for populating columns of fact table.\nCREATE TABLE customer ( `customer_id` UInt32, `first_name` String, `birth_date` Date, `sex` Enum('M' = 1, 'F' = 2) ) ENGINE = MergeTree ORDER BY customer_id CREATE TABLE order ( `order_id` UInt32, `order_date` DateTime DEFAULT now(), `cust_id` UInt32, `amount` Decimal(12, 2) ) ENGINE = MergeTree PARTITION BY toYYYYMM(order_date) ORDER BY (order_date, cust_id, order_id) INSERT INTO customer VALUES(1, 'Mike', now() - INTERVAL 30 YEAR, 'M'); INSERT INTO customer VALUES(2, 'Boris', now() - INTERVAL 40 YEAR, 'M'); INSERT INTO customer VALUES(3, 'Sofie', now() - INTERVAL 24 YEAR, 'F'); INSERT INTO order (order_id, cust_id, amount) VALUES(50, 1, 15); INSERT INTO order (order_id, cust_id, amount) VALUES(30, 1, 10); SELECT * EXCEPT 'order_date' FROM order ┌─order_id─┬─cust_id─┬─amount─┐ │ 30 │ 1 │ 10.00 │ │ 50 │ 1 │ 15.00 │ └──────────┴─────────┴────────┘ CREATE DICTIONARY customer_dict ( `customer_id` UInt32, `first_name` String, `birth_date` Date, `sex` UInt8 ) PRIMARY KEY customer_id SOURCE(CLICKHOUSE(TABLE 'customer')) LIFETIME(MIN 0 MAX 300) LAYOUT(FLAT) ALTER TABLE order ADD COLUMN `cust_first_name` String DEFAULT dictGetString('default.customer_dict', 'first_name', toUInt64(cust_id)), ADD COLUMN `cust_sex` Enum('M' = 1, 'F' = 2) DEFAULT dictGetUInt8('default.customer_dict', 'sex', toUInt64(cust_id)), ADD COLUMN `cust_birth_date` Date DEFAULT dictGetDate('default.customer_dict', 'birth_date', toUInt64(cust_id)); INSERT INTO order (order_id, cust_id, amount) VALUES(10, 3, 30); INSERT INTO order (order_id, cust_id, amount) VALUES(20, 3, 60); INSERT INTO order (order_id, cust_id, amount) VALUES(40, 2, 20); SELECT * EXCEPT 'order_date' FROM order FORMAT PrettyCompactMonoBlock ┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐ │ 30 │ 1 │ 10.00 │ Mike │ M │ 1991-08-05 │ │ 50 │ 1 │ 15.00 │ Mike │ M │ 1991-08-05 │ │ 10 │ 3 │ 30.00 │ Sofie │ F │ 1997-08-05 │ │ 40 │ 2 │ 20.00 │ Boris │ M │ 1981-08-05 │ │ 20 │ 3 │ 60.00 │ Sofie │ F │ 1997-08-05 │ └──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ ALTER TABLE customer UPDATE birth_date = now() - INTERVAL 35 YEAR WHERE customer_id=2; SYSTEM RELOAD DICTIONARY customer_dict; ALTER TABLE order UPDATE cust_birth_date = dictGetDate('default.customer_dict', 'birth_date', toUInt64(cust_id)) WHERE 1 -- or if you do have track of changes it's possible to lower amount of dict calls -- UPDATE cust_birth_date = dictGetDate('default.customer_dict', 'birth_date', toUInt64(cust_id)) WHERE customer_id = 2 SELECT * EXCEPT 'order_date' FROM order FORMAT PrettyCompactMonoBlock ┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐ │ 30 │ 1 │ 10.00 │ Mike │ M │ 1991-08-05 │ │ 50 │ 1 │ 15.00 │ Mike │ M │ 1991-08-05 │ │ 10 │ 3 │ 30.00 │ Sofie │ F │ 1997-08-05 │ │ 40 │ 2 │ 20.00 │ Boris │ M │ 1986-08-05 │ │ 20 │ 3 │ 60.00 │ Sofie │ F │ 1997-08-05 │ └──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ ALTER TABLE order UPDATE would completely overwrite this column in table, so it’s not recommended to run it often.\n","categories":"","description":"Flattened table\n","excerpt":"Flattened table\n","ref":"/altinity-kb-schema-design/flattened-table/","tags":"","title":"Flattened table"},{"body":"Float arithmetics is not accurate: https://floating-point-gui.de/\nIn case you need accurate calculations you should use Decimal datatypes.\nOperations on floats are not associative SELECT (toFloat64(100000000000000000.) + toFloat64(7.5)) - toFloat64(100000000000000000.) AS res ┌─res─┐ │ 0 │ └─────┘ SELECT (toFloat64(100000000000000000.) - toFloat64(100000000000000000.)) + toFloat64(7.5) AS res ┌─res─┐ │ 7.5 │ └─────┘ No problem with Decimals: SELECT (toDecimal64(100000000000000000., 1) + toDecimal64(7.5, 1)) - toDecimal64(100000000000000000., 1) AS res ┌─res─┐ │ 7.5 │ └─────┘ SELECT (toDecimal64(100000000000000000., 1) - toDecimal64(100000000000000000., 1)) + toDecimal64(7.5, 1) AS res ┌─res─┐ │ 7.5 │ └─────┘ Warning Because clickhouse uses MPP order of execution of a single query can vary on each run, and you can get slightly different results from the float column every time you run the query.\nUsually, this deviation is small, but it can be significant when some kind of arithmetic operation is performed on very large and very small numbers at the same time.\nSome decimal numbers has no accurate float representation SELECT sum(toFloat64(0.45)) AS res FROM numbers(10000) ┌───────────────res─┐ │ 4499.999999999948 │ └───────────────────┘ SELECT sumKahan(toFloat64(0.45)) AS res FROM numbers(10000) ┌──res─┐ │ 4500 │ └──────┘ SELECT toFloat32(0.6) * 6 AS res ┌────────────────res─┐ │ 3.6000001430511475 │ └────────────────────┘ No problem with Decimal: SELECT sum(toDecimal64(0.45, 2)) AS res FROM numbers(10000) ┌──res─┐ │ 4500 │ └──────┘ SELECT toDecimal32(0.6, 1) * 6 AS res ┌─res─┐ │ 3.6 │ └─────┘ Direct comparisons of floats may be impossible The same number can have several floating-point representations and because of that you should not compare Floats directly\nSELECT (toFloat32(0.1) * 10) = (toFloat32(0.01) * 100) AS res ┌─res─┐ │ 0 │ └─────┘ SELECT sumIf(0.1, number \u003c 10) AS a, sumIf(0.01, number \u003c 100) AS b, a = b AS a_eq_b FROM numbers(100) ┌──────────────────a─┬──────────────────b─┬─a_eq_b─┐ │ 0.9999999999999999 │ 1.0000000000000004 │ 0 │ └────────────────────┴────────────────────┴────────┘ See also\nhttps://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/ https://stackoverflow.com/questions/4915462/how-should-i-do-floating-point-comparison https://stackoverflow.com/questions/2100490/floating-point-inaccuracy-examples https://stackoverflow.com/questions/10371857/is-floating-point-addition-and-multiplication-associative\nBut:\nhttps://github.com/ClickHouse/ClickHouse/issues/24909\n","categories":"","description":"Floats vs Decimals\n","excerpt":"Floats vs Decimals\n","ref":"/altinity-kb-schema-design/floats-vs-decimals/","tags":"","title":"Floats vs Decimals"},{"body":"migrate migrate is a simple schema migration tool written in golang. No external dependencies are required (like interpreter, jre), only one platform-specific executable. golang-migrate/migrate\nmigrate supports several databases, including ClickHouse (support was introduced by @kshvakov).\nTo store information about migrations state migrate creates one additional table in target database, by default that table is called schema_migrations.\nInstall download the migrate executable for your platform and put it to the folder listed in your %PATH.\n#wget https://github.com/golang-migrate/migrate/releases/download/v3.2.0/migrate.linux-amd64.tar.gz wget https://github.com/golang-migrate/migrate/releases/download/v4.14.1/migrate.linux-amd64.tar.gz tar -xzf migrate.linux-amd64.tar.gz mkdir -p ~/bin mv migrate.linux-amd64 ~/bin/migrate rm migrate.linux-amd64.tar.gz Sample usage mkdir migrations echo 'create table test(id UInt8) Engine = Memory;' \u003e migrations/000001_my_database_init.up.sql echo 'DROP TABLE test;' \u003e migrations/000001_my_database_init.down.sql # you can also auto-create file with new migrations with automatic numbering like that: migrate create -dir migrations -seq -digits 6 -ext sql my_database_init edit migrations/000001_my_database_init.up.sql \u0026 migrations/000001_my_database_init.down.sql migrate -database 'clickhouse://localhost:9000' -path ./migrations up 1/u my_database_init (6.502974ms) migrate -database 'clickhouse://localhost:9000' -path ./migrations down 1/d my_database_init (2.164394ms) # clears the database (use carefully - will not ask any confirmations) ➜ migrate -database 'clickhouse://localhost:9000' -path ./migrations drop Connection string format clickhouse://host:port?username=user\u0026password=qwerty\u0026database=clicks\nURL Query Description x-migrations-table Name of the migrations table x-migrations-table-engine Engine to use for the migrations table, defaults to TinyLog x-cluster-name Name of cluster for creating table cluster wide database The name of the database to connect to username The user to sign in as password The user’s password host The host to connect to. port The port to bind to. secure to use a secure connection (for self-signed also add skip_verify=1) Replicated / Distributed / Cluster environments golang-migrate supports a clustered Clickhouse environment since v4.15.0.\nIf you provide x-cluster-name query param, it will create the table to store migration data on the passed cluster.\nKnown issues could not load time location: unknown time zone Europe/Moscow in line 0:\nIt’s happens due of missing tzdata package in migrate/migrate docker image of golang-migrate. There is 2 possible solutions:\nYou can build your own golang-migrate image from official with tzdata package. If you using it as part of your CI you can add installing tzdata package as one of step in CI before using golang-migrate. Related GitHub issues: https://github.com/golang-migrate/migrate/issues/494 https://github.com/golang-migrate/migrate/issues/201\nUsing database name in x-migrations-table\nCreates table with database.table When running migrations migrate actually uses database from query settings and encapsulate database.table as table name: ``other_database.`database.table``` ","categories":"","description":"golang-migrate\n","excerpt":"golang-migrate\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/golang-migrate/","tags":"","title":"golang-migrate"},{"body":"GCS with the table function - seems to work correctly for simple scenarios.\nEssentially you can follow the steps from the Migrating from Amazon S3 to Cloud Storage.\nSet up a GCS bucket. This bucket must be set as part of the default project for the account. This configuration can be found in settings -\u003e interoperability. Generate a HMAC key for the account, can be done in settings -\u003e interoperability, in the section for user account access keys. In ClickHouse, replace the S3 bucket endpoint with the GCS bucket endpoint This must be done with the path-style GCS endpoint: https://storage.googleapis.com/BUCKET_NAME/OBJECT_NAME. Replace the aws access key id and aws secret access key with the corresponding parts of the HMAC key. ","categories":"","description":"\"Google S3 GCS\"\n","excerpt":"\"Google S3 GCS\"\n","ref":"/altinity-kb-integrations/altinity-kb-google-s3-gcs/","tags":"","title":"Google S3 (GCS)"},{"body":"ClickHouse ClickHouse will use all available hardware to maximize performance. So the more hardware - the better. As of this publication, the hardware requirements are:\nMinimum Hardware: 4-core CPU with support of SSE4.2, 16 Gb RAM, 1Tb HDD. Recommended for development and staging environments. SSE4.2 is required, and going below 4 Gb of RAM is not recommended. Recommended Hardware: \u003e=16-cores, \u003e=64Gb RAM, HDD-raid or SSD. For processing up to hundreds of millions / billions of rows. For clouds: disk throughput is the more important factor compared to IOPS. Be aware of burst / baseline disk speed difference.\nSee also: https://clickhouse.tech/benchmark/hardware/\nZookeeper Zookeeper requires separate servers from those used for ClickHouse. Zookeeper has poor performance when installed on the same node as ClickHouse.\nHardware Requirements for Zookeeper:\nFast disk speed (ideally NVMe, 128Gb should be enough). Any modern CPU (one core, better 2) 4Gb of RAM For clouds - be careful with burstable network disks (like gp2 on aws): you may need up to 1000 IOPs on the disk for on a long run, so gp3 with 3000 IOPs baseline is a better choice.\nThe number of Zookeeper instances depends on the environment:\nProduction: 3 is an optimal number of zookeeper instances. Development and Staging: 1 zookeeper instance is sufficient. See also:\nhttps://docs.altinity.com/operationsguide/clickhouse-zookeeper/ altinity-kb-proper-setup zookeeper-monitoring ClickHouse Hardware Configuration Configure the servers according to those recommendations the ClickHouse Usage Recommendations.\nTest Your Hardware Be sure to test the following:\nRAM speed. Network speed. Storage speed. It’s better to find any performance issues before installing ClickHouse.\n","categories":"","description":"Hardware Requirements\n","excerpt":"Hardware Requirements\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/","tags":"","title":"Hardware Requirements"},{"body":"In general, it is a NORMAL situation for clickhouse that while processing a huge dataset it can use a lot of (or all of) the server resources. It is ‘by design’ - just to make the answers faster.\nThe main directions to reduce the CPU usage is to review the schema / queries to limit the amount of the data which need to be processed, and to plan the resources in a way when single running query will not impact the others.\nAny attempts to reduce the CPU usage will end up with slower queries!\nHow to slow down queries to reduce the CPU usage If it is acceptable for you - please check the following options for limiting the CPU usage:\nsetting max_threads: reducing the number of threads that are allowed to use one request. Fewer threads = more free cores for other requests. By default, it’s allowed to take half of the available CPU cores, adjust only when needed. So if if you have 10 cores then max_threads = 10 will work about twice faster than max_threads=5, but will take 100% or CPU. (max_threads=5 will use half of CPUs so 50%).\nsetting os_thread_priority: increasing niceness for selected requests. In this case, the operating system, when choosing which of the running processes to allocate processor time, will prefer processes with lower niceness. 0 is the default niceness. The higher the niceness, the lower the priority of the process. The maximum niceness value is 19.\nThese are custom settings that can be tweaked in several ways:\nby specifying them when connecting a client, for example\nclickhouse-client --os_thread_priority=19 -q 'SELECT max (number) from numbers (100000000)' echo 'SELECT max(number) from numbers(100000000)' | curl 'http://localhost:8123/?os_thread_priority=19' --data-binary @- via dedicated API / connection parameters in client libraries\nusing the SQL command SET (works only within the session)\nSET os_thread_priority = 19; SELECT max(number) from numbers(100000000) using different profiles of settings for different users. Something like\n\u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e ... \u003c/default\u003e \u003clowcpu\u003e \u003cos_thread_priority\u003e19\u003c/os_thread_priority\u003e \u003cmax_threads\u003e4\u003c/max_threads\u003e \u003c/lowcpu\u003e \u003c/profiles\u003e \u003c!-- Users and ACL. --\u003e \u003cusers\u003e \u003c!-- If user name was not specified, 'default' user is used. --\u003e \u003climited_user\u003e \u003cpassword\u003e123\u003c/password\u003e \u003cnetworks\u003e \u003cip\u003e::/0\u003c/ip\u003e \u003c/networks\u003e \u003cprofile\u003elowcpu\u003c/profile\u003e \u003c!-- Quota for user. --\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003c/limited_user\u003e \u003c/users\u003e \u003c/yandex\u003e There are also plans to introduce a system of more flexible control over the assignment of resources to different requests.\nAlso, if these are manually created queries, then you can try to discipline users by adding quotas to them (they can be formulated as “you can read no more than 100GB of data per hour” or “no more than 10 queries”, etc.)\nIf these are automatically generated queries, it may make sense to check if there is no way to write them in a more efficient way.\n","categories":"","description":"High CPU usage\n","excerpt":"High CPU usage\n","ref":"/altinity-kb-setup-and-maintenance/high-cpu-usage/","tags":"","title":"High CPU usage"},{"body":"Zookeeper use watches to notify a client on znode changes. This article explains how to check watches set by ZooKeeper servers and how it is used.\nSolution:\nZookeeper uses the 'wchc' command to list all watches set on the Zookeeper server.\n# echo wchc | nc zookeeper 2181\nReference\nhttps://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html\nThe wchp and wchc commands are not enabled by default because of their known DOS vulnerability. For more information, see ZOOKEEPER-2693and Zookeeper 3.5.2 - Denial of Service.\nBy default those commands are disabled, they can be enabled via Java system property:\n-Dzookeeper.4lw.commands.whitelist=*\non in zookeeper config: 4lw.commands.whitelist=*\\\n","categories":"","description":"How to check the list of watches\n","excerpt":"How to check the list of watches\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-how-to-check-the-list-of-watches/","tags":"","title":"How to check the list of watches"},{"body":"The following instructions are an example on how to convert a database with the Engine type Atomic to a database with the Engine type Ordinary.\nWarning That can be used only for simple schemas. Schemas with MATERIALIZED views will require extra manipulations. CREATE DATABASE atomic_db ENGINE = Atomic; CREATE DATABASE ordinary_db ENGINE = Ordinary; CREATE TABLE atomic_db.x ENGINE = MergeTree ORDER BY tuple() AS system.numbers; INSERT INTO atomic_db.x SELECT number FROM numbers(100000); RENAME TABLE atomic_db.x TO ordinary_db.x; ls -1 /var/lib/clickhouse/data/ordinary_db/x all_1_1_0 detached format_version.txt DROP DATABASE atomic_db; DETACH DATABASE ordinary_db; mv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql vi /var/lib/clickhouse/metadata/atomic_db.sql mv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db mv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db ATTACH DATABASE atomic_db; SELECT count() FROM atomic_db.x ┌─count()─┐ │ 100000 │ └─────────┘ SHOW CREATE DATABASE atomic_db ┌─statement──────────────────────────────────┐ │ CREATE DATABASE atomic_db ENGINE = Ordinary │ └────────────────────────────────────────────┘ Schemas with Materialized VIEW DROP DATABASE IF EXISTS atomic_db; DROP DATABASE IF EXISTS ordinary_db; CREATE DATABASE atomic_db engine=Atomic; CREATE DATABASE ordinary_db engine=Ordinary; CREATE TABLE atomic_db.x ENGINE = MergeTree ORDER BY tuple() AS system.numbers; CREATE MATERIALIZED VIEW atomic_db.x_mv ENGINE = MergeTree ORDER BY tuple() AS SELECT * FROM atomic_db.x; CREATE MATERIALIZED VIEW atomic_db.y_mv ENGINE = MergeTree ORDER BY tuple() AS SELECT * FROM atomic_db.x; CREATE TABLE atomic_db.z ENGINE = MergeTree ORDER BY tuple() AS system.numbers; CREATE MATERIALIZED VIEW atomic_db.z_mv TO atomic_db.z AS SELECT * FROM atomic_db.x; INSERT INTO atomic_db.x SELECT * FROM numbers(100); --- USE atomic_db; --- --- Query id: 28af886d-a339-4e9c-979c-8bdcfb32fd95 --- --- ┌─name───────────────────────────────────────────┐ --- │ .inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c │ --- │ .inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f │ --- │ x │ --- │ x_mv │ --- │ y_mv │ --- │ z │ --- │ z_mv │ --- └────────────────────────────────────────────────┘ SELECT mv_storage.database, mv_storage.name, mv.database, mv.name FROM system.tables AS mv_storage LEFT JOIN system.tables AS mv ON substring(mv_storage.name, 11) = toString(mv.uuid) WHERE mv_storage.name LIKE '.inner_id.%' AND mv_storage.database = 'atomic_db'; -- ┌─database──┬─name───────────────────────────────────────────┬─mv.database─┬─mv.name─┐ -- │ atomic_db │ .inner_id.81e1a67d-3d02-4b2a-be17-84d8626d2328 │ atomic_db │ y_mv │ -- │ atomic_db │ .inner_id.e428225c-982a-4859-919b-ba5026db101d │ atomic_db │ x_mv │ -- └───────────┴────────────────────────────────────────────────┴─────────────┴─────────┘ /* STEP 1: prepare rename statements, also to rename implicit mv storage table to explicit one */ SELECT if( t.name LIKE '.inner_id.%', 'RENAME TABLE `' || t.database || '`.`' || t.name || '` TO `ordinary_db`.`' || mv.name || '_storage`;', 'RENAME TABLE `' || t.database || '`.`' || t.name || '` TO `ordinary_db`.`' || t.name || '`;' ) FROM system.tables as t LEFT JOIN system.tables mv ON (substring(t.name,11) = toString(mv.uuid) AND t.database = mv.database ) WHERE t.database = 'atomic_db' AND t.engine \u003c\u003e 'MaterializedView' FORMAT TSVRaw; -- RENAME TABLE `atomic_db`.`.inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c` TO `ordinary_db`.`y_mv_storage`; -- RENAME TABLE `atomic_db`.`.inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f` TO `ordinary_db`.`x_mv_storage`; -- RENAME TABLE `atomic_db`.`x` TO `ordinary_db`.`x`; -- RENAME TABLE `atomic_db`.`z` TO `ordinary_db`.`z`; /* STEP 2: prepare statements to reattach MV */ -- Can be done manually: pick existing MV definition (SHOW CREATE TABLE), and change it in the following way: -- 1) add TO keyword 2) remove column names and engine settings after mv name SELECT if( t.name LIKE '.inner_id.%', replaceRegexpOne(mv.create_table_query, '^CREATE MATERIALIZED VIEW ([^ ]+) (.*? AS ', 'CREATE MATERIALIZED VIEW \\\\1 TO \\\\1_storage AS '), mv.create_table_query ) FROM system.tables as mv LEFT JOIN system.tables t ON (substring(t.name,11) = toString(mv.uuid) AND t.database = mv.database) WHERE mv.database = 'atomic_db' AND mv.engine='MaterializedView' FORMAT TSVRaw; -- CREATE MATERIALIZED VIEW atomic_db.x_mv TO atomic_db.x_mv_storage AS SELECT * FROM atomic_db.x -- CREATE MATERIALIZED VIEW atomic_db.y_mv TO atomic_db.y_mv_storage AS SELECT * FROM atomic_db.x /* STEP 3: stop inserts, fire renames statements prepared at the step 1 (hint: use clickhouse-client -mn) */ RENAME ... /* STEP 4: ensure that only MaterializedView left in source db, and drop it. */ SELECT * FROM system.tables WHERE database = 'atomic_db' and engine \u003c\u003e 'MaterializedView'; DROP DATABASE atomic_db; /* STEP 4. rename table to old name: */ DETACH DATABASE ordinary_db; -- rename files / folders: mv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql vi /var/lib/clickhouse/metadata/atomic_db.sql mv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db mv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db -- attach database atomic_db; ATTACH DATABASE atomic_db; /* STEP 5. restore MV using statements created on STEP 2 */ ","categories":"","description":"How to Convert Atomic to Ordinary\n","excerpt":"How to Convert Atomic to Ordinary\n","ref":"/engines/altinity-kb-atomic-database-engine/altinity-kb-how-to-convert-atomic-to-ordinary/","tags":"","title":"How to Convert Atomic to Ordinary"},{"body":"Example Create test_table based on the source table.\nCREATE TABLE test_table AS source_table ENGINE=MergeTree() PARTITION BY ...; If the source table has Replicated*MergeTree engine, you would need to change it to non-replicated.\nAttach one partition with data from the source table to test_table.\nALTER TABLE test_table ATTACH PARTITION ID '20210120' FROM source_table; You can modify the column or create a new one based on the old column value.\nALTER TABLE test_table MODIFY COLUMN column_a CODEC(ZSTD(2)); ALTER TABLE test_table ADD COLUMN column_new UInt32 DEFAULT toUInt32OrZero(column_old) CODEC(T64,LZ4); After that, you would need to populate changed columns with data.\nALTER TABLE test_table UPDATE column_a=column_a, column_new=column_new WHERE 1; You can look status of mutation via the system.mutations table\nSELECT * FROM system.mutations; And it’s also possible to kill mutation if there are some problems with it.\nKILL MUTATION WHERE ... Useful queries SELECT database, table, count() AS parts, uniqExact(partition_id) AS partition_cnt, sum(rows), formatReadableSize(sum(data_compressed_bytes) AS comp_bytes) AS comp, formatReadableSize(sum(data_uncompressed_bytes) AS uncomp_bytes) AS uncomp, uncomp_bytes / comp_bytes AS ratio FROM system.parts WHERE active GROUP BY database, table ORDER BY comp_bytes DESC SELECT database, table, column, type, sum(rows) AS rows, sum(column_data_compressed_bytes) AS compressed_bytes, formatReadableSize(compressed_bytes) AS compressed, formatReadableSize(sum(column_data_uncompressed_bytes)) AS uncompressed, sum(column_data_uncompressed_bytes) / compressed_bytes AS ratio, any(compression_codec) AS codec FROM system.parts_columns AS pc LEFT JOIN system.columns AS c ON (pc.database = c.database) AND (c.table = pc.table) AND (c.name = pc.column) WHERE (database LIKE '%') AND (table LIKE '%') AND active GROUP BY database, table, column, type ORDER BY database, table, sum(column_data_compressed_bytes) DESC ","categories":"","description":"How to test different compression codecs\n","excerpt":"How to test different compression codecs\n","ref":"/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/","tags":"","title":"How to test different compression codecs"},{"body":"\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup27/adaptive_index_granularity.pdf\n","categories":"","description":"index \u0026 column files\n","excerpt":"index \u0026 column files\n","ref":"/engines/mergetree-table-engine-family/index-and-column-files/","tags":"","title":"index \u0026 column files"},{"body":"clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format TSV' \u003e speed.tsv clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format RowBinary' \u003e speed.RowBinary clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format Native' \u003e speed.Native clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format CSV' \u003e speed.csv clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format JSONEachRow' \u003e speed.JSONEachRow clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format Parquet' \u003e speed.parquet clickhouse-client -q 'select toString(number) s, number n, number/1000 f from numbers(100000000) format Avro' \u003e speed.avro -- Engine=Null does not have I/O / sorting overhead -- we test only formats parsing performance. create table n (s String, n UInt64, f Float64) Engine=Null -- clickhouse-client parses formats itself -- it allows to see user CPU time -- time is used in a multithreaded application -- another option is to disable parallelism `--input_format_parallel_parsing=0` -- real -- wall / clock time. time clickhouse-client -t -q 'insert into n format TSV' \u003c speed.tsv 2.693 real 0m2.728s user 0m14.066s time clickhouse-client -t -q 'insert into n format RowBinary' \u003c speed.RowBinary 3.744 real 0m3.773s user 0m4.245s time clickhouse-client -t -q 'insert into n format Native' \u003c speed.Native 2.359 real 0m2.382s user 0m1.945s time clickhouse-client -t -q 'insert into n format CSV' \u003c speed.csv 3.296 real 0m3.328s user 0m18.145s time clickhouse-client -t -q 'insert into n format JSONEachRow' \u003c speed.JSONEachRow 8.872 real 0m8.899s user 0m30.235s time clickhouse-client -t -q 'insert into n format Parquet' \u003c speed.parquet 4.905 real 0m4.929s user 0m5.478s time clickhouse-client -t -q 'insert into n format Avro' \u003c speed.avro 11.491 real 0m11.519s user 0m12.166s As you can see the JSONEachRow is the worst format (user 0m30.235s) for this synthetic dataset. Native is the best (user 0m1.945s). TSV / CSV are good in wall time but spend a lot of CPU (user time).\n","categories":"","description":"","excerpt":"clickhouse-client -q 'select toString(number) s, number n, number/1000 …","ref":"/altinity-kb-schema-design/ingestion-performance-and-formats/","tags":"","title":"Ingestion performance and formats"},{"body":"How do I Store IPv4 and IPv6 Address In One Field? There is a clean and simple solution for that. Any IPv4 has its unique IPv6 mapping:\nIPv4 IP address: 191.239.213.197 IPv4-mapped IPv6 address: ::ffff:191.239.213.197 Find IPs matching CIDR/network mask (IPv4) WITH IPv4CIDRToRange( toIPv4('10.0.0.1'), 8 ) as range SELECT * FROM values('ip IPv4', toIPv4('10.2.3.4'), toIPv4('192.0.2.1'), toIPv4('8.8.8.8')) WHERE ip BETWEEN range.1 AND range.2; Find IPs matching CIDR/network mask (IPv6) WITH IPv6CIDRToRange ( toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32 ) as range SELECT * FROM values('ip IPv6', toIPv6('2001:db8::8a2e:370:7334'), toIPv6('::ffff:192.0.2.1'), toIPv6('::')) WHERE ip BETWEEN range.1 AND range.2; ","categories":"","description":"IPs/masks\n","excerpt":"IPs/masks\n","ref":"/altinity-kb-schema-design/how-to-store-ips/","tags":"","title":"IPs/masks"},{"body":"Sample data CREATE TABLE test_metrics (counter_id Int64, timestamp DateTime, metric UInt64) Engine=Log; INSERT INTO test_metrics SELECT number % 3, toDateTime('2021-01-01 00:00:00'), 1 FROM numbers(20); INSERT INTO test_metrics SELECT number % 3, toDateTime('2021-01-03 00:00:00'), 1 FROM numbers(20); SELECT counter_id, toDate(timestamp) dt, sum(metric) FROM test_metrics GROUP BY counter_id, dt ORDER BY counter_id, dt; ┌─counter_id─┬─────────dt─┬─sum(metric)─┐ │ 0 │ 2021-01-01 │ 7 │ │ 0 │ 2021-01-03 │ 7 │ │ 1 │ 2021-01-01 │ 7 │ │ 1 │ 2021-01-03 │ 7 │ │ 2 │ 2021-01-01 │ 6 │ │ 2 │ 2021-01-03 │ 6 │ └────────────┴────────────┴─────────────┘ Calendar WITH arrayMap(i -\u003e (toDate('2021-01-01') + i), range(4)) AS Calendar SELECT arrayJoin(Calendar); ┌─arrayJoin(Calendar)─┐ │ 2021-01-01 │ │ 2021-01-02 │ │ 2021-01-03 │ │ 2021-01-04 │ └─────────────────────┘ Join with Calendar using arrayJoin SELECT counter_id, tuple.2 dt, sum(tuple.1) sum FROM ( WITH arrayMap(i -\u003e (0, toDate('2021-01-01') + i), range(4)) AS Calendar SELECT counter_id, arrayJoin(arrayConcat(Calendar, [(sum, dt)])) tuple FROM (SELECT counter_id, toDate(timestamp) dt, sum(metric) sum FROM test_metrics GROUP BY counter_id, dt) ) GROUP BY counter_id, dt ORDER BY counter_id, dt; ┌─counter_id─┬─────────dt─┬─sum─┐ │ 0 │ 2021-01-01 │ 7 │ │ 0 │ 2021-01-02 │ 0 │ │ 0 │ 2021-01-03 │ 7 │ │ 0 │ 2021-01-04 │ 0 │ │ 1 │ 2021-01-01 │ 7 │ │ 1 │ 2021-01-02 │ 0 │ │ 1 │ 2021-01-03 │ 7 │ │ 1 │ 2021-01-04 │ 0 │ │ 2 │ 2021-01-01 │ 6 │ │ 2 │ 2021-01-02 │ 0 │ │ 2 │ 2021-01-03 │ 6 │ │ 2 │ 2021-01-04 │ 0 │ └────────────┴────────────┴─────┘ With fill SELECT counter_id, toDate(timestamp) AS dt, sum(metric) AS sum FROM test_metrics GROUP BY counter_id, dt ORDER BY counter_id ASC WITH FILL, dt ASC WITH FILL FROM toDate('2021-01-01') TO toDate('2021-01-05'); ┌─counter_id─┬─────────dt─┬─sum─┐ │ 0 │ 2021-01-01 │ 7 │ │ 0 │ 2021-01-02 │ 0 │ │ 0 │ 2021-01-03 │ 7 │ │ 0 │ 2021-01-04 │ 0 │ │ 1 │ 2021-01-01 │ 7 │ │ 1 │ 2021-01-02 │ 0 │ │ 1 │ 2021-01-03 │ 7 │ │ 1 │ 2021-01-04 │ 0 │ │ 2 │ 2021-01-01 │ 6 │ │ 2 │ 2021-01-02 │ 0 │ │ 2 │ 2021-01-03 │ 6 │ │ 2 │ 2021-01-04 │ 0 │ └────────────┴────────────┴─────┘ ","categories":"","description":"Join with Calendar using Arrays\n","excerpt":"Join with Calendar using Arrays\n","ref":"/altinity-kb-queries-and-syntax/join-with-calendar-using-arrays/","tags":"","title":"Join with Calendar using Arrays"},{"body":"Resources:\nOverview of JOINs (Russian) - Presentation from Meetup 38 in 2019 Notes on JOIN options Join Table Engine The main purpose of JOIN table engine is to avoid building the right table for joining on each query execution. So it’s usually used when you have a high amount of fast queries which share the same right table for joining.\nUpdates It’s possible to update rows with setting join_any_take_last_row enabled.\nCREATE TABLE id_val_join ( `id` UInt32, `val` UInt8 ) ENGINE = Join(ANY, LEFT, id) SETTINGS join_any_take_last_row = 1 Ok. INSERT INTO id_val_join VALUES (1,21)(1,22)(3,23); Ok. SELECT * FROM ( SELECT toUInt32(number) AS id FROM numbers(4) ) AS n ANY LEFT JOIN id_val_join USING (id) ┌─id─┬─val─┐ │ 0 │ 0 │ │ 1 │ 22 │ │ 2 │ 0 │ │ 3 │ 23 │ └────┴─────┘ INSERT INTO id_val_join VALUES (1,40)(2,24); Ok. SELECT * FROM ( SELECT toUInt32(number) AS id FROM numbers(4) ) AS n ANY LEFT JOIN id_val_join USING (id) ┌─id─┬─val─┐ │ 0 │ 0 │ │ 1 │ 40 │ │ 2 │ 24 │ │ 3 │ 23 │ └────┴─────┘ Join table engine documentation\n","categories":"","description":"JOINs\n","excerpt":"JOINs\n","ref":"/altinity-kb-queries-and-syntax/joins/","tags":"","title":"JOINs"},{"body":"Tables with engine Null don’t store data but can be used as a source for materialized views.\nJSONAsString a special input format which allows to ingest JSONs into a String column. If the input has several JSON objects (comma separated) they will be interpreted as separate rows. JSON can be multiline.\ncreate table entrypoint(J String) Engine=Null; create table datastore(a String, i Int64, f Float64) Engine=MergeTree order by a; create materialized view jsonConverter to datastore as select (JSONExtract(J, 'Tuple(String,Tuple(Int64,Float64))') as x), x.1 as a, x.2.1 as i, x.2.2 as f from entrypoint; $ echo '{\"s\": \"val1\", \"b2\": {\"i\": 42, \"f\": 0.1}}' | \\ clickhouse-client -q \"insert into entrypoint format JSONAsString\" $ echo '{\"s\": \"val1\",\"b2\": {\"i\": 33, \"f\": 0.2}},{\"s\": \"val1\",\"b2\": {\"i\": 34, \"f\": 0.2}}' | \\ clickhouse-client -q \"insert into entrypoint format JSONAsString\" SELECT * FROM datastore; ┌─a────┬──i─┬───f─┐ │ val1 │ 42 │ 0.1 │ └──────┴────┴─────┘ ┌─a────┬──i─┬───f─┐ │ val1 │ 33 │ 0.2 │ │ val1 │ 34 │ 0.2 │ └──────┴────┴─────┘ See also: JSONExtract to parse many attributes at a time\n","categories":"","description":"JSONAsString and Mat. View as JSON parser\n","excerpt":"JSONAsString and Mat. View as JSON parser\n","ref":"/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/","tags":"","title":"JSONAsString and Mat. View as JSON parser"},{"body":"WITH JSONExtract(json, 'Tuple(name String, id String, resources Nested(description String, format String, tracking_summary Tuple(total UInt32, recent UInt32)), extras Nested(key String, value String))') AS parsed_json SELECT tupleElement(parsed_json, 'name') AS name, tupleElement(parsed_json, 'id') AS id, tupleElement(tupleElement(parsed_json, 'resources'), 'description') AS `resources.description`, tupleElement(tupleElement(parsed_json, 'resources'), 'format') AS `resources.format`, tupleElement(tupleElement(tupleElement(parsed_json, 'resources'), 'tracking_summary'), 'total') AS `resources.tracking_summary.total`, tupleElement(tupleElement(tupleElement(parsed_json, 'resources'), 'tracking_summary'), 'recent') AS `resources.tracking_summary.recent` FROM url('https://raw.githubusercontent.com/jsonlines/guide/master/datagov100.json', 'JSONAsString', 'json String') ","categories":"","description":"JSONExtract to parse many attributes at a time\n","excerpt":"JSONExtract to parse many attributes at a time\n","ref":"/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time/","tags":"","title":"JSONExtract to parse many attributes at a time"},{"body":"TLDR version use fresh Java version (11 or newer), disable swap and set up (for 4 Gb node):\nJAVA_OPTS=\"-Xms512m -Xmx3G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" If you have a node with more RAM - change it accordingly, for example for 8Gb node:\nJAVA_OPTS=\"-Xms512m -Xmx7G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" Details ZooKeeper runs as in JVM. Depending on version different garbage collectors are available.\nRecent JVM versions (starting from 10) use G1 garbage collector by default (should work fine). On JVM 13-14 using ZGC or Shenandoah garbage collector may reduce pauses. On older JVM version (before 10) you may want to make some tuning to decrease pauses, ParNew + CMS garbage collectors (like in Yandex config) is one of the best options.\nOne of the most important setting for JVM application is heap size. A heap size of \u003e1 GB is recommended for most use cases and monitoring heap usage to ensure no delays are caused by garbage collection. We recommend to use at least 4Gb of RAM for zookeeper nodes (8Gb is better, that will make difference only when zookeeper is heavily loaded).\nSet the Java heap size smaller than available RAM size on the node. This is very important to avoid swapping, which will seriously degrade ZooKeeper performance. Be conservative - use a maximum heap size of 3GB for a 4GB machine.\nAdd XX:+AlwaysPreTouch flag as well to load the memory pages into memory at the start of the zookeeper.\nSet min (Xms) heap size to the values like 512Mb, or even to the same value as max (Xmx) to avoid resizing and returning the RAM to OS. Add XX:+AlwaysPreTouch flag as well to load the memory pages into memory at the start of the zookeeper.\nMaxGCPauseMillis=50 (by default 200) - the ’target’ acceptable pause for garbage collection (milliseconds)\njute.maxbuffer limits the maximum size of znode content. By default it’s 1Mb. In some usecases (lot of partitions in table) ClickHouse may need to create bigger znodes.\n(optional) enable GC logs: -Xloggc:/path_to/gc.log\nZookeeper configurarion used by Yandex Metrika (from 2017) The configuration used by Yandex ( https://clickhouse.tech/docs/en/operations/tips/#zookeeper ) - they use older JVM version (with UseParNewGC garbage collector), and tune GC logs heavily:\nJAVA_OPTS=\"-Xms{{ cluster.get('xms','128M') }} \\ -Xmx{{ cluster.get('xmx','1G') }} \\ -Xloggc:/var/log/$NAME/zookeeper-gc.log \\ -XX:+UseGCLogFileRotation \\ -XX:NumberOfGCLogFiles=16 \\ -XX:GCLogFileSize=16M \\ -verbose:gc \\ -XX:+PrintGCTimeStamps \\ -XX:+PrintGCDateStamps \\ -XX:+PrintGCDetails -XX:+PrintTenuringDistribution \\ -XX:+PrintGCApplicationStoppedTime \\ -XX:+PrintGCApplicationConcurrentTime \\ -XX:+PrintSafepointStatistics \\ -XX:+UseParNewGC \\ -XX:+UseConcMarkSweepGC \\ -XX:+CMSParallelRemarkEnabled\" See also https://wikitech.wikimedia.org/wiki/JVM_Tuning#G1_for_full_gcs https://sematext.com/blog/java-garbage-collection-tuning/ https://www.oracle.com/technical-resources/articles/java/g1gc.html https://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2 https://docs.cloudera.com/runtime/7.2.7/kafka-performance-tuning/topics/kafka-tune-broker-tuning-jvm.html https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm-tune-g1gc.html https://www.maknesium.de/21-most-important-java-8-vm-options-for-servers https://docs.oracle.com/javase/10/gctuning/introduction-garbage-collection-tuning.htm#JSGCT-GUID-326EB4CF-8C8C-4267-8355-21AB04F0D304 https://github.com/chewiebug/GCViewer ","categories":"","description":"JVM sizes and garbage collector settings\n","excerpt":"JVM sizes and garbage collector settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/","tags":"","title":"JVM sizes and garbage collector settings"},{"body":"git log -- contrib/librdkafka | git name-rev --stdin ClickHouse version librdkafka version 21.10+ (#27883) 1.6.1 + snappy fixes + boring ssl + illumos_build fixes + edenhill#3279 fix 21.6+ (#23874) 1.6.1 + snappy fixes + boring ssl + illumos_build fixes 21.1+ (#18671) 1.6.0-RC3 + snappy fixes + boring ssl 20.13+ (#18053) 1.5.0 + msan fixes + snappy fixes + boring ssl 20.7+ (#12991) 1.5.0 + msan fixes 20.5+ (#11256) 1.4.2 20.2+ (#9000) 1.3.0 19.11+ (#5872) 1.1.0 19.5+ (#4799) 1.0.0 19.1+ (#4025) 1.0.0-RC5 v1.1.54382+ (#2276) 0.11.4 ","categories":"","description":"Kafka\n","excerpt":"Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/","tags":"","title":"Kafka"},{"body":"One of the threads from scheduled_pool (pre 20.9) / background_message_broker_schedule_pool (after 20.9) do that in infinite loop:\nBatch poll (time limit: kafka_poll_timeout_ms 500ms, messages limit: kafka_poll_max_batch_size 65536) Parse messages. If we don’t have enough data (rows limit: kafka_max_block_size 1048576) or time limit reached (kafka_flush_interval_ms 7500ms) - continue polling (goto p.1) Write a collected block of data to MV Do commit (commit after write = at-least-once). On any error, during that process, Kafka client is restarted (leading to rebalancing - leave the group and get back in few seconds).\nImportant settings These usually should not be adjusted:\nkafka_poll_max_batch_size = max_block_size (65536) kafka_poll_timeout_ms = stream_poll_timeout_ms (500ms) You may want to adjust those depending on your scenario:\nkafka_flush_interval_ms = stream_poll_timeout_ms (7500ms) kafka_max_block_size = max_insert_block_size / kafka_num_consumers (for the single consumer: 1048576) See also https://github.com/ClickHouse/ClickHouse/pull/11388\nDisable at-least-once delivery kafka_commit_every_batch = 1 will change the loop logic mentioned above. Consumed batch commited to the Kafka and the block of rows send to Materialized Views only after that. It could be resembled as at-most-once delivery mode as prevent duplicate creation but allow loss of data in case of failures.\n","categories":"","description":"Kafka main parsing loop\n","excerpt":"Kafka main parsing loop\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/","tags":"","title":"Kafka main parsing loop"},{"body":"For very large topics when you need more parallelism (especially on the insert side) you may use several tables with the same pipeline (pre 20.9) or enable kafka_thread_per_consumer (after 20.9).\nkafka_num_consumers = N, kafka_thread_per_consumer=1 Notes:\nthe inserts will happen in parallel (without that setting inserts happen linearly) enough partitions are needed. kafka_num_consumers is limited by number of physical cores (half of vCPUs). kafka_disable_num_consumers_limit can be used to override the limit. background_message_broker_schedule_pool_size is 16 by default, you may need to increase if using more than 16 consumers Before increasing kafka_num_consumers with keeping kafka_thread_per_consumer=0 may improve consumption \u0026 parsing speed, but flushing \u0026 committing still happens by a single thread there (so inserts are linear).\n","categories":"","description":"Kafka parallel consuming\n","excerpt":"Kafka parallel consuming\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/","tags":"","title":"Kafka parallel consuming"},{"body":"Unfortunately not all queries can be killed. KILL QUERY only sets a flag that must be checked by the query. A query pipeline is checking this flag before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed. If a query does not stop, the only way to get rid of it is to restart ClickHouse.\nSee also:\nhttps://github.com/ClickHouse/ClickHouse/issues/3964 https://github.com/ClickHouse/ClickHouse/issues/1576 How to replace a running query Q. We are trying to abort running queries when they are being replaced with a new one. We are setting the same query id for this. In some cases this error happens:\nQuery with id = e213cc8c-3077-4a6c-bc78-e8463adad35d is already running and can’t be stopped\nThe query is still being killed but the new one is not being executed. Do you know anything about this and if there is a fix or workaround for it?\nI guess you use replace_running_query + replace_running_query_max_wait_ms.\nUnfortunately it’s not always possible to kill the query at random moment of time.\nKill don’t send any signals, it just set a flag. Which gets (synchronously) checked at certain moments of query execution, mostly after finishing processing one block and starting another.\nOn certain stages (executing scalar sub-query) the query can not be killed at all. This is a known issue and requires an architectural change to fix it.\nI see. Is there a workaround?\nThis is our use case:\nA user requests an analytics report which has a query that takes several settings, the user makes changes to the report (e.g. to filters, metrics, dimensions…). Since the user changed what he is looking for the query results from the initial query are never used and we would like to cancel it when starting the new query (edited)\nYou can just use 2 commands:\nKILL QUERY WHERE query_id = ' ... ' ASYNC SELECT ... new query .... in that case you don’t need to care when the original query will be stopped.\n","categories":"","description":"KILL QUERY\n","excerpt":"KILL QUERY\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-kill-query/","tags":"","title":"KILL QUERY"},{"body":"ClickHouse-copier deployment in kubernetes Clickhouse-copier can be deployed in a kubernetes environment to automate some simple backups or copy fresh data between clusters.\nSome documentation to read:\nhttps://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/ https://clickhouse.com/docs/en/operations/utilities/clickhouse-copier/ Deployment Use a kubernetes job is recommended but a simple pod can be used if you only want to execute the copy one time.\nJust edit/change all the yaml files to your needs.\n1) Create the PVC: First create a namespace in which all the pods and resources are going to be deployed\nkubectl create namespace clickhouse-copier Then create the PVC using a storageClass gp2-encrypted class or use any other storageClass from other providers:\n--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: copier-logs namespace: clickhouse-copier spec: storageClassName: gp2-encrypted accessModes: - ReadWriteOnce resources: requests: storage: 100Mi and deploy:\nkubectl -n clickhouse-copier create -f ./kubernetes/copier-pvc.yaml 2) Create the configmap: The configmap has both files zookeeper.xml and task01.xml with the zookeeper node listing and the parameters for the task respectively.\n--- apiVersion: v1 kind: ConfigMap metadata: name: copier-config namespace: clickhouse-copier data: task01.xml: | \u003cclickhouse\u003e \u003clogger\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003clog remove=\"remove\"/\u003e \u003cerrorlog remove=\"remove\"/\u003e \u003clevel\u003etrace\u003c/level\u003e \u003c/logger\u003e \u003cremote_servers\u003e \u003call-replicated\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003eclickhouse01.svc.cluster.local\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003cuser\u003echcopier\u003c/user\u003e \u003cpassword\u003epass\u003c/password\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse02.svc.cluster.local\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003cuser\u003echcopier\u003c/user\u003e \u003cpassword\u003epass\u003c/password\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/all-replicated\u003e \u003call-sharded\u003e \u003c!-- \u003csecret\u003e\u003c/secret\u003e --\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003eclickhouse03.svc.cluster.local\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003cuser\u003echcopier\u003c/user\u003e \u003cpassword\u003epass\u003c/password\u003e \u003c/replica\u003e \u003c/shard\u003e \u003cshard\u003e \u003creplica\u003e \u003chost\u003eclickhouse03.svc.cluster.local\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003cuser\u003echcopier\u003c/user\u003e \u003cpassword\u003epass\u003c/password\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/all-sharded\u003e \u003c/remote_servers\u003e \u003cmax_workers\u003e1\u003c/max_workers\u003e \u003csettings_pull\u003e \u003creadonly\u003e1\u003c/readonly\u003e \u003c/settings_pull\u003e \u003csettings_push\u003e \u003creadonly\u003e0\u003c/readonly\u003e \u003c/settings_push\u003e \u003csettings\u003e \u003cconnect_timeout\u003e3\u003c/connect_timeout\u003e \u003cinsert_distributed_sync\u003e1\u003c/insert_distributed_sync\u003e \u003c/settings\u003e \u003ctables\u003e \u003ctable_sales\u003e \u003ccluster_pull\u003eall-replicated\u003c/cluster_pull\u003e \u003cdatabase_pull\u003edefault\u003c/database_pull\u003e \u003ctable_pull\u003efact_sales_event\u003c/table_pull\u003e \u003ccluster_push\u003eall-sharded\u003c/cluster_push\u003e \u003cdatabase_push\u003edefault\u003c/database_push\u003e \u003ctable_push\u003efact_sales_event\u003c/table_push\u003e \u003cengine\u003e Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/fact_sales_event', '{replica}') PARTITION BY toYYYYMM(timestamp) ORDER BY (channel_id, product_id) SETTINGS index_granularity = 8192 \u003c/engine\u003e \u003csharding_key\u003erand()\u003c/sharding_key\u003e \u003c/table_ventas\u003e \u003c/tables\u003e \u003c/clickhouse\u003e zookeeper.xml: | \u003cclickhouse\u003e \u003clogger\u003e \u003clevel\u003etrace\u003c/level\u003e \u003csize\u003e100M\u003c/size\u003e \u003ccount\u003e3\u003c/count\u003e \u003c/logger\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003ezookeeper1.svc.cluster.local\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper2.svc.cluster.local\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper3.svc.cluster.local\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003c/clickhouse\u003e and deploy:\nkubectl -n clickhouse-copier create -f ./kubernetes/copier-configmap.yaml The task01.xml file has many parameters to take into account explained in the clickhouse-copier documentation. Important to note that it is needed a FQDN for the zookeeper nodes and clickhouse server that are valid for the cluster. As the deployment creates a new namespace, it is recommended to use a FQDN linked to a service. For example zookeeper01.svc.cluster.local. This file should be adapted to both clusters topologies and to the needs of the user.\nThe zookeeper.xml file is pretty straightforward with a simple 3 node ensemble configuration.\n3) Create the job: Basically the job will download the official clickhouse image and will create a pod with 2 containers:\nclickhouse-copier: This container will run the clickhouse-copier utility.\nsidecar-logging: This container will be used to read the logs of the clickhouse-copier container for different runs (this part can be improved):\n--- apiVersion: batch/v1 kind: Job metadata: name: clickhouse-copier-test namespace: clickhouse-copier spec: # only for kubernetes 1.23 # ttlSecondsAfterFinished: 86400 template: spec: containers: - name: clickhouse-copier image: clickhouse/clickhouse-server:21.8 command: - clickhouse-copier - --task-upload-force=1 - --config-file=$(CH_COPIER_CONFIG) - --task-path=$(CH_COPIER_TASKPATH) - --task-file=$(CH_COPIER_TASKFILE) - --base-dir=$(CH_COPIER_BASEDIR) env: - name: CH_COPIER_CONFIG value: \"/var/lib/clickhouse/tmp/zookeeper.xml\" - name: CH_COPIER_TASKPATH value: \"/clickhouse/copier/tasks/task01\" - name: CH_COPIER_TASKFILE value: \"/var/lib/clickhouse/tmp/task01.xml\" - name: CH_COPIER_BASEDIR value: \"/var/lib/clickhouse/tmp\" resources: limits: cpu: \"1\" memory: 2048Mi volumeMounts: - name: copier-config mountPath: /var/lib/clickhouse/tmp/zookeeper.xml subPath: zookeeper.xml - name: copier-config mountPath: /var/lib/clickhouse/tmp/task01.xml subPath: task01.xml - name: copier-logs mountPath: /var/lib/clickhouse/tmp - name: sidecar-logger image: busybox:1.35 command: ['/bin/sh', '-c', 'tail', '-n', '1000', '-f', '/tmp/copier-logs/clickhouse-copier*/*.log'] resources: limits: cpu: \"1\" memory: 512Mi volumeMounts: - name: copier-logs mountPath: /tmp/copier-logs volumes: - name: copier-config configMap: name: copier-config items: - key: zookeeper.xml path: zookeeper.xml - key: task01.xml path: task01.xml - name: copier-logs persistentVolumeClaim: claimName: copier-logs restartPolicy: Never backoffLimit: 3 Deploy and watch progress checking the logs:\nkubectl -n clickhouse-copier logs \u003cpodname\u003e sidecar-logging ","categories":"","description":"Kubernetes job for clickhouse-copier\n","excerpt":"Kubernetes job for clickhouse-copier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-kubernetes-job/","tags":"","title":"Kubernetes job for clickhouse-copier"},{"body":"Sample data CREATE TABLE llexample ( g Int32, a Date ) ENGINE = Memory; INSERT INTO llexample SELECT number % 3, toDate('2020-01-01') + number FROM numbers(10); SELECT * FROM llexample ORDER BY g,a; ┌─g─┬──────────a─┐ │ 0 │ 2020-01-01 │ │ 0 │ 2020-01-04 │ │ 0 │ 2020-01-07 │ │ 0 │ 2020-01-10 │ │ 1 │ 2020-01-02 │ │ 1 │ 2020-01-05 │ │ 1 │ 2020-01-08 │ │ 2 │ 2020-01-03 │ │ 2 │ 2020-01-06 │ │ 2 │ 2020-01-09 │ └───┴────────────┘ Using arrays select g, (arrayJoin(tuple_ll) as ll).1 a, ll.2 prev, ll.3 next from ( select g, arrayZip( arraySort(groupArray(a)) as aa, arrayPopBack(arrayPushFront(aa, toDate(0))), arrayPopFront(arrayPushBack(aa, toDate(0))) ) tuple_ll from llexample group by g) order by g, a; ┌─g─┬──────────a─┬───────prev─┬───────next─┐ │ 0 │ 2020-01-01 │ 1970-01-01 │ 2020-01-04 │ │ 0 │ 2020-01-04 │ 2020-01-01 │ 2020-01-07 │ │ 0 │ 2020-01-07 │ 2020-01-04 │ 2020-01-10 │ │ 0 │ 2020-01-10 │ 2020-01-07 │ 1970-01-01 │ │ 1 │ 2020-01-02 │ 1970-01-01 │ 2020-01-05 │ │ 1 │ 2020-01-05 │ 2020-01-02 │ 2020-01-08 │ │ 1 │ 2020-01-08 │ 2020-01-05 │ 1970-01-01 │ │ 2 │ 2020-01-03 │ 1970-01-01 │ 2020-01-06 │ │ 2 │ 2020-01-06 │ 2020-01-03 │ 2020-01-09 │ │ 2 │ 2020-01-09 │ 2020-01-06 │ 1970-01-01 │ └───┴────────────┴────────────┴────────────┘ Using window functions (starting from Clickhouse 21.3) SET allow_experimental_window_functions = 1; SELECT g, a, any(a) OVER (PARTITION BY g ORDER BY a ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) AS prev, any(a) OVER (PARTITION BY g ORDER BY a ASC ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING) AS next FROM llexample ORDER BY g ASC, a ASC; ┌─g─┬──────────a─┬───────prev─┬───────next─┐ │ 0 │ 2020-01-01 │ 1970-01-01 │ 2020-01-04 │ │ 0 │ 2020-01-04 │ 2020-01-01 │ 2020-01-07 │ │ 0 │ 2020-01-07 │ 2020-01-04 │ 2020-01-10 │ │ 0 │ 2020-01-10 │ 2020-01-07 │ 1970-01-01 │ │ 1 │ 2020-01-02 │ 1970-01-01 │ 2020-01-05 │ │ 1 │ 2020-01-05 │ 2020-01-02 │ 2020-01-08 │ │ 1 │ 2020-01-08 │ 2020-01-05 │ 1970-01-01 │ │ 2 │ 2020-01-03 │ 1970-01-01 │ 2020-01-06 │ │ 2 │ 2020-01-06 │ 2020-01-03 │ 2020-01-09 │ │ 2 │ 2020-01-09 │ 2020-01-06 │ 1970-01-01 │ └───┴────────────┴────────────┴────────────┘ Using lagInFrame/leadInFrame (starting from ClickHouse 21.4) SELECT g, a, lagInFrame(a) OVER (PARTITION BY g ORDER BY a ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS prev, leadInFrame(a) OVER (PARTITION BY g ORDER BY a ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS next FROM llexample ORDER BY g ASC, a ASC; ┌─g─┬──────────a─┬───────prev─┬───────next─┐ │ 0 │ 2020-01-01 │ 1970-01-01 │ 2020-01-04 │ │ 0 │ 2020-01-04 │ 2020-01-01 │ 2020-01-07 │ │ 0 │ 2020-01-07 │ 2020-01-04 │ 2020-01-10 │ │ 0 │ 2020-01-10 │ 2020-01-07 │ 1970-01-01 │ │ 1 │ 2020-01-02 │ 1970-01-01 │ 2020-01-05 │ │ 1 │ 2020-01-05 │ 2020-01-02 │ 2020-01-08 │ │ 1 │ 2020-01-08 │ 2020-01-05 │ 1970-01-01 │ │ 2 │ 2020-01-03 │ 1970-01-01 │ 2020-01-06 │ │ 2 │ 2020-01-06 │ 2020-01-03 │ 2020-01-09 │ │ 2 │ 2020-01-09 │ 2020-01-06 │ 1970-01-01 │ └───┴────────────┴────────────┴────────────┘ Using neighbor (no grouping, incorrect result over blocks) SELECT g, a, neighbor(a, -1) AS prev, neighbor(a, 1) AS next FROM ( SELECT * FROM llexample ORDER BY g ASC, a ASC ); ┌─g─┬──────────a─┬───────prev─┬───────next─┐ │ 0 │ 2020-01-01 │ 1970-01-01 │ 2020-01-04 │ │ 0 │ 2020-01-04 │ 2020-01-01 │ 2020-01-07 │ │ 0 │ 2020-01-07 │ 2020-01-04 │ 2020-01-10 │ │ 0 │ 2020-01-10 │ 2020-01-07 │ 2020-01-02 │ │ 1 │ 2020-01-02 │ 2020-01-10 │ 2020-01-05 │ │ 1 │ 2020-01-05 │ 2020-01-02 │ 2020-01-08 │ │ 1 │ 2020-01-08 │ 2020-01-05 │ 2020-01-03 │ │ 2 │ 2020-01-03 │ 2020-01-08 │ 2020-01-06 │ │ 2 │ 2020-01-06 │ 2020-01-03 │ 2020-01-09 │ │ 2 │ 2020-01-09 │ 2020-01-06 │ 1970-01-01 │ └───┴────────────┴────────────┴────────────┘ ","categories":"","description":"Lag / Lead\n","excerpt":"Lag / Lead\n","ref":"/altinity-kb-queries-and-syntax/lag-lead/","tags":"","title":"Lag / Lead"},{"body":"In general - one of the simplest option to do load balancing is to implement it on the client side.\nI.e. list several endpoints for clickhouse connections and add some logic to pick one of the nodes.\nMany client libraries support that.\nClickHouse native protocol (port 9000) Currently there are no protocol-aware proxies for clickhouse protocol, so the proxy / load balancer can work only on TCP level.\nOne of the best option for TCP load balancer is haproxy, also nginx can work in that mode.\nHaproxy will pick one upstream when connection is established, and after that it will keep it connected to the same server until the client or server will disconnect (or some timeout will happen).\nIt can’t send different queries coming via a single connection to different servers, as he knows nothing about clickhouse protocol and doesn’t know when one query ends and another start, it just sees the binary stream.\nSo for native protocol, there are only 3 possibilities:\nclose connection after each query client-side close connection after each query server-side (currently there is only one setting for that - idle_connection_timeout=0, which is not exact what you need, but similar). use a clickhouse server with Distributed table as a proxy. HTTP protocol (port 8123) There are many more options and you can use haproxy / nginx / chproxy, etc. chproxy give some extra clickhouse-specific features, you can find a list of them at https://chproxy.org\n","categories":"","description":"Load balancers\n","excerpt":"Load balancers\n","ref":"/altinity-kb-setup-and-maintenance/load-balancers/","tags":"","title":"Load balancers"},{"body":"Settings allow_suspicious_low_cardinality_types In CREATE TABLE statement allows specifying LowCardinality modifier for types of small fixed size (8 or less). Enabling this may increase merge times and memory consumption.\nlow_cardinality_max_dictionary_size default - 8192\nMaximum size (in rows) of shared global dictionary for LowCardinality type.\nlow_cardinality_use_single_dictionary_for_part LowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.\nlow_cardinality_allow_in_native_format Use LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query.\noutput_format_arrow_low_cardinality_as_dictionary Enable output LowCardinality type as Dictionary Arrow type\n","categories":"","description":"LowCardinality\n","excerpt":"LowCardinality\n","ref":"/altinity-kb-schema-design/lowcardinality/","tags":"","title":"LowCardinality"},{"body":"Resources\nMachine Learning in ClickHouse - Presentation from 2019 (Meetup 31) ML discussion: CatBoost / MindsDB / Fast.ai - Brief article from 2021 Machine Learning Forecase (Russian) - Presentation from 2019 (Meetup 38) ","categories":"","description":"Machine learning in ClickHouse\n","excerpt":"Machine learning in ClickHouse\n","ref":"/altinity-kb-queries-and-syntax/machine-learning-in-clickhouse/","tags":"","title":"Machine learning in ClickHouse"},{"body":" Info MATERIALIZED VIEWs in ClickHouse behave like AFTER INSERT TRIGGER to the left-most table listed in its SELECT statement. MATERIALIZED VIEWS Clickhouse and the magic of materialized views. Basics explained with examples: webinar recording Everything you should know about materialized views. Very detailed information about internals: video, annotated presentation, presentation Best practices Use MATERIALIZED VIEW with TO syntax (explicit storage table)\nFirst you create the table which will store the data calculated by MV explicitly, and after that create materialized view itself with TO syntax.\nCREATE TABLE target ( ... ) Engine=[Replicated][Replacing/Summing/...]MergeTree ...; CREATE MATERIALIZED VIEW mv_source2target TO target AS SELECT ... FROM source; That way it’s bit simpler to do schema migrations or build more complicated pipelines when one table is filled by several MV.\nWith engine=Atomic it hard to map undelying table with the MV.\nAvoid using POPULATE when creating MATERIALIZED VIEW on big tables.\nUse manual backfilling (with the same query) instead.\nWith POPULATE the data ingested to the source table during MV populating will not appear in MV. POPULATE doesn’t work with TO syntax. With manual backfilling, you have much better control on the process - you can do it in parts, adjust settings etc. In case of some failure ‘in the middle (for example due to timeouts), it’s hard to understand the state of the MV. CREATE MATERIALIZED VIEW mv_source2target TO target AS SELECT ... FROM source WHERE cond \u003e ... INSERT INTO target SELECT ... FROM source WHERE cond \u003c ... This way you have full control backfilling process (you can backfill in smaller parts to avoid timeouts, do some cross-checks / integrity-checks, change some settings, etc.)\nFAQ Q. Can I attach MATERIALIZED VIEW to the VIEW, or engine=Merge, or engine=MySQL, etc.? Since MATERIALIZED VIEWs are updated on every INSERT to the underlying table and you can not insert anything to the usual VIEW, the materialized view update will never be triggered.\nNormally you should build MATERIALIZED VIEWs on the top of the table with MergeTree engine family.\nQ. I’ve created materialized error with some error, and since it’s it reading from Kafka I don’t understand where the error is Server logs will help you. Also, see the next question.\nQ. How to debug misbehaving MATERIALIZED VIEW? You can also attach the same MV to some dummy table with engine=Log (or even Null) and do some manual inserts there to debug the behavior. Similar way (as the Materialized view often can contain some pieces of the business logic of the application) you can create tests for your schema.\nWarning Always test MATERIALIZED VIEWs first on staging or testing environments Possible test scenario:\ncreate a copy of the original table CREATE TABLE src_copy ... AS src create MV on that copy CREATE MATERIALIZED VIEW ... AS SELECT ... FROM src_copy check if inserts to src_copy work properly, and mv is properly filled. INSERT INTO src_copy SELECT * FROM src LIMIT 100 cleanup the temp stuff and recreate MV on real table. Q. Can I use subqueries / joins in MV? It is possible but it is a very bad idea for most of the use cases**.**\nSo it will most probably work not as you expect and will hit insert performance significantly.\nThe MV will be attached (as AFTER INSERT TRIGGER) to the left-most table in the MV SELECT statement, and it will ‘see’ only freshly inserted rows there. It will ‘see’ the whole set of rows of other tables, and the query will be executed EVERY TIME you do the insert to the left-most table. That will impact the performance speed there significantly. If you really need to update the MV with the left-most table, not impacting the performance so much you can consider using dictionary / engine=Join / engine=Set for right-hand table / subqueries (that way it will be always in memory, ready to use).\nQ. How to alter MV implicit storage (w/o TO syntax) take the existing MV definition\nSHOW CREATE TABLE dbname.mvname; Adjust the query in the following manner:\nreplace ‘CREATE MATERIALIZED VIEW’ to ‘ATTACH MATERIALIZED VIEW’ add needed columns; Detach materialized view with the command:\nDETACH TABLE dbname.mvname ON CLUSTER cluster_name; Add the needed column to the underlying ReplicatedAggregatingMergeTree table\n-- if the Materialized view was created without TO keyword ALTER TABLE dbname.`.inner.mvname` ON CLUSTER cluster_name add column tokens AggregateFunction(uniq, UInt64); -- othewise just alter the target table used in `CREATE MATERIALIZED VIEW ...` `TO ...` clause attach MV back using the query you create at p. 1.\nATTACH MATERIALIZED VIEW dbname.mvname ON CLUSTER cluster_name ( /* ... */ `tokens` AggregateFunction(uniq, UInt64) ) ENGINE = ReplicatedAggregatingMergeTree(...) ORDER BY ... AS SELECT /* ... */ uniqState(rand64()) as tokens FROM /* ... */ GROUP BY /* ... */ As you can see that operation is NOT atomic, so the safe way is to stop data ingestion during that procedure.\nIf you have version 19.16.13 or newer you can change the order of step 2 and 3 making the period when MV is detached and not working shorter (related issue https://github.com/ClickHouse/ClickHouse/issues/7878).\nSee also:\nhttps://github.com/ClickHouse/ClickHouse/issues/1226 https://github.com/ClickHouse/ClickHouse/pull/7533 ","categories":"","description":"MATERIALIZED VIEWS\n","excerpt":"MATERIALIZED VIEWS\n","ref":"/altinity-kb-schema-design/materialized-views/","tags":"","title":"MATERIALIZED VIEWS"},{"body":"max_memory_usage. Single query memory usage max_memory_usage - the maximum amount of memory allowed for a single query to take. By default, it’s 10Gb. The default value is good, don’t adjust it in advance.\nThere are scenarios when you need to relax the limit for particular queries (if you hit ‘Memory limit (for query) exceeded’), or use a lower limit if you need to discipline the users or increase the number of simultaneous queries.\nServer memory usage Server memory usage = constant memory footprint (used by different caches, dictionaries, etc) + sum of memory temporary used by running queries (a theoretical limit is a number of simultaneous queries multiplied by max_memory_usage).\nSince 20.4 you can set up a global limit using the max_server_memory_usage setting. If something will hit that limit you will see ‘Memory limit (total) exceeded’ in random places.\nBy default it 90% of the physical RAM of the server. https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#max_server_memory_usage https://github.com/ClickHouse/ClickHouse/blob/e5b96bd93b53d2c1130a249769be1049141ef386/programs/server/config.xml#L239-L250\nYou can decrease that in some scenarios (like you need to leave more free RAM for page cache or to some other software).\nHow to check what is using my RAM? altinity-kb-who-ate-my-memory.md\nMark cache https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/mark-cache.pdf\n","categories":"","description":"memory configuration settings\n","excerpt":"memory configuration settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/","tags":"","title":"memory configuration settings"},{"body":"Memory Overcommiter From version 22.2+ ClickHouse was updated with enhanced Memory overcommit capabilities. In the past, queries were constrained by the max_memory_usage setting, imposing a rigid limitation. Users had the option to increase this limit, but it came at the potential expense of impacting other users during a single query. With the introduction of Memory overcommit, more memory-intensive queries can now execute, granted there are ample resources available. When the server reaches its maximum memory limit, ClickHouse identifies the most overcommitted queries and attempts to terminate them. It’s important to note that the terminated query might not be the one causing the condition. If it’s not, the query will undergo a waiting period to allow the termination of the high-memory query before resuming its execution. This setup ensures that low-memory queries always have the opportunity to run, while more resource-intensive queries can execute during server idle times when resources are abundant. Users have the flexibility to fine-tune this behavior at both the server and user levels.\nIf the memory overcommitter is not being used you’ll get something like this:\nReceived exception from server (version 22.8.20): Code: 241. DB::Exception: Received from altinity.cloud:9440. DB::Exception: Received from chi-replica1-2-0:9000. DB::Exception: Memory limit (for query) exceeded: would use 5.00 GiB (attempt to allocate chunk of 4196736 bytes), maximum: 5.00 GiB. OvercommitTracker decision: Memory overcommit isn't used. OvercommitTracker isn't set.: (avg_value_size_hint = 0, avg_chars_size = 1, limit = 8192): while receiving packet from chi-replica1-1-0:9000: While executing Remote. (MEMORY_LIMIT_EXCEEDED) So to enable Memory Overcommit you need to get rid of the max_memory_usage and max_memory_usage_for_user (set them to 0) and configure overcommit specific settings (usually defaults are ok, so read carefully the documentation)\nmemory_overcommit_ratio_denominator: It represents soft memory limit on the user level. This value is used to compute query overcommit ratio. memory_overcommit_ratio_denominator_for_user: It represents soft memory limit on the global level. This value is used to compute query overcommit ratio. memory_usage_overcommit_max_wait_microseconds: Maximum time thread will wait for memory to be freed in the case of memory overcommit. If timeout is reached and memory is not freed, exception is thrown Please check https://clickhouse.com/docs/en/operations/settings/memory-overcommit\nAlso you will check/need to configure global memory server setting. These are by default:\n\u003cclickhouse\u003e \u003c!-- when max_server_memory_usage is set to non-zero, max_server_memory_usage_to_ram_ratio is ignored--\u003e \u003cmax_server_memory_usage\u003e0\u003c/max_server_memory_usage\u003e \u003cmax_server_memory_usage_to_ram_ratio\u003e0.9\u003c/max_server_memory_usage_to_ram_ratio\u003e \u003c/clickhouse\u003e With these set, now if you execute some queries with bigger memory needs than your max_server_memory_usage you’ll get something like this:\nReceived exception from server (version 22.8.20): Code: 241. DB::Exception: Received from altinity.cloud:9440. DB::Exception: Received from chi-test1-2-0:9000. DB::Exception: Memory limit (total) exceeded: would use 12.60 GiB (attempt to allocate chunk of 4280448 bytes), maximum: 12.60 GiB. OvercommitTracker decision: Query was selected to stop by OvercommitTracker.: while receiving packet from chi-replica1-2-0:9000: While executing Remote. (MEMORY_LIMIT_EXCEEDED) This will allow you to know that the Overcommit memory tracker is set and working.\nAlso to note that maybe you don’t need the Memory Overcommit system because with max_memory_usage per query you’re ok.\nThe good thing about memory overcommit is that you let ClickHouse handle the memory limitations instead of doing it manually, but there may be some scenarios where you don’t want to use it and using max_memory_usage or max_memory_usage_for_user is a better fit. For example, if your workload has a lot of small/medium queries that are not memory intensive and you need to run few memory intensive queries for some users with a fixed memory limit. This is a common scenario for dbt or other ETL tools that usually run big memory intensive queries.\n","categories":"","description":"Enable Memory overcommiter instead of ussing `max_memory_usage` per query\n","excerpt":"Enable Memory overcommiter instead of ussing `max_memory_usage` per …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-memory-overcommit/","tags":"","title":"Memory Overcommiter"},{"body":"Merge Performance Main things affecting the merge speed are:\nSchema (especially compression codecs, some bad types, sorting order…) Horizontal vs Vertical merge Horizontal = reads all columns at once, do merge sort, write new part Vertical = first read columns from order by, do merge sort, write them to disk, remember permutation, then process the rest of columns on by one, applying permutation. compact vs wide parts Other things like server load, concurrent merges… SELECT name, value FROM system.merge_tree_settings WHERE name LIKE '%vert%'; │ enable_vertical_merge_algorithm │ 1 │ vertical_merge_algorithm_min_rows_to_activate │ 131072 │ vertical_merge_algorithm_min_columns_to_activate │ 11 Vertical merge will be used if part has more than 131072 rows and more than 11 columns in the table. -- Disable Vertical Merges ALTER TABLE test MODIFY SETTING enable_vertical_merge_algorithm = 0 Horizontal merge used by default, will use more memory if there are more than 80 columns in the table OPTIMIZE TABLE example FINAL DEDUPLICATE BY expr When using deduplicate feature in OPTIMIZE FINAL, the question is which row will remain and won’t be deduped?\nFor SELECT operations Clickhouse does not guarantee the order of the resultset unless you specify ORDER BY. This random ordering is affected by different parameters, like for example max_threads.\nIn a merge operation ClickHouse reads rows sequentially in storage order, which is determined by ORDER BY specified in CREATE TABLE statement, and only the first unique row in that order survives deduplication. So it is a bit different from how SELECT actually works. As FINAL clause is used then ClickHouse will merge all rows across all partitions (If it is not specified then the merge operation will be done per partition), and so the first unique row of the first partition will survive deduplication. Merges are single-threaded because it is too complicated to apply merge ops in-parallel, and it generally makes no sense.\nhttps://github.com/ClickHouse/ClickHouse/pull/17846 https://clickhouse.com/docs/en/sql-reference/statements/optimize/ ","categories":"","description":"Merge performance and OPTIMIZE FINAL DEDUPLICATE BY expr\n","excerpt":"Merge performance and OPTIMIZE FINAL DEDUPLICATE BY expr\n","ref":"/engines/mergetree-table-engine-family/merge-performance-final-optimize-by/","tags":"","title":"Merge performance and OPTIMIZE FINAL"},{"body":"Internals:\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/merge_tree.pdf\nhttps://youtu.be/1UIl7FpNo2M?t=2467\n","categories":"","description":"MergeTree table engine family\n","excerpt":"MergeTree table engine family\n","ref":"/engines/mergetree-table-engine-family/","tags":"","title":"MergeTree table engine family"},{"body":"Suppose we mount a new device at path /mnt/disk_1 and want to move table_4 to it.\nCreate directory on new device for ClickHouse data. /in shell mkdir /mnt/disk_1/clickhouse Change ownership of created directory to ClickHouse user. /in shell chown -R clickhouse:clickhouse /mnt/disk_1/clickhouse Create a special storage policy which should include both disks: old and new. /in shell nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdefault\u003e \u003cdisk\u003edefault\u003c/disk\u003e \u003cmax_data_part_size_bytes\u003e10000000\u003c/max_data_part_size_bytes\u003e \u003c/default\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e ######################################################################################### Update storage_policy setting of tables to new policy. ALTER TABLE table_4 MODIFY SETTING storage_policy='move_from_default_to_disk_1'; Wait till all parts of tables change their disk_name to new disk. SELECT name,disk_name, path from system.parts WHERE table='table_4' and active; SELECT disk_name, path, sum(rows), sum(bytes_on_disk), uniq(partition), count() FROM system.parts WHERE table='table_4' and active GROUP BY disk_name, path ORDER BY disk_name, path; Remove ‘default’ disk from new storage policy. In server shell: nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e ######################################################################################### ClickHouse wouldn’t auto reload config, because we removed some disks from storage policy, so we need to restart it by hand.\nRestart ClickHouse server. Make sure that storage policy uses the right disks. SELECT * FROM system.storage_policies WHERE policy_name='move_from_default_to_disk_1'; ","categories":"","description":"Moving a table to another device.\n","excerpt":"Moving a table to another device.\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-moving-table-to-another-device./","tags":"","title":"Moving a table to another device."},{"body":"So the basic pipeline depicted is a Kafka table with 2 MVs attached. The Kafka broker has 2 topics and 4 partitions.\nkafka_thread_per_consumer = 0 Kafka engine table will act as 2 consumers but only 1 thread for both consumers. For this scenario we use these settings:\nkafka_num_consumers = 2 kafka_thread_per_consumer = 0 The same Kafka engine will create 2 streams, 1 for each consumer and will join them in a union stream. And it will use 1 thread [ 2385 ] This is how we can see it in the logs:\n2022.11.09 17:49:34.282077 [ 2385 ] {} \u003cDebug\u003e StorageKafka (kafka_table): Started streaming to 2 attached views How ClickHouse calculates the number of threads depending on the thread_per_consumer setting:\nauto stream_count = thread_per_consumer ? 1 : num_created_consumers; sources.reserve(stream_count); pipes.reserve(stream_count); for (size_t i = 0; i \u003c stream_count; ++i) { ...... } Details:\nhttps://github.com/ClickHouse/ClickHouse/blob/1b49463bd297ade7472abffbc931c4bb9bf213d0/src/Storages/Kafka/StorageKafka.cpp#L834\nAlso a detailed graph of the pipeline:\nWith this approach if the number of consumers are increased, still Kafka engine will use only 1 thread to flush. The consuming/processing rate will probably be increased but not linearly, for example 5 consumers will not consume 5 times faster. Also a good property of this approach is the linearization of INSERTS, which means that the order of the inserts is preserved and it is sequential. This option is good for small/medium kafka topics.\nkafka_thread_per_consumer = 1 Kafka engine table will act as 2 consumers and 1 thread per consumers For this scenario we use these settings:\nkafka_num_consumers = 2 kafka_thread_per_consumer = 1 Here the pipeline works like this:\nWith this approach the number of consumers are increased and each consumer will use a thread and so the consuming/processing rate. In this scenario it is important to remark that topic needs to have as many partitions as consumers (threads) to achieve the maximum performance. Also if the number of consumers(threads) needs to be raised to more than 16 you need to change the background pool of threads setting background_message_broker_schedule_pool_size to a higher value than 16 (which is the default). This option is good for large kafka topics with millions of messages per second.\n","categories":"","description":"How Multiple MVs attached to Kafka table consume and how they are affected by kafka_num_consumers/kafka_thread_per_consumer\n","excerpt":"How Multiple MVs attached to Kafka table consume and how they are …","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-mv-consuming/","tags":"","title":"Multiple MVs attached to Kafka table"},{"body":"How to know if ALTER TABLE … DELETE/UPDATE mutation ON CLUSTER was finished successfully on all the nodes? A. mutation status in system.mutations is local to each replica, so use\nSELECT hostname(), * FROM clusterAllReplicas('your_cluster_name', system.mutations); -- you can also add WHERE conditions to that query if needed. Look on is_done and latest_fail_reason columns\nAre mutations being run in parallel or they are sequential in ClickHouse (in scope of one table) ClickHouse runs mutations sequentially, but it can combine several mutations in a single and apply all of them in one merge. Sometimes, it can lead to problems, when a combined expression which ClickHouse needs to execute becomes really big. (If ClickHouse combined thousands of mutations in one)\nBecause ClickHouse stores data in independent parts, ClickHouse is able to run mutation(s) merges for each part independently and in parallel. It also can lead to high resource utilization, especially memory usage if you use x IN (SELECT ... FROM big_table) statements in mutation, because each merge will run and keep in memory its own HashSet. You can avoid this problem, if you will use Dictionary approach for such mutations.\nParallelism of mutations controlled by settings:\nSELECT * FROM system.merge_tree_settings WHERE name LIKE '%mutation%' ┌─name───────────────────────────────────────────────┬─value─┬─changed─┬─description──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─type───┐ │ max_replicated_mutations_in_queue │ 8 │ 0 │ How many tasks of mutating parts are allowed simultaneously in ReplicatedMergeTree queue. │ UInt64 │ │ number_of_free_entries_in_pool_to_execute_mutation │ 20 │ 0 │ When there is less than specified number of free entries in pool, do not execute part mutations. This is to leave free threads for regular merges and avoid \"Too many parts\" │ UInt64 │ └────────────────────────────────────────────────────┴───────┴─────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────┘ ","categories":"","description":"ALTER UPDATE / DELETE\n","excerpt":"ALTER UPDATE / DELETE\n","ref":"/altinity-kb-queries-and-syntax/mutations/","tags":"","title":"Mutations"},{"body":"Authorization MySQL8 used default authorization plugin caching_sha2_password. Unfortunately, libmysql which currently used (21.4-) in clickhouse is not.\nYou can fix it during create custom user with mysql_native_password authentication plugin.\nCREATE USER IF NOT EXISTS 'clickhouse'@'%' IDENTIFIED WITH mysql_native_password BY 'clickhouse_user_password'; CREATE DATABASE IF NOT EXISTS test; GRANT ALL PRIVILEGES ON test.* TO 'clickhouse'@'%'; Table schema changes As an example, in ClickHouse, run SHOW TABLE STATUS LIKE 'table_name' and try to figure out was table schema changed or not from MySQL response field Update_time.\nBy default, to properly data loading from MySQL8 source to dictionaries, please turn off the information_schema cache.\nYou can change default behavior with create /etc/mysql/conf.d/information_schema_cache.cnfwith following content:\n[mysqld] information_schema_stats_expiry=0 Or setup it via SQL query:\nSET GLOBAL information_schema_stats_expiry=0; ","categories":"","description":"MySQL8 source for dictionaries\n","excerpt":"MySQL8 source for dictionaries\n","ref":"/altinity-kb-dictionaries/mysql8-source-for-dictionaries/","tags":"","title":"MySQL8 source for dictionaries"},{"body":"Networking And Server Room Planning The network used for your ClickHouse cluster should be a fast network, ideally 10 Gbit or more. ClickHouse nodes generate a lot of traffic to exchange the data between nodes (port 9009 for replication, and 9000 for distributed queries). Zookeeper traffic in normal circumstanses is moderate, but in some special cases can also be very significant.\nFor the zookeeper low latency is more important than bandwidth.\nKeep the replicas isolated on the hardware level. This allows for cluster failover from possible outages.\nFor Physical Environments: Avoid placing 2 ClickHouse replicas on the same server rack. Ideally, they should be on isolated network switches and an isolated power supply. For Clouds Environments: Use different availability zones between the ClickHouse replicas when possible (but be aware of the interzone traffic costs) These considerations are the same as the Zookeeper nodes.\nFor example:\nRack Server Server Server Server Rack 1 CH_SHARD1_R1 CH_SHARD2_R1 CH_SHARD3_R1 ZOO_1 Rack 2 CH_SHARD1_R2 CH_SHARD2_R2 CH_SHARD3_R2 ZOO_2 Rack 3 ZOO3 Network Ports And Firewall ClickHouse listens the following ports:\n9000: clickhouse-client, native clients, other clickhouse-servers connect to here. 8123: HTTP clients 9009: Other replicas will connect here to download data. For more information, see CLICKHOUSE NETWORKING, PART 1.\nZookeeper listens the following ports:\n2181: Client connections. 2888: Inter-ensemble connections. 3888: Leader election. Outbound traffic from ClickHouse connects to the following ports:\nZooKeeper: On port 2181. Other CH nodes in the cluster: On port 9000 and 9009. Dictionary sources: Depending on what was configured such as HTTP, MySQL, Mongo, etc. Kafka or Hadoop: If those integrations were enabled. SSL For non-trusted networks enable SSL/HTTPS. If acceptable, it is better to keep interserver communications unencrypted for performance reasons.\nNaming Schema The best time to start creating a naming schema for the servers is before they’re created and configured.\nThere are a few features based on good server naming in ClickHouse:\nclickhouse-client prompts: Allows a different prompt for clickhouse-client per server hostname. Nearest hostname load balancing: For more information, see Nearest Hostname. A good option is to use the following:\n{datacenter}-{serverroom}-{rack identifier}-{clickhouse cluster identifier}-{shard number or server number}.\nOther examples:\nrxv-olap-ch-master-sh01-r01: rxv - location (rack#15) olap - product name ch = clickhouse master = stage sh01 = shard 1 r01 = replica 1 hetnzerde1-ch-prod-01.local: hetnzerde1 - location (also replica id) ch = clickhouse prod = stage 01 - server number / shard number in that DC sh01.ch-front.dev.aws-east1a.example.com: sh01 - shard 01 ch-front - cluster name dev = stage aws = cloud provider east1a = region and availability zone Host Name References What are the best practices for domain names (dev, staging, production)? 9 Best Practices and Examples for Working with Kubernetes Labels Thoughts On Hostname Nomenclature Additional Hostname Tips Hostnames configured on the server should not change. If you do need to change the host name, one reference to use is How to Change Hostname on Ubuntu 18.04. The server should be accessible to other servers in the cluster via it’s hostname. Otherwise you will need to configure interserver_hostname in your config. Ensure that hostname --fqdn and getent hosts $(hostname --fqdn) return the correct name and ip. ","categories":"","description":"Network Configuration\n","excerpt":"Network Configuration\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/","tags":"","title":"Network Configuration"},{"body":" It is NOT RECOMMENDED for a general use Use on your own risk Use latest ClickHouse version if you need that. CREATE TABLE x ( `a` Nullable(UInt32), `b` Nullable(UInt32), `cnt` UInt32 ) ENGINE = SummingMergeTree ORDER BY (a, b) SETTINGS allow_nullable_key = 1; INSERT INTO x VALUES (Null,2,1), (Null,Null,1), (3, Null, 1), (4,4,1); INSERT INTO x VALUES (Null,2,1), (Null,Null,1), (3, Null, 1), (4,4,1); SELECT * FROM x; ┌────a─┬────b─┬─cnt─┐ │ 3 │ null │ 2 │ │ 4 │ 4 │ 2 │ │ null │ 2 │ 2 │ │ null │ null │ 2 │ └──────┴──────┴─────┘ ","categories":"","description":"Nulls in order by\n","excerpt":"Nulls in order by\n","ref":"/engines/mergetree-table-engine-family/altinity-kb-nulls-in-order-by/","tags":"","title":"Nulls in order by"},{"body":"Q: Why do I have several active parts in a partition? Why Clickhouse does not merge them immediately? A: CH does not merge parts by time Merge scheduler selects parts by own algorithm based on the current node workload / number of parts / size of parts.\nCH merge scheduler balances between a big number of parts and a wasting resources on merges.\nMerges are CPU/DISK IO expensive. If CH will merge every new part then all resources will be spend on merges and will no resources remain on queries (selects ).\nCH will not merge parts with a combined size greater than 100 GB.\nSELECT database, table, partition, sum(rows) AS rows, count() AS part_count FROM system.parts WHERE (active = 1) AND (table LIKE '%') AND (database LIKE '%') GROUP BY database, table, partition ORDER BY part_count DESC limit 20 ","categories":"","description":"Number of active parts in a partition\n","excerpt":"Number of active parts in a partition\n","ref":"/altinity-kb-useful-queries/altinity-kb-number-of-active-parts-in-a-partition/","tags":"","title":"Number of active parts in a partition"},{"body":"List of missing tables\nWITH ( SELECT groupArray(FQDN()) FROM clusterAllReplicas('{cluster}',system,one) ) AS hosts SELECT database, table, arrayFilter( i-\u003e NOT has(groupArray(host),i), hosts) miss_table FROM ( SELECT FQDN() host, database, name table FROM clusterAllReplicas('{cluster}',system,tables) WHERE engine NOT IN ('Log','Memory','TinyLog') ) GROUP BY database, table HAVING miss_table \u003c\u003e [] SETTINGS skip_unavailable_shards=1; ┌─database─┬─table─┬─miss_table────────────────┐ │ default │ test │ ['host366.mynetwork.net'] │ └──────────┴───────┴───────────────────────────┘ List of inconsistent tables\nSELECT database, name, engine, uniqExact(create_table_query) AS ddl FROM clusterAllReplicas('{cluster}',system.tables) GROUP BY database, name, engine HAVING ddl \u003e 1 List of inconsistent columns\nWITH ( SELECT groupArray(FQDN()) FROM clusterAllReplicas('{cluster}',system,one) ) AS hosts SELECT database, table, column, arrayStringConcat(arrayMap( i -\u003e i.2 ||': '|| i.1, (groupArray( (type,host) ) AS g)),', ') diff FROM ( SELECT FQDN() host, database, table, name column, type FROM clusterAllReplicas('{cluster}',system,columns) ) GROUP BY database, table, column HAVING length(arrayDistinct(g.1)) \u003e 1 OR length(g.1) \u003c\u003e length(hosts) SETTINGS skip_unavailable_shards=1; ┌─database─┬─table───┬─column────┬─diff────────────────────────────────┐ │ default │ z │ A │ ch-host22: Int64, ch-host21: String │ └──────────┴─────────┴───────────┴─────────────────────────────────────┘ List of inconsistent dictionaries\nWITH ( SELECT groupArray(FQDN()) FROM clusterAllReplicas('{cluster}',system,one) ) AS hosts SELECT database, dictionary, arrayFilter( i-\u003e NOT has(groupArray(host),i), hosts) miss_dict, arrayReduce('min', (groupArray((element_count, host)) AS ec).1) min, arrayReduce('max', (groupArray((element_count, host)) AS ec).1) max FROM ( SELECT FQDN() host, database, name dictionary, element_count FROM clusterAllReplicas('{cluster}',system,dictionaries) ) GROUP BY database, dictionary HAVING miss_dict \u003c\u003e [] or min \u003c\u003e max SETTINGS skip_unavailable_shards=1; ; ","categories":"","description":"Object consistency in a cluster\n","excerpt":"Object consistency in a cluster\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-object-consistency-in-a-cluster/","tags":"","title":"Object consistency in a cluster"},{"body":"OPTIMIZE TABLE xyz – this initiates an unscheduled merge.\nExample You have 40 parts in 3 partitions. This unscheduled merge selects some partition (i.e. February) and selects 3 small parts to merge, then merge them into a single part. You get 38 parts in the result.\nOPTIMIZE TABLE xyz FINAL – initiates a cycle of unscheduled merges.\nClickHouse merges parts in this table until will remains 1 part in each partition (if a system has enough free disk space). As a result, you get 3 parts, 1 part per partition. In this case, CH rewrites parts even if they are already merged into a single part. It creates a huge CPU / Disk load if the table ( XYZ) is huge. ClickHouse reads / uncompress / merge / compress / writes all data in the table.\nIf this table has size 1TB it could take around 3 hours to complete.\nSo we don’t recommend running OPTIMIZE TABLE xyz FINAL against tables with more than 10million rows.\n","categories":"","description":"OPTIMIZE vs OPTIMIZE FINAL\n","excerpt":"OPTIMIZE vs OPTIMIZE FINAL\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-optimize-vs-optimize-final/","tags":"","title":"OPTIMIZE vs OPTIMIZE FINAL"},{"body":"ClickHouse version 23.1+ (23.1.6.42, 23.2.5.46, 23.3.1.2823) Have inbuild support for parametrized views:\nCREATE VIEW my_new_view AS SELECT * FROM deals WHERE category_id IN ( SELECT category_id FROM deal_categories WHERE category = {category:String} ) SELECT * FROM my_new_view(category = 'hot deals'); One more example CREATE OR REPLACE VIEW v AS SELECT 1::UInt32 x WHERE x IN ({xx:Array(UInt32)}); select * from v(xx=[1,2,3]); ┌─x─┐ │ 1 │ └───┘ ClickHouse versions per 23.1 Custom settings allows to emulate parameterized views.\nYou need to enable custom settings and define any prefixes for settings.\n$ cat /etc/clickhouse-server/config.d/custom_settings_prefixes.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003ccustom_settings_prefixes\u003emy,my2\u003c/custom_settings_prefixes\u003e \u003c/yandex\u003e You can also set the default value for user settings in the default section of the user configuration.\ncat /etc/clickhouse-server/users.d/custom_settings_default.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmy2_category\u003e'hot deals'\u003c/my2_category\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e See also: https://kb.altinity.com/altinity-kb-setup-and-maintenance/custom_settings/\nA server restart is required for the default value to be applied\n$ systemctl restart clickhouse-server Now you can set settings as any other settings, and query them using getSetting() function.\nSET my2_category='hot deals'; SELECT getSetting('my2_category'); ┌─getSetting('my2_category')─┐ │ hot deals │ └────────────────────────────┘ -- you can query ClickHouse settings as well SELECT getSetting('max_threads') ┌─getSetting('max_threads')─┐ │ 8 │ └───────────────────────────┘ Now we can create a view\nCREATE VIEW my_new_view AS SELECT * FROM deals WHERE category_id IN ( SELECT category_id FROM deal_categories WHERE category = getSetting('my2_category') ); And query it\nSELECT * FROM my_new_view SETTINGS my2_category = 'hot deals'; If the custom setting is not set when the view is being created, you need to explicitly define the list of columns for the view:\nCREATE VIEW my_new_view (c1 Int, c2 String, ...) AS SELECT * FROM deals WHERE category_id IN ( SELECT category_id FROM deal_categories WHERE category = getSetting('my2_category') ); ","categories":"","description":"Parameterized views\n","excerpt":"Parameterized views\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-parameterized-views/","tags":"","title":"Parameterized views"},{"body":"Clickhouse is able to fetch from a source only updated rows. You need to define update_field section.\nAs an example, We have a table in an external source MySQL, PG, HTTP, … defined with the following code sample:\nCREATE TABLE cities ( `polygon` Array(Tuple(Float64, Float64)), `city` String, `updated_at` DateTime DEFAULT now() ) ENGINE = MergeTree ORDER BY city When you add new row and update some rows in this table you should update updated_at with the new timestamp.\n-- fetch updated rows every 30 seconds CREATE DICTIONARY cities_dict ( polygon Array(Tuple(Float64, Float64)), city String ) PRIMARY KEY polygon SOURCE(CLICKHOUSE( TABLE cities DB 'default' update_field 'updated_at')) LAYOUT(POLYGON()) LIFETIME(MIN 30 MAX 30) A dictionary with update_field updated_at will fetch only updated rows. A dictionary saves the current time (now) time of the last successful update and queries the source where updated_at \u003e= previous_update - 1 (shift = 1 sec.).\nIn case of HTTP source Clickhouse will send get requests with update_field as an URL parameter \u0026updated_at=2020-01-01%2000:01:01\n","categories":"","description":"Partial updates\n","excerpt":"Partial updates\n","ref":"/altinity-kb-dictionaries/partial-updates/","tags":"","title":"Partial updates"},{"body":"CREATE TABLE default.metric ( `key_a` UInt8, `key_b` UInt32, `date` Date, `value` UInt32, PROJECTION monthly ( SELECT key_a, key_b, min(date), sum(value) GROUP BY key_a, key_b ) ) ENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY (key_a, key_b, date) SETTINGS index_granularity = 8192; INSERT INTO metric SELECT key_a, key_b, date, rand() % 100000 AS value FROM ( SELECT arrayJoin(range(8)) AS key_a, number % 500000 AS key_b, today() - intDiv(number, 500000) AS date FROM numbers_mt(1080000000) ); OPTIMIZE TABLE metric FINAL; SET max_threads = 8; WITH toDate('2015-02-27') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM metric WHERE (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 6.561 sec. Processed 4.32 billion rows, 47.54 GB (658.70 million rows/s., 7.25 GB/s.) WITH toDate('2015-02-27') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM ( SELECT key_b, value FROM metric WHERE indexHint(_partition_id IN CAST([toYYYYMM(start_date), toYYYYMM(end_date)], 'Array(String)')) AND (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond UNION ALL SELECT key_b, sum(value) AS value FROM metric WHERE indexHint(_partition_id IN CAST(range(toYYYYMM(start_date) + 1, toYYYYMM(end_date)), 'Array(String)')) AND key_a_cond GROUP BY key_b ) GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 1.038 sec. Processed 181.86 million rows, 4.56 GB (175.18 million rows/s., 4.40 GB/s.) WITH (toDate('2016-02-27'), toDate('2017-02-15')) AS period_1, (toDate('2018-05-27'), toDate('2022-08-15')) AS period_2, (date \u003e (period_1.1)) AND (date \u003c (period_1.2)) AS period_1_cond, (date \u003e (period_2.1)) AND (date \u003c (period_2.2)) AS period_2_cond, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sumIf(value, period_1_cond) AS sum_per_1, sumIf(value, period_2_cond) AS sum_per_2 FROM metric WHERE (period_1_cond OR period_2_cond) AND key_a_cond GROUP BY key_b ORDER BY sum_per_2 / sum_per_1 DESC LIMIT 25 25 rows in set. Elapsed: 5.717 sec. Processed 3.47 billion rows, 38.17 GB (606.93 million rows/s., 6.68 GB/s.) WITH (toDate('2016-02-27'), toDate('2017-02-15')) AS period_1, (toDate('2018-05-27'), toDate('2022-08-15')) AS period_2, (date \u003e (period_1.1)) AND (date \u003c (period_1.2)) AS period_1_cond, (date \u003e (period_2.1)) AND (date \u003c (period_2.2)) AS period_2_cond, CAST([toYYYYMM(period_1.1), toYYYYMM(period_1.2), toYYYYMM(period_2.1), toYYYYMM(period_2.2)], 'Array(String)') AS daily_parts, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sumIf(value, period_1_cond) AS sum_per_1, sumIf(value, period_2_cond) AS sum_per_2 FROM ( SELECT key_b, date, value FROM metric WHERE indexHint(_partition_id IN (daily_parts)) AND (period_1_cond OR period_2_cond) AND key_a_cond UNION ALL SELECT key_b, min(date) AS date, sum(value) AS value FROM metric WHERE indexHint(_partition_id IN CAST(arrayConcat(range(toYYYYMM(period_1.1) + 1, toYYYYMM(period_1.2)), range(toYYYYMM(period_2.1) + 1, toYYYYMM(period_2.1))), 'Array(String)')) AND indexHint(_partition_id NOT IN (daily_parts)) AND key_a_cond GROUP BY key_b ) GROUP BY key_b ORDER BY sum_per_2 / sum_per_1 DESC LIMIT 25 25 rows in set. Elapsed: 0.444 sec. Processed 140.34 million rows, 2.11 GB (316.23 million rows/s., 4.77 GB/s.) WITH toDate('2022-01-03') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM metric WHERE (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 0.208 sec. Processed 100.06 million rows, 1.10 GB (481.06 million rows/s., 5.29 GB/s.) WITH toDate('2022-01-03') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM ( SELECT key_b, value FROM metric WHERE indexHint(_partition_id IN CAST([toYYYYMM(start_date), toYYYYMM(end_date)], 'Array(String)')) AND (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond UNION ALL SELECT key_b, sum(value) AS value FROM metric WHERE indexHint(_partition_id IN CAST(range(toYYYYMM(start_date) + 1, toYYYYMM(end_date)), 'Array(String)')) AND key_a_cond GROUP BY key_b ) GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 0.216 sec. Processed 100.06 million rows, 1.10 GB (462.68 million rows/s., 5.09 GB/s.) WITH toDate('2021-12-03') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM metric WHERE (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 0.308 sec. Processed 162.09 million rows, 1.78 GB (526.89 million rows/s., 5.80 GB/s.) WITH toDate('2021-12-03') AS start_date, toDate('2022-02-15') AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM ( SELECT key_b, value FROM metric WHERE indexHint(_partition_id IN CAST([toYYYYMM(start_date), toYYYYMM(end_date)], 'Array(String)')) AND (date \u003e start_date) AND (date \u003c end_date) AND key_a_cond UNION ALL SELECT key_b, sum(value) AS value FROM metric WHERE indexHint(_partition_id IN CAST(range(toYYYYMM(start_date) + 1, toYYYYMM(end_date)), 'Array(String)')) AND key_a_cond GROUP BY key_b ) GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set. Elapsed: 0.268 sec. Processed 102.08 million rows, 1.16 GB (381.46 million rows/s., 4.33 GB/s.) ","categories":"","description":"How to write queries, which will use both data from projection and raw table.\n","excerpt":"How to write queries, which will use both data from projection and raw …","ref":"/altinity-kb-queries-and-syntax/partial-projection-optimization/","tags":"","title":"Use both projection and raw data in single query"},{"body":"Check if there are blocks missing SELECT database, table, partition_id, ranges.1 AS previous_part, ranges.2 AS next_part, ranges.3 AS previous_block_number, ranges.4 AS next_block_number, range(toUInt64(previous_block_number + 1), toUInt64(next_block_number)) AS missing_block_numbers FROM ( WITH arrayPopFront(groupArray(min_block_number) AS min) AS min_adj, arrayPopBack(groupArray(max_block_number) AS max) AS max_adj, arrayFilter((x, y, z) -\u003e (y != (z + 1)), arrayZip(arrayPopBack(groupArray(name) AS name_arr), arrayPopFront(name_arr), max_adj, min_adj), min_adj, max_adj) AS missing_ranges SELECT database, table, partition_id, missing_ranges FROM ( SELECT * FROM system.parts WHERE active AND (table = 'query_thread_log') AND (partition_id = '202108') AND active ORDER BY min_block_number ASC ) GROUP BY database, table, partition_id ) ARRAY JOIN missing_ranges AS ranges ┌─database─┬─table────────────┬─partition_id─┬─previous_part───────┬─next_part──────────┬─previous_block_number─┬─next_block_number─┬─missing_block_numbers─┐ │ system │ query_thread_log │ 202108 │ 202108_864_1637_556 │ 202108_1639_1639_0 │ 1637 │ 1639 │ [1638] │ └──────────┴──────────────────┴──────────────┴─────────────────────┴────────────────────┴───────────────────────┴───────────────────┴───────────────────────┘ Find the number of blocks in a table SELECT database, table, partition_id, sum(max_block_number - min_block_number) AS blocks_count FROM system.parts WHERE active AND (table = 'query_thread_log') AND (partition_id = '202108') AND active GROUP BY database, table, partition_id ┌─database─┬─table────────────┬─partition_id─┬─blocks_count─┐ │ system │ query_thread_log │ 202108 │ 1635 │ └──────────┴──────────────────┴──────────────┴──────────────┘ Compare the list of parts in ZooKeeper with the list of parts on disk select zoo.p_path as part_zoo, zoo.ctime, zoo.mtime, disk.p_path as part_disk from ( select concat(path,'/',name) as p_path, ctime, mtime from system.zookeeper where path in (select concat(replica_path,'/parts') from system.replicas) ) zoo left join ( select concat(replica_path,'/parts/',name) as p_path from system.parts inner join system.replicas using (database, table) ) disk on zoo.p_path = disk.p_path where part_disk='' and zoo.mtime \u003c= now() - interval 1 hour order by part_zoo; You can clean that orphan zk records (need to execute using delete in zkCli, rm in zk-shell):\nselect 'delete '||part_zoo from ( select zoo.p_path as part_zoo, zoo.ctime, zoo.mtime, disk.p_path as part_disk from ( select concat(path,'/',name) as p_path, ctime, mtime from system.zookeeper where path in (select concat(replica_path,'/parts') from system.replicas) ) zoo left join ( select concat(replica_path,'/parts/',name) as p_path from system.parts inner join system.replicas using (database, table) ) disk on zoo.p_path = disk.p_path where part_disk='' and zoo.mtime \u003c= now() - interval 1 day order by part_zoo) format TSVRaw; ","categories":"","description":"","excerpt":"Check if there are blocks missing SELECT database, table, …","ref":"/altinity-kb-useful-queries/parts-consistency/","tags":"","title":"Parts consistency"},{"body":"PIVOT CREATE TABLE sales(suppkey UInt8, category String, quantity UInt32) ENGINE=Memory(); INSERT INTO sales VALUES (2, 'AA' ,7500),(1, 'AB' , 4000),(1, 'AA' , 6900),(1, 'AB', 8900), (1, 'AC', 8300), (1, 'AA', 7000), (1, 'AC', 9000), (2,'AA', 9800), (2,'AB', 9600), (1,'AC', 8900),(1, 'AD', 400), (2,'AD', 900), (2,'AD', 1200), (1,'AD', 2600), (2, 'AC', 9600),(1, 'AC', 6200); Using Map data type (starting from Clickhouse 21.1) WITH CAST(sumMap([category], [quantity]), 'Map(String, UInt32)') AS map SELECT suppkey, map['AA'] AS AA, map['AB'] AS AB, map['AC'] AS AC, map['AD'] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ WITH CAST(sumMap(map(category, quantity)), 'Map(LowCardinality(String), UInt32)') AS map SELECT suppkey, map['AA'] AS AA, map['AB'] AS AB, map['AC'] AS AC, map['AD'] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ Using -If combinator SELECT suppkey, sumIf(quantity, category = 'AA') AS AA, sumIf(quantity, category = 'AB') AS AB, sumIf(quantity, category = 'AC') AS AC, sumIf(quantity, category = 'AD') AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ Using -Resample combinator WITH sumResample(0, 4, 1)(quantity, transform(category, ['AA', 'AB', 'AC', 'AD'], [0, 1, 2, 3], 4)) AS sum SELECT suppkey, sum[1] AS AA, sum[2] AS AB, sum[3] AS AC, sum[4] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ UNPIVOT CREATE TABLE sales_w(suppkey UInt8, brand String, AA UInt32, AB UInt32, AC UInt32, AD UInt32) ENGINE=Memory(); INSERT INTO sales_w VALUES (1, 'BRAND_A', 1500, 4200, 1600, 9800), (2, 'BRAND_B', 6200, 1300, 5800, 3100), (3, 'BRAND_C', 5000, 8900, 6900, 3400); SELECT suppkey, brand, category, quantity FROM sales_w ARRAY JOIN [AA, AB, AC, AD] AS quantity, splitByString(', ', 'AA, AB, AC, AD') AS category ORDER BY suppkey ASC ┌─suppkey─┬─brand───┬─category─┬─quantity─┐ │ 1 │ BRAND_A │ AA │ 1500 │ │ 1 │ BRAND_A │ AB │ 4200 │ │ 1 │ BRAND_A │ AC │ 1600 │ │ 1 │ BRAND_A │ AD │ 9800 │ │ 2 │ BRAND_B │ AA │ 6200 │ │ 2 │ BRAND_B │ AB │ 1300 │ │ 2 │ BRAND_B │ AC │ 5800 │ │ 2 │ BRAND_B │ AD │ 3100 │ │ 3 │ BRAND_C │ AA │ 5000 │ │ 3 │ BRAND_C │ AB │ 8900 │ │ 3 │ BRAND_C │ AC │ 6900 │ │ 3 │ BRAND_C │ AD │ 3400 │ └─────────┴─────────┴──────────┴──────────┘ SELECT suppkey, brand, tpl.1 AS category, tpl.2 AS quantity FROM sales_w ARRAY JOIN tupleToNameValuePairs(CAST((AA, AB, AC, AD), 'Tuple(AA UInt32, AB UInt32, AC UInt32, AD UInt32)')) AS tpl ORDER BY suppkey ASC ┌─suppkey─┬─brand───┬─category─┬─quantity─┐ │ 1 │ BRAND_A │ AA │ 1500 │ │ 1 │ BRAND_A │ AB │ 4200 │ │ 1 │ BRAND_A │ AC │ 1600 │ │ 1 │ BRAND_A │ AD │ 9800 │ │ 2 │ BRAND_B │ AA │ 6200 │ │ 2 │ BRAND_B │ AB │ 1300 │ │ 2 │ BRAND_B │ AC │ 5800 │ │ 2 │ BRAND_B │ AD │ 3100 │ │ 3 │ BRAND_C │ AA │ 5000 │ │ 3 │ BRAND_C │ AB │ 8900 │ │ 3 │ BRAND_C │ AC │ 6900 │ │ 3 │ BRAND_C │ AD │ 3400 │ └─────────┴─────────┴──────────┴──────────┘ ","categories":"","description":"PIVOT / UNPIVOT\n","excerpt":"PIVOT / UNPIVOT\n","ref":"/altinity-kb-queries-and-syntax/pivot-unpivot/","tags":"","title":"PIVOT / UNPIVOT"},{"body":"In version 19.14 a serious issue was found: a race condition that can lead to server deadlock. The reason for that was quite fundamental, and a temporary workaround for that was added (“possible deadlock avoided”).\nThose locks are one of the fundamental things that the core team was actively working on in 2020.\nIn 20.3 some of the locks leading to that situation were removed as a part of huge refactoring.\nIn 20.4 more locks were removed, the check was made configurable (see lock_acquire_timeout ) so you can say how long to wait before returning that exception\nIn 20.5 heuristics of that check (“possible deadlock avoided”) was improved.\nIn 20.6 all table-level locks which were possible to remove were removed, so alters are totally lock-free.\n20.10 enables database=Atomic by default which allows running even DROP commands without locks.\nTypically issue was happening when doing some concurrent select on system.parts / system.columns / system.table with simultaneous table manipulations (doing some kind of ALTERS / TRUNCATES / DROP)I\nIf that exception happens often in your use-case:\nuse recent clickhouse versions ensure you use Atomic engine for the database (not Ordinary) (can be checked in system.databases) Sometime you can try to workaround issue by finding the queries which uses that table concurenly (especially to system.tables / system.parts and other system tables) and try killing them (or avoiding them).\n","categories":"","description":"Possible deadlock avoided. Client should retry\n","excerpt":"Possible deadlock avoided. Client should retry\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-possible-deadlock-avoided.-client-should-retry/","tags":"","title":"Possible deadlock avoided. Client should retry"},{"body":"The biggest problem with running ClickHouse in k8s, happens when clickhouse-server can’t start for some reason and pod is falling in CrashloopBackOff, so you can’t easily get in the pod and check/fix/restart ClickHouse.\nThere is multiple possible reasons for this, some of them can be fixed without manual intervention in pod:\nWrong configuration files Fix: Check templates which are being used for config file generation and fix them. While upgrade some backward incompatible changes prevents ClickHouse from start. Fix: Downgrade and check backward incompatible changes for all versions in between. Next reasons would require to have manual intervention in pod/volume. There is two ways, how you can get access to data:\nChange entry point of ClickHouse pod to something else, so pod wouldn’t be terminated due ClickHouse error. Attach ClickHouse data volume to some generic pod (like Ubuntu). Unclear restart which produced broken files and/or state on disk is differs too much from state in zookeeper for replicated tables. Fix: Create force_restore_data flag. Wrong file permission for ClickHouse files in pod. Fix: Use chown to set right ownership for files and directories. Errors in ClickHouse table schema prevents ClickHouse from start. Fix: Rename problematic table.sql scripts to table.sql.bak Occasional failure of distributed queries because of wrong user/password. Due nature of k8s with dynamic ip allocations, it’s possible that ClickHouse would cache wrong ip-\u003e hostname combination and disallow connections because of mismatched hostname. Fix: run SYSTEM DROP DNS CACHE; \u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml. Caveats:\nNot all configuration/state folders are being covered by persistent volumes. (geobases) Page cache belongs to k8s node and pv are being mounted to pod, in case of fast shutdown there is possibility to loss some data(needs to be clarified) Some cloud providers (GKE) can have slow unlink command, which is important for clickhouse because it’s needed for parts management. (max_part_removal_threads setting) Useful commands:\nkubectl logs chi-chcluster-2-1-0 -c clickhouse-pod -n chcluster --previous kubectl describe pod chi-chcluster-2-1-0 -n chcluster Q. Clickhouse is caching the Kafka pod’s IP and trying to connect to the same ip even when there is a new Kafka pod running and the old one is deprecated. Is there some setting where we could refresh the connection\n\u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml\nClickHouse init process failed It’s due to low value for env CLICKHOUSE_INIT_TIMEOUT value. Consider increasing it up to 1 min. https://github.com/ClickHouse/ClickHouse/blob/9f5cd35a6963cc556a51218b46b0754dcac7306a/docker/server/entrypoint.sh#L120\n","categories":"","description":"Possible issues with running ClickHouse in k8s\n","excerpt":"Possible issues with running ClickHouse in k8s\n","ref":"/altinity-kb-kubernetes/altinity-kb-possible-issues-with-running-clickhouse-in-k8s/","tags":"","title":"Possible issues with running ClickHouse in k8s"},{"body":"Moving from a single ClickHouse server to a clustered format provides several benefits:\nReplication guarantees data integrity. Provides redundancy. Failover by being able to restart half of the nodes without encountering downtime. Moving from an unsharded ClickHouse environment to a sharded cluster requires redesign of schema and queries. Starting with a sharded cluster from the beginning makes it easier in the future to scale the cluster up.\nSetting up a ClickHouse cluster for a production environment requires the following stages:\nHardware Requirements Network Configuration Create Host Names Monitoring Considerations Configuration Steps Setting Up Backups Staging Plans Upgrading The Cluster ","categories":"","description":"Production Cluster Configuration Guide\n","excerpt":"Production Cluster Configuration Guide\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/","tags":"","title":"Production Cluster Configuration Guide"},{"body":"Aggregating projections create table z(Browser String, Country UInt8, F Float64) Engine=MergeTree order by Browser; insert into z select toString(number%9999), number%33, 1 from numbers(100000000); --Q1) select sum(F), Browser from z group by Browser format Null; Elapsed: 0.205 sec. Processed 100.00 million rows --Q2) select sum(F), Browser, Country from z group by Browser,Country format Null; Elapsed: 0.381 sec. Processed 100.00 million rows --Q3) select sum(F),count(), Browser, Country from z group by Browser,Country format Null; Elapsed: 0.398 sec. Processed 100.00 million rows alter table z add projection pp (select Browser,Country, count(), sum(F) group by Browser,Country); alter table z materialize projection pp; ---- 0 = don't use proj, 1 = use projection set allow_experimental_projection_optimization=1; --Q1) select sum(F), Browser from z group by Browser format Null; Elapsed: 0.003 sec. Processed 22.43 thousand rows --Q2) select sum(F), Browser, Country from z group by Browser,Country format Null; Elapsed: 0.004 sec. Processed 22.43 thousand rows --Q3) select sum(F),count(), Browser, Country from z group by Browser,Country format Null; Elapsed: 0.005 sec. Processed 22.43 thousand rows Emulation of an inverted index using orderby projection You can create an orderby projection and include all columns of a table, but if a table is very wide it will double of stored data. This example demonstrate a trick, we create an orderby projection and include primary key columns and the target column and sort by the target column. This allows using subquery to find primary key values and after that to query the table using the primary key.\nCREATE TABLE test_a ( `src` String, `dst` String, `other_cols` String, PROJECTION p1 ( SELECT src, dst ORDER BY dst ) ) ENGINE = MergeTree ORDER BY src; insert into test_a select number, -number, 'other_col '||toString(number) from numbers(1e8); select * from test_a where src='42'; ┌─src─┬─dst─┬─other_cols───┐ │ 42 │ -42 │ other_col 42 │ └─────┴─────┴──────────────┘ 1 row in set. Elapsed: 0.005 sec. Processed 16.38 thousand rows, 988.49 KB (3.14 million rows/s., 189.43 MB/s.) select * from test_a where dst='-42'; ┌─src─┬─dst─┬─other_cols───┐ │ 42 │ -42 │ other_col 42 │ └─────┴─────┴──────────────┘ 1 row in set. Elapsed: 0.625 sec. Processed 100.00 million rows, 1.79 GB (160.05 million rows/s., 2.86 GB/s.) -- optimization using projection select * from test_a where src in (select src from test_a where dst='-42') and dst='-42'; ┌─src─┬─dst─┬─other_cols───┐ │ 42 │ -42 │ other_col 42 │ └─────┴─────┴──────────────┘ 1 row in set. Elapsed: 0.013 sec. Processed 32.77 thousand rows, 660.75 KB (2.54 million rows/s., 51.26 MB/s.) Elapsed: 0.625 sec. Processed 100.00 million rows – not optimized\nVS\nElapsed: 0.013 sec. Processed 32.77 thousand rows – optimized\nSee also Amos Bird - kuaishou.com - Projections in ClickHouse. slides. video Documentation tinybird blog article ClickHouse presentation on Projections https://www.youtube.com/watch?v=QDAJTKZT8y4 ","categories":"","description":"Projections examples\n","excerpt":"Projections examples\n","ref":"/altinity-kb-queries-and-syntax/projections-examples/","tags":"","title":"Projections examples"},{"body":"Main docs article https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/\nHardware requirements TLDR version:\nUSE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup). use 3 nodes (more nodes = slower quorum, less = no HA). low network latency between zookeeper nodes is very important (latency, not bandwidth). have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings. ensure that zookeeper will not be CPU-starved by some other processes monitor zookeeper. Side note: in many cases, the slowness of the zookeeper is actually a symptom of some issue with clickhouse schema/usage pattern (the most typical issues: an enormous number of partitions/tables/databases with real-time inserts, tiny \u0026 frequent inserts).\nSome doc about that subject:\nhttps://docs.confluent.io/platform/current/zookeeper/deployment.html https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.html#sc_commonProblems https://clickhouse.tech/docs/en/operations/tips/#zookeeper https://lucene.apache.org/solr/guide/7_4/setting-up-an-external-zookeeper-ensemble.html https://cwiki.apache.org/confluence/display/ZOOKEEPER/Troubleshooting Cite from https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_commonProblems :\nThings to Avoid Here are some common problems you can avoid by configuring ZooKeeper correctly:\ninconsistent lists of servers : The list of ZooKeeper servers used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has. Things work okay if the client list is a subset of the real list, but things will really act strange if clients have a list of ZooKeeper servers that are in different ZooKeeper clusters. Also, the server lists in each Zookeeper server configuration file should be consistent with one another. incorrect placement of transaction log : The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log. incorrect Java heap size : You should take special care to set your Java max heap size correctly. In particular, you should not create a situation in which ZooKeeper swaps to disk. The disk is death to ZooKeeper. Everything is ordered, so if processing one request swaps the disk, all other queued requests will probably do the same. the disk. DON’T SWAP. Be conservative in your estimates: if you have 4G of RAM, do not set the Java max heap size to 6G or even 4G. For example, it is more likely you would use a 3G heap for a 4G machine, as the operating system and the cache also need memory. The best and only recommend practice for estimating the heap size your system needs is to run load tests, and then make sure you are well below the usage limit that would cause the system to swap. Publicly accessible deployment : A ZooKeeper ensemble is expected to operate in a trusted computing environment. It is thus recommended to deploy ZooKeeper behind a firewall. ","categories":"","description":"Proper setup\n","excerpt":"Proper setup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/","tags":"","title":"Proper setup"},{"body":"The following example shows a range_hashed example at open intervals.\nDROP TABLE IF EXISTS rates; DROP DICTIONARY IF EXISTS rates_dict; CREATE TABLE rates ( id UInt64, date_start Nullable(Date), date_end Nullable(Date), rate Decimal64(4) ) engine=Log; INSERT INTO rates VALUES (1, Null, '2021-03-13',99), (1, '2021-03-14','2021-03-16',100), (1, '2021-03-17', Null, 101), (2, '2021-03-14', Null, 200), (3, Null, '2021-03-14', 300), (4, '2021-03-14', '2021-03-14', 400); CREATE DICTIONARY rates_dict ( id UInt64, date_start Date, date_end Date, rate Decimal64(4) ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 USER 'default' TABLE 'rates')) LIFETIME(MIN 1 MAX 1000) LAYOUT(RANGE_HASHED()) RANGE(MIN date_start MAX date_end); SELECT * FROM rates_dict order by id, date_start; ┌─id─┬─date_start─┬───date_end─┬─────rate─┐ │ 1 │ 1970-01-01 │ 2021-03-13 │ 99.0000 │ │ 1 │ 2021-03-14 │ 2021-03-16 │ 100.0000 │ │ 1 │ 2021-03-17 │ 1970-01-01 │ 101.0000 │ │ 2 │ 2021-03-14 │ 1970-01-01 │ 200.0000 │ │ 3 │ 1970-01-01 │ 2021-03-14 │ 300.0000 │ │ 4 │ 2021-03-14 │ 2021-03-14 │ 400.0000 │ └────┴────────────┴────────────┴──────────┘ WITH toDate('2021-03-10') + INTERVAL number DAY as date select date, dictGet(currentDatabase() || '.rates_dict', 'rate', toUInt64(1), date) as rate1, dictGet(currentDatabase() || '.rates_dict', 'rate', toUInt64(2), date) as rate2, dictGet(currentDatabase() || '.rates_dict', 'rate', toUInt64(3), date) as rate3, dictGet(currentDatabase() || '.rates_dict', 'rate', toUInt64(4), date) as rate4 FROM numbers(10); ┌───────date─┬────rate1─┬────rate2─┬────rate3─┬────rate4─┐ │ 2021-03-10 │ 99.0000 │ 0.0000 │ 300.0000 │ 0.0000 │ │ 2021-03-11 │ 99.0000 │ 0.0000 │ 300.0000 │ 0.0000 │ │ 2021-03-12 │ 99.0000 │ 0.0000 │ 300.0000 │ 0.0000 │ │ 2021-03-13 │ 99.0000 │ 0.0000 │ 300.0000 │ 0.0000 │ │ 2021-03-14 │ 100.0000 │ 200.0000 │ 300.0000 │ 400.0000 │ │ 2021-03-15 │ 100.0000 │ 200.0000 │ 0.0000 │ 0.0000 │ │ 2021-03-16 │ 100.0000 │ 200.0000 │ 0.0000 │ 0.0000 │ │ 2021-03-17 │ 101.0000 │ 200.0000 │ 0.0000 │ 0.0000 │ │ 2021-03-18 │ 101.0000 │ 200.0000 │ 0.0000 │ 0.0000 │ │ 2021-03-19 │ 101.0000 │ 200.0000 │ 0.0000 │ 0.0000 │ └────────────┴──────────┴──────────┴──────────┴──────────┘ ","categories":"","description":"range_hashed example - open intervals\n","excerpt":"range_hashed example - open intervals\n","ref":"/altinity-kb-dictionaries/altinity-kb-range_hashed-example-open-intervals/","tags":"","title":"range_hashed example - open intervals"},{"body":"Problem Every ClickHouse user experienced a loss of ZooKeeper one day. While the data is available and replicas respond to queries, inserts are no longer possible. ClickHouse uses ZooKeeper in order to store the reference version of the table structure and part of data, and when it is not available can not guarantee data consistency anymore. Replicated tables turn to the read-only mode. In this article we describe step-by-step instructions of how to restore ZooKeeper metadata and bring ClickHouse cluster back to normal operation.\nIn order to restore ZooKeeper we have to solve two tasks. First, we need to restore table metadata in ZooKeeper. Currently, the only way to do it is to recreate the table with the CREATE TABLE DDL statement.\nCREATE TABLE table_name ... ENGINE=ReplicatedMergeTree('zookeeper_path','replica_name'); The second and more difficult task is to populate zookeeper with information of clickhouse data parts. As mentioned above, ClickHouse stores the reference data about all parts of replicated tables in ZooKeeper, so we have to traverse all partitions and re-attach them to the recovered replicated table in order to fix that.\nInfo Starting from ClickHouse version 21.7 there is SYSTEM RESTORE REPLICA command https://altinity.com/blog/a-new-way-to-restore-clickhouse-after-zookeeper-metadata-is-lost\nTest case Let’s say we have replicated table table_repl.\nCREATE TABLE table_repl ( `number` UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl','{replica}') PARTITION BY intDiv(number, 1000) ORDER BY number; And populate it with some data\nSELECT * FROM system.zookeeper WHERE path='/clickhouse/cluster_1/tables/01/'; INSERT INTO table_repl SELECT * FROM numbers(1000,2000); SELECT partition, sum(rows) AS rows, count() FROM system.parts WHERE table='table_repl' AND active GROUP BY partition; Now let’s remove metadata in zookeeper using ZkCli.sh at ZooKeeper host:\ndeleteall /clickhouse/cluster_1/tables/01/table_repl And try to resync clickhouse replica state with zookeeper:\nSYSTEM RESTART REPLICA table_repl; If we try to insert some data in the table, error happens:\nINSERT INTO table_repl SELECT number AS number FROM numbers(1000,2000) WHERE number % 2 = 0; And now we have an exception that we lost all metadata in zookeeper. It is time to recover!\nCurrent Solution Detach replicated table.\nDETACH TABLE table_repl; Save the table’s attach script and change engine of replicated table to non-replicated *mergetree analogue. Table definition is located in the ‘metadata’ folder, ‘/var/lib/clickhouse/metadata/default/table_repl.sql’ in our example. Please make a backup copy and modify the file as follows:\nATTACH TABLE table_repl ( `number` UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl', '{replica}') PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192 Needs to be replaced with this:\nATTACH TABLE table_repl ( `number` UInt32 ) ENGINE = MergeTree() PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192 Attach non-replicated table.\nATTACH TABLE table_repl; Rename non-replicated table.\nRENAME TABLE table_repl TO table_repl_old; Create a new replicated table. Take the saved attach script and replace ATTACH with CREATE, and run it.\nCREATE TABLE table_repl ( `number` UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl', '{replica}') PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192 Attach parts from old table to new.\nALTER TABLE table_repl ATTACH PARTITION 1 FROM table_repl_old; ALTER TABLE table_repl ATTACH PARTITION 2 FROM table_repl_old; If the table has many partitions, it may require some shell script to make it easier.\nAutomated approach For a large number of tables, you can use script https://github.com/Altinity/clickhouse-zookeeper-recovery which partially automates the above approach.\n","categories":"","description":"Recovering from complete metadata loss in ZooKeeper\n","excerpt":"Recovering from complete metadata loss in ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-recovering-from-complete-metadata-loss-in-zookeeper/","tags":"","title":"Recovering from complete metadata loss in ZooKeeper"},{"body":"remote(…) table function Suitable for moving up to hundreds of gigabytes of data.\nWith bigger tables recommended approach is to slice the original data by some WHERE condition, ideally - apply the condition on partitioning key, to avoid writing data to many partitions at once.\nINSERT INTO staging_table SELECT * FROM remote(...) WHERE date='2021-04-13'; INSERT INTO staging_table SELECT * FROM remote(...) WHERE date='2021-04-12'; INSERT INTO staging_table SELECT * FROM remote(...) WHERE date='2021-04-11'; .... OR INSERT INTO FUNCTION remote(...) SELECT * FROM staging_table WHERE date='2021-04-11'; .... Q. Can it create a bigger load on the source system? Yes, it may use disk read \u0026 network write bandwidth. But typically write speed is worse than the read speed, so most probably the receiver side will be a bottleneck, and the sender side will not be overloaded.\nWhile of course it should be checked, every case is different.\nQ. Can I tune INSERT speed to make it faster? Yes, by the cost of extra memory usage (on the receiver side).\nClickhouse tries to form blocks of data in memory and while one of limit: min_insert_block_size_rows or min_insert_block_size_bytes being hit, clickhouse dump this block on disk. If clickhouse tries to execute insert in parallel (max_insert_threads \u003e 1), it would form multiple blocks at one time.\nSo maximum memory usage can be calculated like this: max_insert_threads * first(min_insert_block_size_rows OR min_insert_block_size_bytes)\nDefault values:\n┌─name────────────────────────┬─value─────┐ │ min_insert_block_size_rows │ 1048545 │ │ min_insert_block_size_bytes │ 268427520 │ │ max_insert_threads │ 0 │ \u003c- Values 0 or 1 means that INSERT SELECT is not run in parallel. └─────────────────────────────┴───────────┘ Tune those settings depending on your table average row size and amount of memory which are safe to occupy by INSERT SELECT query.\nQ. I’ve got the error “All connection tries failed” SELECT count() FROM remote('server.from.remote.dc:9440', 'default.table', 'admin', 'password') Received exception from server (version 20.8.11): Code: 519. DB::Exception: Received from localhost:9000. DB::Exception: All attempts to get table structure failed. Log: Code: 279, e.displayText() = DB::NetException: All connection tries failed. Log: Code: 209, e.displayText() = DB::NetException: Timeout: connect timed out: 192.0.2.1:9440 (server.from.remote.dc:9440) (version 20.8.11.17 (official build)) Code: 209, e.displayText() = DB::NetException: Timeout: connect timed out: 192.0.2.1:9440 (server.from.remote.dc:9440) (version 20.8.11.17 (official build)) Code: 209, e.displayText() = DB::NetException: Timeout: connect timed out: 192.0.2.1:9440 (server.from.remote.dc:9440) (version 20.8.11.17 (official build)) Using remote(…) table function with secure TCP port (default values is 9440). There is remoteSecure() function for that. High (\u003e50ms) ping between servers, values for connect_timeout_with_failover_ms, connect_timeout_with_failover_secure_ms need’s to be adjusted accordingly. Default values:\n┌─name────────────────────────────────────┬─value─┐ │ connect_timeout_with_failover_ms │ 50 │ │ connect_timeout_with_failover_secure_ms │ 100 │ └─────────────────────────────────────────┴───────┘ Example #!/bin/bash table='...' database='bvt' local='...' remote='...' CH=\"clickhouse-client\" # you may add auth here settings=\" max_insert_threads=20, max_threads=20, min_insert_block_size_bytes = 536870912, min_insert_block_size_rows = 16777216, max_insert_block_size = 16777216, optimize_on_insert=0\"; # need it to create temp table with same structure (suitable for attach) params=$($CH -h $remote -q \"select partition_key,sorting_key,primary_key from system.tables where table='$table' and database = '$database' \" -f TSV) IFS=$'\\t' read -r partition_key sorting_key primary_key \u003c\u003c\u003c $params $CH -h $local \\ # get list of source partitions -q \"select distinct partition from system.parts where table='$table' and database = '$database' \" while read -r partition; do # check that the partition is already copied if [ `$CH -h $remote -q \" select count() from system.parts table='$table' and database = '$database' and partition='$partition'\"` -eq 0 ] ; then $CH -n -h $remote -q \" create temporary table temp as $database.$table engine=MergeTree -- 23.3 required for temporary table partition by ($partition_key) primary key ($primary_key) order by ($sorting_key); -- SYSTEM STOP MERGES temp; -- maybe.... set $settings; insert into temp select * from remote($local,$database.$table) where _partition='$partition' -- order by ($sorting_key) -- maybe.... ; alter table $database.$table attach partition $partition from temp \" fi done ","categories":"","description":"Remote table function\n","excerpt":"Remote table function\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-table-function/","tags":"","title":"Remote table function"},{"body":"Removing of empty parts is a new feature introduced in 20.12. Earlier versions leave empty parts (with 0 rows) if TTL removes all rows from a part (https://github.com/ClickHouse/ClickHouse/issues/5491). If you set up TTL for your data it is likely that there are quite many empty parts in your system.\nThe new version notices empty parts and tries to remove all of them immediately. This is a one-time operation which runs right after an upgrade. After that TTL will remove empty parts on its own.\nThere is a problem when different replicas of the same table start to remove empty parts at the same time. Because of the bug they can block each other (https://github.com/ClickHouse/ClickHouse/issues/23292).\nWhat we can do to avoid this problem during an upgrade:\nDrop empty partitions before upgrading to decrease the number of empty parts in the system.\nSELECT concat('alter table ',database, '.', table, ' drop partition id ''', partition_id, ''';') FROM system.parts WHERE active GROUP BY database, table, partition_id HAVING count() = countIf(rows=0) Upgrade/restart one replica (in a shard) at a time. If only one replica is cleaning empty parts there will be no deadlock because of replicas waiting for one another. Restart one replica, wait for replication queue to process, then restart the next one.\nRemoving of empty parts can be disabled by adding remove_empty_parts=0 to the default profile.\n$ cat /etc/clickhouse-server/users.d/remove_empty_parts.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cremove_empty_parts\u003e0\u003c/remove_empty_parts\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e ","categories":"","description":"Removing empty parts\n","excerpt":"Removing empty parts\n","ref":"/upgrade/removing-empty-parts/","tags":"","title":"Removing empty parts"},{"body":"There might be parts left in ZooKeeper that don’t exist on disk The explanation is here https://github.com/ClickHouse/ClickHouse/pull/26716\nThe problem is introduced in 20.1.\nThe problem is fixed in 21.8 and backported to 21.3.16, 21.6.9, 21.7.6.\nRegarding the procedure to reproduce the issue: The procedure was not confirmed, but I think it should work.\nWait for a merge on a particular partition (or run an OPTIMIZE to trigger one) At this point you can collect the names of parts participating in the merge from the system.merges table, or the system.parts table.\nWhen the merge finishes, stop one of the replicas before the inactive parts are dropped (or detach the table).\nBring the replica back up (or attach the table). Check that there are no inactive parts in system.parts, but they stayed in ZooKeeper. Also check that the inactive parts got removed from ZooKeeper for another replica. Here is the query to check ZooKeeper:\nselect name, ctime from system.zookeeper where path='\u003ctable_zpath\u003e/replicas/\u003creplica_name\u003e/parts/' and name like '\u003cput an expression for the parts that were merged\u003e' Drop the partition on the replica that DOES NOT have those extra parts in ZooKeeper. Check the list of parts in ZooKeeper. We hope that after this the parts on disk will be removed on all replicas, but one of the replicas will still have some parts left in ZooKeeper. If this happens, then we think that after a restart of the replica with extra parts in ZooKeeper it will try to download them from another replica. A query to find ‘forgotten’ parts https://kb.altinity.com/altinity-kb-useful-queries/parts-consistency/#compare-the-list-of-parts-in-zookeeper-with-the-list-of-parts-on-disk\nA query to drop empty partitions with failing replication tasks select 'alter table '||database||'.'||table||' drop partition id '''||partition_id||''';' from ( select database, table, splitByChar('_',new_part_name)[1] partition_id from system.replication_queue where type='GET_PART' and not is_currently_executing and create_time \u003c toStartOfDay(yesterday()) group by database, table, partition_id) q left join (select database, table, partition_id, countIf(active) cnt_active, count() cnt_total from system.parts group by database, table, partition_id ) p using database, table, partition_id where cnt_active=0 ","categories":"","description":"Removing lost parts\n","excerpt":"Removing lost parts\n","ref":"/upgrade/removing-lost-parts/","tags":"","title":"Removing lost parts"},{"body":"ReplacingMergeTree is a powerful ClickHouse MergeTree engine. It is one of the techniques that can be used to guarantee unicity or exactly once delivery in ClickHouse.\nGeneral Operations Engine Parameters Engine = ReplacingMergeTree([version_column],[is_deleted_column]) ORDER BY \u003clist_of_columns\u003e ORDER BY – The ORDER BY defines the columns that need to be unique at merge time. Since merge time can not be decided most of the time, the FINAL keyword is required to remove duplicates. version_column – An monotonically increasing number, which can be based on a timestamp. Used for make sure sure updates are executed in a right order. is_deleted_column (23.2+ see https://github.com/ClickHouse/ClickHouse/pull/41005) – the column used to delete rows. DML operations CREATE – INSERT INTO t values(..) READ – SELECT FROM t final UPDATE – INSERT INTO t(..., _version) values (...), insert with incremented version DELETE – INSERT INTO t(..., _version, is_deleted) values(..., 1) FINAL ClickHouse does not guarantee that merge will fire and replace rows using ReplacingMergeTree logic. FINAL keyword should be used in order to apply merge in a query time. It works reasonably fast when PK filter is used, but maybe slow for SELECT * type of queries:\nSee these links for reference:\nFINAL clause speed Handling Real-Time Updates in ClickHouse Since 23.2, profile level final=1 can force final automatically, see https://github.com/ClickHouse/ClickHouse/pull/40945\nClickhouse merge parts only in scope of single partition, so if two rows with the same replacing key would land in different partitions, they would never be merged in single row. FINAL keyword works in other way, it merge all rows across all partitions. But that behavior can be changed viado_not_merge_across_partitions_select_final setting.\nCREATE TABLE repl_tbl_part ( `key` UInt32, `value` UInt32, `part_key` UInt32 ) ENGINE = ReplacingMergeTree PARTITION BY part_key ORDER BY key; INSERT INTO repl_tbl_part SELECT 1 AS key, number AS value, number % 2 AS part_key FROM numbers(4) SETTINGS optimize_on_insert = 0; SELECT * FROM repl_tbl_part; ┌─key─┬─value─┬─part_key─┐ │ 1 │ 1 │ 1 │ │ 1 │ 3 │ 1 │ └─────┴───────┴──────────┘ ┌─key─┬─value─┬─part_key─┐ │ 1 │ 0 │ 0 │ │ 1 │ 2 │ 0 │ └─────┴───────┴──────────┘ SELECT * FROM repl_tbl_part FINAL; ┌─key─┬─value─┬─part_key─┐ │ 1 │ 3 │ 1 │ └─────┴───────┴──────────┘ SELECT * FROM repl_tbl_part FINAL SETTINGS do_not_merge_across_partitions_select_final=1; ┌─key─┬─value─┬─part_key─┐ │ 1 │ 3 │ 1 │ └─────┴───────┴──────────┘ ┌─key─┬─value─┬─part_key─┐ │ 1 │ 2 │ 0 │ └─────┴───────┴──────────┘ OPTIMIZE TABLE repl_tbl_part FINAL; SELECT * FROM repl_tbl_part; ┌─key─┬─value─┬─part_key─┐ │ 1 │ 3 │ 1 │ └─────┴───────┴──────────┘ ┌─key─┬─value─┬─part_key─┐ │ 1 │ 2 │ 0 │ └─────┴───────┴──────────┘ Deleting the data Delete in partition: ALTER TABLE t DELETE WHERE ... in PARTITION 'partition' – slow and asynchronous, rebuilds the partition Filter is_deleted in queries: SELECT ... WHERE is_deleted = 0 Before 23.2, use ROW POLICY to apply a filter automatically: CREATE ROW POLICY delete_masking on t using is_deleted = 0 for ALL; 23.2+ ReplacingMergeTree(version, is_deleted) ORDER BY .. SETTINGS clean_deleted_rows='Always' (see https://github.com/ClickHouse/ClickHouse/pull/41005) Other options:\nPartition operations: ALTER TABLE t DROP PARTITION 'partition' – locks the table, drops full partition only Lightwieght delete: DELETE FROM t WHERE ... – experimental Use cases Last state Tested on ClickHouse 23.6 version FINAL is good in all cases\nCREATE TABLE repl_tbl ( `key` UInt32, `val_1` UInt32, `val_2` String, `val_3` String, `val_4` String, `val_5` UUID, `ts` DateTime ) ENGINE = ReplacingMergeTree(ts) ORDER BY key SYSTEM STOP MERGES repl_tbl; INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, now() as ts FROM numbers(10000000); INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, now() as ts FROM numbers(10000000); INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, now() as ts FROM numbers(10000000); INSERT INTO repl_tbl SELECT number as key, rand() as val_1, randomStringUTF8(10) as val_2, randomStringUTF8(5) as val_3, randomStringUTF8(4) as val_4, generateUUIDv4() as val_5, now() as ts FROM numbers(10000000); SELECT count() FROM repl_tbl ┌──count()─┐ │ 40000000 │ └──────────┘ Single key -- GROUP BY SELECT key, argMax(val_1, ts) as val_1, argMax(val_2, ts) as val_2, argMax(val_3, ts) as val_3, argMax(val_4, ts) as val_4, argMax(val_5, ts) as val_5, max(ts) FROM repl_tbl WHERE key = 10 GROUP BY key; 1 row in set. Elapsed: 0.008 sec. -- ORDER BY LIMIT BY SELECT * FROM repl_tbl WHERE key = 10 ORDER BY ts DESC LIMIT 1 BY key ; 1 row in set. Elapsed: 0.006 sec. -- Subquery SELECT * FROM repl_tbl WHERE key = 10 AND ts = (SELECT max(ts) FROM repl_tbl WHERE key = 10); 1 row in set. Elapsed: 0.009 sec. -- FINAL SELECT * FROM repl_tbl FINAL WHERE key = 10; 1 row in set. Elapsed: 0.008 sec. Multiple keys -- GROUP BY SELECT key, argMax(val_1, ts) as val_1, argMax(val_2, ts) as val_2, argMax(val_3, ts) as val_3, argMax(val_4, ts) as val_4, argMax(val_5, ts) as val_5, max(ts) FROM repl_tbl WHERE key IN (SELECT toUInt32(number) FROM numbers(1000000) WHERE number % 100) GROUP BY key FORMAT Null; Peak memory usage (for query): 2.19 GiB. 0 rows in set. Elapsed: 1.043 sec. Processed 5.08 million rows, 524.38 MB (4.87 million rows/s., 502.64 MB/s.) -- SET optimize_aggregation_in_order=1; Peak memory usage (for query): 349.94 MiB. 0 rows in set. Elapsed: 0.901 sec. Processed 4.94 million rows, 506.55 MB (5.48 million rows/s., 562.17 MB/s.) -- ORDER BY LIMIT BY SELECT * FROM repl_tbl WHERE key IN (SELECT toUInt32(number)　FROM numbers(1000000) WHERE number % 100) ORDER BY ts DESC LIMIT 1 BY key FORMAT Null; Peak memory usage (for query): 1.12 GiB. 0 rows in set. Elapsed: 1.171 sec. Processed 5.08 million rows, 524.38 MB (4.34 million rows/s., 447.95 MB/s.) -- Subquery SELECT * FROM repl_tbl WHERE (key, ts) IN (SELECT key, max(ts) FROM repl_tbl WHERE key IN (SELECT toUInt32(number) FROM numbers(1000000) WHERE number % 100) GROUP BY key) FORMAT Null; Peak memory usage (for query): 197.30 MiB. 0 rows in set. Elapsed: 0.484 sec. Processed 8.72 million rows, 507.33 MB (18.04 million rows/s., 1.05 GB/s.) -- SET optimize_aggregation_in_order=1; Peak memory usage (for query): 171.93 MiB. 0 rows in set. Elapsed: 0.465 sec. Processed 8.59 million rows, 490.55 MB (18.46 million rows/s., 1.05 GB/s.) -- FINAL SELECT * FROM repl_tbl FINAL WHERE key IN (SELECT toUInt32(number) FROM numbers(1000000) WHERE number % 100) FORMAT Null; Peak memory usage (for query): 537.13 MiB. 0 rows in set. Elapsed: 0.357 sec. Processed 4.39 million rows, 436.28 MB (12.28 million rows/s., 1.22 GB/s.) Full table -- GROUP BY SELECT key, argMax(val_1, ts) as val_1, argMax(val_2, ts) as val_2, argMax(val_3, ts) as val_3, argMax(val_4, ts) as val_4, argMax(val_5, ts) as val_5, max(ts) FROM repl_tbl GROUP BY key FORMAT Null; Peak memory usage (for query): 16.08 GiB. 0 rows in set. Elapsed: 11.600 sec. Processed 40.00 million rows, 5.12 GB (3.45 million rows/s., 441.49 MB/s.) -- SET optimize_aggregation_in_order=1; Peak memory usage (for query): 865.76 MiB. 0 rows in set. Elapsed: 9.677 sec. Processed 39.82 million rows, 5.10 GB (4.12 million rows/s., 526.89 MB/s.) -- ORDER BY LIMIT BY SELECT * FROM repl_tbl ORDER BY ts DESC LIMIT 1 BY key FORMAT Null; Peak memory usage (for query): 8.39 GiB. 0 rows in set. Elapsed: 14.489 sec. Processed 40.00 million rows, 5.12 GB (2.76 million rows/s., 353.45 MB/s.) -- Subquery SELECT * FROM repl_tbl WHERE (key, ts) IN (SELECT key, max(ts) FROM repl_tbl GROUP BY key) FORMAT Null; Peak memory usage (for query): 2.40 GiB. 0 rows in set. Elapsed: 5.225 sec. Processed 79.65 million rows, 5.40 GB (15.24 million rows/s., 1.03 GB/s.) -- SET optimize_aggregation_in_order=1; Peak memory usage (for query): 924.39 MiB. 0 rows in set. Elapsed: 4.126 sec. Processed 79.67 million rows, 5.40 GB (19.31 million rows/s., 1.31 GB/s.) -- FINAL SELECT * FROM repl_tbl FINAL FORMAT Null; Peak memory usage (for query): 834.09 MiB. 0 rows in set. Elapsed: 2.314 sec. Processed 38.80 million rows, 4.97 GB (16.77 million rows/s., 2.15 GB/s.) ","categories":"","description":"ReplacingMergeTree\n","excerpt":"ReplacingMergeTree\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/","tags":"","title":"ReplacingMergeTree"},{"body":"Hi there, I have a question about replacing merge trees. I have set up a Materialized View with ReplacingMergeTree table, but even if I call optimize on it, the parts don’t get merged. I filled that table yesterday, nothing happened since then. What should I do?\nMerges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts. If the total size of input parts are greater than the maximum part size then they will never be merged.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings/#max-bytes-to-merge-at-max-space-in-pool\nhttps://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replacingmergetree/ ReplacingMergeTree is suitable for clearing out duplicate data in the background in order to save space, but it doesn’t guarantee the absence of duplicates.\n","categories":"","description":"ReplacingMergeTree does not collapse duplicates\n","excerpt":"ReplacingMergeTree does not collapse duplicates\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates/","tags":"","title":"ReplacingMergeTree does not collapse duplicates"},{"body":"SELECT database, table, type, max(last_exception), max(postpone_reason), min(create_time), max(last_attempt_time), max(last_postpone_time), max(num_postponed) AS max_postponed, max(num_tries) AS max_tries, min(num_tries) AS min_tries, countIf(last_exception != '') AS count_err, countIf(num_postponed \u003e 0) AS count_postponed, countIf(is_currently_executing) AS count_executing, count() AS count_all FROM system.replication_queue GROUP BY database, table, type ORDER BY count_all DESC ","categories":"","description":"Replication queue\n","excerpt":"Replication queue\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/","tags":"","title":"Replication queue"},{"body":" Step 1: Detach Kafka tables in ClickHouse Step 2: kafka-consumer-groups.sh --bootstrap-server kafka:9092 --topic topic:0,1,2 --group id1 --reset-offsets --to-latest --execute More samples: https://gist.github.com/filimonov/1646259d18b911d7a1e8745d6411c0cc Step: Attach Kafka tables back See also these configuration settings:\n\u003ckafka\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003c/kafka\u003e About Offset Consuming When a consumer joins the consumer group, the broker will check if it has a commited offset. If that is the case, then it will start from the latest offset. Both ClickHouse and librdKafka documentation state that the default value for auto_offset_reset is largest (or latest in new Kafka versions) but it is not, if the consumer is new:\nhttps://github.com/ClickHouse/ClickHouse/blob/f171ad93bcb903e636c9f38812b6aaf0ab045b04/src/Storages/Kafka/StorageKafka.cpp#L506\nconf.set(\"auto.offset.reset\", \"earliest\"); // If no offset stored for this group, read all messages from the start\nIf there is no offset stored or it is out of range, for that particular consumer group, the consumer will start consuming from the beginning (earliest), and if there is some offset stored then it should use the latest. The log retention policy influences which offset values correspond to the earliest and latest configurations. Consider a scenario where a topic has a retention policy set to 1 hour. Initially, you produce 5 messages, and then, after an hour, you publish 5 more messages. In this case, the latest offset will remain unchanged from the previous example. However, due to Kafka removing the earlier messages, the earliest available offset will not be 0; instead, it will be 5.\n","categories":"","description":"Rewind / fast-forward / replay\n","excerpt":"Rewind / fast-forward / replay\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/","tags":"","title":"Rewind / fast-forward / replay"},{"body":"CREATE TABLE test_roaring_bitmap ENGINE = MergeTree ORDER BY h AS SELECT intDiv(number, 5) AS h, groupArray(toUInt16(number - (2 * intDiv(number, 5)))) AS vals, groupBitmapState(toUInt16(number - (2 * intDiv(number, 5)))) AS vals_bitmap FROM numbers(40) GROUP BY h SELECT h, vals, hex(vals_bitmap) FROM test_roaring_bitmap ┌─h─┬─vals─────────────┬─hex(vals_bitmap)─────────┐ │ 0 │ [0,1,2,3,4] │ 000500000100020003000400 │ │ 1 │ [3,4,5,6,7] │ 000503000400050006000700 │ │ 2 │ [6,7,8,9,10] │ 000506000700080009000A00 │ │ 3 │ [9,10,11,12,13] │ 000509000A000B000C000D00 │ │ 4 │ [12,13,14,15,16] │ 00050C000D000E000F001000 │ │ 5 │ [15,16,17,18,19] │ 00050F001000110012001300 │ │ 6 │ [18,19,20,21,22] │ 000512001300140015001600 │ │ 7 │ [21,22,23,24,25] │ 000515001600170018001900 │ └───┴──────────────────┴──────────────────────────┘ SELECT groupBitmapAnd(vals_bitmap) AS uniq, bitmapToArray(groupBitmapAndState(vals_bitmap)) AS vals FROM test_roaring_bitmap WHERE h IN (0, 1) ┌─uniq─┬─vals──┐ │ 2 │ [3,4] │ └──────┴───────┘ See also A primer on roaring bitmaps\n","categories":"","description":"","excerpt":"CREATE TABLE test_roaring_bitmap ENGINE = MergeTree ORDER BY h AS …","ref":"/altinity-kb-queries-and-syntax/roaring-bitmaps-for-calculating-retention/","tags":"","title":"Roaring bitmaps for calculating retention"},{"body":"Short Instructions These instructions apply to ClickHouse using default locations for storage.\nDo FREEZE TABLE on needed table, partition. It produces a consistent snapshot of table data.\nRun rsync command.\nrsync -ravlW --bwlimit=100000 /var/lib/clickhouse/data/shadow/N/database/table root@remote_host:/var/lib/clickhouse/data/database/table/detached --bwlimit is transfer limit in KBytes per second.\nRun ATTACH PARTITION for each partition from ./detached directory.\nIMPORTANT NOTE: If you are using a mount point different from /var/lib/clickhouse/data, adjust the rsync command accordingly to point the correct location. For example, suppose you reconfigure the storage path as follows in /etc/clickhouse-server/config.d/config.xml.\n\u003cclickhouse\u003e \u003c!-- Path to data directory, with trailing slash. --\u003e \u003cpath\u003e/data1/clickhouse/\u003c/path\u003e ... \u003c/clickhouse\u003e You’ll need to use /data1/clickhouse instead of /var/lib/clickhouse in the rsync paths.\n","categories":"","description":"rsync\n","excerpt":"rsync\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/","tags":"","title":"rsync"},{"body":"The execution pipeline is embedded in the partition reading code.\nSo that works this way:\nClickHouse does partition pruning based on WHERE conditions. For every partition, it picks a columns ranges (aka ‘marks’ / ‘granulas’) based on primary key conditions. Here the sampling logic is applied: a) in case of SAMPLE k (k in 0..1 range) it adds conditions WHERE sample_key \u003c k * max_int_of_sample_key_type b) in case of SAMPLE k OFFSET m it adds conditions WHERE sample_key BETWEEN m * max_int_of_sample_key_type AND (m + k) * max_int_of_sample_key_typec) in case of SAMPLE N (N\u003e1) if first estimates how many rows are inside the range we need to read and based on that convert it to 3a case (calculate k based on number of rows in ranges and desired number of rows) on the data returned by those other conditions are applied (so here the number of rows can be decreased here) Source Code SAMPLE by Docs Source Code SAMPLE key Must be:\nIncluded in the primary key. Uniformly distributed in the domain of its data type: Bad: Timestamp; Good: intHash32(UserID); Cheap to calculate: Bad: cityHash64(URL); Good: intHash32(UserID); Not after high granular fields in primary key: Bad: ORDER BY (Timestamp, sample_key); Good: ORDER BY (CounterID, Date, sample_key). Sampling is:\nDeterministic Works in a consistent way for different tables. Allows reading less amount of data from disk. SAMPLE key, bonus SAMPLE 1/10 Select data for 1/10 of all possible sample keys; SAMPLE 1000000 Select from about (not less than) 1 000 000 rows on each shard; You can use _sample_factor virtual column to determine the relative sample factor; SAMPLE 1/10 OFFSET 1/10 Select second 1/10 of all possible sample keys; SET max_parallel_replicas = 3 Select from multiple replicas of each shard in parallel; SAMPLE emulation via WHERE condition Sometimes, it’s easier to emulate sampling via conditions in WHERE clause instead of using SAMPLE key.\nSELECT count() FROM table WHERE ... AND cityHash64(some_high_card_key) % 10 = 0; -- Deterministic SELECT count() FROM table WHERE ... AND rand() % 10 = 0; -- Non-deterministic ClickHouse will read more data from disk compared to an example with a good SAMPLE key, but it’s more universal and can be used if you can’t change table ORDER BY key. (To learn more about ClickHouse internals, ClickHouse Administrator Training is available.)\n","categories":"","description":"SAMPLE by\n","excerpt":"SAMPLE by\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-sample-by/","tags":"","title":"SAMPLE by"},{"body":"The most important idea about sampling that the primary index must have LowCardinality. (For more information, see the Altinity Knowledge Base article on LowCardinality or a ClickHouse user's lessons learned from LowCardinality).\nThe following example demonstrates how sampling can be setup correctly, and an example if it being set up incorrectly as a comparison.\nSampling requires sample by expression . This ensures a range of sampled column types fit within a specified range, which ensures the requirement of low cardinality. In this example, I cannot use transaction_id because I can not ensure that the min value of transaction_id = 0 and max value = MAX_UINT64. Instead, I used cityHash64(transaction_id)to expand the range within the minimum and maximum values.\nFor example if all values of transaction_id are from 0 to 10000 sampling will be inefficient. But cityHash64(transaction_id) expands the range from 0 to 18446744073709551615:\nSELECT cityHash64(10000) ┌────cityHash64(10000)─┐ │ 14845905981091347439 │ └──────────────────────┘ If I used transaction_id without knowing that they matched the allowable ranges, the results of sampled queries would be skewed. For example, when using sample 0.5, ClickHouse requests where sample_col \u003e= 0 and sample_col \u003c= MAX_UINT64/2.\nAlso you can include multiple columns into a hash function of the sampling expression to improve randomness of the distribution cityHash64(transaction_id, banner_id).\nSampling Friendly Table CREATE TABLE table_one ( timestamp UInt64, transaction_id UInt64, banner_id UInt16, value UInt32 ) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(toDateTime(timestamp)) ORDER BY (banner_id, toStartOfHour(toDateTime(timestamp)), cityHash64(transaction_id)) SAMPLE BY cityHash64(transaction_id) SETTINGS index_granularity = 8192 insert into table_one select 1602809234+intDiv(number,100000), number, number%991, toUInt32(rand()) from numbers(10000000000); I reduced the granularity of the timestamp column to one hour with toStartOfHour(toDateTime(timestamp)) , otherwise sampling will not work.\nVerifying Sampling Works The following shows that sampling works with the table and parameters described above. Notice the Elapsed time when invoking sampling:\n-- Q1. No where filters. -- The query is 10 times faster with SAMPLE 0.01 select banner_id, sum(value), count(value), max(value) from table_one group by banner_id format Null; 0 rows in set. Elapsed: 11.490 sec. Processed 10.00 billion rows, 60.00 GB (870.30 million rows/s., 5.22 GB/s.) select banner_id, sum(value), count(value), max(value) from table_one SAMPLE 0.01 group by banner_id format Null; 0 rows in set. Elapsed: 1.316 sec. Processed 452.67 million rows, 6.34 GB (343.85 million rows/s., 4.81 GB/s.) -- Q2. Filter by the first column in index (banner_id = 42) -- The query is 20 times faster with SAMPLE 0.01 -- reads 20 times less rows: 10.30 million rows VS Processed 696.32 thousand rows select banner_id, sum(value), count(value), max(value) from table_one WHERE banner_id = 42 group by banner_id format Null; 0 rows in set. Elapsed: 0.020 sec. Processed 10.30 million rows, 61.78 MB (514.37 million rows/s., 3.09 GB/s.) select banner_id, sum(value), count(value), max(value) from table_one SAMPLE 0.01 WHERE banner_id = 42 group by banner_id format Null; 0 rows in set. Elapsed: 0.008 sec. Processed 696.32 thousand rows, 9.75 MB (92.49 million rows/s., 1.29 GB/s.) -- Q3. No filters -- The query is 10 times faster with SAMPLE 0.01 -- reads 20 times less rows. select banner_id, toStartOfHour(toDateTime(timestamp)) hr, sum(value), count(value), max(value) from table_one group by banner_id, hr format Null; 0 rows in set. Elapsed: 36.660 sec. Processed 10.00 billion rows, 140.00 GB (272.77 million rows/s., 3.82 GB/s.) select banner_id, toStartOfHour(toDateTime(timestamp)) hr, sum(value), count(value), max(value) from table_one SAMPLE 0.01 group by banner_id, hr format Null; 0 rows in set. Elapsed: 3.741 sec. Processed 452.67 million rows, 9.96 GB (121.00 million rows/s., 2.66 GB/s.) -- Q4. Filter by not indexed column -- The query is 6 times faster with SAMPLE 0.01 -- reads 20 times less rows. select count() from table_one where value = 666 format Null; 1 rows in set. Elapsed: 6.056 sec. Processed 10.00 billion rows, 40.00 GB (1.65 billion rows/s., 6.61 GB/s.) select count() from table_one SAMPLE 0.01 where value = 666 format Null; 1 rows in set. Elapsed: 1.214 sec. Processed 452.67 million rows, 5.43 GB (372.88 million rows/s., 4.47 GB/s.) Non-Sampling Friendly Table CREATE TABLE table_one ( timestamp UInt64, transaction_id UInt64, banner_id UInt16, value UInt32 ) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(toDateTime(timestamp)) ORDER BY (banner_id, timestamp, cityHash64(transaction_id)) SAMPLE BY cityHash64(transaction_id) SETTINGS index_granularity = 8192 insert into table_one select 1602809234+intDiv(number,100000), number, number%991, toUInt32(rand()) from numbers(10000000000); This is the same as our other table, BUT granularity of timestamp column is not reduced.\nVerifying Sampling Does Not Work The following tests shows that sampling is not working because of the lack of timestamp granularity. The Elapsed time is longer when sampling is used.\n-- Q1. No where filters. -- The query is 2 times SLOWER!!! with SAMPLE 0.01 -- Because it needs to read excessive column with sampling data! select banner_id, sum(value), count(value), max(value) from table_one group by banner_id format Null; 0 rows in set. Elapsed: 11.196 sec. Processed 10.00 billion rows, 60.00 GB (893.15 million rows/s., 5.36 GB/s.) select banner_id, sum(value), count(value), max(value) from table_one SAMPLE 0.01 group by banner_id format Null; 0 rows in set. Elapsed: 24.378 sec. Processed 10.00 billion rows, 140.00 GB (410.21 million rows/s., 5.74 GB/s.) -- Q2. Filter by the first column in index (banner_id = 42) -- The query is SLOWER with SAMPLE 0.01 select banner_id, sum(value), count(value), max(value) from table_one WHERE banner_id = 42 group by banner_id format Null; 0 rows in set. Elapsed: 0.022 sec. Processed 10.27 million rows, 61.64 MB (459.28 million rows/s., 2.76 GB/s.) select banner_id, sum(value), count(value), max(value) from table_one SAMPLE 0.01 WHERE banner_id = 42 group by banner_id format Null; 0 rows in set. Elapsed: 0.037 sec. Processed 10.27 million rows, 143.82 MB (275.16 million rows/s., 3.85 GB/s.) -- Q3. No filters -- The query is SLOWER with SAMPLE 0.01 select banner_id, toStartOfHour(toDateTime(timestamp)) hr, sum(value), count(value), max(value) from table_one group by banner_id, hr format Null; 0 rows in set. Elapsed: 21.663 sec. Processed 10.00 billion rows, 140.00 GB (461.62 million rows/s., 6.46 GB/s.) select banner_id, toStartOfHour(toDateTime(timestamp)) hr, sum(value), count(value), max(value) from table_one SAMPLE 0.01 group by banner_id, hr format Null; 0 rows in set. Elapsed: 26.697 sec. Processed 10.00 billion rows, 220.00 GB (374.57 million rows/s., 8.24 GB/s.) -- Q4. Filter by not indexed column -- The query is SLOWER with SAMPLE 0.01 select count() from table_one where value = 666 format Null; 0 rows in set. Elapsed: 7.679 sec. Processed 10.00 billion rows, 40.00 GB (1.30 billion rows/s., 5.21 GB/s.) select count() from table_one SAMPLE 0.01 where value = 666 format Null; 0 rows in set. Elapsed: 21.668 sec. Processed 10.00 billion rows, 120.00 GB (461.51 million rows/s., 5.54 GB/s.) ","categories":"","description":"ClickHouse table sampling example\n","excerpt":"ClickHouse table sampling example\n","ref":"/altinity-kb-queries-and-syntax/sampling-example/","tags":"","title":"Sampling Example"},{"body":" atlas https://atlasgo.io/guides/clickhouse golang-migrate tool - see golang-migrate liquibase https://github.com/mediarithmics/liquibase-clickhouse https://johntipper.org/how-to-execute-liquibase-changesets-against-clickhouse/ Flyway Official community supported plugin git Old pull requests (latest at the top): https://github.com/flyway/flyway/pull/3333 Сlickhouse support https://github.com/flyway/flyway/pull/3134 Сlickhouse support https://github.com/flyway/flyway/pull/3133 Add support clickhouse https://github.com/flyway/flyway/pull/2981 Clickhouse replicated https://github.com/flyway/flyway/pull/2640 Yet another ClickHouse support https://github.com/flyway/flyway/pull/2166 Clickhouse support (#1772) https://github.com/flyway/flyway/pull/1773 Fixed #1772: Add support for ClickHouse (https://clickhouse.yandex/) alembic see https://clickhouse-sqlalchemy.readthedocs.io/en/latest/migrations.html bytebase https://bytebase.com custom tool for ClickHouse for python https://github.com/delium/clickhouse-migrator https://github.com/zifter/clickhouse-migrations https://github.com/trushad0w/clickhouse-migrate phpMigrations https://github.com/smi2/phpMigrationsClickhouse https://habrahabr.ru/company/smi2/blog/317682/ dbmate https://github.com/amacneil/dbmate#clickhouse know more?\nhttps://clickhouse.com/docs/knowledgebase/schema_migration_tools\nArticle on migrations in ClickHouse https://posthog.com/blog/async-migrations\n","categories":"","description":"Schema migration tools for ClickHouse\n","excerpt":"Schema migration tools for ClickHouse\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/","tags":"","title":"Schema migration tools for ClickHouse"},{"body":"Dictionary with Clickhouse table as a source with named collections Data for connecting to external sources can be stored in named collections \u003cclickhouse\u003e \u003cnamed_collections\u003e \u003clocal_host\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003cdatabase\u003edefault\u003c/database\u003e \u003cuser\u003ech_dict\u003c/user\u003e \u003cpassword\u003emypass\u003c/password\u003e \u003c/local_host\u003e \u003c/named_collections\u003e \u003c/clickhouse\u003e Dictionary DROP DICTIONARY IF EXISTS named_coll_dict; CREATE DICTIONARY named_coll_dict ( key UInt64, val String ) PRIMARY KEY key SOURCE(CLICKHOUSE(NAME local_host TABLE my_table DB default)) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); INSERT INTO my_table(key, val) VALUES(1, 'first row'); SELECT dictGet('named_coll_dict', 'b', 1); ┌─dictGet('named_coll_dict', 'b', 1)─┐ │ first row │ └────────────────────────────────────┘ ","categories":"","description":"Security named collections\n","excerpt":"Security named collections\n","ref":"/altinity-kb-dictionaries/security-named-collections/","tags":"","title":"Security named collections"},{"body":"Question What will happen, if we would run SELECT query from working Kafka table with MV attached? Would data showed in SELECT query appear later in MV destination table?\nAnswer Most likely SELECT query would show nothing. If you lucky enough and something would show up, those rows wouldn’t appear in MV destination table. So it’s not recommended to run SELECT queries on working Kafka tables.\nIn case of debug it’s possible to use another Kafka table with different consumer_group, so it wouldn’t affect your main pipeline.\n","categories":"","description":"SELECTs from engine=Kafka\n","excerpt":"SELECTs from engine=Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/","tags":"","title":"SELECTs from engine=Kafka"},{"body":"Question I expect the sequence here to only match once as a is only directly after a once - but it matches with gaps. Why is that?\nSELECT sequenceCount('(?1)(?2)')(sequence, page ILIKE '%a%', page ILIKE '%a%') AS sequences FROM values('page String, sequence UInt16', ('a', 1), ('a', 2), ('b', 3), ('b', 4), ('a', 5), ('b', 6), ('a', 7)) 2 # ?? Answer sequenceMatch just ignores the events which don’t match the condition. Check that:\nSELECT sequenceMatch('(?1)(?2)')(sequence,page='a',page='b') AS sequences　FROM values( 'page String, sequence UInt16' , ('a', 1), ('c',2), ('b', 3)); 1 # ?? SELECT sequenceMatch('(?1).(?2)')(sequence,page='a',page='b') AS sequences　FROM values( 'page String, sequence UInt16' , ('a', 1), ('c',2), ('b', 3)); 0 # ??? SELECT sequenceMatch('(?1)(?2)')(sequence,page='a',page='b', page NOT IN ('a','b')) AS sequences　from values( 'page String, sequence UInt16' , ('a', 1), ('c',2), ('b', 3)); 0 # ! SELECT sequenceMatch('(?1).(?2)')(sequence,page='a',page='b', page NOT IN ('a','b')) AS sequences　from values( 'page String, sequence UInt16' , ('a', 1), ('c',2), ('b', 3)); 1 # So for your example - just introduce one more ’nothing matched’ condition:\nSELECT sequenceCount('(?1)(?2)')(sequence, page ILIKE '%a%', page ILIKE '%a%', NOT (page ILIKE '%a%')) AS sequences FROM values('page String, sequence UInt16', ('a', 1), ('a', 2), ('b', 3), ('b', 4), ('a', 5), ('b', 6), ('a', 7)) ","categories":"","description":"sequenceMatch\n","excerpt":"sequenceMatch\n","ref":"/altinity-kb-functions/altinity-kb-sequencematch/","tags":"","title":"sequenceMatch"},{"body":"Сonfig management (recommended structure) Clickhouse server config consists of two parts server settings (config.xml) and users settings (users.xml).\nBy default they are stored in the folder /etc/clickhouse-server/ in two files config.xml \u0026 users.xml.\nWe suggest never change vendor config files and place your changes into separate .xml files in sub-folders. This way is easier to maintain and ease Clickhouse upgrades.\n/etc/clickhouse-server/users.d – sub-folder for user settings.\n/etc/clickhouse-server/config.d – sub-folder for server settings.\n/etc/clickhouse-server/conf.d – sub-folder for any (both) settings.\nFile names of your xml files can be arbitrary but they are applied in alphabetical order.\nExamples:\n$ cat /etc/clickhouse-server/config.d/listen_host.xml \u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003clisten_host\u003e::\u003c/listen_host\u003e \u003c/clickhouse\u003e $ cat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003cmacros\u003e \u003ccluster\u003etest\u003c/cluster\u003e \u003creplica\u003ehost22\u003c/replica\u003e \u003cshard\u003e0\u003c/shard\u003e \u003cserver_id\u003e41295\u003c/server_id\u003e \u003cserver_name\u003ehost22.server.com\u003c/server_name\u003e \u003c/macros\u003e \u003c/clickhouse\u003e cat /etc/clickhouse-server/config.d/zoo.xml \u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/test/task_queue/ddl\u003c/path\u003e \u003c/distributed_ddl\u003e \u003c/clickhouse\u003e cat /etc/clickhouse-server/users.d/enable_access_management_for_user_default.xml \u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003cusers\u003e \u003cdefault\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/default\u003e \u003c/users\u003e \u003c/clickhouse\u003e cat /etc/clickhouse-server/users.d/memory_usage.xml \u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_bytes_before_external_group_by\u003e25290221568\u003c/max_bytes_before_external_group_by\u003e \u003cmax_memory_usage\u003e50580443136\u003c/max_memory_usage\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e BTW, you can define any macro in your configuration and use them in Zookeeper paths\nReplicatedMergeTree('/clickhouse/{cluster}/tables/my_table','{replica}') or in your code using function getMacro:\nCREATE OR REPLACE VIEW srv_server_info SELECT (SELECT getMacro('shard')) AS shard_num, (SELECT getMacro('server_name')) AS server_name, (SELECT getMacro('server_id')) AS server_key Settings can be appended to an XML tree (default behaviour) or replaced or removed.\nExample how to delete tcp_port \u0026 http_port defined on higher level in the main config.xml (it disables open tcp \u0026 http ports if you configured secure ssl):\ncat /etc/clickhouse-server/config.d/disable_open_network.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003chttp_port remove=\"1\"/\u003e \u003ctcp_port remove=\"1\"/\u003e \u003c/clickhouse\u003e Example how to replace remote_servers section defined on higher level in the main config.xml (it allows to remove default test clusters.\n\u003c?xml version=\"1.0\" ?\u003e \u003cclickhouse\u003e \u003cremote_servers replace=\"1\"\u003e \u003cmycluster\u003e .... \u003c/mycluster\u003e \u003c/remote_servers\u003e \u003c/clickhouse\u003e Settings \u0026 restart General ‘rule of thumb’:\nserver settings (config.xml and config.d) changes require restart; user settings (users.xml and users.d) changes don’t require restart. But there are exceptions from those rules (see below).\nServer config (config.xml) sections which don’t require restart \u003cmax_server_memory_usage\u003e \u003cmax_server_memory_usage_to_ram_ratio\u003e \u003cmax_table_size_to_drop\u003e (since 19.12) \u003cmax_partition_size_to_drop\u003e (since 19.12) \u003cmax_concurrent_queries\u003e (since 21.11) \u003cmacros\u003e \u003cremote_servers\u003e \u003cdictionaries_config\u003e \u003cuser_defined_executable_functions_config\u003e \u003cmodels_config\u003e \u003ckeeper_server\u003e \u003czookeeper\u003e (but reconnect don’t happen automatically) \u003cstorage_configuration\u003e \u003cuser_directories\u003e \u003caccess_control_path\u003e \u003cencryption_codecs\u003e \u003clogger\u003e (since 21.11) Those sections (live in separate files):\n\u003cdictionaries\u003e \u003cfunctions\u003e \u003cmodels\u003e See also https://github.com/ClickHouse/ClickHouse/blob/445b0ba7cc6b82e69fef28296981fbddc64cd634/programs/server/Server.cpp#L809-L883\nUser settings which require restart. Most of user setting changes don’t require restart, but they get applied at the connect time, so existing connection may still use old user-level settings. That means that that new setting will be applied to new sessions / after reconnect.\nThe list of user setting which require server restart:\n\u003cbackground_buffer_flush_schedule_pool_size\u003e \u003cbackground_pool_size\u003e \u003cbackground_merges_mutations_concurrency_ratio\u003e \u003cbackground_move_pool_size\u003e \u003cbackground_fetches_pool_size\u003e \u003cbackground_common_pool_size\u003e \u003cbackground_schedule_pool_size\u003e \u003cbackground_message_broker_schedule_pool_size\u003e \u003cbackground_distributed_schedule_pool_size\u003e \u003cmax_replicated_fetches_network_bandwidth_for_server\u003e \u003cmax_replicated_sends_network_bandwidth_for_server\u003e See also select * from system.settings where description ilike '%start%'\nAlso there are several ’long-running’ user sessions which are almost never restarted and can keep the setting from the server start (it’s DDLWorker, Kafka, and some other service things).\nDictionaries We suggest to store each dictionary description in a separate (own) file in a /etc/clickhouse-server/dict sub-folder.\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003chttp\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e and add to the configuration\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cdictionaries_config\u003edict/*.xml\u003c/dictionaries_config\u003e \u003cdictionaries_lazy_load\u003etrue\u003c/dictionaries_lazy_load\u003e \u003c/clickhouse\u003e dict/*.xml – relative path, servers seeks files in the folder /etc/clickhouse-server/dict. More info in Multiple Clickhouse instances.\nincl attribute \u0026 metrica.xml incl attribute allows to include some XML section from a special include file multiple times.\nBy default include file is /etc/metrika.xml. You can use many include files for each XML section.\nFor example to avoid repetition of user/password for each dictionary you can create an XML file:\n$ cat /etc/clickhouse-server/dict_sources.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cmysql_config\u003e \u003cport\u003e3306\u003c/port\u003e \u003cuser\u003euser\u003c/user\u003e \u003cpassword\u003e123\u003c/password\u003e \u003creplica\u003e \u003chost\u003emysql_host\u003c/host\u003e \u003cpriority\u003e1\u003c/priority\u003e \u003c/replica\u003e \u003cdb\u003emy_database\u003c/db\u003e \u003c/mysql_config\u003e \u003c/clickhouse\u003e Include this file:\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e ... \u003cinclude_from\u003e/etc/clickhouse-server/dict_sources.xml\u003c/include_from\u003e \u003c/clickhouse\u003e And use in dictionary descriptions (incl=“mysql_config”):\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003cmysql incl=\"mysql_config\"\u003e \u003ctable\u003emy_table\u003c/table\u003e \u003cinvalidate_query\u003eselect max(id) from my_table\u003c/invalidate_query\u003e \u003c/mysql\u003e \u003c/source\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e Multiple Clickhouse instances at one host By default Clickhouse server configs are in /etc/clickhouse-server/ because clickhouse-server runs with a parameter –config-file /etc/clickhouse-server/config.xml\nconfig-file is defined in startup scripts:\n/etc/init.d/clickhouse-server – init-V /etc/systemd/system/clickhouse-server.service – systemd Clickhouse uses the path from config-file parameter as base folder and seeks for other configs by relative path. All sub-folders users.d / config.d are relative.\nYou can start multiple clickhouse-server each with own –config-file.\nFor example:\n/usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node1/config.xml /etc/clickhouse-server-node1/ config.xml ... users.xml /etc/clickhouse-server-node1/config.d/disable_open_network.xml /etc/clickhouse-server-node1/users.d/.... /usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node2/config.xml /etc/clickhouse-server-node2/ config.xml ... users.xml /etc/clickhouse-server-node2/config.d/disable_open_network.xml /etc/clickhouse-server-node2/users.d/.... If you need to run multiple servers for CI purposes you can combine all settings in a single fat XML file and start ClickHouse without config folders/sub-folders.\n/usr/bin/clickhouse-server --config-file /tmp/ch1.xml /usr/bin/clickhouse-server --config-file /tmp/ch2.xml /usr/bin/clickhouse-server --config-file /tmp/ch3.xml Each ClickHouse instance must work with own data-folder and tmp-folder.\nBy default ClickHouse uses /var/lib/clickhouse/. It can be overridden in path settings\n\u003cpath\u003e/data/clickhouse-ch1/\u003c/path\u003e \u003ctmp_path\u003e/data/clickhouse-ch1/tmp/\u003c/tmp_path\u003e \u003cuser_files_path\u003e/data/clickhouse-ch1/user_files/\u003c/user_files_path\u003e \u003clocal_directory\u003e \u003cpath\u003e/data/clickhouse-ch1/access/\u003c/path\u003e \u003c/local_directory\u003e \u003cformat_schema_path\u003e/data/clickhouse-ch1/format_schemas/\u003c/format_schema_path\u003e preprocessed_configs Clickhouse server watches config files and folders. When you change, add or remove XML files Clickhouse immediately assembles XML files into a combined file. These combined files are stored in /var/lib/clickhouse/preprocessed_configs/ folders.\nYou can verify that your changes are valid by checking /var/lib/clickhouse/preprocessed_configs/config.xml, /var/lib/clickhouse/preprocessed_configs/users.xml.\nIf something wrong with with your settings e.g. unclosed XML element or typo you can see alerts about this mistakes in /var/log/clickhouse-server/clickhouse-server.log\nIf you see your changes in preprocessed_configs it does not mean that changes are applied on running server, check Settings \u0026amp; restart\n","categories":"","description":"How to manage server config files in Clickhouse\n","excerpt":"How to manage server config files in Clickhouse\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-server-config-files/","tags":"","title":"Server config files"},{"body":" query_log and other _log tables - set up TTL, or some other cleanup procedures.\ncat /etc/clickhouse-server/config.d/query_log.xml \u003cclickhouse\u003e \u003cquery_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY event_date ORDER BY (event_time) TTL event_date + interval 90 day SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/query_log\u003e \u003c/clickhouse\u003e query_thread_log - typically is not too useful for end users, you can disable it (or set up TTL). We do not recommend removing this table completely as you might need it for debug one day and the threads’ logging can be easily disabled/enabled without a restart through user profiles:\n$ cat /etc/clickhouse-server/users.d/z_log_queries.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e If you have a good monitoring outside ClickHouse you don’t need to store the history of metrics in ClickHouse\ncat /etc/clickhouse-server/config.d/disable_metric_logs.xml \u003cclickhouse\u003e \u003cmetric_log remove=\"1\" /\u003e \u003casynchronous_metric_log remove=\"1\" /\u003e \u003c/clickhouse\u003e part_log - may be nice, especially at the beginning / during system tuning/analyze.\ncat /etc/clickhouse-server/config.d/part_log.xml \u003cclickhouse\u003e \u003cpart_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003epart_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_time) TTL toStartOfMonth(event_date) + INTERVAL 3 MONTH SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/part_log\u003e \u003c/clickhouse\u003e on older versions log_queries is disabled by default, it’s worth having it enabled always.\n$ cat /etc/clickhouse-server/users.d/log_queries.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e quite often you want to have on-disk group by / order by enabled (both disabled by default).\ncat /etc/clickhouse-server/users.d/enable_on_disk_operations.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_bytes_before_external_group_by\u003e2000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e2000000000\u003c/max_bytes_before_external_sort\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e quite often you want to create more users with different limitations. The most typical is \u003cmax_execution_time\u003e It’s actually also not a way to plan/share existing resources better, but it at least disciplines users.\nAlso introducing some restrictions on query complexity can be a good option to discipline users.\nYou can find the preset example here. Also, force_index_by_date + force_primary_key can be a nice idea to avoid queries that ‘accidentally’ do full scans, max_concurrent_queries_for_user\nmerge_tree settings: max_bytes_to_merge_at_max_space_in_pool (may be reduced in some scenarios), inactive_parts_to_throw_insert - can be enabled, replicated_deduplication_window - can be extended if single insert create lot of parts , merge_with_ttl_timeout - when you use ttl\ninsert_distributed_sync - for small clusters you may sometimes want to enable it\nwhen the durability is the main requirement (or server / storage is not stable) - you may want to enable fsync_* setting (impacts the write performance significantly!!), and insert_quorum\nIf you use FINAL queries - usually you want to enable do_not_merge_across_partitions_select_final\nmemory usage per server / query / user: memory configuration settings\nif you use async_inserts - you often may want to increase max_concurrent_queries\n\u003cclickhouse\u003e \u003cmax_concurrent_queries\u003e500\u003c/max_concurrent_queries\u003e \u003cmax_concurrent_insert_queries\u003e400\u003c/max_concurrent_insert_queries\u003e \u003cmax_concurrent_select_queries\u003e100\u003c/max_concurrent_select_queries\u003e \u003c/clickhouse\u003e materialize_ttl_after_modify=0 access_management=1 secret in \u003cremote_servers\u003e See also:\nhttps://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\n","categories":"","description":"Settings to adjust\n","excerpt":"Settings to adjust\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/","tags":"","title":"Settings to adjust"},{"body":"It’s possible to shutdown server on fly, but that would lead to failure of some queries.\nMore safer way:\nRemove server (which is going to be disabled) from remote_server section of config.xml on all servers.\navoid removing the last replica of the shard (that can lead to incorrect data placement if you use non-random distribution) Remove server from load balancer, so new queries wouldn’t hit it.\nDetach Kafka / Rabbit / Buffer tables (if used), and Materialized* databases.\nWait until all already running queries would finish execution on it. It’s possible to check it via query:\nSHOW PROCESSLIST; Ensure there is no pending data in distributed tables\nSELECT * FROM system.distribution_queue; SYSTEM FLUSH DISTRIBUTED \u003ctable_name\u003e; Run sync replica query in related shard replicas (others than the one you remove) via query:\nSYSTEM SYNC REPLICA db.table; Shutdown server.\nSYSTEM SHUTDOWN query by default doesn’t wait until query completion and tries to kill all queries immediately after receiving signal, if you want to change this behavior, you need to enable setting shutdown_wait_unfinished_queries.\nhttps://github.com/ClickHouse/ClickHouse/blob/d705f8ead4bdc837b8305131844f558ec002becc/programs/server/Server.cpp#L1682\n","categories":"","description":"Shutting down a node\n","excerpt":"Shutting down a node\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-shutting-down-a-node/","tags":"","title":"Shutting down a node"},{"body":"Q. What is SimpleAggregateFunction? Are there advantages to use it instead of AggregateFunction in AggregatingMergeTree? SimpleAggregateFunction can be used for those aggregations when the function state is exactly the same as the resulting function value. Typical example is max function: it only requires storing the single value which is already maximum, and no extra steps needed to get the final value. In contrast avg need to store two numbers - sum \u0026 count, which should be divided to get the final value of aggregation (done by the -Merge step at the very end).\nSimpleAggregateFunction AggregateFunction inserting accepts the value of underlying type OR\na value of corresponding SimpleAggregateFunction type CREATE TABLE saf_test\n( x SimpleAggregateFunction(max, UInt64) )\nENGINE=AggregatingMergeTree\nORDER BY tuple();\nINSERT INTO saf_test VALUES (1);\nINSERT INTO saf_test SELECT max(number) FROM numbers(10);\nINSERT INTO saf_test SELECT maxSimpleState(number) FROM numbers(20); ONLY accepts the state of same aggregate function calculated using -State combinator storing Internally store just a value of underlying type function-specific state storage usage typically is much better due to better compression/codecs in very rare cases it can be more optimal than raw values\nadaptive granularity doesn't work for large states\nreading raw value per row you can access it directly you need to use finalizeAgggregation function using aggregated value just\nselect max(x) from test; you need to use -Merge combinator select maxMerge(x) from test; memory usage typically less memory needed (in some corner cases even 10 times) typically uses more memory, as every state can be quite complex performance typically better, due to lower overhead worse See also:\nAltinity Knowledge Base article on AggregatingMergeTree https://github.com/ClickHouse/ClickHouse/pull/4629 https://github.com/ClickHouse/ClickHouse/issues/3852 Q. How maxSimpleState combinator result differs from plain max? They produce the same result, but types differ (the first have SimpleAggregateFunction datatype). Both can be pushed to SimpleAggregateFunction or to the underlying type. So they are interchangeable.\nInfo -SimpleState is useful for implicit Materialized View creation, like CREATE MATERIALIZED VIEW mv ENGINE = AggregatingMergeTree ORDER BY date AS SELECT date, sumSimpleState(1) AS cnt, sumSimpleState(revenue) AS rev FROM table GROUP BY date Warning -SimpleState supported since 21.1. See https://github.com/ClickHouse/ClickHouse/pull/16853/ Q. Can I use -If combinator with SimpleAggregateFunction? Something like SimpleAggregateFunction(maxIf, UInt64, UInt8) is NOT possible. But is 100% ok to push maxIf (or maxSimpleStateIf) into SimpleAggregateFunction(max, UInt64)\nThere is one problem with that approach: -SimpleStateIf Would produce 0 as result in case of no-match, and it can mess up some aggregate functions state. It wouldn’t affect functions like max/argMax/sum, but could affect functions like min/argMin/any/anyLast\nSELECT minIfMerge(state_1), min(state_2) FROM ( SELECT minIfState(number, number \u003e 5) AS state_1, minSimpleStateIf(number, number \u003e 5) AS state_2 FROM numbers(5) UNION ALL SELECT minIfState(toUInt64(2), 2), minIf(2, 2) ) ┌─minIfMerge(state_1)─┬─min(state_2)─┐ │ 2 │ 0 │ └─────────────────────┴──────────────┘ You can easily workaround that:\nUsing Nullable datatype. Set result to some big number in case of no-match, which would be bigger than any possible value, so it would be safe to use. But it would work only for min/argMin SELECT min(state_1), min(state_2) FROM ( SELECT minSimpleState(if(number \u003e 5, number, 1000)) AS state_1, minSimpleStateIf(toNullable(number), number \u003e 5) AS state_2 FROM numbers(5) UNION ALL SELECT minIf(2, 2), minIf(2, 2) ) ┌─min(state_1)─┬─min(state_2)─┐ │ 2 │ 2 │ └──────────────┴──────────────┘ Extra example WITH minIfState(number, number \u003e 5) AS state_1, minSimpleStateIf(number, number \u003e 5) AS state_2 SELECT byteSize(state_1), toTypeName(state_1), byteSize(state_2), toTypeName(state_2) FROM numbers(10) FORMAT Vertical -- For UInt64 Row 1: ────── byteSize(state_1): 24 toTypeName(state_1): AggregateFunction(minIf, UInt64, UInt8) byteSize(state_2): 8 toTypeName(state_2): SimpleAggregateFunction(min, UInt64) -- For UInt32 ────── byteSize(state_1): 16 byteSize(state_2): 4 -- For UInt16 ────── byteSize(state_1): 12 byteSize(state_2): 2 -- For UInt8 ────── byteSize(state_1): 10 byteSize(state_2): 1 See also https://gist.github.com/filimonov/a4f6754497f02fcef78e9f23a4d170ee\n","categories":"","description":"Simple aggregate functions \u0026 combinators\n","excerpt":"Simple aggregate functions \u0026 combinators\n","ref":"/altinity-kb-queries-and-syntax/simplestateif-or-ifstate-for-simple-aggregate-functions/","tags":"","title":"Simple aggregate functions \u0026 combinators"},{"body":" Warning When you are creating skip indexes in non-regular (Replicated)MergeTree tables over non ORDER BY columns. ClickHouse applies index condition on the first step of query execution, so it’s possible to get outdated rows. --(1) create test table drop table if exists test; create table test ( version UInt32 ,id UInt32 ,state UInt8 ,INDEX state_idx (state) type set(0) GRANULARITY 1 ) ENGINE ReplacingMergeTree(version) ORDER BY (id); --(2) insert sample data INSERT INTO test (version, id, state) VALUES (1,1,1); INSERT INTO test (version, id, state) VALUES (2,1,0); INSERT INTO test (version, id, state) VALUES (3,1,1); --(3) check the result: -- expected 3, 1, 1 select version, id, state from test final; ┌─version─┬─id─┬─state─┐ │ 3 │ 1 │ 1 │ └─────────┴────┴───────┘ -- expected empty result select version, id, state from test final where state=0; ┌─version─┬─id─┬─state─┐ │ 2 │ 1 │ 0 │ └─────────┴────┴───────┘ ","categories":"","description":"Skip index\n","excerpt":"Skip index\n","ref":"/engines/mergetree-table-engine-family/skip-index/","tags":"","title":"Skip index"},{"body":"tested with 20.8.17.25\nhttps://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes\nLet’s create test data create table bftest (k Int64, x Array(Int64)) Engine=MergeTree order by k; insert into bftest select number, arrayMap(i-\u003erand64()%565656, range(10)) from numbers(10000000); insert into bftest select number, arrayMap(i-\u003erand64()%565656, range(10)) from numbers(100000000); Base point (no index) select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.495 sec. Processed 110.00 million rows, 9.68 GB (222.03 million rows/s., 19.54 GB/s.) select count() from bftest where has(x, -42); ┌─count()─┐ │ 0 │ └─────────┘ 1 rows in set. Elapsed: 0.505 sec. Processed 110.00 million rows, 9.68 GB (217.69 million rows/s., 19.16 GB/s.) As you can see Clickhouse read 110.00 million rows and the query elapsed Elapsed: 0.505 sec.\nLet’s add an index alter table bftest add index ix1(x) TYPE bloom_filter GRANULARITY 3; -- GRANULARITY 3 means how many table granules will be in the one index granule -- In our case 1 granule of skip index allows to check and skip 3*8192 rows. -- Every dataset is unique sometimes GRANULARITY 1 is better, sometimes -- GRANULARITY 10. -- Need to test on the real data. optimize table bftest final; -- I need to optimize my table because an index is created for only -- new parts (inserted or merged) -- optimize table final re-writes all parts, but with an index. -- probably in your production you don't need to optimize -- because your data is rotated frequently. -- optimize is a heavy operation, better never run optimize table final in a -- production. test bloom_filter GRANULARITY 3 select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.063 sec. Processed 5.41 million rows, 475.79 MB (86.42 million rows/s., 7.60 GB/s.) select count() from bftest where has(x, -42); ┌─count()─┐ │ 0 │ └─────────┘ 1 rows in set. Elapsed: 0.042 sec. Processed 1.13 million rows, 99.48 MB (26.79 million rows/s., 2.36 GB/s.) As you can see I got 10 times boost.\nLet’s try to reduce GRANULARITY to drop by 1 table granule alter table bftest drop index ix1; alter table bftest add index ix1(x) TYPE bloom_filter GRANULARITY 1; optimize table bftest final; select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.051 sec. Processed 3.64 million rows, 320.08 MB (71.63 million rows/s., 6.30 GB/s.) select count() from bftest where has(x, -42); ┌─count()─┐ │ 0 │ └─────────┘ 1 rows in set. Elapsed: 0.050 sec. Processed 2.06 million rows, 181.67 MB (41.53 million rows/s., 3.65 GB/s.) No improvement :(\nLet’s try to change the false/true probability of the bloom_filter bloom_filter(0.05) alter table bftest drop index ix1; alter table bftest add index ix1(x) TYPE bloom_filter(0.05) GRANULARITY 3; optimize table bftest final; select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.079 sec. Processed 8.95 million rows, 787.22 MB (112.80 million rows/s., 9.93 GB/s.) select count() from bftest where has(x, -42); ┌─count()─┐ │ 0 │ └─────────┘ 1 rows in set. Elapsed: 0.058 sec. Processed 3.86 million rows, 339.54 MB (66.83 million rows/s., 5.88 GB/s.) No improvement.\nbloom_filter(0.01) alter table bftest drop index ix1; alter table bftest add index ix1(x) TYPE bloom_filter(0.01) GRANULARITY 3; optimize table bftest final; select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.069 sec. Processed 5.26 million rows, 462.82 MB (76.32 million rows/s., 6.72 GB/s.) select count() from bftest where has(x, -42); ┌─count()─┐ │ 0 │ └─────────┘ 1 rows in set. Elapsed: 0.047 sec. Processed 737.28 thousand rows, 64.88 MB (15.72 million rows/s., 1.38 GB/s.) Also no improvement :(\nOutcome: I would use TYPE bloom_filter GRANULARITY 3.\n2021 Altinity Inc. All rights reserved.\n","categories":"","description":"Example: skip index bloom_filter \u0026 array column\n","excerpt":"Example: skip index bloom_filter \u0026 array column\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/skip-index-bloom_filter-for-array-column/","tags":"","title":"Skip index bloom_filter Example"},{"body":"ClickHouse provides a type of index that in specific circumstances can significantly improve query speed. These structures are labeled “skip” indexes because they enable ClickHouse to skip reading significant chunks of data that are guaranteed to have no matching values.\n","categories":"","description":"Skip indexes\n","excerpt":"Skip indexes\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/","tags":"","title":"Skip indexes"},{"body":"bloom_filter create table bftest (k Int64, x Int64) Engine=MergeTree order by k; insert into bftest select number, rand64()%565656 from numbers(10000000); insert into bftest select number, rand64()%565656 from numbers(100000000); select count() from bftest where x = 42; ┌─count()─┐ │ 201 │ └─────────┘ 1 rows in set. Elapsed: 0.243 sec. Processed 110.00 million rows alter table bftest add index ix1(x) TYPE bloom_filter GRANULARITY 1; alter table bftest materialize index ix1; select count() from bftest where x = 42; ┌─count()─┐ │ 201 │ └─────────┘ 1 rows in set. Elapsed: 0.056 sec. Processed 3.68 million rows minmax create table bftest (k Int64, x Int64) Engine=MergeTree order by k; -- data is in x column is correlated with the primary key insert into bftest select number, number * 2 from numbers(100000000); alter table bftest add index ix1(x) TYPE minmax GRANULARITY 1; alter table bftest materialize index ix1; select count() from bftest where x = 42; 1 rows in set. Elapsed: 0.004 sec. Processed 8.19 thousand rows projection create table bftest (k Int64, x Int64, S String) Engine=MergeTree order by k; insert into bftest select number, rand64()%565656, '' from numbers(10000000); insert into bftest select number, rand64()%565656, '' from numbers(100000000); alter table bftest add projection p1 (select k,x order by x); alter table bftest materialize projection p1 settings mutations_sync=1; set allow_experimental_projection_optimization=1 ; -- projection select count() from bftest where x = 42; 1 rows in set. Elapsed: 0.002 sec. Processed 24.58 thousand rows -- no projection select * from bftest where x = 42 format Null; 0 rows in set. Elapsed: 0.432 sec. Processed 110.00 million rows -- projection select * from bftest where k in (select k from bftest where x = 42) format Null; 0 rows in set. Elapsed: 0.316 sec. Processed 1.50 million rows ","categories":"","description":"","excerpt":"bloom_filter create table bftest (k Int64, x Int64) Engine=MergeTree …","ref":"/altinity-kb-queries-and-syntax/skip-indexes/skip-indexes-examples/","tags":"","title":"Skip indexes examples"},{"body":"Sparse_hashed and hashed_array layouts are supposed to save memory but has some downsides. We can test it with the following:\ncreate table orders(id UInt64, price Float64) Engine = MergeTree() order by id; insert into orders select number, 0 from numbers(5000000); CREATE DICTIONARY orders_hashed (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE orders DB 'default' USER 'default')) LIFETIME(MIN 0 MAX 0) LAYOUT(HASHED()); CREATE DICTIONARY orders_sparse (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE orders DB 'default' USER 'default')) LIFETIME(MIN 0 MAX 0) LAYOUT(SPARSE_HASHED()); CREATE DICTIONARY orders_hashed_array (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT 9000 TABLE orders DB 'default' USER 'default')) LIFETIME(MIN 0 MAX 0) LAYOUT(HASHED_ARRAY()); SELECT name, type, status, element_count, formatReadableSize(bytes_allocated) AS RAM FROM system.dictionaries WHERE name LIKE 'orders%' ┌─name────────────────┬─type─────────┬─status─┬─element_count─┬─RAM────────┐ │ orders_hashed_array │ HashedArray │ LOADED │ 5000000 │ 68.77 MiB │ │ orders_sparse │ SparseHashed │ LOADED │ 5000000 │ 76.30 MiB │ │ orders_hashed │ Hashed │ LOADED │ 5000000 │ 256.00 MiB │ └─────────────────────┴──────────────┴────────┴───────────────┴────────────┘ SELECT sum(dictGet('default.orders_hashed', 'price', toUInt64(number))) AS res FROM numbers(10000000) ┌─res─┐ │ 0 │ └─────┘ 1 rows in set. Elapsed: 0.546 sec. Processed 10.01 million rows ... SELECT sum(dictGet('default.orders_sparse', 'price', toUInt64(number))) AS res FROM numbers(10000000) ┌─res─┐ │ 0 │ └─────┘ 1 rows in set. Elapsed: 1.422 sec. Processed 10.01 million rows ... SELECT sum(dictGet('default.orders_hashed_array', 'price', toUInt64(number))) AS res FROM numbers(10000000) ┌─res─┐ │ 0 │ └─────┘ 1 rows in set. Elapsed: 0.558 sec. Processed 10.01 million rows ... As you can see SPARSE_HASHED is memory efficient and use about 3 times less memory (!!!) but is almost 3 times slower as well. On the other side HASHED_ARRAY is even more efficient in terms of memory usage and maintains almost the same performance as HASHED layout.\n","categories":"","description":"SPARSE_HASHED VS HASHED VS HASHED_ARRAY\n","excerpt":"SPARSE_HASHED VS HASHED VS HASHED_ARRAY\n","ref":"/altinity-kb-dictionaries/altinity-kb-sparse_hashed-vs-hashed/","tags":"","title":"SPARSE_HASHED VS HASHED vs HASHED_ARRAY"},{"body":"ClickHouse doesn’t probe CA path which is default on CentOS and Amazon Linux.\nClickHouse client cat /etc/clickhouse-client/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cclient\u003e \u003c!-- Used for connection to server's secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e ClickHouse server cat /etc/clickhouse-server/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cserver\u003e \u003c!-- Used for https server AND secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/server\u003e \u003cclient\u003e \u003c!-- Used for connecting to https dictionary source and secured Zookeeper communication --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e https://github.com/ClickHouse/ClickHouse/issues/17803\nhttps://github.com/ClickHouse/ClickHouse/issues/18869\n","categories":"","description":"SSL connection unexpectedly closed\n","excerpt":"SSL connection unexpectedly closed\n","ref":"/altinity-kb-setup-and-maintenance/ssl-connection-unexpectedly-closed/","tags":"","title":"SSL connection unexpectedly closed"},{"body":"Nested structures In certain conditions it could make sense to collapse one of dimensions to set of arrays. It’s usually profitable to do if this dimension is not commonly used in queries. It would reduce amount of rows in aggregated table and speed up queries which doesn’t care about this dimension in exchange of aggregation performance by collapsed dimension.\nCREATE TABLE traffic ( `key1` UInt32, `key2` UInt32, `port` UInt16, `bits_in` UInt32 CODEC (T64,LZ4), `bits_out` UInt32 CODEC (T64,LZ4), `packets_in` UInt32 CODEC (T64,LZ4), `packets_out` UInt32 CODEC (T64,LZ4) ) ENGINE = SummingMergeTree ORDER BY (key1, key2, port); INSERT INTO traffic SELECT number % 1000, intDiv(number, 10000), rand() % 20, rand() % 753, rand64() % 800, rand() % 140, rand64() % 231 FROM numbers(100000000); CREATE TABLE default.traffic_map ( `key1` UInt32, `key2` UInt32, `bits_in` UInt32 CODEC(T64, LZ4), `bits_out` UInt32 CODEC(T64, LZ4), `packets_in` UInt32 CODEC(T64, LZ4), `packets_out` UInt32 CODEC(T64, LZ4), `portMap.port` Array(UInt16), `portMap.bits_in` Array(UInt32) CODEC(T64, LZ4), `portMap.bits_out` Array(UInt32) CODEC(T64, LZ4), `portMap.packets_in` Array(UInt32) CODEC(T64, LZ4), `portMap.packets_out` Array(UInt32) CODEC(T64, LZ4) ) ENGINE = SummingMergeTree ORDER BY (key1, key2); INSERT INTO traffic_map WITH rand() % 20 AS port SELECT number % 1000 AS key1, intDiv(number, 10000) AS key2, rand() % 753 AS bits_in, rand64() % 800 AS bits_out, rand() % 140 AS packets_in, rand64() % 231 AS packets_out, [port], [bits_in], [bits_out], [packets_in], [packets_out] FROM numbers(100000000); ┌─table───────┬─column──────────────┬─────rows─┬─compressed─┬─uncompressed─┬──ratio─┐ │ traffic │ bits_out │ 80252317 │ 109.09 MiB │ 306.14 MiB │ 2.81 │ │ traffic │ bits_in │ 80252317 │ 108.34 MiB │ 306.14 MiB │ 2.83 │ │ traffic │ port │ 80252317 │ 99.21 MiB │ 153.07 MiB │ 1.54 │ │ traffic │ packets_out │ 80252317 │ 91.36 MiB │ 306.14 MiB │ 3.35 │ │ traffic │ packets_in │ 80252317 │ 84.61 MiB │ 306.14 MiB │ 3.62 │ │ traffic │ key2 │ 80252317 │ 47.88 MiB │ 306.14 MiB │ 6.39 │ │ traffic │ key1 │ 80252317 │ 1.38 MiB │ 306.14 MiB │ 221.42 │ │ traffic_map │ portMap.bits_out │ 10000000 │ 108.96 MiB │ 306.13 MiB │ 2.81 │ │ traffic_map │ portMap.bits_in │ 10000000 │ 108.32 MiB │ 306.13 MiB │ 2.83 │ │ traffic_map │ portMap.port │ 10000000 │ 92.00 MiB │ 229.36 MiB │ 2.49 │ │ traffic_map │ portMap.packets_out │ 10000000 │ 90.95 MiB │ 306.13 MiB │ 3.37 │ │ traffic_map │ portMap.packets_in │ 10000000 │ 84.19 MiB │ 306.13 MiB │ 3.64 │ │ traffic_map │ key2 │ 10000000 │ 23.46 MiB │ 38.15 MiB │ 1.63 │ │ traffic_map │ bits_in │ 10000000 │ 15.59 MiB │ 38.15 MiB │ 2.45 │ │ traffic_map │ bits_out │ 10000000 │ 15.59 MiB │ 38.15 MiB │ 2.45 │ │ traffic_map │ packets_out │ 10000000 │ 13.22 MiB │ 38.15 MiB │ 2.89 │ │ traffic_map │ packets_in │ 10000000 │ 12.62 MiB │ 38.15 MiB │ 3.02 │ │ traffic_map │ key1 │ 10000000 │ 180.29 KiB │ 38.15 MiB │ 216.66 │ └─────────────┴─────────────────────┴──────────┴────────────┴──────────────┴────────┘ -- Queries SELECT key1, sum(packets_in), sum(bits_out) FROM traffic GROUP BY key1 FORMAT `Null` 0 rows in set. Elapsed: 0.488 sec. Processed 80.25 million rows, 963.03 MB (164.31 million rows/s., 1.97 GB/s.) SELECT key1, sum(packets_in), sum(bits_out) FROM traffic_map GROUP BY key1 FORMAT `Null` 0 rows in set. Elapsed: 0.063 sec. Processed 10.00 million rows, 120.00 MB (159.43 million rows/s., 1.91 GB/s.) SELECT key1, port, sum(packets_in), sum(bits_out) FROM traffic GROUP BY key1, port FORMAT `Null` 0 rows in set. Elapsed: 0.668 sec. Processed 80.25 million rows, 1.12 GB (120.14 million rows/s., 1.68 GB/s.) WITH arrayJoin(arrayZip(untuple(sumMap(portMap.port, portMap.packets_in, portMap.bits_out)))) AS tpl SELECT key1, tpl.1 AS port, tpl.2 AS packets_in, tpl.3 AS bits_out FROM traffic_map GROUP BY key1 FORMAT `Null` 0 rows in set. Elapsed: 0.915 sec. Processed 10.00 million rows, 1.08 GB (10.93 million rows/s., 1.18 GB/s.) ","categories":"","description":"SummingMergeTree\n","excerpt":"SummingMergeTree\n","ref":"/engines/mergetree-table-engine-family/summingmergetree/","tags":"","title":"SummingMergeTree"},{"body":"Symptom: clickhouse don’t start with a message DB::Exception: Suspiciously many broken parts to remove.\nCause: That exception is just a safeguard check/circuit breaker, triggered when clickhouse detects a lot of broken parts during server startup.\nParts are considered broken if they have bad checksums or some files are missing or malformed. Usually, that means the data was corrupted on the disk.\nWhy data could be corrupted?\nthe most often reason is a hard restart of the system, leading to a loss of the data which was not fully flushed to disk from the system page cache. Please be aware that by default ClickHouse doesn’t do fsync, so data is considered inserted after it was passed to the Linux page cache. See fsync-related settings in ClickHouse.\nit can also be caused by disk failures, maybe there are bad blocks on hard disk, or logical problems, or some raid issue. Check system journals, use fsck / mdadm and other standard tools to diagnose the disk problem.\nother reasons: manual intervention/bugs etc, for example, the data files or folders are removed by mistake or moved to another folder.\nAction: If you ok to accept the data loss: set up force_restore_data flag and clickhouse will move the parts to detached. Data loss is possible if the issue is a result of misconfiguration (i.e. someone accidentally has fixed xml configs with incorrect shard/replica macros, data will be moved to detached folder and can be recovered).\nsudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data then restart clickhouse, the table will be attached, and the broken parts will be detached, which means the data from those parts will not be available for the selects. You can see the list of those parts in the system.detached_parts table and drop them if needed using ALTER TABLE ... DROP DETACHED PART ... commands.\nIf you are ok to tolerate bigger losses automatically you can change that safeguard configuration to be less sensitive by increasing max_suspicious_broken_parts setting:\ncat /etc/clickhouse-server/config.d/max_suspicious_broken_parts.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cmerge_tree\u003e \u003cmax_suspicious_broken_parts\u003e50\u003c/max_suspicious_broken_parts\u003e \u003c/merge_tree\u003e \u003c/yandex\u003e this limit is set to 10 by default, we can set a bigger value (50 or 100 or more), but the data will lose because of the corruption.\nCheck also a similar setting max_suspicious_broken_parts_bytes.\nSee https://clickhouse.com/docs/en/operations/settings/merge-tree-settings/\nIf you can’t accept the data loss - you should recover data from backups / re-insert it once again etc.\nIf you don’t want to tolerate automatic detaching of broken parts, you can set max_suspicious_broken_parts_bytes and max_suspicious_broken_parts to 0.\nScenario illustrating / testing Create table create table t111(A UInt32) Engine=MergeTree order by A settings max_suspicious_broken_parts=1; insert into t111 select number from numbers(100000); Detach the table and make Data corruption detach table t111; cd /var/lib/clickhouse/data/default/t111/all_*** make data file corruption:\n\u003e data.bin repeat for 2 or more data files.\nAttach the table: attach table t111; Received exception from server (version 21.12.3): Code: 231. DB::Exception: Received from localhost:9000. DB::Exception: Suspiciously many (2) broken parts to remove.. (TOO_MANY_UNEXPEC setup force_restrore_data flag sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data sudo service clickhouse-server restart then the table t111 will be attached lost the corrupted data.\n","categories":"","description":"Suspiciously many broken parts error during the server startup.\n","excerpt":"Suspiciously many broken parts error during the server startup.\n","ref":"/altinity-kb-setup-and-maintenance/suspiciously-many-broken-parts/","tags":"","title":"Suspiciously many broken parts"},{"body":" Note 1: System database stores virtual tables (parts, tables, columns, etc.) and *_log tables.\nVirtual tables do not persist on disk. They reflect ClickHouse memory (c++ structures). They cannot be changed or removed.\nLog tables are named with postfix *_log and have the MergeTree engine. Clickhouse does not use information stored in these tables, this data is for you only.\nYou can drop / rename / truncate *_log tables at any time. ClickHouse will recreate them in about 7 seconds (flush period).\nNote 2: Log tables with numeric postfixes (_1 / 2 / 3 …) query_log_1 query_thread_log_3 are results of Clickhouse upgrades. When a new version of Clickhouse starts and discovers that a system log table’s schema is incompatible with a new schema, then Clickhouse renames the old *_log table to the name with the prefix and creates a table with the new schema. You can drop such tables if you don’t need such historic data.\nYou can disable all / any of them Do not create log tables at all (a restart is needed for these changes to take effect).\n$ cat /etc/clickhouse-server/config.d/z_log_disable.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003casynchronous_metric_log remove=\"1\"/\u003e \u003cmetric_log remove=\"1\"/\u003e \u003cquery_thread_log remove=\"1\" /\u003e \u003cquery_log remove=\"1\" /\u003e \u003cquery_views_log remove=\"1\" /\u003e \u003cpart_log remove=\"1\"/\u003e \u003csession_log remove=\"1\"/\u003e \u003ctext_log remove=\"1\" /\u003e \u003ctrace_log remove=\"1\"/\u003e \u003ccrash_log remove=\"1\"/\u003e \u003copentelemetry_span_log remove=\"1\"/\u003e \u003czookeeper_log remove=\"1\"/\u003e \u003c/clickhouse\u003e We do not recommend removing query_log and query_thread_log as queries’ (they have very useful information for debugging), and logging can be easily turned off without a restart through user profiles:\n$ cat /etc/clickhouse-server/users.d/z_log_queries.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003c!-- normally it's better to keep it turned on! --\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e Hint: z_log_disable.xml is named with z_ in the beginning, it means this config will be applied the last and will override all other config files with these sections (config are applied in alphabetical order).\nYou can also configure these settings to reduce the amount of data in the system.query_log table:\nname | value | description ----------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------- log_queries_min_type | QUERY_START | Minimal type in query_log to log, possible values (from low to high): QUERY_START, QUERY_FINISH, EXCEPTION_BEFORE_START, EXCEPTION_WHILE_PROCESSING. log_queries_min_query_duration_ms | 0 | Minimal time for the query to run, to get to the query_log/query_thread_log. log_queries_cut_to_length | 100000 | If query length is greater than specified threshold (in bytes), then cut query when writing to query log. Also limit length of printed query in ordinary text log. log_profile_events | 1 | Log query performance statistics into the query_log and query_thread_log. log_query_settings | 1 | Log query settings into the query_log. log_queries_probability | 1 | Log queries with the specified probabality. You can configure TTL Example for query_log. It drops partitions with data older than 14 days:\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cquery_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cengine\u003eENGINE = MergeTree PARTITION BY (event_date) ORDER BY (event_time) TTL event_date + INTERVAL 14 DAY DELETE \u003c/engine\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003c/query_log\u003e \u003c/clickhouse\u003e After that you need to restart ClickHouse and if using old clickhouse versions like 20 or less, drop or rename the existing system.query_log table and then CH creates a new table with these settings. This is automatically done in newer versions 21+.\nRENAME TABLE system.query_log TO system.query_log_1; Important part here is a daily partitioning PARTITION BY (event_date) in this case TTL expression event_date + INTERVAL 14 DAY DELETE expires all rows at the same time. In this case ClickHouse drops whole partitions. Dropping of partitions is very easy operation for CPU / Disk I/O.\nUsual TTL processing (when table partitioned by toYYYYMM and TTL by day) is heavy CPU / Disk I/O consuming operation which re-writes data parts without expired rows.\nYou can add TTL without ClickHouse restart (and table dropping or renaming):\nALTER TABLE system.query_log MODIFY TTL event_date + INTERVAL 14 DAY; But in this case ClickHouse will drop only whole monthly partitions (will store data older than 14 days).\nOne more way to configure TTL for system tables This way just adds TTL to a table and leaves monthly (default) partitioning (will store data older than 14 days).\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cclickhouse\u003e \u003cquery_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cttl\u003eevent_date + INTERVAL 30 DAY DELETE\u003c/ttl\u003e \u003c/query_log\u003e \u003c/clickhouse\u003e 💡 For the clickhouse-operator, the above method of using only the \u003cengine\u003e tag without \u003cttl\u003e or \u003cpartition\u003e is recommended, because of possible configuration clashes.\nAfter that you need to restart ClickHouse and if using old clickhouse versions like 20 or less, drop or rename the existing system.query_log table and then CH creates a new table with these settings. This is automatically done in newer versions 21+.\nYou can disable logging on a session level or in user’s profile (for all or specific users) But only for logs generated on session level (query_log / query_thread_log)\nIn this case a restart is not needed.\nLet’s disable query logging for all users (profile = default, all other profiles inherit it).\ncat /etc/clickhouse-server/users.d/log_queries.xml \u003cclickhouse\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/clickhouse\u003e ","categories":"","description":"System tables ate my disk\n","excerpt":"System tables ate my disk\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-system-tables-eat-my-disk/","tags":"","title":"System tables ate my disk"},{"body":"Sometimes your Distributed DDL queries are being stuck, and not executing on all or subset of nodes, there are a lot of possible reasons for that kind of behavior, so it would take some time and effort to investigate.\nPossible reasons Clickhouse node can’t recognize itself SELECT * FROM system.clusters; -- check is_local column, it should have 1 for itself getent hosts clickhouse.local.net # or other name which should be local hostname --fqdn cat /etc/hosts cat /etc/hostname Debian / Ubuntu There is an issue in Debian based images, when hostname being mapped to 127.0.1.1 address which doesn’t literally match network interface and clickhouse fails to detect this address as local.\nhttps://github.com/ClickHouse/ClickHouse/issues/23504\nPrevious task is being executed and taking some time It’s usually some heavy operations like merges, mutations, alter columns, so it make sense to check those tables:\nSHOW PROCESSLIST; SELECT * FROM system.merges; SELECT * FROM system.mutations; In that case, you can just wait completion of previous task.\nPrevious task is stuck because of some error In that case, the first step is to understand which exact task is stuck and why. There are some queries which can help with that.\n-- list of all distributed ddl queries, path can be different in your installation SELECT * FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/'; -- information about specific task. SELECT * FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/query-0000001000/'; SELECT * FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/' AND name = 'query-0000001000'; -- 22.3 SELECT * FROM system.zookeeper WHERE path like '/clickhouse/task_queue/ddl/query-0000001000/%' ORDER BY ctime, path SETTINGS allow_unrestricted_reads_from_keeper='true' -- 22.6 SELECT path, name, value, ctime, mtime FROM system.zookeeper WHERE path like '/clickhouse/task_queue/ddl/query-0000001000/%' ORDER BY ctime, path SETTINGS allow_unrestricted_reads_from_keeper='true' -- How many nodes executed this task SELECT name, numChildren as finished_nodes FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/query-0000001000/' AND name = 'finished'; ┌─name─────┬─finished_nodes─┐ │ finished │ 0 │ └──────────┴────────────────┘ -- The nodes that are running the task SELECT name, value, ctime, mtime FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/query-0000001000/active/'; -- What was the result for the finished nodes SELECT name, value, ctime, mtime FROM system.zookeeper WHERE path = '/clickhouse/task_queue/ddl/query-0000001000/finished/'; -- Latest successfull executed tasks from query_log. SELECT query FROM system.query_log WHERE query LIKE '%ddl_entry%' AND type = 2 ORDER BY event_time DESC LIMIT 5; SELECT FQDN(), * FROM clusterAllReplicas('cluster', system.metrics) WHERE metric LIKE '%MaxDDLEntryID%' ┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐ │ chi-ab.svc.cluster.local │ MaxDDLEntryID │ 1468 │ Max processed DDL entry of DDLWorker. │ └──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘ ┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐ │ chi-ab.svc.cluster.local │ MaxDDLEntryID │ 1468 │ Max processed DDL entry of DDLWorker. │ └──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘ ┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐ │ chi-ab.svc.cluster.local │ MaxDDLEntryID │ 1468 │ Max processed DDL entry of DDLWorker. │ └──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘ -- Information about task execution from logs. grep -C 40 \"ddl_entry\" /var/log/clickhouse-server/clickhouse-server*.log Issues that can prevent task execution Obsolete Replicas Obsolete replicas left in zookeeper.\nSELECT database, table, zookeeper_path, replica_path zookeeper FROM system.replicas WHERE total_replicas != active_replicas; SELECT * FROM system.zookeeper WHERE path = '/clickhouse/cluster/tables/01/database/table/replicas'; SYSTEM DROP REPLICA 'replica_name'; SYSTEM STOP REPLICATION QUEUES; SYSTEM START REPLICATION QUEUES; https://clickhouse.tech/docs/en/sql-reference/statements/system/#query_language-system-drop-replica\nTasks manually removed from DDL queue Task were removed from DDL queue, but left in Replicated*MergeTree table queue.\ngrep -C 40 \"ddl_entry\" /var/log/clickhouse-server/clickhouse-server*.log /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:28.956888 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053555 [ 599 ] {} \u003cError\u003e DDLWorker: ZooKeeper error: Code: 999, e.displayText() = Coordination::Exception: No node, Stack trace (when copying this message, always include the lines below): /var/log/clickhouse-server/clickhouse-server.log- /var/log/clickhouse-server/clickhouse-server.log-0. Coordination::Exception::Exception(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, Coordination::Error, int) @ 0xfb2f6b3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-1. Coordination::Exception::Exception(Coordination::Error) @ 0xfb2fb56 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:2. DB::DDLWorker::createStatusDirs(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::shared_ptr\u003czkutil::ZooKeeper\u003e const\u0026) @ 0xeb3127a in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:3. DB::DDLWorker::processTask(DB::DDLTask\u0026) @ 0xeb36c96 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:4. DB::DDLWorker::enqueueTask(std::__1::unique_ptr\u003cDB::DDLTask, std::__1::default_delete\u003cDB::DDLTask\u003e \u003e) @ 0xeb35f22 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-5. ? @ 0xeb47aed in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-6. ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::worker(std::__1::__list_iterator\u003cThreadFromGlobalPool, void*\u003e) @ 0x8633bcd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-7. ThreadFromGlobalPool::ThreadFromGlobalPool\u003cvoid ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u003e(void\u0026\u0026, void ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u0026\u0026...)::'lambda'()::operator()() @ 0x863612f in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-8. ThreadPoolImpl\u003cstd::__1::thread\u003e::worker(std::__1::__list_iterator\u003cstd::__1::thread, void*\u003e) @ 0x8630ffd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-9. ? @ 0x8634bb3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-10. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so /var/log/clickhouse-server/clickhouse-server.log-11. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so /var/log/clickhouse-server/clickhouse-server.log- (version 21.1.8.30 (official build)) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053951 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) Context of this problem is:\nConstant pressure of cheap ON CLUSTER DELETE queries. One replica was down for a long amount of time (multiple days). Because of pressure on the DDL queue, it purged old records due to the task_max_lifetime setting. When a lagging replica comes up, it’s fail’s execute old queries from DDL queue, because at this point they were purged from it. Solution:\nReload/Restore this replica from scratch. DDL path was changed in Zookeeper without restarting ClickHouse Changing the DDL queue path in Zookeeper without restarting ClickHouse will make ClickHouse confused. If you need to do this ensure that you restart ClickHouse before submitting additional distributed DDL commands. Here’s an example.\n-- Path before change: SELECT * FROM system.zookeeper WHERE path = '/clickhouse/clickhouse101/task_queue' ┌─name─┬─value─┬─path─────────────────────────────────┐ │ ddl │ │ /clickhouse/clickhouse101/task_queue │ └──────┴───────┴──────────────────────────────────────┘ -- Path after change SELECT * FROM system.zookeeper WHERE path = '/clickhouse/clickhouse101/task_queue' ┌─name─┬─value─┬─path─────────────────────────────────┐ │ ddl2 │ │ /clickhouse/clickhouse101/task_queue │ └──────┴───────┴──────────────────────────────────────┘ The reason is that ClickHouse will not “see” this change and will continue to look for tasks in the old path. Altering paths in Zookeeper should be avoided if at all possible. If necessary it must be done very carefuly.\n","categories":"","description":"\"There are N unfinished hosts (0 of them are currently active).\"\n","excerpt":"\"There are N unfinished hosts (0 of them are currently active).\"\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/there-are-n-unfinished-hosts-0-of-them-are-currently-active/","tags":"","title":"There are N unfinished hosts (0 of them are currently active)."},{"body":"Count threads used by clickhouse-server cat /proc/$(pidof -s clickhouse-server)/status | grep Threads Threads: 103 ps hH $(pidof -s clickhouse-server) | wc -l 103 ps hH -AF | grep clickhouse | wc -l 116 Thread counts by type (using ps \u0026 clickhouse-local) ps H -o 'tid comm' $(pidof -s clickhouse-server) | tail -n +2 | awk '{ printf(\"%s\\t%s\\n\", $1, $2) }' | clickhouse-local -S \"threadid UInt16, name String\" -q \"SELECT name, count() FROM table GROUP BY name WITH TOTALS ORDER BY count() DESC FORMAT PrettyCompact\" Threads used by running queries: SELECT query, length(thread_ids) AS threads_count FROM system.processes ORDER BY threads_count; Thread pools limits \u0026 usage SELECT name, value FROM system.settings WHERE name LIKE '%pool%' ┌─name─────────────────────────────────────────┬─value─┐ │ connection_pool_max_wait_ms │ 0 │ │ distributed_connections_pool_size │ 1024 │ │ background_buffer_flush_schedule_pool_size │ 16 │ │ background_pool_size │ 16 │ │ background_move_pool_size │ 8 │ │ background_fetches_pool_size │ 8 │ │ background_schedule_pool_size │ 16 │ │ background_message_broker_schedule_pool_size │ 16 │ │ background_distributed_schedule_pool_size │ 16 │ │ postgresql_connection_pool_size │ 16 │ │ postgresql_connection_pool_wait_timeout │ -1 │ │ odbc_bridge_connection_pool_size │ 16 │ └──────────────────────────────────────────────┴───────┘ SELECT metric, value FROM system.metrics WHERE metric LIKE 'Background%' ┌─metric──────────────────────────────────┬─value─┐ │ BackgroundPoolTask │ 0 │ │ BackgroundFetchesPoolTask │ 0 │ │ BackgroundMovePoolTask │ 0 │ │ BackgroundSchedulePoolTask │ 0 │ │ BackgroundBufferFlushSchedulePoolTask │ 0 │ │ BackgroundDistributedSchedulePoolTask │ 0 │ │ BackgroundMessageBrokerSchedulePoolTask │ 0 │ └─────────────────────────────────────────┴───────┘ SELECT * FROM system.asynchronous_metrics WHERE lower(metric) LIKE '%thread%' ORDER BY metric ASC ┌─metric───────────────────────────────────┬─value─┐ │ HTTPThreads │ 0 │ │ InterserverThreads │ 0 │ │ MySQLThreads │ 0 │ │ OSThreadsRunnable │ 2 │ │ OSThreadsTotal │ 2910 │ │ PostgreSQLThreads │ 0 │ │ TCPThreads │ 1 │ │ jemalloc.background_thread.num_runs │ 0 │ │ jemalloc.background_thread.num_threads │ 0 │ │ jemalloc.background_thread.run_intervals │ 0 │ └──────────────────────────────────────────┴───────┘ SELECT * FROM system.metrics WHERE lower(metric) LIKE '%thread%' ORDER BY metric ASC Query id: 6acbb596-e28f-4f89-94b2-27dccfe88ee9 ┌─metric─────────────┬─value─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ GlobalThread │ 151 │ Number of threads in global thread pool. │ │ GlobalThreadActive │ 144 │ Number of threads in global thread pool running a task. │ │ LocalThread │ 0 │ Number of threads in local thread pools. The threads in local thread pools are taken from the global thread pool. │ │ LocalThreadActive │ 0 │ Number of threads in local thread pools running a task. │ │ QueryThread │ 0 │ Number of query processing threads │ └────────────────────┴───────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ Stack traces of the working threads from the pools SET allow_introspection_functions = 1; WITH arrayMap(x -\u003e demangle(addressToSymbol(x)), trace) AS all SELECT thread_id, query_id, arrayStringConcat(all, '\\n') AS res FROM system.stack_trace WHERE res ILIKE '%Pool%' FORMAT Vertical; ","categories":"","description":"Threads\n","excerpt":"Threads\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-threads/","tags":"","title":"Threads"},{"body":"Important things to know:\nDateTime inside clickhouse is actually UNIX timestamp always, i.e. number of seconds since 1970-01-01 00:00:00 GMT. Conversion from that UNIX timestamp to a human-readable form and reverse can happen on the client (for native clients) and on the server (for HTTP clients, and for some type of queries, like toString(ts)) Depending on the place where that conversion happened rules of different timezones may be applied. You can check server timezone using SELECT timezone() clickhouse-client also by default tries to use server timezone (see also --use_client_time_zone flag) If you want you can store the timezone name inside the data type, in that case, timestamp \u003c-\u003e human-readable time rules of that timezone will be applied. SELECT timezone(), toDateTime(now()) AS t, toTypeName(t), toDateTime(now(), 'UTC') AS t_utc, toTypeName(t_utc), toUnixTimestamp(t), toUnixTimestamp(t_utc) Row 1: ────── timezone(): Europe/Warsaw t: 2021-07-16 12:50:28 toTypeName(toDateTime(now())): DateTime t_utc: 2021-07-16 10:50:28 toTypeName(toDateTime(now(), 'UTC')): DateTime('UTC') toUnixTimestamp(toDateTime(now())): 1626432628 toUnixTimestamp(toDateTime(now(), 'UTC')): 1626432628 Since version 20.4 clickhouse uses embedded tzdata (see https://github.com/ClickHouse/ClickHouse/pull/10425 )\nYou get used tzdata version\nSELECT * FROM system.build_options WHERE name = 'TZDATA_VERSION' Query id: 0a9883f0-dadf-4fb1-8b42-8fe93f561430 ┌─name───────────┬─value─┐ │ TZDATA_VERSION │ 2020e │ └────────────────┴───────┘ and list of available time zones\nSELECT * FROM system.time_zones WHERE time_zone LIKE '%Anta%' Query id: 855453d7-eccd-44cb-9631-f63bb02a273c ┌─time_zone─────────────────┐ │ Antarctica/Casey │ │ Antarctica/Davis │ │ Antarctica/DumontDUrville │ │ Antarctica/Macquarie │ │ Antarctica/Mawson │ │ Antarctica/McMurdo │ │ Antarctica/Palmer │ │ Antarctica/Rothera │ │ Antarctica/South_Pole │ │ Antarctica/Syowa │ │ Antarctica/Troll │ │ Antarctica/Vostok │ │ Indian/Antananarivo │ └───────────────────────────┘ 13 rows in set. Elapsed: 0.002 sec. Clickhouse uses system timezone info from tzdata package if it exists, and uses own builtin tzdata if it is missing in the system.\ncd /usr/share/zoneinfo/Canada ln -s ../America/Halifax A TZ=Canada/A clickhouse-local -q 'select timezone()' Canada/A When the conversion using different rules happen SELECT timezone() ┌─timezone()─┐ │ UTC │ └────────────┘ create table t_with_dt_utc ( ts DateTime64(3,'Europe/Moscow') ) engine=Log; create table x (ts String) engine=Null; create materialized view x_mv to t_with_dt_utc as select parseDateTime64BestEffort(ts) as ts from x; $ echo '2021-07-15T05:04:23.733' | clickhouse-client -q 'insert into t_with_dt_utc format CSV' -- here client checks the type of the columns, see that it's 'Europe/Moscow' and use conversion according to moscow rules $ echo '2021-07-15T05:04:23.733' | clickhouse-client -q 'insert into x format CSV' -- here client check tha type of the columns (it is string), and pass string value to the server. -- parseDateTime64BestEffort(ts) uses server default timezone (UTC in my case), and convert the value using UTC rules. -- and the result is 2 different timestamps (when i selecting from that is shows both in 'desired' timezone, forced by column type, i.e. Moscow): SELECT * FROM t_with_dt_utc ┌──────────────────────ts─┐ │ 2021-07-15 05:04:23.733 │ │ 2021-07-15 08:04:23.733 │ └─────────────────────────┘ Best practice here: use UTC timezone everywhere, OR use the same default timezone for clickhouse server as used by your data\n","categories":"","description":"Time zones\n","excerpt":"Time zones\n","ref":"/altinity-kb-queries-and-syntax/time-zones/","tags":"","title":"Time zones"},{"body":"DROP TABLE test_ts_interpolation; --- generate test data CREATE TABLE test_ts_interpolation ENGINE = Log AS SELECT ((number * 100) + 50) - (rand() % 100) AS timestamp, transform(rand() % 2, [0, 1], ['A', 'B'], '') AS ts, if(ts = 'A', timestamp * 10, timestamp * 100) AS value FROM numbers(1000000); SELECT * FROM test_ts_interpolation; -- interpolation select with window functions SELECT timestamp, if( ts = 'A', toFloat64(value), prev_a.2 + (timestamp - prev_a.1 ) * (next_a.2 - prev_a.2) / ( next_a.1 - prev_a.1) ) as a_value, if( ts = 'B', toFloat64(value), prev_b.2 + (timestamp - prev_b.1 ) * (next_b.2 - prev_b.2) / ( next_b.1 - prev_b.1) ) as b_value FROM ( SELECT timestamp, ts, value, anyLastIf((timestamp,value), ts='A') OVER (ORDER BY timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS prev_a, anyLastIf((timestamp,value), ts='A') OVER (ORDER BY timestamp DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS next_a, anyLastIf((timestamp,value), ts='B') OVER (ORDER BY timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS prev_b, anyLastIf((timestamp,value), ts='B') OVER (ORDER BY timestamp DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS next_b FROM test_ts_interpolation ) ","categories":"","description":"Time-series alignment with interpolation\n","excerpt":"Time-series alignment with interpolation\n","ref":"/altinity-kb-queries-and-syntax/ts-interpolation/","tags":"","title":"Time-series alignment with interpolation"},{"body":"CREATE TABLE top_with_rest ( `k` String, `number` UInt64 ) ENGINE = Memory; INSERT INTO top_with_rest SELECT toString(intDiv(number, 10)), number FROM numbers_mt(10000); Using UNION ALL SELECT * FROM ( SELECT k, sum(number) AS res FROM top_with_rest GROUP BY k ORDER BY res DESC LIMIT 10 UNION ALL SELECT NULL, sum(number) AS res FROM top_with_rest WHERE k NOT IN ( SELECT k FROM top_with_rest GROUP BY k ORDER BY sum(number) DESC LIMIT 10 ) ) ORDER BY res ASC ┌─k───┬───res─┐ │ 990 │ 99045 │ │ 991 │ 99145 │ │ 992 │ 99245 │ │ 993 │ 99345 │ │ 994 │ 99445 │ │ 995 │ 99545 │ │ 996 │ 99645 │ │ 997 │ 99745 │ │ 998 │ 99845 │ │ 999 │ 99945 │ └─────┴───────┘ ┌─k────┬──────res─┐ │ null │ 49000050 │ └──────┴──────────┘ Using arrays WITH toUInt64(sumIf(sum, isNull(k)) - sumIf(sum, isNotNull(k))) AS total SELECT (arrayJoin(arrayPushBack(groupArrayIf(10)((k, sum), isNotNull(k)), (NULL, total))) AS tpl).1 AS key, tpl.2 AS res FROM ( SELECT toNullable(k) AS k, sum(number) AS sum FROM top_with_rest GROUP BY k WITH CUBE ORDER BY sum DESC LIMIT 11 ) ORDER BY res ASC ┌─key──┬──────res─┐ │ 990 │ 99045 │ │ 991 │ 99145 │ │ 992 │ 99245 │ │ 993 │ 99345 │ │ 994 │ 99445 │ │ 995 │ 99545 │ │ 996 │ 99645 │ │ 997 │ 99745 │ │ 998 │ 99845 │ │ 999 │ 99945 │ │ null │ 49000050 │ └──────┴──────────┘ Using window functions (starting from 21.1) SET allow_experimental_window_functions = 1; SELECT k AS key, If(isNotNull(key), sum, toUInt64(sum - wind)) AS res FROM ( SELECT *, sumIf(sum, isNotNull(k)) OVER () AS wind FROM ( SELECT toNullable(k) AS k, sum(number) AS sum FROM top_with_rest GROUP BY k WITH CUBE ORDER BY sum DESC LIMIT 11 ) ) ORDER BY res ASC ┌─key──┬──────res─┐ │ 990 │ 99045 │ │ 991 │ 99145 │ │ 992 │ 99245 │ │ 993 │ 99345 │ │ 994 │ 99445 │ │ 995 │ 99545 │ │ 996 │ 99645 │ │ 997 │ 99745 │ │ 998 │ 99845 │ │ 999 │ 99945 │ │ null │ 49000050 │ └──────┴──────────┘ SELECT k, sum(sum) AS res FROM ( SELECT if(rn \u003e 10, NULL, k) AS k, sum FROM ( SELECT k, sum, row_number() OVER () AS rn FROM ( SELECT k, sum(number) AS sum FROM top_with_rest GROUP BY k ORDER BY sum DESC ) ) ) GROUP BY k ORDER BY res ┌─k────┬──────res─┐ │ 990 │ 99045 │ │ 991 │ 99145 │ │ 992 │ 99245 │ │ 993 │ 99345 │ │ 994 │ 99445 │ │ 995 │ 99545 │ │ 996 │ 99645 │ │ 997 │ 99745 │ │ 998 │ 99845 │ │ 999 │ 99945 │ │ null │ 49000050 │ └──────┴──────────┘ Using WITH TOTALS The total number will include the top rows as well so the remainder must be calculated by the application\nSELECT k, sum(number) AS res FROM top_with_rest GROUP BY k WITH TOTALS ORDER BY res DESC LIMIT 10 ┌─k───┬───res─┐ │ 999 │ 99945 │ │ 998 │ 99845 │ │ 997 │ 99745 │ │ 996 │ 99645 │ │ 995 │ 99545 │ │ 994 │ 99445 │ │ 993 │ 99345 │ │ 992 │ 99245 │ │ 991 │ 99145 │ │ 990 │ 99045 │ └─────┴───────┘ Totals: ┌─k─┬──────res─┐ │ │ 49995000 │ └───┴──────────┘ ","categories":"","description":"Top N \u0026 Remain\n","excerpt":"Top N \u0026 Remain\n","ref":"/altinity-kb-queries-and-syntax/top-n-and-remain/","tags":"","title":"Top N \u0026 Remain"},{"body":"Log of query execution Controlled by session level setting send_logs_level Possible values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none' Can be used with clickhouse-client in both interactive and non-interactive mode.\n$ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" [LAPTOP] 2021.04.29 00:05:31.425842 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e executeQuery: (from 127.0.0.1:42590, using production parser) SELECT sum(number) FROM numbers(1000) [LAPTOP] 2021.04.29 00:05:31.426281 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.* [LAPTOP] 2021.04.29 00:05:31.426648 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete [LAPTOP] 2021.04.29 00:05:31.427132 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e AggregatingTransform: Aggregating [LAPTOP] 2021.04.29 00:05:31.427187 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Aggregation method: without_key [LAPTOP] 2021.04.29 00:05:31.427220 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e AggregatingTransform: Aggregated. 1000 to 1 rows (from 7.81 KiB) in 0.0004469 sec. (2237637.0552696353 rows/sec., 17.07 MiB/sec.) [LAPTOP] 2021.04.29 00:05:31.427233 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Merging aggregated data [LAPTOP] 2021.04.29 00:05:31.427875 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cInformation\u003e executeQuery: Read 1000 rows, 7.81 KiB in 0.0019463 sec., 513795 rows/sec., 3.92 MiB/sec. [LAPTOP] 2021.04.29 00:05:31.427898 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B. 499500 $ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" 2\u003e ./query.log LAPTOP.localdomain :) SET send_logs_level='trace'; SET send_logs_level = 'trace' Query id: cbbffc02-283e-48ef-93e2-8b3baced6689 Ok. 0 rows in set. Elapsed: 0.003 sec. LAPTOP.localdomain :) SELECT sum(number) FROM numbers(1000); SELECT sum(number) FROM numbers(1000) Query id: d3db767b-34e9-4252-9f90-348cf958f822 [LAPTOP] 2021.04.29 00:06:51.673836 [ 25316 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cDebug\u003e executeQuery: (from 127.0.0.1:43116, using production parser) SELECT sum(number) FROM numbers(1000); [LAPTOP] 2021.04.29 00:06:51.674167 [ 25316 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cTrace\u003e ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.* [LAPTOP] 2021.04.29 00:06:51.674419 [ 25316 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete [LAPTOP] 2021.04.29 00:06:51.674748 [ 25449 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cTrace\u003e AggregatingTransform: Aggregating [LAPTOP] 2021.04.29 00:06:51.674781 [ 25449 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cTrace\u003e Aggregator: Aggregation method: without_key [LAPTOP] 2021.04.29 00:06:51.674855 [ 25449 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cDebug\u003e AggregatingTransform: Aggregated. 1000 to 1 rows (from 7.81 KiB) in 0.0003299 sec. (3031221.582297666 rows/sec., 23.13 MiB/sec.) [LAPTOP] 2021.04.29 00:06:51.674883 [ 25449 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cTrace\u003e Aggregator: Merging aggregated data ┌─sum(number)─┐ │ 499500 │ └─────────────┘ [LAPTOP] 2021.04.29 00:06:51.675481 [ 25316 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cInformation\u003e executeQuery: Read 1000 rows, 7.81 KiB in 0.0015799 sec., 632951 rows/sec., 4.83 MiB/sec. [LAPTOP] 2021.04.29 00:06:51.675508 [ 25316 ] {d3db767b-34e9-4252-9f90-348cf958f822} \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B. 1 rows in set. Elapsed: 0.007 sec. Processed 1.00 thousand rows, 8.00 KB (136.43 thousand rows/s., 1.09 MB/s.) system tables SELECT sum(number) FROM numbers(1000); Query id: 34c61093-3303-47d0-860b-0d644fa7264b ┌─sum(number)─┐ │ 499500 │ └─────────────┘ 1 row in set. Elapsed: 0.002 sec. Processed 1.00 thousand rows, 8.00 KB (461.45 thousand rows/s., 3.69 MB/s.) SELECT * FROM system.query_log WHERE (event_date = today()) AND (query_id = '34c61093-3303-47d0-860b-0d644fa7264b'); If query_thread_log enabled (SET log_query_threads = 1) SELECT * FROM system.query_thread_log WHERE (event_date = today()) AND (query_id = '34c61093-3303-47d0-860b-0d644fa7264b'); If opentelemetry_span_log enabled (SET opentelemetry_start_trace_probability = 1, opentelemetry_trace_processors = 1) SELECT * FROM system.opentelemetry_span_log WHERE (trace_id, finish_date) IN ( SELECT trace_id, finish_date FROM system.opentelemetry_span_log WHERE ((attribute['clickhouse.query_id']) = '34c61093-3303-47d0-860b-0d644fa7264b') AND (finish_date = today()) ); Flamegraph https://www.speedscope.app/\nWITH '95578e1c-1e93-463c-916c-a1a8cdd08198' AS query, min(min) AS start_value, max(max) AS end_value, groupUniqArrayArrayArray(trace_arr) AS uniq_frames, arrayMap((x, a, b) -\u003e ('sampled', b, 'none', start_value, end_value, arrayMap(s -\u003e reverse(arrayMap(y -\u003e toUInt32(indexOf(uniq_frames, y) - 1), s)), x), a), groupArray(trace_arr), groupArray(weights), groupArray(trace_type)) AS samples SELECT concat('clickhouse-server@', version()) AS exporter, 'https://www.speedscope.app/file-format-schema.json' AS `$schema`, concat('Clickhouse query id: ', query) AS name, CAST(samples, 'Array(Tuple(type String, name String, unit String, startValue UInt64, endValue UInt64, samples Array(Array(UInt32)), weights Array(UInt32)))') AS profiles, CAST(tuple(arrayMap(x -\u003e (demangle(addressToSymbol(x)), addressToLine(x)), uniq_frames)), 'Tuple(frames Array(Tuple(name String, line String)))') AS shared FROM ( SELECT min(min_ns) AS min, trace_type, max(max_ns) AS max, groupArray(trace) AS trace_arr, groupArray(cnt) AS weights FROM ( SELECT min(timestamp_ns) AS min_ns, max(timestamp_ns) AS max_ns, trace, trace_type, count() AS cnt FROM system.trace_log WHERE query_id = query GROUP BY trace_type, trace ) GROUP BY trace_type ) SETTINGS allow_introspection_functions = 1, output_format_json_named_tuples_as_objects = 1 FORMAT JSONEachRow SETTINGS output_format_json_named_tuples_as_objects = 1 ","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/altinity-kb-queries-and-syntax/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"TTL\n","excerpt":"TTL\n","ref":"/altinity-kb-queries-and-syntax/ttl/","tags":"","title":"TTL"},{"body":"Example with MergeTree table CREATE TABLE test_ttl_group_by ( `key` UInt32, `ts` DateTime, `value` UInt32, `min_value` UInt32 DEFAULT value, `max_value` UInt32 DEFAULT value ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, toStartOfDay(ts)) TTL ts + interval 30 day GROUP BY key, toStartOfDay(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfDay(ts)); During TTL merges Clickhouse re-calculates values of columns in the SET section.\nGROUP BY section should be a prefix of a table’s PRIMARY KEY (the same as ORDER BY, if no separate PRIMARY KEY defined).\n-- stop merges to demonstrate data before / after -- a rolling up SYSTEM STOP TTL MERGES test_ttl_group_by; SYSTEM STOP MERGES test_ttl_group_by; INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() + number, 1 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 60 day + number, 2 FROM numbers(100); SELECT toYYYYMM(ts) AS m, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY m; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202102 │ 100 │ 200 │ 2 │ 2 │ │ 202104 │ 100 │ 100 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ SYSTEM START TTL MERGES test_ttl_group_by; SYSTEM START MERGES test_ttl_group_by; OPTIMIZE TABLE test_ttl_group_by FINAL; SELECT toYYYYMM(ts) AS m, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY m; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202102 │ 5 │ 200 │ 2 │ 2 │ │ 202104 │ 100 │ 100 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ As you can see 100 rows were rolled up into 5 rows (key has 5 values) for rows older than 30 days.\nExample with SummingMergeTree table CREATE TABLE test_ttl_group_by ( `key1` UInt32, `key2` UInt32, `ts` DateTime, `value` UInt32, `min_value` SimpleAggregateFunction(min, UInt32) DEFAULT value, `max_value` SimpleAggregateFunction(max, UInt32) DEFAULT value ) ENGINE = SummingMergeTree PARTITION BY toYYYYMM(ts) PRIMARY KEY (key1, key2, toStartOfDay(ts)) ORDER BY (key1, key2, toStartOfDay(ts), ts) TTL ts + interval 30 day GROUP BY key1, key2, toStartOfDay(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfDay(ts)); -- stop merges to demonstrate data before / after -- a rolling up SYSTEM STOP TTL MERGES test_ttl_group_by; SYSTEM STOP MERGES test_ttl_group_by; INSERT INTO test_ttl_group_by (key1, key2, ts, value) SELECT 1, 1, toStartOfMinute(now() + number*60), 1 FROM numbers(100); INSERT INTO test_ttl_group_by (key1, key2, ts, value) SELECT 1, 1, toStartOfMinute(now() + number*60), 1 FROM numbers(100); INSERT INTO test_ttl_group_by (key1, key2, ts, value) SELECT 1, 1, toStartOfMinute(now() + number*60 - toIntervalDay(60)), 2 FROM numbers(100); INSERT INTO test_ttl_group_by (key1, key2, ts, value) SELECT 1, 1, toStartOfMinute(now() + number*60 - toIntervalDay(60)), 2 FROM numbers(100); SELECT toYYYYMM(ts) AS m, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY m; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202102 │ 200 │ 400 │ 2 │ 2 │ │ 202104 │ 200 │ 200 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ SYSTEM START TTL MERGES test_ttl_group_by; SYSTEM START MERGES test_ttl_group_by; OPTIMIZE TABLE test_ttl_group_by FINAL; SELECT toYYYYMM(ts) AS m, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY m; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202102 │ 1 │ 400 │ 2 │ 2 │ │ 202104 │ 100 │ 200 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ During merges Clickhouse re-calculates ts columns as min(toStartOfDay(ts)). It’s possible only for the last column of SummingMergeTree ORDER BY section ORDER BY (key1, key2, toStartOfDay(ts), ts) otherwise it will break the order of rows in the table.\nExample with AggregatingMergeTree table CREATE TABLE test_ttl_group_by_agg ( `key1` UInt32, `key2` UInt32, `ts` DateTime, `counter` AggregateFunction(count, UInt32) ) ENGINE = AggregatingMergeTree PARTITION BY toYYYYMM(ts) PRIMARY KEY (key1, key2, toStartOfDay(ts)) ORDER BY (key1, key2, toStartOfDay(ts), ts) TTL ts + interval 30 day GROUP BY key1, key2, toStartOfDay(ts) SET counter = countMergeState(counter), ts = min(toStartOfDay(ts)); CREATE TABLE test_ttl_group_by_raw ( `key1` UInt32, `key2` UInt32, `ts` DateTime ) ENGINE = Null; CREATE MATERIALIZED VIEW test_ttl_group_by_mv TO test_ttl_group_by_agg AS SELECT `key1`, `key2`, `ts`, countState() as counter FROM test_ttl_group_by_raw GROUP BY key1, key2, ts; -- stop merges to demonstrate data before / after -- a rolling up SYSTEM STOP TTL MERGES test_ttl_group_by_agg; SYSTEM STOP MERGES test_ttl_group_by_agg; INSERT INTO test_ttl_group_by_raw (key1, key2, ts) SELECT 1, 1, toStartOfMinute(now() + number*60) FROM numbers(100); INSERT INTO test_ttl_group_by_raw (key1, key2, ts) SELECT 1, 1, toStartOfMinute(now() + number*60) FROM numbers(100); INSERT INTO test_ttl_group_by_raw (key1, key2, ts) SELECT 1, 1, toStartOfMinute(now() + number*60 - toIntervalDay(60)) FROM numbers(100); INSERT INTO test_ttl_group_by_raw (key1, key2, ts) SELECT 1, 1, toStartOfMinute(now() + number*60 - toIntervalDay(60)) FROM numbers(100); SELECT toYYYYMM(ts) AS m, count(), countMerge(counter) FROM test_ttl_group_by_agg GROUP BY m; ┌──────m─┬─count()─┬─countMerge(counter)─┐ │ 202307 │ 200 │ 200 │ │ 202309 │ 200 │ 200 │ └────────┴─────────┴─────────────────────┘ SYSTEM START TTL MERGES test_ttl_group_by_agg; SYSTEM START MERGES test_ttl_group_by_agg; OPTIMIZE TABLE test_ttl_group_by_agg FINAL; SELECT toYYYYMM(ts) AS m, count(), countMerge(counter) FROM test_ttl_group_by_agg GROUP BY m; ┌──────m─┬─count()─┬─countMerge(counter)─┐ │ 202307 │ 1 │ 200 │ │ 202309 │ 100 │ 200 │ └────────┴─────────┴─────────────────────┘ Multilevel TTL Group by CREATE TABLE test_ttl_group_by ( `key` UInt32, `ts` DateTime, `value` UInt32, `min_value` UInt32 DEFAULT value, `max_value` UInt32 DEFAULT value ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, toStartOfWeek(ts), toStartOfDay(ts), toStartOfHour(ts)) TTL ts + interval 1 hour GROUP BY key, toStartOfWeek(ts), toStartOfDay(ts), toStartOfHour(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfHour(ts)), ts + interval 1 day GROUP BY key, toStartOfWeek(ts), toStartOfDay(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfDay(ts)), ts + interval 30 day GROUP BY key, toStartOfWeek(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfWeek(ts)); SYSTEM STOP TTL MERGES test_ttl_group_by; SYSTEM STOP MERGES test_ttl_group_by; INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() + number, 1 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 2 hour + number, 2 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 2 day + number, 3 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 2 month + number, 4 FROM numbers(100); SELECT toYYYYMMDD(ts) AS d, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY d ORDER BY d; ┌────────d─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 20210616 │ 100 │ 400 │ 4 │ 4 │ │ 20210814 │ 100 │ 300 │ 3 │ 3 │ │ 20210816 │ 200 │ 300 │ 1 │ 2 │ └──────────┴─────────┴────────────┴────────────────┴────────────────┘ SYSTEM START TTL MERGES test_ttl_group_by; SYSTEM START MERGES test_ttl_group_by; OPTIMIZE TABLE test_ttl_group_by FINAL; SELECT toYYYYMMDD(ts) AS d, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY d ORDER BY d; ┌────────d─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 20210613 │ 5 │ 400 │ 4 │ 4 │ │ 20210814 │ 5 │ 300 │ 3 │ 3 │ │ 20210816 │ 105 │ 300 │ 1 │ 2 │ └──────────┴─────────┴────────────┴────────────────┴────────────────┘ TTL GROUP BY + DELETE CREATE TABLE test_ttl_group_by ( `key` UInt32, `ts` DateTime, `value` UInt32, `min_value` UInt32 DEFAULT value, `max_value` UInt32 DEFAULT value ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, toStartOfDay(ts)) TTL ts + interval 180 day, ts + interval 30 day GROUP BY key, toStartOfDay(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfDay(ts)); -- stop merges to demonstrate data before / after -- a rolling up SYSTEM STOP TTL MERGES test_ttl_group_by; SYSTEM STOP MERGES test_ttl_group_by; INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() + number, 1 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 60 day + number, 2 FROM numbers(100); INSERT INTO test_ttl_group_by (key, ts, value) SELECT number % 5, now() - interval 200 day + number, 3 FROM numbers(100); SELECT toYYYYMM(ts) AS m, count(), sum(value), min(min_value), max(max_value) FROM test_ttl_group_by GROUP BY m; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202101 │ 100 │ 300 │ 3 │ 3 │ │ 202106 │ 100 │ 200 │ 2 │ 2 │ │ 202108 │ 100 │ 100 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ SYSTEM START TTL MERGES test_ttl_group_by; SYSTEM START MERGES test_ttl_group_by; OPTIMIZE TABLE test_ttl_group_by FINAL; ┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐ │ 202106 │ 5 │ 200 │ 2 │ 2 │ │ 202108 │ 100 │ 100 │ 1 │ 1 │ └────────┴─────────┴────────────┴────────────────┴────────────────┘ Also see the Altinity Knowledge Base pages on the MergeTree table engine family.\n","categories":"","description":"TTL GROUP BY Examples\n","excerpt":"TTL GROUP BY Examples\n","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/","tags":"","title":"TTL GROUP BY Examples"},{"body":"See also the Altinity Knowledge Base article on testing different compression codecs.\nExample how to create a table and define recompression rules CREATE TABLE hits ( `banner_id` UInt64, `event_time` DateTime CODEC(Delta, Default), `c_name` String, `c_cost` Float64 ) ENGINE = MergeTree PARTITION BY toYYYYMM(event_time) ORDER BY (banner_id, event_time) TTL event_time + toIntervalMonth(1) RECOMPRESS CODEC(ZSTD(1)), event_time + toIntervalMonth(6) RECOMPRESS CODEC(ZSTD(6); Default comression is LZ4 https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-compression\nThese TTL rules recompress data after 1 and 6 months.\nCODEC(Delta, Default) – Default means to use default compression (LZ4 -\u003e ZSTD1 -\u003e ZSTD6) in this case.\nExample how to define recompression rules for an existing table CREATE TABLE hits ( `banner_id` UInt64, `event_time` DateTime CODEC(Delta, LZ4), `c_name` String, `c_cost` Float64 ) ENGINE = MergeTree PARTITION BY toYYYYMM(event_time) ORDER BY (banner_id, event_time); ALTER TABLE hits modify column event_time DateTime CODEC(Delta, Default), modify TTL event_time + toIntervalMonth(1) RECOMPRESS CODEC(ZSTD(1)), event_time + toIntervalMonth(6) RECOMPRESS CODEC(ZSTD(6)); All columns have implicite default compression from server config, except event_time, that’s why need to change to compression to Default for this column otherwise it won’t be recompressed.\n","categories":"","description":"TTL Recompress example\n","excerpt":"TTL Recompress example\n","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-recompress-example/","tags":"","title":"TTL Recompress example"},{"body":"CREATE TABLE test_update ( `key` UInt32, `value` String ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_update SELECT number, concat('value ', toString(number)) FROM numbers(20); SELECT * FROM test_update; ┌─key─┬─value────┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ │ 4 │ value 4 │ │ 5 │ value 5 │ │ 6 │ value 6 │ │ 7 │ value 7 │ │ 8 │ value 8 │ │ 9 │ value 9 │ │ 10 │ value 10 │ │ 11 │ value 11 │ │ 12 │ value 12 │ │ 13 │ value 13 │ │ 14 │ value 14 │ │ 15 │ value 15 │ │ 16 │ value 16 │ │ 17 │ value 17 │ │ 18 │ value 18 │ │ 19 │ value 19 │ └─────┴──────────┘ CREATE TABLE test_update_source ( `key` UInt32, `value` String ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_update_source VALUES (1,'other value'), (10, 'new value'); CREATE DICTIONARY update_dict ( `key` UInt32, `value` String ) PRIMARY KEY key SOURCE(CLICKHOUSE(TABLE 'test_update_source')) LIFETIME(MIN 0 MAX 10) LAYOUT(FLAT); SELECT dictGet('default.update_dict', 'value', toUInt64(1)); ┌─dictGet('default.update_dict', 'value', toUInt64(1))─┐ │ other value │ └──────────────────────────────────────────────────────┘ ALTER TABLE test_update UPDATE value = dictGet('default.update_dict', 'value', toUInt64(key)) WHERE dictHas('default.update_dict', toUInt64(key)); SELECT * FROM test_update ┌─key─┬─value───────┐ │ 0 │ value 0 │ │ 1 │ other value │ │ 2 │ value 2 │ │ 3 │ value 3 │ │ 4 │ value 4 │ │ 5 │ value 5 │ │ 6 │ value 6 │ │ 7 │ value 7 │ │ 8 │ value 8 │ │ 9 │ value 9 │ │ 10 │ new value │ │ 11 │ value 11 │ │ 12 │ value 12 │ │ 13 │ value 13 │ │ 14 │ value 14 │ │ 15 │ value 15 │ │ 16 │ value 16 │ │ 17 │ value 17 │ │ 18 │ value 18 │ │ 19 │ value 19 │ └─────┴─────────────┘ Info In case of Replicated installation, Dictionary should be created on all nodes and source tables should use the ReplicatedMergeTree engine and be replicated across all nodes. Info Starting from 20.4, ClickHouse forbid by default any potential non-deterministic mutations. This behavior controlled by setting allow_nondeterministic_mutations. You can append it to query like this ALTER TABLE xxx UPDATE ... WHERE ... SETTINGS allow_nondeterministic_mutations = 1; For ON CLUSTER queries, you would need to put this setting in default profile and restart ClickHouse servers. ","categories":"","description":"UPDATE via Dictionary\n","excerpt":"UPDATE via Dictionary\n","ref":"/altinity-kb-queries-and-syntax/update-via-dictionary/","tags":"","title":"UPDATE via Dictionary"},{"body":"SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(transform(number % 3, [0, 1, 2, 3], ['aa', 'ab', 'ad', 'af'], 'a0')) 1 rows in set. Elapsed: 4.668 sec. Processed 1.00 billion rows, 8.00 GB (214.21 million rows/s., 1.71 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(multiIf((number % 3) = 0, 'aa', (number % 3) = 1, 'ab', (number % 3) = 2, 'ad', (number % 3) = 3, 'af', 'a0')) 1 rows in set. Elapsed: 7.333 sec. Processed 1.00 billion rows, 8.00 GB (136.37 million rows/s., 1.09 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(CAST(number % 3 AS Enum('aa' = 0, 'ab' = 1, 'ad' = 2, 'af' = 3)')) 1 rows in set. Elapsed: 1.152 sec. Processed 1.00 billion rows, 8.00 GB (867.79 million rows/s., 6.94 GB/s.) ","categories":"","description":"Values mapping\n","excerpt":"Values mapping\n","ref":"/altinity-kb-queries-and-syntax/values-mapping/","tags":"","title":"Values mapping"},{"body":"Update itself is simple: update packages, restart clickhouse-server service afterwards.\nCheck if the version you want to upgrade to is stable. We highly recommend the Altinity ClickHouse Stable Releases. Review the changelog to ensure that no configuration changes are needed. Update staging and test to verify all systems are working. Prepare and test downgrade procedures so the server can be returned to the previous version if necessary. Start with a “canary” update. This is one replica with one shard that is upgraded to make sure that the procedure works. Test and verify that everything works properly. Check for any errors in the log files. If everything is working well, update the rest of the cluster. For small clusters, the BlueGreenDeployment technique is also a good option.\n","categories":"","description":"Version Upgrades\n","excerpt":"Version Upgrades\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/version-upgrades/","tags":"","title":"Version Upgrades"},{"body":"When you have an incoming event stream with duplicates and updates you have a big challenge building a consistent row state inside the Clickhouse table.\nReplacingMergeTree is a great engine for that and there are a lot of blog posts on how to apply it for that particular purpose. But there is a serious problem - you can’t use another very important feature - aggregating rows by Materialized Views or Projections on top of the ReplacingMT table, because duplicates and updates will not be deduplicated and calculated aggregates (like sum or count) will be incorrect. For big amounts of data, it’s become critical because aggregating raw data during report queries will take too much time.\nAnother drawback of ReplacingMergeTree is unfinished support for DELETEs. While for the newest versions of Clickhouse, it’s possible to add the is_deleted to ReplacingMergeTree parameters, the necessity of manually filtering out deleted rows even after FINAL processing makes it less useful.\nClickhouse has other table engines that can be used quite well for dealing with UPDATEs and DELETEs - CollapsingMergeTree and VersionedCollapsingMergeTree.\nBoth of them use the concept of inserting a “rollback row” to compensate for the previous insert. The difference between CollapsingMergeTree and VersionedCollapsingMergeTree is in the algorithm of collapsing. For Cluster configurations, it’s very important to understand what row came first and who should replace whom. That is why using ReplicatedVersionedCollapsingMergeTree is mandatory for Replicated Clusters.\nWhen dealing with such complicated data streams, it needs to be solved 3 tasks simultaneously:\nremove duplicates process updates and deletes calculate correct aggregates Let’s explain in several steps how to do it in Clickhouse with some tricky SQL code.\nRow deduplication There are a lot of ways to remove duplicates from the event stream. The most effective is block deduplication when Clickhouse drop inserts blocks with the same checksum (or tag). But it requires building a smart ingest procedure.\nBut it’s possible to use another method - checking that particular row is already presented in the destination table and not insert it again. To get reliable results, such a process should be executed in 1 thread on 1 cluster node. That can be possible only for not-too-active event streams (like 100k/sec). For heavier streams some sort of partitioning is needed while inserting data with different PK to different shards or replicas or even on the same node.\nThe example of row deduplication:\ncreate table Example1 (id Int64, metric UInt64) engine = MergeTree order by id; create table Example1Null engine = Null as Example1; create materialized view __Example1 to Example1 as select * from Example1Null where id not in ( select id from Example1 where id in ( select id from Example1Null ) ); Here is the trick:\nuse Null table and MatView to be able access both insert block and the dest table check existance of ids in dest table with fast index scan by primary key using IN operator filter existing rows from insert block by NOT IN operator Insert block in most cases has not too many rows (like 1000-100k), so checking dest table for their existence by scanning Primary Key (residing in memory) won’t take much time, but due to high table’s index granularity can be still noticeble on high load. If it’s possible better to reduce index granularity at least to 4096 (from default 8192).\nLast row state To process updates in CollapsingMergeTree it needs to know “last row state” to insert the “compensation row”. Sometimes it’s possible - CDC events coming from MySQL’s binlog or Postgres’s WAL contains not only “new” data but also “old” values. If one of columns contains timestamp of row’s update time it can be used as row’s “version”. But in most cases incoming event stream does not have old metric values and suitable version information. In this case we can get that data by looking into Clickhouse table the same way as we do for row deduplication in previous example.\ncreate table Example2 (id Int64, metric UInt64, sign Int8) engine = CollapsingMergeTree(sign) order by id; create table Example2Null engine = Null as Example2; create materialized view __Example2 to Example3 as with _old as ( select *, arrayJoin([-1,1]) as _sign from Example2 where id in (select id from Example2Null) ) select id, if(_old._sign=-1, _old.metric, _new.metric) as metric from Example2Null as _new join _old using id; Here I read more data from Example2 table compared to Example1. Instead of simple checking the row existance by IN operator, a JOIN with existed rows used for building “compensate row”.\nThe trick with arrayJoin is needed to insert two rows as it required for CollapsingMergeTree table.\nDon’t try to run code above. It’s just a short explanation of the idea, lucking many needed elements.\nReplace by collapsing Here is more realistic example, that can be played with:\ncreate table Example3 ( id Int32, metric1 UInt32, metric2 UInt32, _version UInt64, sign Int8 default 1 ) engine = VersionedCollapsingMergeTree(sign, _version) ORDER BY id ; create table Stage engine=Null as Example3 ; create materialized view Example3Transform to Example3 as with __new as ( SELECT * FROM Stage order by sign desc, _version desc limit 1 by id ), __old AS ( SELECT *, arrayJoin([-1,1]) AS _sign from ( select * FROM Example3 final PREWHERE id IN (SELECT id FROM __new) where sign = 1 ) ) select id, if(__old._sign = -1, __old.metric1, __new.metric1) AS metric1, if(__old._sign = -1, __old.metric2, __new.metric2) AS metric2, if(__old._sign = -1, __old._version, __new._version) AS _version, if(__old._sign = -1, -1, 1) AS sign from __new left join __old using id where if(__new.sign=-1, __old._sign = -1, -- insert only delete row if it's found in old data __new._version \u003e __old._version -- skip duplicates for updates ); -- original insert into Stage values (1,1,1,1,1), (2,2,2,1,1); select 'step1',* from Example3 ; -- no duplicates (with the same version) inserted insert into Stage values (1,3,1,1,1),(2,3,2,1,1); select 'step2',* from Example3 ; -- delete a row with id=2. version for delete row does not have any meaning insert into Stage values (2,2,2,0,-1); select 'step3',* from Example3 final; -- replace a row with id=1. row with sign=-1 not needed, but can be in the insert blocks (will be skipped) insert into Stage values (1,1,1,0,-1),(1,3,3,2,1); select 'step4',* from Example3 final; Output:\nstep1\t1\t1\t1\t1\t1 step1\t2\t2\t2\t1\t1 step2\t1\t1\t1\t1\t1 step2\t2\t2\t2\t1\t1 step3\t1\t1\t1\t1\t1 step4\t1\t3\t3\t2\t1 Important additions:\nfiltering insert block to get only 1 (latest) row, if there are inserted many rows with same id using FINAL and PREWHERE (to speed up FINAL) while reading main (dest) table filter to skip out-of-order events by checking version DELETE event processing (inside last WHERE) Adding projections Let’s finally add aggregating projection together with more useful updated_at timestamp instead of abstract _version.\nhttps://fiddle.clickhouse.com/3140d341-ccc5-4f57-8fbf-55dbf4883a21\ncreate table Example4 ( id Int32, metric1 UInt32, Smetric1 alias metric1*sign, metric2 UInt32, dim1 LowCardinality(String), updated_at DateTime64(3) default now64(3), sign Int8 default 1, -- incoming event stream is deduplicated so I can do stream aggregation PROJECTION byDim1 ( select dim1, sum(metric1*sign) group by dim1 ) ) engine = VersionedCollapsingMergeTree(sign, updated_at) ORDER BY id ; create table Stage engine=Null as Example4 ; create materialized view Example4Transform to Example4 as with __new as ( SELECT * FROM Stage order by sign desc, updated_at desc limit 1 by id ), __old AS ( SELECT *, arrayJoin([-1,1]) AS _sign from ( select * FROM Example4 final PREWHERE id IN (SELECT id FROM __new) where sign = 1 ) ) select id, if(__old._sign = -1, __old.metric1, __new.metric1) AS metric1, if(__old._sign = -1, __old.metric2, __new.metric2) AS metric2, if(__old._sign = -1, __old.dim1, __new.dim1) AS dim1, if(__old._sign = -1, __old.updated_at, __new.updated_at) AS updated_at, if(__old._sign = -1, -1, 1) AS sign from __new left join __old using id where if(__new.sign=-1, __old._sign = -1, -- insert only delete row if it's found in old data __new.updated_at \u003e __old.updated_at -- skip duplicates for updates ); -- original insert into Stage(id,metric1,metric2,dim1) values (1,1,1,'d'), (2,2,2,'d'); select 'step1',* from Example4 ; select 'proj1',dim1, sum(Smetric1) from Example4 group by dim1; -- delete a row with id=2 insert into Stage(id,metric1,metric2,sign) values (2,2,2,-1); select 'step2',* from Example4 final; select 'proj2',dim1, sum(Smetric1) from Example4 group by dim1; -- replace a row with id=1. row with sign=-1 not needed, but can be in the insert blocks (will be skipped) insert into Stage(id,metric1,metric2,dim1,sign) values (1,1,1,'',-1),(1,3,3,'d',1); select 'step3',* from Example4 final; select 'proj3',dim1, sum(Smetric1) from Example4 group by dim1; Output:\nstep1\t1\t1\t1\td\t2024-03-03 15:58:23.232\t1 step1\t2\t2\t2\td\t2024-03-03 15:58:23.232\t1 proj1\td\t3 step2\t1\t1\t1\td\t2024-03-03 15:58:23.232\t1 proj2\td\t1 step3\t1\t3\t3\td\t2024-03-03 15:58:23.292\t1 proj3\td\t3 Combine old and new As the bonus I will use presented techique to reimplement AggregatingMergeTree algorithm with combining old row with new row with VersionedCollapsingMergeTree.\nhttps://fiddle.clickhouse.com/e1d7e04c-f1d6-4a25-9aac-1fe2b543c693\ncreate table Example5 ( id Int32, metric1 UInt32, metric2 Nullable(UInt32), updated_at DateTime64(3) default now64(3), sign Int8 default 1 ) engine = VersionedCollapsingMergeTree(sign, updated_at) ORDER BY id ; create table Stage engine=Null as Example5 ; create materialized view Example5Transform to Example5 as with __new as ( SELECT * FROM Stage order by sign desc, updated_at desc limit 1 by id ), __old AS ( SELECT *, arrayJoin([-1,1]) AS _sign from ( select * FROM Example5 final PREWHERE id IN (SELECT id FROM __new) where sign = 1 ) ) select id, if(__old._sign = -1, __old.metric1, greatest(__new.metric1, __old.metric1)) AS metric1, if(__old._sign = -1, __old.metric2, ifNull(__new.metric2, __old.metric2)) AS metric2, if(__old._sign = -1, __old.updated_at, __new.updated_at) AS updated_at, if(__old._sign = -1, -1, 1) AS sign from __new left join __old using id where if(__new.sign=-1, __old._sign = -1, -- insert only delete row if it's found in old data __new.updated_at \u003e __old.updated_at -- skip duplicates for updates ); -- original insert into Stage(id) values (1), (2); select 'step0',* from Example5 ; insert into Stage(id,metric1) values (1,1), (2,2); select 'step1',* from Example5 final; insert into Stage(id,metric2) values (1,11), (2,12); select 'step2',* from Example5 final ; Output:\nstep0\t1\t0\t\\N\t2024-03-03 15:48:21.588\t1 step0\t2\t0\t\\N\t2024-03-03 15:48:21.588\t1 step1\t1\t1\t\\N\t2024-03-03 15:48:21.599\t1 step1\t2\t2\t\\N\t2024-03-03 15:48:21.599\t1 step2\t1\t1\t11\t2024-03-03 15:48:21.612\t1 step2\t2\t2\t12\t2024-03-03 15:48:21.612\t1 Complex Primary Key In the examples above I use for PK a very simple a compact column with In64 type. When it’s possible better to go such a way. SnowFlakeId is the best variant and can be easily created during INSERT from DateTime and hash of one or several important columns. But sometimes it needs to have a more complicated PK as when storing data for multiple Tenant (Customer, Partners, etc) in the same table. It’s not a problem for suggested technique - just use all the needed columns in all filter and JOIN operations.\ncreate table Example1 ( id Int64, tenant_id Int32, metric1 UInt32, _version UInt64, sign Int8 default 1 ) engine = VersionedCollapsingMergeTree(sign, _version) ORDER BY (tenant_id,id) ; create table Stage engine=Null as Example1 ; create materialized view Example1Transform to Example1 as with __new as ( SELECT * FROM Stage order by sign desc, _version desc limit 1 by tenant_id,id ), __old AS ( SELECT *, arrayJoin([-1,1]) AS _sign from ( select * FROM Example1 final PREWHERE (tenant_id,id) IN (SELECT tenant_id,id FROM __new) where sign = 1 ) ) select id,tenant_id, if(__old._sign = -1, __old.metric1, __new.metric1) AS metric1, if(__old._sign = -1, __old._version, __new._version) AS _version, if(__old._sign = -1, -1, 1) AS sign from __new left join __old using (tenant_id,id) where if(__new.sign=-1, __old._sign = -1, -- insert only delete row if it's found in old data __new._version \u003e __old._version -- skip duplicates for updates ); ","categories":"","description":"VersionedCollapsingMergeTree","excerpt":"VersionedCollapsingMergeTree","ref":"/engines/mergetree-table-engine-family/versioned-collapsing-mergetree/","tags":"","title":"VersionedCollapsingMergeTree"},{"body":"SELECT *, formatReadableSize(value) FROM system.asynchronous_metrics WHERE metric like '%Cach%' or metric like '%Mem%' order by metric format PrettyCompactMonoBlock; SELECT event_time, metric, value, formatReadableSize(value) FROM system.asynchronous_metric_log WHERE event_time \u003e now() - 600 and (metric like '%Cach%' or metric like '%Mem%') and value \u003c\u003e 0 order by metric, event_time format PrettyCompactMonoBlock; SELECT formatReadableSize(sum(bytes_allocated)) FROM system.dictionaries; SELECT database, name, formatReadableSize(total_bytes) FROM system.tables WHERE engine IN ('Memory','Set','Join'); SELECT sumIf(data_uncompressed_bytes, part_type = 'InMemory') as memory_parts, formatReadableSize(sum(primary_key_bytes_in_memory)) AS primary_key_bytes_in_memory, formatReadableSize(sum(primary_key_bytes_in_memory_allocated)) AS primary_key_bytes_in_memory_allocated FROM system.parts; SELECT formatReadableSize(sum(memory_usage)) FROM system.merges; SELECT formatReadableSize(sum(memory_usage)) FROM system.processes; select formatReadableSize(sum(result_size)) FROM system.query_cache; SELECT initial_query_id, elapsed, formatReadableSize(memory_usage), formatReadableSize(peak_memory_usage), query FROM system.processes ORDER BY peak_memory_usage DESC LIMIT 10; SELECT type, event_time, initial_query_id, formatReadableSize(memory_usage), query FROM system.query_log WHERE (event_date \u003e= today()) AND (event_time \u003e= (now() - 7200)) ORDER BY memory_usage DESC LIMIT 10; for i in `seq 1 600`; do clickhouse-client --empty_result_for_aggregation_by_empty_set=0 -q \"select (select 'Merges: \\ '||formatReadableSize(sum(memory_usage)) from system.merges), (select \\ 'Processes: '||formatReadableSize(sum(memory_usage)) from system.processes)\";\\ sleep 3; done Merges: 96.57 MiB\tProcesses: 41.98 MiB Merges: 82.24 MiB\tProcesses: 41.91 MiB Merges: 66.33 MiB\tProcesses: 41.91 MiB Merges: 66.49 MiB\tProcesses: 37.13 MiB Merges: 67.78 MiB\tProcesses: 37.13 MiB echo \" Merges Processes PrimaryK TempTabs Dicts\"; \\ for i in `seq 1 600`; do clickhouse-client --empty_result_for_aggregation_by_empty_set=0 -q \"select \\ (select leftPad(formatReadableSize(sum(memory_usage)),15, ' ') from system.merges)|| (select leftPad(formatReadableSize(sum(memory_usage)),15, ' ') from system.processes)|| (select leftPad(formatReadableSize(sum(primary_key_bytes_in_memory_allocated)),15, ' ') from system.parts)|| \\ (select leftPad(formatReadableSize(sum(total_bytes)),15, ' ') from system.tables \\ WHERE engine IN ('Memory','Set','Join'))|| (select leftPad(formatReadableSize(sum(bytes_allocated)),15, ' ') FROM system.dictionaries) \"; sleep 3; done Merges Processes PrimaryK TempTabs Dicts 0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB 0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB 0.00 B 0.00 B 21.35 MiB 1.58 GiB 911.07 MiB 0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB retrospection analysis of the RAM usage based on query_log and part_log (shows peaks) WITH now() - INTERVAL 24 HOUR AS min_time, -- you can adjust that now() AS max_time, -- you can adjust that INTERVAL 1 HOUR as time_frame_size SELECT toStartOfInterval(event_timestamp, time_frame_size) as timeframe, formatReadableSize(max(mem_overall)) as peak_ram, formatReadableSize(maxIf(mem_by_type, event_type='Insert')) as inserts_ram, formatReadableSize(maxIf(mem_by_type, event_type='Select')) as selects_ram, formatReadableSize(maxIf(mem_by_type, event_type='MergeParts')) as merge_ram, formatReadableSize(maxIf(mem_by_type, event_type='MutatePart')) as mutate_ram, formatReadableSize(maxIf(mem_by_type, event_type='Alter')) as alter_ram, formatReadableSize(maxIf(mem_by_type, event_type='Create')) as create_ram, formatReadableSize(maxIf(mem_by_type, event_type not IN ('Insert', 'Select', 'MergeParts','MutatePart', 'Alter', 'Create') )) as other_types_ram, groupUniqArrayIf(event_type, event_type not IN ('Insert', 'Select', 'MergeParts','MutatePart', 'Alter', 'Create') ) as other_types FROM ( SELECT toDateTime( toUInt32(ts) ) as event_timestamp, t as event_type, SUM(mem) OVER (PARTITION BY t ORDER BY ts) as mem_by_type, SUM(mem) OVER (ORDER BY ts) as mem_overall FROM ( WITH arrayJoin([(toFloat64(event_time_microseconds) - (duration_ms / 1000), toInt64(peak_memory_usage)), (toFloat64(event_time_microseconds), -peak_memory_usage)]) AS data SELECT CAST(event_type,'LowCardinality(String)') as t, data.1 as ts, data.2 as mem FROM system.part_log WHERE event_time BETWEEN min_time AND max_time AND peak_memory_usage != 0 UNION ALL WITH arrayJoin([(toFloat64(query_start_time_microseconds), toInt64(memory_usage)), (toFloat64(event_time_microseconds), -memory_usage)]) AS data SELECT query_kind, data.1 as ts, data.2 as mem FROM system.query_log WHERE event_time BETWEEN min_time AND max_time AND memory_usage != 0 UNION ALL WITH arrayJoin([(toFloat64(event_time_microseconds) - (view_duration_ms / 1000), toInt64(peak_memory_usage)), (toFloat64(event_time_microseconds), -peak_memory_usage)]) AS data SELECT CAST(toString(view_type)||'View','LowCardinality(String)') as t, data.1 as ts, data.2 as mem FROM system.query_views_log WHERE event_time BETWEEN min_time AND max_time AND peak_memory_usage != 0 ) ) GROUP BY timeframe ORDER BY timeframe FORMAT PrettyCompactMonoBlock; retrospection analysis of trace_log WITH now() - INTERVAL 24 HOUR AS min_time, -- you can adjust that now() AS max_time -- you can adjust that SELECT trace_type, count(), topK(20)(query_id) FROM system.trace_log WHERE event_time BETWEEN min_time AND max_time GROUP BY trace_type; -- later on you can check particular query_ids in query_log analysis of the server text logs grep MemoryTracker /var/log/clickhouse-server.log zgrep MemoryTracker /var/log/clickhouse-server.log.*.gz ","categories":"","description":"Who ate my memory\n","excerpt":"Who ate my memory\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/","tags":"","title":"Who ate my memory"},{"body":"Resources: Tutorial: ClickHouse Window Functions Video: Fun with ClickHouse Window Functions Blog: Battle of the Views: ClickHouse Window View vs. Live View How Do I Simulate Window Functions Using Arrays on older versions of clickhouse? Group with groupArray. Calculate the needed metrics. Ungroup back using arrayJoin. NTILE SELECT intDiv((num - 1) - (cnt % 3), 3) AS ntile FROM ( SELECT row_number() OVER (ORDER BY number ASC) AS num, count() OVER () AS cnt FROM numbers(11) ) ┌─ntile─┐ │ 0 │ │ 0 │ │ 0 │ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ │ 2 │ └───────┘ ","categories":"","description":"Window functions\n","excerpt":"Window functions\n","ref":"/altinity-kb-queries-and-syntax/window-functions/","tags":"","title":"Window functions"},{"body":" Warning The local set of parts of table doesn’t look like the set of parts in ZooKeeper. 100.00 rows of 150.00 total rows in filesystem are suspicious. There are 1 unexpected parts with 100 rows (1 of them is not just-written with 100 rows), 0 missing parts (with 0 blocks).: Cannot attach table. ClickHouse has a registry of parts in ZooKeeper.\nAnd during the start ClickHouse compares that list of parts on a local disk is consistent with a list in ZooKeeper. If the lists are too different ClickHouse denies to start because it could be an issue with settings, wrong Shard or wrong Replica macros. But this safe-limiter throws an exception if the difference is more 50% (in rows).\nIn your case the table is very small and the difference \u003e50% ( 100.00 vs 150.00 ) is only a single part mismatch, which can be the result of hard restart.\nSELECT * FROM system.merge_tree_settings WHERE name = 'replicated_max_ratio_of_wrong_parts' ┌─name────────────────────────────────┬─value─┬─changed─┬─description──────────────────────────────────────────────────────────────────────────┬─type──┐ │ replicated_max_ratio_of_wrong_parts │ 0.5 │ 0 │ If ratio of wrong parts to total number of parts is less than this - allow to start. │ Float │ └─────────────────────────────────────┴───────┴─────────┴──────────────────────────────────────────────────────────────────────────────────────┴───────┘ You can set another value of replicated_max_ratio_of_wrong_parts for all MergeTree tables or per table.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings\nAfter manipulation with storage_policies and disks When storage policy changes (one disk was removed from it), ClickHouse compared parts on disk and this replica state in ZooKeeper and found out that a lot of parts (from removed disk) disappeared. So ClickHouse removed them from the replica state in ZooKeeper and scheduled to fetch them from other replicas.\nAfter we add the removed disk to storage_policy back, ClickHouse finds missing parts, but at this moment they are not registered for that replica. ClickHouse produce error message like this:\nWarning Application: DB::Exception: The local set of parts of table default.tbl doesn’t look like the set of parts in ZooKeeper: 14.96 billion rows of 16.24 billion total rows in filesystem are suspicious. There are 45 unexpected parts with 14960302620 rows (43 of them is not just-written with 14959824636 rows), 0 missing parts (with 0 blocks).: Cannot attach table default.tbl from metadata file /var/lib/clickhouse/metadata/default/tbl.sql from query ATTACH TABLE default.tbl … ENGINE=ReplicatedMergeTree(’/clickhouse/tables/0/default/tbl’, ‘replica-0’)… SETTINGS index_granularity = 1024, storage_policy = ’ebs_hot_and_cold’: while loading database default from path /var/lib/clickhouse/metadata/data At this point, it’s possible to either tune setting replicated_max_ratio_of_wrong_parts or do force restore, but it will end up downloading all “missing” parts from other replicas, which can take a lot of time for big tables.\nClickHouse 21.7+ Rename table SQL attach script in order to prevent ClickHouse from attaching it at startup. mv /var/lib/clickhouse/metadata/default/tbl.sql /var/lib/clickhouse/metadata/default/tbl.sql.bak Start ClickHouse server.\nRemove metadata for this replica from ZooKeeper.\nSYSTEM DROP REPLICA 'replica-0' FROM ZKPATH '/clickhouse/tables/0/default/tbl'; SELECT * FROM system.zookeeper WHERE path = '/clickhouse/tables/0/default/tbl/replicas'; Rename table SQL attach script back to normal name. mv /var/lib/clickhouse/metadata/default/tbl.sql.bak /var/lib/clickhouse/metadata/default/tbl.sql Attach table to ClickHouse server, because there is no metadata in ZooKeeper, ClickHouse will attach it in read only state. ATTACH TABLE default.tbl; Run SYSTEM RESTORE REPLICA in order to sync state on disk and in ZooKeeper. SYSTEM RESTORE REPLICA default.tbl; Run SYSTEM SYNC REPLICA to download missing parts from other replicas. SYSTEM SYNC REPLICA default.tbl; ","categories":"","description":"X rows of Y total rows in filesystem are suspicious\n","excerpt":"X rows of Y total rows in filesystem are suspicious\n","ref":"/altinity-kb-setup-and-maintenance/x-rows-of-y-total-rows-in-filesystem-are-suspicious/","tags":"","title":"X rows of Y total rows in filesystem are suspicious"},{"body":"Requirements TLDR version:\nUSE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup). use 3 nodes (more nodes = slower quorum, less = no HA). low network latency between zookeeper nodes is very important (latency, not bandwidth). have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings ensure that zookeeper will not be CPU-starved by some other processes monitor zookeeper. Side note: in many cases, the slowness of the zookeeper is actually a symptom of some issue with clickhouse schema/usage pattern (the most typical issues: an enormous number of partitions/tables/databases with real-time inserts, tiny \u0026 frequent inserts).\nHow to install https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/ altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/ Random links on best practices https://docs.confluent.io/platform/current/zookeeper/deployment.html https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.html#sc_commonProblems https://clickhouse.tech/docs/en/operations/tips/#zookeeper https://lucene.apache.org/solr/guide/7_4/setting-up-an-external-zookeeper-ensemble.html https://cwiki.apache.org/confluence/display/ZOOKEEPER/Troubleshooting Cite from https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_commonProblems :\nThings to Avoid Here are some common problems you can avoid by configuring ZooKeeper correctly:\ninconsistent lists of servers : The list of ZooKeeper servers used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has. Things work okay if the client list is a subset of the real list, but things will really act strange if clients have a list of ZooKeeper servers that are in different ZooKeeper clusters. Also, the server lists in each Zookeeper server configuration file should be consistent with one another. incorrect placement of transaction log : The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log. incorrect Java heap size : You should take special care to set your Java max heap size correctly. In particular, you should not create a situation in which ZooKeeper swaps to disk. The disk is death to ZooKeeper. Everything is ordered, so if processing one request swaps the disk, all other queued requests will probably do the same. the disk. DON’T SWAP. Be conservative in your estimates: if you have 4G of RAM, do not set the Java max heap size to 6G or even 4G. For example, it is more likely you would use a 3G heap for a 4G machine, as the operating system and the cache also need memory. The best and only recommend practice for estimating the heap size your system needs is to run load tests, and then make sure you are well below the usage limit that would cause the system to swap. Publicly accessible deployment : A ZooKeeper ensemble is expected to operate in a trusted computing environment. It is thus recommended to deploy ZooKeeper behind a firewall. How to check number of followers: echo mntr | nc zookeeper 2187 | grep foll zk_synced_followers 2 zk_synced_non_voting_followers 0 zk_avg_follower_sync_time 0.0 zk_min_follower_sync_time 0 zk_max_follower_sync_time 0 zk_cnt_follower_sync_time 0 zk_sum_follower_sync_time 0 Tools https://github.com/apache/zookeeper/blob/master/zookeeper-docs/src/main/resources/markdown/zookeeperTools.md\nAlternative for zkCli https://github.com/go-zkcli/zkcli Web UI https://github.com/elkozmon/zoonavigator-api https://github.com/tobilg/docker-zookeeper-webui https://github.com/vran-dev/PrettyZoo ","categories":"","description":"ZooKeeper\n","excerpt":"ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/","tags":"","title":"ZooKeeper"},{"body":"Question: Do I need to backup Zookeeper Database, because it’s pretty important for ClickHouse?\nTLDR answer: NO, just backup ClickHouse data itself, and do SYSTEM RESTORE REPLICA during recovery to recreate zookeeper data\nDetails:\nZookeeper does not store any data, it stores the STATE of the distributed system (“that replica have those parts”, “still need 2 merges to do”, “alter is being applied” etc). That state always changes, and you can not capture / backup / and recover that state in a safe manner. So even backup from few seconds ago is represending some ‘old state from the past’ which is INCONSISTENT with actual state of the data.\nIn other words - if clickhouse is working - then the state of distributed system always changes, and it’s almost impossible to collect the current state of zookeeper (while you collecting it it will change many times). The only exception is ‘stop-the-world’ scenario - i.e. shutdown all clickhouse nodes, with all other zookeeper clients, then shutdown all the zookeeper, and only then take the backups, in that scenario and backups of zookeeper \u0026 clickhouse will be consistent. In that case restoring the backup is as simple (and is equal to) as starting all the nodes which was stopped before. But usually that scenario is very non-practical because it requires huge downtime.\nSo what to do instead? It’s enought if you will backup clickhouse data itself, and to recover the state of zookeeper you can just run the command SYSTEM RESTORE REPLICA command AFTER restoring the clickhouse data itself. That will recreate the state of the replica in the zookeeper as it exists on the filesystem after backup recovery.\nNormally Zookeeper ensemble consists of 3 nodes, which is enough to survive hardware failures.\nOn older verion (which don’t have SYSTEM RESTORE REPLICA command - it can be done manually, using instruction https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#converting-from-mergetree-to-replicatedmergetree), on scale you can try https://github.com/Altinity/clickhouse-zookeeper-recovery\n","categories":"","description":"ZooKeeper backup\n","excerpt":"ZooKeeper backup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-backup/","tags":"","title":"ZooKeeper backup"},{"body":"Here is a plan for ZK 3.4.9 (no dynamic reconfiguration):\nAdd the 3 new ZK nodes to the old cluster. No changes needed for the 3 old ZK nodes at this time. Configure one of the new ZK nodes as a cluster of 4 nodes (3 old + 1 new), start it. Configure the other two new ZK nodes as a cluster of 6 nodes (3 old + 3 new), start them. Make sure the 3 new ZK nodes connected to the old ZK cluster as followers (run echo stat | nc localhost 2181 on the 3 new ZK nodes) Confirm that the leader has 5 synced followers (run echo mntr | nc localhost 2181 on the leader, look for zk_synced_followers) Stop data ingestion in CH (this is to minimize errors when CH loses ZK). Change the zookeeper section in the configs on the CH nodes (remove the 3 old ZK servers, add the 3 new ZK servers) Make sure that there are no connections from CH to the 3 old ZK nodes (run echo stat | nc localhost 2181 on the 3 old nodes, check their Clients section). Restart all CH nodes if necessary (In some cases CH can reconnect to different ZK servers without a restart). Remove the 3 old ZK nodes from zoo.cfg on the 3 new ZK nodes. Restart the 3 new ZK nodes. They should form a cluster of 3 nodes. When CH reconnects to ZK, start data loading. Turn off the 3 old ZK nodes. This plan works, but it is not the only way to do this, it can be changed if needed.\n","categories":"","description":"ZooKeeper cluster migration\n","excerpt":"ZooKeeper cluster migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration/","tags":"","title":"ZooKeeper cluster migration"},{"body":"ZooKeeper scrape metrics embedded exporter since version 3.6.0 https://zookeeper.apache.org/doc/r3.6.2/zookeeperMonitor.html standalone exporter https://github.com/dabealu/zookeeper-exporter Install dashboards embedded exporter https://grafana.com/grafana/dashboards/10465 dabealu exporter https://grafana.com/grafana/dashboards/11442 See also https://grafana.com/grafana/dashboards?search=ZooKeeper\u0026amp;dataSource=prometheus\nsetup alert rules embedded exporter link See also https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/#zookeeper-metrics https://dzone.com/articles/monitoring-apache-zookeeper-servers - note exhibitor is no longer maintained https://github.com/samber/awesome-prometheus-alerts/blob/c3ba0cf1997c7e952369a090aeb10343cdca4878/_data/rules.yml#L1146-L1170 (or https://awesome-prometheus-alerts.grep.to/rules.html#zookeeper ) https://alex.dzyoba.com/blog/prometheus-alerts/ https://docs.datadoghq.com/integrations/zk/?tab=host https://statuslist.app/uptime-monitoring/zookeeper/ ","categories":"","description":"ZooKeeper Monitoring\n","excerpt":"ZooKeeper Monitoring\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring/","tags":"","title":"ZooKeeper Monitoring"},{"body":"/metadata Table schema.\ndate column -\u003e legacy MergeTree partition expression. sampling expression -\u003e SAMPLE BY index granularity -\u003e index_granularity mode -\u003e type of MergeTree table sign column -\u003e sign - CollapsingMergeTree / VersionedCollapsingMergeTree primary key -\u003e ORDER BY key if PRIMARY KEY not defined. sorting key -\u003e ORDER BY key if PRIMARY KEY defined. data format version -\u003e 1 partition key -\u003e PARTITION BY granularity bytes -\u003e index_granularity_bytes types of MergeTree tables: Ordinary = 0 Collapsing = 1 Summing = 2 Aggregating = 3 Replacing = 5 Graphite = 6 VersionedCollapsing = 7 /mutations Log of latest mutations\n/columns List of columns for latest (reference) table version. Replicas would try to reach this state.\n/log Log of latest actions with table.\nRelated settings:\n┌─name────────────────────────┬─value─┬─changed─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─type───┐ │ max_replicated_logs_to_keep │ 1000 │ 0 │ How many records may be in log, if there is inactive replica. Inactive replica becomes lost when when this number exceed. │ UInt64 │ │ min_replicated_logs_to_keep │ 10 │ 0 │ Keep about this number of last records in ZooKeeper log, even if they are obsolete. It doesn't affect work of tables: used only to diagnose ZooKeeper log before cleaning. │ UInt64 │ └─────────────────────────────┴───────┴─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────┘ /replicas List of table replicas.\n/replicas/replica_name/ /replicas/replica_name/mutation_pointer Pointer to the latest mutation executed by replica\n/replicas/replica_name/log_pointer Pointer to the latest task from replication_queue executed by replica\n/replicas/replica_name/max_processed_insert_time /replica/replica_name/metadata Table schema of specific replica\n/replica/replica_name/columns Columns list of specific replica.\n/quorum Used for quorum inserts.\n","categories":"","description":"ZooKeeper schema\n","excerpt":"ZooKeeper schema\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-schema/","tags":"","title":"ZooKeeper schema"}]