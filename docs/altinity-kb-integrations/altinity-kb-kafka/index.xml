<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on Altinity Beta Knowledgebase</title>
    <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/</link>
    <description>Recent content in Kafka on Altinity Beta Knowledgebase</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adjusting librdkafka settings</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/</guid>
      <description>To set rdkafka options - add to &amp;lt;kafka&amp;gt; section in config.xml or preferably use a separate file in config.d/:  https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md    Some random example:
&amp;lt;kafka&amp;gt; &amp;lt;max_poll_interval_ms&amp;gt;60000&amp;lt;/max_poll_interval_ms&amp;gt; &amp;lt;session_timeout_ms&amp;gt;60000&amp;lt;/session_timeout_ms&amp;gt; &amp;lt;heartbeat_interval_ms&amp;gt;10000&amp;lt;/heartbeat_interval_ms&amp;gt; &amp;lt;reconnect_backoff_ms&amp;gt;5000&amp;lt;/reconnect_backoff_ms&amp;gt; &amp;lt;reconnect_backoff_max_ms&amp;gt;60000&amp;lt;/reconnect_backoff_max_ms&amp;gt; &amp;lt;request_timeout_ms&amp;gt;20000&amp;lt;/request_timeout_ms&amp;gt; &amp;lt;retry_backoff_ms&amp;gt;500&amp;lt;/retry_backoff_ms&amp;gt; &amp;lt;message_max_bytes&amp;gt;20971520&amp;lt;/message_max_bytes&amp;gt; &amp;lt;debug&amp;gt;all&amp;lt;/debug&amp;gt;&amp;lt;!-- only to get the errors --&amp;gt; &amp;lt;security_protocol&amp;gt;SSL&amp;lt;/security_protocol&amp;gt; &amp;lt;ssl_ca_location&amp;gt;/etc/clickhouse-server/ssl/kafka-ca-qa.crt&amp;lt;/ssl_ca_location&amp;gt; &amp;lt;ssl_certificate_location&amp;gt;/etc/clickhouse-server/ssl/client_clickhouse_client.pem&amp;lt;/ssl_certificate_location&amp;gt; &amp;lt;ssl_key_location&amp;gt;/etc/clickhouse-server/ssl/client_clickhouse_client.key&amp;lt;/ssl_key_location&amp;gt; &amp;lt;ssl_key_password&amp;gt;pass&amp;lt;/ssl_key_password&amp;gt; &amp;lt;/kafka&amp;gt; Authentication / connectivity  Amazon MSK  &amp;lt;yandex&amp;gt; &amp;lt;kafka&amp;gt; &amp;lt;security_protocol&amp;gt;sasl_ssl&amp;lt;/security_protocol&amp;gt; &amp;lt;sasl_username&amp;gt;root&amp;lt;/sasl_username&amp;gt; &amp;lt;sasl_password&amp;gt;toor&amp;lt;/sasl_password&amp;gt; &amp;lt;/kafka&amp;gt; &amp;lt;/yandex&amp;gt; https://leftjoin.ru/all/clickhouse-as-a-consumer-to-amazon-msk/
Inline Kafka certs  To connect to some Kafka cloud services you may need to use certificates.</description>
    </item>
    
    <item>
      <title>Error handling</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/error-handling/</guid>
      <description>Pre 21.6 There are couple options:
Certain formats which has schema in built in them (like JSONEachRow) could silently skip any unexpected fields after enabling setting input_format_skip_unknown_fields
It&amp;rsquo;s also possible to skip up to N malformed messages for each block, with used setting kafka_skip_broken_messages but it&amp;rsquo;s also does not support all possible formats.
After 21.6 It&amp;rsquo;s possible to stream messages which could not be parsed, this behavior could be enabled via setting: kafka_handle_error_mode=&#39;stream&#39; and clickhouse wil write error and message from Kafka itself to two new virtual columns: _error, _raw_message.</description>
    </item>
    
    <item>
      <title>Exactly once semantics</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/</guid>
      <description>EOS consumer (isolation.level=read_committed) is enabled by default since librdkafka 1.2.0, so for ClickHouse - since 20.2
See:
 edenhill/librdkafka@6b2a155 9de5dff  There was a report recently that it was giving some duplicates #18668 and in should be fixed in 21.2.
BUT: while EOS semantics will guarantee you that no duplicates will happen on the Kafka side (i.e. even if you produce the same messages few times it will be consumed once), but ClickHouse as a Kafka client can currently guarantee only at-least-once.</description>
    </item>
    
    <item>
      <title>Kafka main parsing loop</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/</guid>
      <description>One of the threads from scheduled_pool (pre 20.9) / background_message_broker_schedule_pool (after 20.9) do that in infinite loop:
 Batch poll (time limit: kafka_poll_timeout_ms 500ms, messages limit: kafka_poll_max_batch_size 65536) Parse messages. If we don&amp;rsquo;t have enough data (rows limit: kafka_max_block_size 1048576) or time limit reached (kafka_flush_interval_ms 7500ms) - continue polling (goto p.1) Write a collected block of data to MV Do commit (commit after write = at-least-once).  On any error, during that process, Kafka client is restarted (leading to rebalancing - leave the group and get back in few seconds).</description>
    </item>
    
    <item>
      <title>Kafka parallel consuming</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/</guid>
      <description>For very large topics when you need more parallelism (especially on the insert side) you may use several tables with the same pipeline (pre 20.9) or enable kafka_thread_per_consumer (after 20.9).
kafka_num_consumers = N, kafka_thread_per_consumer=1 Notes:
 the inserts will happen in parallel (without that setting inserts happen linearly) enough partitions are needed.  Before increasing kafka_num_consumers with keeping kafka_thread_per_consumer=0 may improve consumption &amp;amp; parsing speed, but flushing &amp;amp; committing still happens by a single thread there (so inserts are linear).</description>
    </item>
    
    <item>
      <title>Rewind / fast-forward / replay</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/</guid>
      <description>Step 1: Detach Kafka tables in ClickHouse Step 2: kafka-consumer-groups.sh --bootstrap-server kafka:9092 --topic topic:0,1,2 --group id1 --reset-offsets --to-latest --execute  More samples: https://gist.github.com/filimonov/1646259d18b911d7a1e8745d6411c0cc   Step: Attach Kafka tables back  See also these configuration settings:
&amp;lt;kafka&amp;gt; &amp;lt;auto_offset_reset&amp;gt;smallest&amp;lt;/auto_offset_reset&amp;gt; &amp;lt;/kafka&amp;gt; Â© 2021 Altinity Inc. All rights reserved.</description>
    </item>
    
    <item>
      <title>SELECTs from engine=Kafka</title>
      <link>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/</guid>
      <description>Question What will happen, if we would run SELECT query from working Kafka table with MV attached? Would data showed in SELECT query appear later in MV destination table?
Answer  Most likely SELECT query would show nothing. If you lucky enough and something would show up, those rows wouldn&amp;rsquo;t appear in MV destination table.  So it&amp;rsquo;s not recommended to run SELECT queries on working Kafka tables.
In case of debug it&amp;rsquo;s possible to use another Kafka table with different consumer_group, so it wouldn&amp;rsquo;t affect your main pipeline.</description>
    </item>
    
  </channel>
</rss>
