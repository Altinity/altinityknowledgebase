<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Migration on Altinity Beta Knowledgebase</title>
    <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/</link>
    <description>Recent content in Data Migration on Altinity Beta Knowledgebase</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-...-table-function/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-...-table-function/</guid>
      <description>remote(&amp;hellip;) table function Suitable for moving up to hundreds of gigabytes of data.
With bigger tables recommended approach is to slice the original data by some WHERE condition, ideally - apply the condition on partitioning key, to avoid writing data to many partitions at once.
INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate=&amp;#39;2021-04-13&amp;#39;;INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate=&amp;#39;2021-04-12&amp;#39;;INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate=&amp;#39;2021-04-11&amp;#39;;....�Q. Can it create a bigger load on the source system? Yes, it may use disk read &amp;amp; network write bandwidth. But typically write speed is worse than the read speed, so most probably the receiver side will be a bottleneck, and the sender side will not be overloaded.</description>
    </item>
    
    <item>
      <title>clickhouse-copier</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/readme/</guid>
      <description>The description of the utility and its parameters, as well as examples of the config files that you need to create for the copier are in the doc https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/
The steps to run a task:
  Create a config file for clickhouse-copier (zookeeper.xml)
https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#format-of-zookeeper-xml
  Create a config file for the task (task1.xml)
https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#configuration-of-copying-tasks
  Create the task in ZooKeeper and start an instance of clickhouse-copierclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.</description>
    </item>
    
    <item>
      <title>clickhouse-copier 20.3 and earlier</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/</guid>
      <description>Clickhouse-copier was created to move data between clusters.
It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards.
In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions.
Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.</description>
    </item>
    
    <item>
      <title>clickhouse-copier 20.4&#43;</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4&#43;/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4&#43;/</guid>
      <description>Clickhouse-copier was created to move data between clusters.
It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards.
In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions.
Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.</description>
    </item>
    
    <item>
      <title>rsync</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/</guid>
      <description>Short Instruction   Do FREEZE TABLE on needed table, partition. It would produce consistent snapshot of table data.
  Run rsync command.
rsync -ravlW --bwlimit=100000 /var/lib/clickhouse/data/shadow/N/database/table root@remote_host:/var/lib/clickhouse/data/database/table/detached --bwlimit is transfer limit in KBytes per second.
  RunATTACH PARTITION for each partition from ./detached directory.
  © 2021 Altinity Inc. All rights reserved.</description>
    </item>
    
  </channel>
</rss>
