<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Production Cluster Configuration Guide on Altinity Beta Knowledgebase</title>
    <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/</link>
    <description>Recent content in Production Cluster Configuration Guide on Altinity Beta Knowledgebase</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backups</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/</guid>
      <description>ClickHouse is currently at the design stage of creating some universal backup solution. Some custom backup strategies are:
 Each shard is backed up separately. FREEZE the table/partition. For more information, see Alter Freeze Partition.  This creates hard links in shadow subdirectory.   rsync that directory to a backup location, then remove that subfolder from shadow.  Cloud users are recommended to use Rclone.   Always add the full contents of the metadata subfolder that contains the current DB schema and clickhouse configs to your backup.</description>
    </item>
    
    <item>
      <title>Cluster Configuration FAQ</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/</guid>
      <description>ClickHouse does not start, some other unexpected behavior happening. Check clickhouse logs, they are your friends:
tail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log | less
tail -n 10000 /var/log/clickhouse-server/clickhouse-server.log | less
How Do I Restrict Memory Usage? See our knowledge base article and official documentation for more information.
ClickHouse died during big query execution Misconfigured clickhouse can try to allocate more RAM than is available on the system.
In that case an OS component called oomkiller can kill the clickhouse process.</description>
    </item>
    
    <item>
      <title>Cluster Configuration Process</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/</guid>
      <description>So you set up 3 nodes with zookeeper (zookeeper1, zookeeper2, zookeeper3 - How to install zookeer?), and and 4 nodes with ClickHouse (clickhouse-sh1r1,clickhouse-sh1r2,clickhouse-sh2r1,clickhouse-sh2r2 - how to install ClickHouse?). Now we need to make them work together.
Use ansible/puppet/salt or other systems to control the serversâ€™ configurations.
 Configure ClickHouse access to Zookeeper by adding the file zookeeper.xml in /etc/clickhouse-server/config.d/ folder. This file must be placed on all ClickHouse servers.  &amp;lt;yandex&amp;gt; &amp;lt;zookeeper&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper1&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper2&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper3&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;/zookeeper&amp;gt; &amp;lt;/yandex&amp;gt;  On each server put the file macros.</description>
    </item>
    
    <item>
      <title>Hardware Requirements</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/</guid>
      <description>ClickHouse ClickHouse will use all available hardware to maximize performance. So the more hardware - the better. As of this publication, the hardware requirements are:
 Minimum Hardware: 4-core CPU with support of SSE4.2, 16 Gb RAM, 1Tb HDD.  Recommended for development and staging environments. SSE4.2 is required, and going below 4 Gb of RAM is not recommended.   Recommended Hardware: &amp;gt;=16-cores, &amp;gt;=64Gb RAM, HDD-raid or SSD.  For processing up to hundreds of millions / billions of rows.</description>
    </item>
    
    <item>
      <title>Monitoring Considerations</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/monitoring-considerations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/monitoring-considerations/</guid>
      <description>Monitoring helps to track potential issues in your cluster before they cause a critical error.
External Monitoring External monitoring collects data from the ClickHouse cluster and uses it for analysis and review. Recommended external monitoring systems include:
 Prometheus: Use embedded exporter or clickhouse-exporter Graphite: Use the embedded exporter. See config.xml. InfluxDB: Use the embedded exporter, plus Telegraf. For more information, see Graphite protocol support in InfluxDB.  ClickHouse can collect the recording of metrics internally by enabling system.</description>
    </item>
    
    <item>
      <title>Network Configuration</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/</guid>
      <description>Networking And Server Room Planning The network used for your ClickHouse cluster should be a fast network, ideally 10 Gbit. ClickHouse nodes generate a lot of traffic along with the Zookeeper connections and inter-Zookeeper communications.
For the zookeeper low latency is more important than bandwidth.
Keep the replicas isolated on the hardware level. This allows for cluster failover from possible outages.
 For Physical Environments: Avoid placing 2 ClickHouse replicas on the same server rack.</description>
    </item>
    
    <item>
      <title>Version Upgrades</title>
      <link>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/version-upgrades/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://beta.kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/version-upgrades/</guid>
      <description>Update itself is simple: update packages, restart clickhouse-server service afterwards.
 Check if the version you want to upgrade to is stable. We highly recommend the Altinity ClickHouse Stable Releases.  Review the changelog to ensure that no configuration changes are needed.   Update staging and test to verify all systems are working. Prepare and test downgrade procedures so the server can be returned to the previous version if necessary.</description>
    </item>
    
  </channel>
</rss>
