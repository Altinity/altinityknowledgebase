
























































































































































[{"body":" title: “Mutations” linkTitle: “Mutations” description: \u003e Mutations Q. How to know if ALTER TABLE … DELETE/UPDATE mutation ON CLUSTER was finished successfully on all the nodes?\nA. mutation status in system.mutations is local to each replica, so use\nSELECT hostname(), * FROM clusterAllReplicas('your_cluster_name', system.mutations); -- you can also add WHERE conditions to that query if needed. Look on is_done and latest_fail_reason columns\n","categories":"","description":"ALTER UPDATE / DELETE","excerpt":"ALTER UPDATE / DELETE","ref":"/altinity-kb-queries-and-syntax/mutations/","tags":"","title":""},{"body":" title: “PIVOT / UNPIVOT” linkTitle: “PIVOT / UNPIVOT” description: \u003e PIVOT / UNPIVOT PIVOT CREATEORREPLACETABLEmonthly_sales(empidINT,amountINT,monthTEXT)ENGINE=Memory();INSERTINTOmonthly_salesVALUES(1,10000,'JAN'),(1,400,'JAN'),(2,4500,'JAN'),(2,35000,'JAN'),(1,5000,'FEB'),(1,3000,'FEB'),(2,200,'FEB'),(2,90500,'FEB'),(1,6000,'MAR'),(1,5000,'MAR'),(2,2500,'MAR'),(2,9500,'MAR'),(1,8000,'APR'),(1,10000,'APR'),(2,800,'APR'),(2,4500,'APR');SETallow_experimental_map_type=1;WITHCAST(sumMap([month],[amount]),'Map(String, UInt32)')ASmapSELECTempid,map['JAN']ASJAN,map['FEB']ASFEB,map['MAR']ASMAR,map['APR']ASAPRFROMmonthly_salesGROUPBYempidORDERBYempidASC┌─empid─┬───JAN─┬───FEB─┬───MAR─┬───APR─┐│1│10400│8000│11000│18000││2│39500│90700│12000│5300│└───────┴───────┴───────┴───────┴───────┘SELECTempid,sumIf(amount,month='JAN')ASJAN,sumIf(amount,month='FEB')ASFEB,sumIf(amount,month='MAR')ASMAR,sumIf(amount,month='APR')ASAPRFROMmonthly_salesGROUPBYempidORDERBYempidASC┌─empid─┬───JAN─┬───FEB─┬───MAR─┬───APR─┐│1│10400│8000│11000│18000││2│39500│90700│12000│5300│└───────┴───────┴───────┴───────┴───────┘UNPIVOT CREATEORREPLACETABLEmonthly_sales(empidINT,deptTEXT,janINT,febINT,marINT,aprilINT)ENGINE=Memory();INSERTINTOmonthly_salesVALUES(1,'electronics',100,200,300,100),(2,'clothes',100,300,150,200),(3,'cars',200,400,100,50);SELECTempid,dept,month,salesFROMmonthly_salesARRAYJOIN[jan,feb,mar,april]ASsales,splitByString(', ','jan, feb, mar, april')ASmonthORDERBYempidASC┌─empid─┬─dept────────┬─month─┬─sales─┐│1│electronics│jan│100││1│electronics│feb│200││1│electronics│mar│300││1│electronics│april│100││2│clothes│jan│100││2│clothes│feb│300││2│clothes│mar│150││2│clothes│april│200││3│cars│jan│200││3│cars│feb│400││3│cars│mar│100││3│cars│april│50│└───────┴─────────────┴───────┴───────┘","categories":"","description":"https://docs.snowflake.com/en/sql-reference/constructs/pivot.html","excerpt":"https://docs.snowflake.com/en/sql-reference/constructs/pivot.html","ref":"/altinity-kb-queries-and-syntax/pivot-unpivot/","tags":"","title":""},{"body":" title: “Sampling Example” linkTitle: “Sampling Example” description: \u003e Sampling Example The most important idea about sampling that the primary index must have low cardinality. The following example demonstrates how sampling can be setup correctly, and an example if it being set up incorrectly as a comparison.\nSampling requires sample by expression . This ensures a range of sampled column types fit within a specified range, which ensures the requirement of low cardinality. In this example, I cannot use transaction_id because I can not ensure that the min value of transaction_id = 0 and max value = MAX_UINT64. Instead, I used cityHash64(transaction_id)to expand the range within the minimum and maximum values.\nFor example if all values of transaction_id are from 0 to 10000 sampling will be inefficient. But cityHash64(transaction_id) expands the range from 0 to 18446744073709551615:\nSELECTcityHash64(10000)┌────cityHash64(10000)─┐│14845905981091347439│└──────────────────────┘If I used transaction_id without knowing that they matched the allowable ranges, the results of sampled queries would be skewed. For example, when using sample 0.5, ClickHouse requests where sample_col \u003e= 0 and sample_col \u003c= MAX_UINT64/2.\nAlso you can include multiple columns into a hash function of the sampling expression to improve randomness of the distribution cityHash64(transaction_id, banner_id).\nSampling Friendly Table CREATETABLEtable_one(timestampUInt64,transaction_idUInt64,banner_idUInt16,valueUInt32)ENGINE=MergeTree()PARTITIONBYtoYYYYMMDD(toDateTime(timestamp))ORDERBY(banner_id,toStartOfHour(toDateTime(timestamp)),cityHash64(transaction_id))SAMPLEBYcityHash64(transaction_id)SETTINGSindex_granularity=8192insertintotable_oneselect1602809234+intDiv(number,100000),number,number%991,toUInt32(rand())fromnumbers(10000000000);I reduced the granularity of the timestamp column to one hour with toStartOfHour(toDateTime(timestamp)) , otherwise sampling will not work.\nVerifying Sampling Works The following shows that sampling works with the table and parameters described above. Notice the Elapsed time when invoking sampling:\n-- Q1. No where filters. -- The query is 10 times faster with SAMPLE 0.01 selectbanner_id,sum(value),count(value),max(value)fromtable_onegroupbybanner_idformatNull;0rowsinset.Elapsed:11.490sec.Processed10.00billionrows,60.00GB(870.30millionrows/s.,5.22GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_idformatNull;0rowsinset.Elapsed:1.316sec.Processed452.67millionrows,6.34GB(343.85millionrows/s.,4.81GB/s.)-- Q2. Filter by the first column in index (banner_id = 42) -- The query is 20 times faster with SAMPLE 0.01 -- reads 20 times less rows: 10.30 million rows VS Processed 696.32 thousand rows selectbanner_id,sum(value),count(value),max(value)fromtable_oneWHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.020sec.Processed10.30millionrows,61.78MB(514.37millionrows/s.,3.09GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01WHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.008sec.Processed696.32thousandrows,9.75MB(92.49millionrows/s.,1.29GB/s.)-- Q3. No filters -- The query is 10 times faster with SAMPLE 0.01 -- reads 20 times less rows. selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_onegroupbybanner_id,hrformatNull;0rowsinset.Elapsed:36.660sec.Processed10.00billionrows,140.00GB(272.77millionrows/s.,3.82GB/s.)selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_id,hrformatNull;0rowsinset.Elapsed:3.741sec.Processed452.67millionrows,9.96GB(121.00millionrows/s.,2.66GB/s.)-- Q4. Filter by not indexed column -- The query is 6 times faster with SAMPLE 0.01 -- reads 20 times less rows. selectcount()fromtable_onewherevalue=666formatNull;1rowsinset.Elapsed:6.056sec.Processed10.00billionrows,40.00GB(1.65billionrows/s.,6.61GB/s.)selectcount()fromtable_oneSAMPLE0.01wherevalue=666formatNull;1rowsinset.Elapsed:1.214sec.Processed452.67millionrows,5.43GB(372.88millionrows/s.,4.47GB/s.)Non-Sampling Friendly Table CREATETABLEtable_one(timestampUInt64,transaction_idUInt64,banner_idUInt16,valueUInt32)ENGINE=MergeTree()PARTITIONBYtoYYYYMMDD(toDateTime(timestamp))ORDERBY(banner_id,timestamp,cityHash64(transaction_id))SAMPLEBYcityHash64(transaction_id)SETTINGSindex_granularity=8192insertintotable_oneselect1602809234+intDiv(number,100000),number,number%991,toUInt32(rand())fromnumbers(10000000000);This is the same as our other table, BUT granularity of timestamp column is not reduced.\nVerifying Sampling Does Not Work The following tests shows that sampling is not working because of the lack of timestamp granularity. The Elapsed time is longer when sampling is used.\n-- Q1. No where filters. -- The query is 2 times SLOWER!!! with SAMPLE 0.01 -- Because it needs to read excessive column with sampling data! selectbanner_id,sum(value),count(value),max(value)fromtable_onegroupbybanner_idformatNull;0rowsinset.Elapsed:11.196sec.Processed10.00billionrows,60.00GB(893.15millionrows/s.,5.36GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_idformatNull;0rowsinset.Elapsed:24.378sec.Processed10.00billionrows,140.00GB(410.21millionrows/s.,5.74GB/s.)-- Q2. Filter by the first column in index (banner_id = 42) -- The query is SLOWER with SAMPLE 0.01 selectbanner_id,sum(value),count(value),max(value)fromtable_oneWHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.022sec.Processed10.27millionrows,61.64MB(459.28millionrows/s.,2.76GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01WHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.037sec.Processed10.27millionrows,143.82MB(275.16millionrows/s.,3.85GB/s.)-- Q3. No filters -- The query is SLOWER with SAMPLE 0.01 selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_onegroupbybanner_id,hrformatNull;0rowsinset.Elapsed:21.663sec.Processed10.00billionrows,140.00GB(461.62millionrows/s.,6.46GB/s.)selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_id,hrformatNull;0rowsinset.Elapsed:26.697sec.Processed10.00billionrows,220.00GB(374.57millionrows/s.,8.24GB/s.)-- Q4. Filter by not indexed column -- The query is SLOWER with SAMPLE 0.01 selectcount()fromtable_onewherevalue=666formatNull;0rowsinset.Elapsed:7.679sec.Processed10.00billionrows,40.00GB(1.30billionrows/s.,5.21GB/s.)selectcount()fromtable_oneSAMPLE0.01wherevalue=666formatNull;0rowsinset.Elapsed:21.668sec.Processed10.00billionrows,120.00GB(461.51millionrows/s.,5.54GB/s.)","categories":"","description":"Clickhouse table sampling example","excerpt":"Clickhouse table sampling example","ref":"/altinity-kb-queries-and-syntax/sampling-example/","tags":"","title":""},{"body":" title: “skip index bloom_filter for array column” linkTitle: “skip index bloom_filter for array column” description: \u003e skip index bloom_filter for array column tested with 20.8.17.25\n{% embed url=“https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes” %}\nLet’s create test data createtablebftest(kInt64,xArray(Int64))Engine=MergeTreeorderbyk;insertintobftestselectnumber,arrayMap(i-\u003erand64()%565656,range(10))fromnumbers(10000000);insertintobftestselectnumber,arrayMap(i-\u003erand64()%565656,range(10))fromnumbers(100000000);Base point (no index) selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.495sec.Processed110.00millionrows,9.68GB(222.03millionrows/s.,19.54GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.505sec.Processed110.00millionrows,9.68GB(217.69millionrows/s.,19.16GB/s.)As you can see Clickhouse read 110.00 million rows and the query elapsed Elapsed: 0.505 sec.\nLet’s add an index altertablebftestaddindexix1(x)TYPEbloom_filterGRANULARITY3;-- GRANULARITY 3 means how many table granules will be in the one index granule -- In our case 1 granule of skip index allows to check and skip 3*8192 rows. -- Every dataset is unique sometimes GRANULARITY 1 is better, sometimes -- GRANULARITY 10. -- Need to test on the real data. optimizetablebftestfinal;-- I need to optimize my table because an index is created for only -- new parts (inserted or merged) -- optimize table final re-writes all parts, but with an index. -- probably in your production you don't need to optimize -- because your data is rotated frequently. -- optimize is a heavy operation, better never run optimize table final in a -- production. test bloom_filter GRANULARITY 3 selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.063sec.Processed5.41millionrows,475.79MB(86.42millionrows/s.,7.60GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.042sec.Processed1.13millionrows,99.48MB(26.79millionrows/s.,2.36GB/s.)As you can see I got 10 times boost.\nLet’s try to reduce GRANULARITY to drop by 1 table granule altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filterGRANULARITY1;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.051sec.Processed3.64millionrows,320.08MB(71.63millionrows/s.,6.30GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.050sec.Processed2.06millionrows,181.67MB(41.53millionrows/s.,3.65GB/s.)No improvement :(\nLet’s try to change the false/true probability of the bloom_filter bloom_filter(0.05) altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filter(0.05)GRANULARITY3;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.079sec.Processed8.95millionrows,787.22MB(112.80millionrows/s.,9.93GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.058sec.Processed3.86millionrows,339.54MB(66.83millionrows/s.,5.88GB/s.)No improvement.\nbloom_filter(0.01) altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filter(0.01)GRANULARITY3;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.069sec.Processed5.26millionrows,462.82MB(76.32millionrows/s.,6.72GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.047sec.Processed737.28thousandrows,64.88MB(15.72millionrows/s.,1.38GB/s.)Also no improvement :(\nOutcome: I would use TYPE bloom_filter GRANULARITY 3.\n2021 Altinity Inc. All rights reserved.\n","categories":"","description":"Example: skip index bloom_filter \u0026 array column","excerpt":"Example: skip index bloom_filter \u0026 array column","ref":"/altinity-kb-queries-and-syntax/skip-indexes/skip-index-bloom_filter-for-array-column/","tags":"","title":""},{"body":" title: “TTL GROUP BY Examples” linkTitle: “TTL GROUP BY Examples” description: \u003e TTL GROUP BY Examples Example with MergeTree table CREATETABLEtest_ttl_group_by(`key`UInt32,`ts`DateTime,`value`UInt32,`min_value`UInt32DEFAULTvalue,`max_value`UInt32DEFAULTvalue)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,toStartOfDay(ts))TTLts+interval30dayGROUPBYkey,toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts));During TTL merges Clickhouse re-calculates values of columns in the SET section.\nGROUP BY section should be a prefix of a table’s ORDER BY.\n-- stop merges to demonstrate data before / after -- a rolling up SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()+number,1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval60day+number,2FROMnumbers(100);SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│100│200│2│2││202104│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│5│200│2│2││202104│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘As you can see 100 rows were rolled up into 5 rows (key has 5 values) for rows older than 30 days.\nExample with SummingMergeTree table CREATETABLEtest_ttl_group_by(`key1`UInt32,`key2`UInt32,`ts`DateTime,`value`UInt32,`min_value`SimpleAggregateFunction(min,UInt32)DEFAULTvalue,`max_value`SimpleAggregateFunction(max,UInt32)DEFAULTvalue)ENGINE=SummingMergeTreePARTITIONBYtoYYYYMM(ts)PRIMARYKEY(key1,key2,toStartOfDay(ts))ORDERBY(key1,key2,toStartOfDay(ts),ts)TTLts+interval30dayGROUPBYkey1,key2,toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts));-- stop merges to demonstrate data before / after -- a rolling up SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60),1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60),1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60-toIntervalDay(60)),2FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60-toIntervalDay(60)),2FROMnumbers(100);SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│200│400│2│2││202104│200│200│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│1│400│2│2││202104│100│200│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘During merges Clickhouse re-calculates ts columns as min(toStartOfDay(ts)). It’s possible only for the last column of SummingMergeTree ORDER BY section ORDER BY (key1, key2, toStartOfDay(ts), ts) otherwise it will break the order of rows in the table.\n","categories":"","description":"TTL GROUP BY -- a feature which allows to roll-up old rows. For example to reduce data granularity to one hour for rows older than one month.","excerpt":"TTL GROUP BY -- a feature which allows to roll-up old rows. For …","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/","tags":"","title":""},{"body":" title: “TTL Recompress example” linkTitle: “TTL Recompress example” description: \u003e TTL Recompress example CREATETABLEhits(`banner_id`UInt64,`event_time`DateTimeCODEC(Delta,Default),`c_name`String,`c_cost`Float64)ENGINE=MergeTreePARTITIONBYtoYYYYMM(event_time)ORDERBY(banner_id,event_time)TTLevent_time+toIntervalMonth(1)RECOMPRESSCODEC(ZSTD(1)),event_time+toIntervalMonth(6)RECOMPRESSCODEC(ZSTD(6);Default comression is LZ4 https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-compression\nThese TTL rules recompress data after 1 and 6 months.\nCODEC(Delta, Default) – Default means to use default compression (LZ4 -\u003e ZSTD1 -\u003e ZSTD6) in this case.\n","categories":"","description":"Clickhouse able to recomress old data with more \"heavy\" compressor.","excerpt":"Clickhouse able to recomress old data with more \"heavy\" compressor.","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-recompress-example/","tags":"","title":""},{"body":"remote(…) table function Suitable for moving up to hundreds of gigabytes of data.\nWith bigger tables recommended approach is to slice the original data by some WHERE condition, ideally - apply the condition on partitioning key, to avoid writing data to many partitions at once.\nINSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-13';INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-12';INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-11';....\u0000Q. Can it create a bigger load on the source system? Yes, it may use disk read \u0026 network write bandwidth. But typically write speed is worse than the read speed, so most probably the receiver side will be a bottleneck, and the sender side will not be overloaded.\nWhile of course it should be checked, every case is different.\nQ. Can I tune INSERT speed to make it faster? Yes, by the cost of extra memory usage (on the receiver side).\nClickhouse tries to form blocks of data in memory and while one of limit: min_insert_block_size_rows or min_insert_block_size_bytes being hit, clickhouse dump this block on disk. If clickhouse tries to execute insert in parallel (max_insert_threads \u003e 1), it would form multiple blocks at one time.\nSo maximum memory usage can be calculated like this: max_insert_threads * first(min_insert_block_size_rows OR min_insert_block_size_bytes)\nDefault values:\n┌─name────────────────────────┬─value─────┐│min_insert_block_size_rows│1048545││min_insert_block_size_bytes│268427520││max_insert_threads│0│\u003c-Values0or1meansthatINSERTSELECTisnotruninparallel.└─────────────────────────────┴───────────┘Tune those settings depending on your table average row size and amount of memory which are safe to occupy by INSERT SELECT query.\nQ. I’ve got the error “All connection tries failed” SELECTcount()FROMremote('server.from.remote.dc:9440','default.table','admin','password')Receivedexceptionfromserver(version20.8.11):Code:519.DB::Exception:Receivedfromlocalhost:9000.DB::Exception:Allattemptstogettablestructurefailed.Log:Code:279,e.displayText()=DB::NetException:Allconnectiontriesfailed.Log:Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild))Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild))Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild)) Using remote(…) table function with secure TCP port (default values is 9440). There is remoteSecure() function for that. High (\u003e50ms) ping between servers, values for connect_timeout_with_failover_ms, connect_timeout_with_failover_secure_ms need’s to be adjusted accordingly.  Default values:\n┌─name────────────────────────────────────┬─value─┐│connect_timeout_with_failover_ms│50││connect_timeout_with_failover_secure_ms│100│└─────────────────────────────────────────┴───────┘© 2021 Altinity Inc. All rights reserved.\n","categories":"","description":"INSERT INTO destination SELECT * FROM remote('ch-server',database,source,user,password)","excerpt":"INSERT INTO destination SELECT * FROM …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-...-table-function/","tags":"","title":""},{"body":"ZooKeeper Monitoring ZooKeeper scrape metrics:\n embedded exporter since version 3.6.0  https://zookeeper.apache.org/doc/r3.6.2/zookeeperMonitor.html   standalone exporter  https://github.com/dabealu/zookeeper-exporter    Install dashboards\n embedded exporter https://grafana.com/grafana/dashboards/10465 dabealu exporter https://grafana.com/grafana/dashboards/11442  see also https://grafana.com/grafana/dashboards?search=ZooKeeper\u0026amp;dataSource=prometheus\nsetup alert rules:\n embedded exporter https://github.com/Altinity/clickhouse-operator/blob/master/deploy/prometheus/prometheus-alert-rules.yaml#L480-L805  See also\n https://blog.serverdensity.com/how-to-monitor-zookeeper/ https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/#zookeeper-metrics https://dzone.com/articles/monitoring-apache-zookeeper-servers https://docs.signalfx.com/en/latest/integrations/integrations-reference/integrations.zookeeper.html https://github.com/samber/awesome-prometheus-alerts/blob/c3ba0cf1997c7e952369a090aeb10343cdca4878/_data/rules.yml#L1146-L1170 (or https://awesome-prometheus-alerts.grep.to/rules.html#zookeeper ) https://alex.dzyoba.com/blog/prometheus-alerts/ https://docs.datadoghq.com/integrations/zk/?tab=host\u0000  © 2021 Altinity Inc. All rights reserved.\n","categories":"","description":"","excerpt":"ZooKeeper Monitoring ZooKeeper scrape metrics:\n embedded exporter …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring/","tags":"","title":""},{"body":" title: “clickhouse-backup” linkTitle: “clickhouse-backup” description: \u003e clickhouse-backup Installation and configuration Download the latest clickhouse-backup.tar.gz from assets from https://github.com/AlexAkulov/clickhouse-backup/releases\nThis tar.gz contains a single binary of clickhouse-backup and an example of config file.\nBackblaze has s3 compatible API but requires empty acl parameter acl: \"\".\nhttps://www.backblaze.com/ has 15 days and free 10Gb S3 trial.\n$ mkdir clickhouse-backup$ cd clickhouse-backup$ wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/1.0.0-beta2/clickhouse-backup.tar.gz$ tar zxf clickhouse-backup.tar.gz$ rm clickhouse-backup.tar.gz$ cat config.ymlgeneral:remote_storage:s3max_file_size:1099511627776disable_progress_bar:falsebackups_to_keep_local:0backups_to_keep_remote:0log_level:infoallow_empty_backups:falseclickhouse:username:defaultpassword:\"\"host:localhostport:9000disk_mapping:{}skip_tables:- system.*timeout:5mfreeze_by_part:falsesecure:falseskip_verify:falsesync_replicated_tables:truelog_sql_queries:falses3:access_key:0****1secret_key:K****1bucket:\"mybucket\"endpoint:s3.us-west-000.backblazeb2.comregion:us-west-000acl:\"\"force_path_style:falsepath:clickhouse-backupdisable_ssl:falsepart_size:536870912compression_level:1compression_format:tarsse:\"\"disable_cert_verification:falsestorage_class:STANDARDI have a database test with table test\nselectcount()fromtest.test;┌─count()─┐│400000│└─────────┘clickhouse-backup list should work without errors (it scans local and remote (s3) folders):\n$ sudo ./clickhouse-backup list -c config.yml $ Backup  create a local backup of database test upload this backup to remote remove the local backup drop the source database  $ sudo ./clickhouse-backup create --tables='test.*' bkp01 -c config.yml 2021/05/31 23:11:13 info done backup=bkp01 operation=create table=test.test 2021/05/31 23:11:13 info done backup=bkp01 operation=create $ sudo ./clickhouse-backup upload bkp01 -c config.yml 1.44 MiB / 1.44 MiB [=====================] 100.00% 2s 2021/05/31 23:12:13 info done backup=bkp01 operation=upload table=test.test 2021/05/31 23:12:17 info done backup=bkp01 operation=upload $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 local bkp01 1.44MiB 31/05/2021 23:11:13 remote tar $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:13:29 info delete 'bkp01' DROPDATABASEtest;Restore  download the remote backup restore database  $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 remote tar $ sudo ./clickhouse-backup download bkp01 -c config.yml 2021/05/31 23:14:41 info done backup=bkp01 operation=download table=test.test 1.47 MiB / 1.47 MiB [=====================] 100.00% 0s 2021/05/31 23:14:43 info done backup=bkp01 operation=download table=test.test 2021/05/31 23:14:43 info done backup=bkp01 operation=download $ sudo ./clickhouse-backup restore bkp01 -c config.yml 2021/05/31 23:16:04 info done backup=bkp01 operation=restore table=test.test 2021/05/31 23:16:04 info done backup=bkp01 operation=restore SELECTcount()FROMtest.test;┌─count()─┐│400000│└─────────┘Delete backups $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:17:05 info delete 'bkp01' $ sudo ./clickhouse-backup delete remote bkp01 -c config.yml ","categories":"","description":"clickhouse-backup + backblaze","excerpt":"clickhouse-backup + backblaze","ref":"/altinity-kb-setup-and-maintenance/clickhouse-backup/","tags":"","title":""},{"body":"Logging Q. I get errors:\nFile not found: /var/log/clickhouse-server/clickhouse-server.log.0. File not found: /var/log/clickhouse-server/clickhouse-server.log.8.gz. ... File not found: /var/log/clickhouse-server/clickhouse-server.err.log.0, Stack trace (when copying this message, always include the lines below): 0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c2b345 in /usr/bin/clickhouse 1. Poco::PurgeOneFileStrategy::purge(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c84618 in /usr/bin/clickhouse 2. Poco::FileChannel::log(Poco::Message const\u0026) @ 0x11c314cc in /usr/bin/clickhouse 3. DB::OwnFormattingChannel::logExtended(DB::ExtendedLogMessage const\u0026) @ 0x8681402 in /usr/bin/clickhouse 4. DB::OwnSplitChannel::logSplit(Poco::Message const\u0026) @ 0x8682fa8 in /usr/bin/clickhouse 5. DB::OwnSplitChannel::log(Poco::Message const\u0026) @ 0x8682e41 in /usr/bin/clickhouse \u0000A. Check if you have proper permission to a log files folder, and enough disk space (\u0026 inode numbers) on the block device used for logging.\nls -la /var/log/clickhouse-server/ df -Th df -Thi \u0000Q. How to configure logging in clickhouse?\nA. See https://github.com/ClickHouse/ClickHouse/blob/ceaf6d57b7f00e1925b85754298cf958a278289a/programs/server/config.xml#L9-L62\n","categories":"","description":"","excerpt":"Logging Q. I get errors:\nFile not found: …","ref":"/altinity-kb-setup-and-maintenance/logging/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":""},{"body":"-State combinator doesn’t actually store information about -If combinator, so aggregate functions with -If and without have the same serialized data.\n$clickhouse-local--query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(10) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 99$clickhouse-local--query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(11) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 910-State combinator have the same serialized data footprint regardless of parameters used in definition of aggregate function. That’s true for quantile* and sequenceMatch/sequenceCount functions.\n$clickhouse-local--query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantileTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantileTDigestWeightedMerge(0.4)(x) FROM table\" 400000$clickhouse-local--query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantilesTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantilesTDigestWeightedMerge(0.4,0.8)(x) FROM table\" [400000,800000]SELECTquantileMerge(0.9)(x)FROM(SELECTquantileState(0.1)(number)ASxFROMnumbers(1000))┌─quantileMerge(0.9)(x)─┐│899.1│└───────────────────────┘SELECTsequenceMatchMerge('(?2)(?3)')(x)AS`2_3`,sequenceMatchMerge('(?1)(?4)')(x)AS`1_4`,sequenceMatchMerge('(?1)(?2)(?3)')(x)AS`1_2_3`FROM(SELECTsequenceMatchState('(?1)(?2)(?3)')(number,number=8,number=5,number=6,number=9)ASxFROMnumbers(10))┌─2_3─┬─1_4─┬─1_2_3─┐│1│1│0│└─────┴─────┴───────┘SELECTsequenceCountMerge('(?1)(?2)')(x)AS`2_3`,sequenceCountMerge('(?1)(?4)')(x)AS`1_4`,sequenceCountMerge('(?1)(?2)(?3)')(x)AS`1_2_3`FROM(WITHnumber%4AScondSELECTsequenceCountState('(?1)(?2)(?3)')(number,cond=1,cond=2,cond=3,cond=5)ASxFROMnumbers(11))┌─2_3─┬─1_4─┬─1_2_3─┐│3│0│2│└─────┴─────┴───────┘","categories":"","description":"-State \u0026 -Merge combinators\n","excerpt":"-State \u0026 -Merge combinators\n","ref":"/altinity-kb-queries-and-syntax/state-and-merge-combinators/","tags":"","title":"-State \u0026 -Merge combinators"},{"body":" To set rdkafka options - add to \u003ckafka\u003e section in config.xml or preferably use a separate file in config.d/:  https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md    Some random example:\n\u003ckafka\u003e \u003cmax_poll_interval_ms\u003e60000\u003c/max_poll_interval_ms\u003e \u003csession_timeout_ms\u003e60000\u003c/session_timeout_ms\u003e \u003cheartbeat_interval_ms\u003e10000\u003c/heartbeat_interval_ms\u003e \u003creconnect_backoff_ms\u003e5000\u003c/reconnect_backoff_ms\u003e \u003creconnect_backoff_max_ms\u003e60000\u003c/reconnect_backoff_max_ms\u003e \u003crequest_timeout_ms\u003e20000\u003c/request_timeout_ms\u003e \u003cretry_backoff_ms\u003e500\u003c/retry_backoff_ms\u003e \u003cmessage_max_bytes\u003e20971520\u003c/message_max_bytes\u003e \u003cdebug\u003eall\u003c/debug\u003e\u003c!-- only to get the errors --\u003e \u003csecurity_protocol\u003eSSL\u003c/security_protocol\u003e \u003cssl_ca_location\u003e/etc/clickhouse-server/ssl/kafka-ca-qa.crt\u003c/ssl_ca_location\u003e \u003cssl_certificate_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.pem\u003c/ssl_certificate_location\u003e \u003cssl_key_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.key\u003c/ssl_key_location\u003e \u003cssl_key_password\u003epass\u003c/ssl_key_password\u003e \u003c/kafka\u003e Authentication / connectivity  Amazon MSK  \u003cyandex\u003e \u003ckafka\u003e \u003csecurity_protocol\u003esasl_ssl\u003c/security_protocol\u003e \u003csasl_username\u003eroot\u003c/sasl_username\u003e \u003csasl_password\u003etoor\u003c/sasl_password\u003e \u003c/kafka\u003e \u003c/yandex\u003e https://leftjoin.ru/all/clickhouse-as-a-consumer-to-amazon-msk/\nInline Kafka certs  To connect to some Kafka cloud services you may need to use certificates.\nIf needed they can be converted to pem format and inlined into ClickHouse config.\nExample:\n\u003ckafka\u003e \u003cssl_key_pem\u003e\u003c![CDATA[ RSA Private-Key: (3072 bit, 2 primes) .... -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- ]]\u003e\u003c/ssl_key_pem\u003e \u003cssl_certificate_pem\u003e\u003c![CDATA[ -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ]]\u003e\u003c/ssl_certificate_pem\u003e \u003c/kafka\u003e See also\nhttps://help.aiven.io/en/articles/489572-getting-started-with-aiven-kafka\nhttps://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files\nAzure Event Hub  See https://github.com/ClickHouse/ClickHouse/issues/12609\nKerberos   https://clickhouse.tech/docs/en/engines/table-engines/integrations/kafka/#kafka-kerberos-support https://github.com/ClickHouse/ClickHouse/blob/master/tests/integration/test_storage_kerberized_kafka/configs/kafka.xml  \u003c!-- Kerberos-aware Kafka --\u003e \u003ckafka\u003e \u003csecurity_protocol\u003eSASL_PLAINTEXT\u003c/security_protocol\u003e \u003csasl_kerberos_keytab\u003e/home/kafkauser/kafkauser.keytab\u003c/sasl_kerberos_keytab\u003e \u003csasl_kerberos_principal\u003ekafkauser/kafkahost@EXAMPLE.COM\u003c/sasl_kerberos_principal\u003e \u003c/kafka\u003e confluent cloud \u003cyandex\u003e \u003ckafka\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003csecurity_protocol\u003eSASL_SSL\u003c/security_protocol\u003e \u003cssl_endpoint_identification_algorithm\u003ehttps\u003c/ssl_endpoint_identification_algorithm\u003e \u003csasl_mechanism\u003ePLAIN\u003c/sasl_mechanism\u003e \u003csasl_username\u003eusername\u003c/sasl_username\u003e \u003csasl_password\u003epassword\u003c/sasl_password\u003e \u003cssl_ca_location\u003eprobe\u003c/ssl_ca_location\u003e \u003c!-- \u003cssl_ca_location\u003e/path/to/cert.pem\u003c/ssl_ca_location\u003e --\u003e \u003c/kafka\u003e \u003c/yandex\u003e https://docs.confluent.io/cloud/current/client-apps/config-client.html\nHow to test connection settings Use kafkacat utility - it internally uses same library to access Kafla as clickhouse itself and allows easily to test different settings\nkafkacat -b my_broker:9092 -C -o -10 -t my_topic \\ -X security.protocol=SASL_SSL \\ -X sasl.mechanisms=PLAIN \\ -X sasl.username=uerName \\ -X sasl.password=Password ","categories":"","description":"Adjusting librdkafka settings\n","excerpt":"Adjusting librdkafka settings\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/","tags":"","title":"Adjusting librdkafka settings"},{"body":"Q. What happens with columns which are nor the part of ORDER BY key, nor have the AggregateFunction type?\nA. it picks the first value met, (similar to any)\nCREATETABLEagg_test(`a`String,`b`UInt8,`c`SimpleAggregateFunction(max,UInt8))ENGINE=AggregatingMergeTreeORDERBYa;INSERTINTOagg_testVALUES('a',1,1);INSERTINTOagg_testVALUES('a',2,2);SELECT*FROMagg_testFINAL;┌─a─┬─b─┬─c─┐│a│1│2│└───┴───┴───┘INSERTINTOagg_testVALUES('a',3,3);SELECT*FROMagg_test;┌─a─┬─b─┬─c─┐│a│1│2│└───┴───┴───┘┌─a─┬─b─┬─c─┐│a│3│3│└───┴───┴───┘OPTIMIZETABLEagg_testFINAL;SELECT*FROMagg_test;┌─a─┬─b─┬─c─┐│a│1│3│└───┴───┴───┘","categories":"","description":"AggregatingMergeTree\n","excerpt":"AggregatingMergeTree\n","ref":"/engines/mergetree-table-engine-family/aggregatingmergetree/","tags":"","title":"AggregatingMergeTree"},{"body":"Problem You have table:\nCREATETABLEmodify_column(column_nString)ENGINE=MergeTree()ORDERBYtuple();Populate it with data:\nINSERTINTOmodify_columnVALUES('key_a');INSERTINTOmodify_columnVALUES('key_b');INSERTINTOmodify_columnVALUES('key_c');Tried to apply alter table query with changing column type:\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nEnum8('key_a'=1,'key_b'=2);But it didn’t succeed and you see an error in system.mutations table:\nSELECT*FROMsystem.mutationsWHERE(table='modify_column')AND(is_done=0)FORMATVerticalRow1:──────database:defaulttable:modify_columnmutation_id:mutation_4.txtcommand:MODIFYCOLUMN`column_n`Enum8('key_a'=1,'key_b'=2)create_time:2021-03-0318:38:09block_numbers.partition_id:['']block_numbers.number:[4]parts_to_do_names:['all_3_3_0']parts_to_do:1is_done:0latest_failed_part:all_3_3_0latest_fail_time:2021-03-0318:38:59latest_fail_reason:Code:36,e.displayText()=DB::Exception:Unknownelement'key_c'fortypeEnum8('key_a'=1,'key_b'=2):whileexecuting'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)') Enum8('key_a' = 1, 'key_b' = 2) : 2':(whilereadingfrompart/var/lib/clickhouse/data/default/modify_column/all_3_3_0/):WhileexecutingMergeTree(version21.3.1.6041)And you can’t query that column anymore:\nSELECTcolumn_nFROMmodify_column┌─column_n─┐│key_a│└──────────┘┌─column_n─┐│key_b│└──────────┘↓Progress:2.00rows,2.00B(19.48rows/s.,19.48B/s.)2rowsinset.Elapsed:0.104sec.Receivedexceptionfromserver(version21.3.1):Code:36.DB::Exception:Receivedfromlocalhost:9000.DB::Exception:Unknownelement'key_c'fortypeEnum8('key_a'=1,'key_b'=2):whileexecuting'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)') Enum8('key_a' = 1, 'key_b' = 2) : 2':(whilereadingfrompart/var/lib/clickhouse/data/default/modify_column/all_3_3_0/):WhileexecutingMergeTreeThread.Solution You should do the following:\nCheck which mutation is stuck and kill it:\nSELECT*FROMsystem.mutationstable='modify_column'ANDis_done=0FORMATVertical;KILLMUTATIONWHEREtable='modify_column'ANDmutation_id='id_of_stuck_mutation';Apply reverting modify column query to convert table to previous column type:\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nString;Check if column is accessible now:\nSELECTcolumn_n,count()FROMmodify_columnGROUPBYcolumn_n;Run fixed ALTER MODIFY COLUMN query.\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nEnum8('key_a'=1,'key_b'=2,'key_c'=3);You can monitor progress of column type change with system.mutations or system.parts_columns tables:\nSELECTcommand,parts_to_do,is_doneFROMsystem.mutationsWHEREtable='modify_column'SELECTcolumn,type,count()ASparts,sum(rows)ASrows,sum(bytes_on_disk)ASbytesFROMsystem.parts_columnsWHERE(table='modify_column')AND(column='column_n')ANDactiveGROUPBYcolumn,type","categories":"","description":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","excerpt":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-alter-modify-column-is-stuck-the-column-is-inaccessible/","tags":"","title":"ALTER MODIFY COLUMN is stuck, the column is inaccessible."},{"body":"Welcome to the Altinity ClickHouse Knowledge Base (KB) This knowledge base is supported by Altinity engineers to provide quick answers to common questions and issues involving ClickHouse.\nFor more detailed information about Altinity services support, see the following:\n Altinity: Providers of Altinity.Cloud, providing SOC-2 certified support for ClickHouse. Altinity ClickHouse Documentation: Detailed guides on installing and connecting ClickHouse to other services. Altinity Resources: News, blog posts, and webinars about ClickHouse and Altinity services.  The following sites are also useful references regarding ClickHouse:\n ClickHouse.tech documentation: From Yandex, the creators of ClickHouse ClickHouse at Stackoverflow: Community driven responses to questions regarding ClickHouse Google groups (Usenet) yes we remember it: The grandparent of all modern discussion boards.  ","categories":"","description":"","excerpt":"Welcome to the Altinity ClickHouse Knowledge Base (KB) This knowledge …","ref":"/","tags":"","title":"Altinity Beta Knowledgebase"},{"body":"Working with Altinity \u0026 Yandex packaging together Since version 21.1 Altinity switches to the same packaging as used by Yandex. That is needed for syncing things and introduces several improvements (like adding systemd service file).\nUnfortunately, that change leads to compatibility issues - automatic dependencies resolution gets confused by the conflicting package names: both when you update ClickHouse to the new version (the one which uses older packaging) and when you want to install older altinity packages (20.8 and older).\nInstalling old clickhouse version (with old packaging schema) When you try to install versions 20.8 or older from Altinity repo -\nversion=20.8.12.2-1.el7 yum install clickhouse-client-${version} clickhouse-server-${version} yum outputs smth like\nyum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: centos.hitme.net.pl * extras: centos1.hti.pl * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 --\u003e Running transaction check ---\u003e Package clickhouse-common-static.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) Requires: clickhouse-server-common = 20.8.12.2-1.el7 Available: clickhouse-server-common-1.1.54370-2.x86_64 (clickhouse-stable) clickhouse-server-common = 1.1.54370-2 Available: clickhouse-server-common-1.1.54378-2.x86_64 (clickhouse-stable) clickhouse-server-common = 1.1.54378-2 ... Available: clickhouse-server-common-20.8.11.17-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.11.17-1.el7 Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.12.2-1.el7 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest As you can see yum has an issue with resolving clickhouse-server-common dependency, which marked as obsoleted by newer packages.\nSolution add --setopt=obsoletes=0 flag to the yum call.\nversion=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} --- title: \"installation succeeded\" linkTitle: \"installation succeeded\" description: \u003e installation succeeded --- Alternatively, you can add obsoletes=0 into /etc/yum.conf.\nTo update to new ClickHouse version (from old packaging schema to new packaging schema) version=21.1.7.1-2 yum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: centos.hitme.net.pl * extras: centos1.hti.pl * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Nothing to do It is caused by wrong dependencies resolution.\nSolution To update to the latest available version - just add clickhouse-server-common:\nyum install clickhouse-client clickhouse-server clickhouse-server-common This way the latest available version will be installed (even if you will request some other version explicitly).\nTo install some specific version remove old packages first, then install new ones.\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=21.1.7.1 yum install clickhouse-client-${version} clickhouse-server-${version} Downgrade from new version to old one version=20.8.12.2-1.el7 yum downgrade clickhouse-client-${version} clickhouse-server-${version} will not work:\nLoaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile * base: ftp.agh.edu.pl * extras: ftp.agh.edu.pl * updates: centos.wielun.net Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be a downgrade ---\u003e Package clickhouse-client.noarch 0:21.1.7.1-2 will be erased ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be a downgrade --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common-20.8.12.2-1.el7.x86_64 is obsoleted by clickhouse-server-21.1.7.1-2.noarch which is already installed --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 ---\u003e Package clickhouse-server.noarch 0:21.1.7.1-2 will be erased --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) Requires: clickhouse-common-static = 20.8.12.2-1.el7 Installed: clickhouse-common-static-21.1.7.1-2.x86_64 (@clickhouse-stable) clickhouse-common-static = 21.1.7.1-2 Available: clickhouse-common-static-1.1.54378-2.x86_64 (clickhouse-stable) clickhouse-common-static = 1.1.54378-2 Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) ... Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) clickhouse-server-common = 20.8.12.2-1.el7 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest Solution Remove packages first, then install older versions:\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} ","categories":"","description":"Altinity packaging compatibility \u0026gt;21.x and earlier\n","excerpt":"Altinity packaging compatibility \u003e21.x and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-packaging-compatibility-greater-than-21.x-and-earlier/","tags":"","title":"Altinity packaging compatibility \u0026gt;21.x and earlier"},{"body":"It’s possible to tune some settings which would make ClickHouse more ANSI SQL compatible(and slower):\nSETjoin_use_nulls=1;-- introduced long ago SETcast_keep_nullable=1;-- introduced in 20.5 SETunion_default_mode='DISTINCT';-- introduced in 21.1 SETallow_experimental_window_functions=1;-- introduced in 21.3 SETprefer_column_name_to_alias=1;-- introduced in 21.4; ","categories":"","description":"ANSI SQL mode\n","excerpt":"ANSI SQL mode\n","ref":"/altinity-kb-queries-and-syntax/ansi-sql-mode/","tags":"","title":"ANSI SQL mode"},{"body":"assumeNotNull result is implementation specific:\nWITHCAST(NULL,'Nullable(UInt8)')AScolumnSELECTcolumn,assumeNotNull(column+999)ASx;┌─column─┬─x─┐│ᴺᵁᴸᴸ│0│└────────┴───┘WITHCAST(NULL,'Nullable(UInt8)')AScolumnSELECTcolumn,assumeNotNull(materialize(column)+999)ASx;┌─column─┬───x─┐│ᴺᵁᴸᴸ│999│└────────┴─────┘CREATETABLEtest_null(`key`UInt32,`value`Nullable(String))ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_nullSELECTnumber,concat('value ',toString(number))FROMnumbers(4);SELECT*FROMtest_null;┌─key─┬─value───┐│0│value0││1│value1││2│value2││3│value3│└─────┴─────────┘ALTERTABLEtest_nullUPDATEvalue=NULLWHEREkey=3;SELECT*FROMtest_null;┌─key─┬─value───┐│0│value0││1│value1││2│value2││3│ᴺᵁᴸᴸ│└─────┴─────────┘SELECTkey,assumeNotNull(value)FROMtest_null;┌─key─┬─assumeNotNull(value)─┐│0│value0││1│value1││2│value2││3│value3│└─────┴──────────────────────┘WITHCAST(NULL,'Nullable(Enum8(\\'a\\' = 1, \\'b\\' = 0))')AStestSELECTassumeNotNull(test)┌─assumeNotNull(test)─┐│b│└─────────────────────┘WITHCAST(NULL,'Nullable(Enum8(\\'a\\' = 1))')AStestSELECTassumeNotNull(test)Erroronprocessingquery'with CAST(null, 'Nullable(Enum8(\\'a\\'=1))') as test select assumeNotNull(test); ;':Code:36,e.displayText()=DB::Exception:Unexpectedvalue0inenum,Stacktrace(whencopyingthismessage,alwaysincludethelinesbelow):{% hint style=“info” %} Null values in ClickHouse are stored in a separate dictionary: is this value Null. And for faster dispatch of functions there is no check on Null value while function execution, so functions like plus can modify internal column value (which has default value). In normal conditions it’s not a problem because on read attempt, ClickHouse first would check the Null dictionary and return value from column itself for non-Nulls only. And assumeNotNull function just ignores this Null dictionary. So it would return only column values, and in certain cases it’s possible to have unexpected results. {% endhint %}\nIf it’s possible to have Null values, it’s better to use ifNull function instead.\nSELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(ifNull(toNullable(number),0))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.705sec.Processed1.00billionrows,8.00GB(1.42billionrows/s.,11.35GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(coalesce(toNullable(number),0))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:2.383sec.Processed1.00billionrows,8.00GB(419.56millionrows/s.,3.36GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(assumeNotNull(toNullable(number)))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.051sec.Processed1.00billionrows,8.00GB(19.62billionrows/s.,156.98GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(toNullable(number))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.050sec.Processed1.00billionrows,8.00GB(20.19billionrows/s.,161.56GB/s.){% hint style=“info” %} There is no overhead for assumeNotNull at all. {% endhint %}\n","categories":"","description":"assumeNotNull and friends\n","excerpt":"assumeNotNull and friends\n","ref":"/altinity-kb-functions/assumenotnull-and-friends/","tags":"","title":"assumeNotNull and friends"},{"body":"In version 20.5 ClickHouse first introduced database engine=Atomic.\nSince version 20.10 it is a default database engine (before engine=Ordinary was used).\nThose 2 database engine differs in a way how they store data on a filesystem, and engine Atomic allows to resolve some of the issues existed in engine=Ordinary.\nengine=Atomic supports\n non-blocking drop table / rename table tables delete (\u0026detach) async (wait for selects finish but invisible for new selects) atomic drop table (all files / folders removed) atomic table swap (table swap by “EXCHANGE TABLES t1 AND t2;\") rename dictionary / rename database unique automatic UUID paths in FS and ZK for Replicated  FAQ Q. Data is not removed immediately A. UseDROP TABLE t SYNC;\nOr use parameter (user level) database_atomic_wait_for_drop_and_detach_synchronously:\nSETdatabase_atomic_wait_for_drop_and_detach_synchronously=1;Also, you can decrease the delay used by Atomic for real table drop (it’s 8 minutes by default)\ncat /etc/clickhouse-server/config.d/database_atomic_delay_before_drop_table.xml \u003cyandex\u003e \u003cdatabase_atomic_delay_before_drop_table_sec\u003e1\u003c/database_atomic_delay_before_drop_table_sec\u003e \u003c/yandex\u003e Q. I cannot reuse zookeeper path after dropping the table. A. This happens because real table deletion occurs with a controlled delay. See the previous question to remove the table immediately.\nWith engine=Atomic it’s possible (and is a good practice if you do it correctly) to include UUID into zookeeper path, i.e. :\nCREATE...ONCLUSTER...ENGINE=ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}/','{replica}')See also: https://github.com/ClickHouse/ClickHouse/issues/12135#issuecomment-653932557\nIt’s very important that the table will have the same UUID cluster-wide.\nWhen the table is created using ON CLUSTER - all tables will get the same UUID automatically.\nWhen it needs to be done manually (for example - you need to add one more replica), pick CREATE TABLE statement with UUID from one of the existing replicas.\nsetshow_table_uuid_in_table_create_qquery_if_not_nil=1;SHOWCREATETABLExxx;/* or SELECT create_table_query FROM system.tables WHERE ... */Q. Should I use Atomic or Ordinary for new setups?  All things inside clickhouse itself should work smoothly with Atomic.\nBut some external tools - backup tools, things involving other kinds of direct manipulations with clickhouse files \u0026 folders may have issues with Atomic.\nOrdinary layout on the filesystem is simpler. And the issues which address Atomic (lock-free renames, drops, atomic exchange of table) are not so critical in most cases.\n    Ordinary Atomic     filesystem layout very simple more complicated   external tool support (like clickhouse-backup) good / mature limited / beta   some DDL queries (DROP / RENAME) may\nhang for a long time (waiting for some other things)\n yes 👎 no 👍   Possibility to swap 2 tables rename a to a_old, b to a,\na_old to b;\nOperation is not atomic, and can break in the middle (while chances are low).\n \nEXCHANGE TABLES t1 AND t2\nAtomic, have no intermediate states.\n   uuid in zookeeper path Not possible to use.\nThe typical pattern is to add version suffix to zookeeper path when you need to create the new version of the same table.\n You can use uuid in zookeeper paths. That requires some extra care when you expand the cluster, and makes zookeeper paths harder to map to real table.\nBut allows to to do any kind of manipulations on tables (rename, recreate with same name etc).\n   Materialized view without TO syntax\n(!we recommend using TO syntax always!)\n .inner.mv_name\nThe name is predictable, easy to match with MV.\n .inner_id.{uuid}\nThe name is unpredictable, hard to match with MV (maybe problematic for MV chains, and similar scenarios)\n    Using Ordinary by default instead of Atomic --- title: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" linkTitle: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" description: \u003e cat /etc/clickhouse-server/users.d/disable_atomic_database.xml --- \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cdefault_database_engine\u003eOrdinary\u003c/default_database_engine\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e Other sources Presentation https://youtu.be/1LVJ_WcLgF8?t=2744\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup46/database_engines.pdf” %}\n","categories":"","description":"Atomic Database Engine\n","excerpt":"Atomic Database Engine\n","ref":"/engines/altinity-kb-atomic-database-engine/readme/","tags":"","title":"Atomic Database Engine"},{"body":"Insert would be atomic only if those conditions met:\n Insert data only in single partition. Numbers of rows is less than max_insert_block_size. Table doesn’t have Materialized Views (there is no atomicity Table \u003c\u003e MV) For TSV, TKSV, CSV, and JSONEachRow formats, setting input_format_parallel_parsing=0 is set.  {% embed url=“https://github.com/ClickHouse/ClickHouse/issues/9195#issuecomment-587500824” %}\n","categories":"","description":"Atomic insert\n","excerpt":"Atomic insert\n","ref":"/altinity-kb-queries-and-syntax/atomic-insert/","tags":"","title":"Atomic insert"},{"body":"   Volume type  gp3 gp2     Max throughput per volume  1000 MiB/s 250 MiB/s   Price  $0.08/GB-month\n3,000 IOPS free and\n$0.005/provisioned IOPS-month over 3,000;\n125 MB/s free and\n$0.04/provisioned MB/s-month over 125\n $0.10/GB-month    GP2 In usual conditions ClickHouse being limited by throughput of volumes only and amount of provided IOPS doesn’t make any big difference for performance. So the most native choice for clickhouse is gp2 and gp3 volumes.\n‌Because gp2 volumes have a hard limit of 250 MiB/s per volume (for volumes bigger than 334 GB), it usually makes sense to split one big volume in multiple smaller volumes larger than 334GB in order to have maximum possible throughput.\n‌EC2 instances also have an EBS throughput limit, it depends on the size of the EC2 instance. That means if you would attach multiple volumes which would have high potential throughput, you would be limited by your EC2 instance, so usually there is no reason to have more than 4-5 volumes per node.\nIt’s pretty straightforward to set up a ClickHouse for using multiple EBS volumes with storage_policies.\nGP3 It’s a new type of volume, which is 20% cheaper than gp2 per GB-month and has lower free throughput: only 125 MB/s vs 250 MB/s. But you can buy additional throughput for volume and gp3 pricing became comparable with multiple gp2 volumes starting from 1000-1500GB size.\n{% embed url=“https://altinity.com/blog/2019/11/27/amplifying-clickhouse-capacity-with-multi-volume-storage-part-1\" %}\n{% embed url=“https://altinity.com/blog/2019/11/29/amplifying-clickhouse-capacity-with-multi-volume-storage-part-2\" %}\n{% embed url=“https://calculator.aws/#/createCalculator/EBS?nc2=h_ql_pr_calc” %}\n{% embed url=“https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html\" %}\n{% embed url=“https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\" %}\n{% embed url=“https://aws.amazon.com/ebs/general-purpose/\" %}\n","categories":"","description":"AWS EBS\n","excerpt":"AWS EBS\n","ref":"/altinity-kb-setup-and-maintenance/aws-ebs/","tags":"","title":"AWS EBS"},{"body":"Q. How to populate MV create with TO syntax? INSERT INTO mv SELECT * FROM huge_table? Will it work if the source table has billions of rows?\nA. single huge insert ... select ... actually will work, but it will take A LOT of time, and during that time lot of bad things can happen (lack of disk space, hard restart etc). Because of that, it’s better to do such backfill in a more controlled manner and in smaller pieces.\nOne of the best options is to fill one partition at a time, and if it breaks you can drop the partition and refill it.\nIf you need to construct a single partition from several sources - then the following approach may be the best.\nCREATE TABLE mv_import AS mv; INSERT INTO mv_import SELECT * FROM huge_table WHERE toYYYYMM(ts) = 202105; /* or other partition expresssion*/ /* that insert select may take a lot of time, if something bad will happen during that - just truncate mv_import and restart the process */ /* after successful loading of mv_import do*/ ALTER TABLE mv ATTACH PARTITION ID '202105' FROM mv_import; See also https://clickhouse.tech/docs/en/sql-reference/statements/alter/partition/#alter_attach-partition-from\n","categories":"","description":"Backfill/populate MV in a controlled manner\n","excerpt":"Backfill/populate MV in a controlled manner\n","ref":"/altinity-kb-schema-design/materialized-views/backfill-populate-mv-in-a-controlled-manner/","tags":"","title":"Backfill/populate MV in a controlled manner"},{"body":"ClickHouse is currently at the design stage of creating some universal backup solution. Some custom backup strategies are:\n Each shard is backed up separately. FREEZE the table/partition. For more information, see Alter Freeze Partition.  This creates hard links in shadow subdirectory.   rsync that directory to a backup location, then remove that subfolder from shadow.  Cloud users are recommended to use Rclone.   Always add the full contents of the metadata subfolder that contains the current DB schema and clickhouse configs to your backup. For a second replica, it’s enough to copy metadata and configuration. Data in clickhouse is already compressed with lz4, backup can be compressed bit better, but avoid using cpu-heavy compression algorythms like gzip, use something like zstd instead.  The tool automating that process clickhouse-backup.\n","categories":"","description":"Backups\n","excerpt":"Backups\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/","tags":"","title":"Backups"},{"body":"Picking the best schema for storing many metrics registered from single source is quite a common problem.\n1 One row per metric i.e.: timestamp, sourceid, metric_name, metric_value\nPros and cons:\n simple well normalized schema easy to extend that is quite typical pattern for timeseries databases   different metrics values stored in same columns (worse compression) to use values of different datatype you need to cast everything to string or introduce few columns for values of different types. not always nice as you need to repeat all ‘common’ fields for each row if you need to select all data for one time point you need to scan several ranges of data.  2 Each measurement (with lot of metrics) in it’s own row In that way you need to put all the metrics in one row (i.e.: timestamp, sourceid, ….)\nThat approach is usually a source of debates about how to put all the metrics in one row.\n2a Every metric in it’s own column i.e.: timestamp, sourceid, metric1_value, … , metricN_value\nPros and cons:\n simple really easy to access / scan for rows with particular metric specialized and well adjusted datatypes for every metric. good for dense recording (each time point can have almost 100% of all the possible metrics)   adding new metric = changing the schema (adding new column). not suitable when set of metric changes dynamically not applicable when there are too many metrics (when you have more than 100-200) when each timepoint have only small subset of metrics recorded - if will create a lot of sparse filled columns. you need to store ‘lack of value’ somehow (NULLs or default values) to read full row - you need to read a lot of column files.  2b Using arrays / Nested / Map i.e.: timestamp, sourceid, array_of_metric_names, array_of_metric_values\nPros and cons:\n easy to extend, you can have very dynamic / huge number of metrics. you can use Array(LowCardinality(String)) for storing metric names efficiently good for sparse recording (each time point can have only 1% of all the possible metrics)   you need to extract all metrics for row to reach a single metric not very handy / complicated non-standard syntax different metrics values stored in single array (bad compression) to use values of different datatype you need to cast everything to string or introduce few arrays for values of different types.  2c Using JSON i.e.: timestamp, sourceid, metrics_data_json\nPros and cons:\n easy to extend, you can have very dynamic / huge number of metrics. the only option to store hierarchical / complicated data structures, also with arrays etc. inside. good for sparse recording (each time point can have only 1% of all the possible metrics) ClickHouse has efficient API to work with JSON nice if your data originally came in JSON (don’t need to reformat)   uses storage non efficiently different metrics values stored in single array (bad compression) you need to extract whole JSON field to reach single metric slower than arrays  2d Using querystring-format URLs i.e.: timestamp, sourceid, metrics_querystring\nSame pros/cons as raw JSON, but usually bit more compact than JSON\nPros and cons:\n clickhouse has efficient API to work with URLs (extractURLParameter etc) can have sense if you data came in such format (i.e. you can store GET / POST request data directly w/o reprocessing)   slower than arrays  2e Several ‘baskets’ of arrays i.e.: timestamp, sourceid, metric_names_basket1, metric_values_basker1, …, metric_names_basketN, metric_values_basketN\nThe same as 2b, but there are several key-value arrays (‘basket’), and metric go to one particular basket depending on metric name (and optionally by metric type)\nPros and cons:\n address some disadvantages of 2b (you need to read only single, smaller basket for reaching a value, better compression - less unrelated metrics are mixed together)   complex  2f Combined approach In real life Pareto principle is correct for many fields.\nFor that particular case: usually you need only about 20% of metrics 80% of the time. So you can pick the metrics which are used intensively, and which have a high density, and extract them into separate columns (like in option 2a), leaving the rest in a common ‘trash bin’ (like in variants 2b-2e).\nWith that approach you can have as many metrics as you need and they can be very dynamic. At the same time the most used metrics are stored in special, fine-tuned columns.\nAt any time you can decide to move one more metric to a separate column ALTER TABLE ... ADD COLUMN metricX Float64 MATERIALIZED metrics.value[indexOf(metrics.names,'metricX')];\n2e Subcolumns [future] https://github.com/ClickHouse/ClickHouse/issues/23516\nWIP currently, ETA of first beta = autumn 2021\nRelated links:\n{% embed url=“https://www.altinity.com/blog/2019/5/23/handling-variable-time-series-efficiently-in-clickhouse\" caption=“There is one article on our blog on this subject with some benchmarks.” %}\n{% embed url=“https://www.percona.com/sites/default/files/ple19-slides/day1-pm/clickhouse-for-timeseries.pdf\" caption=“Slides from Percona Live” %}\n{% embed url=“https://eng.uber.com/logging/\" caption=“Uber article about how they adapted combined approach” %}\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup40/uber.pdf\" caption=“Slides for Uber log storage approach” %}\n","categories":"","description":"Best schema for storing many metrics registered from the single source\n","excerpt":"Best schema for storing many metrics registered from the single source …","ref":"/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/","tags":"","title":"Best schema for storing many metrics registered from the single source"},{"body":"  Superset: https://superset.apache.org/docs/databases/clickhouse\n  Metabase: https://github.com/enqueue/metabase-clickhouse-driver\n  Querybook: https://www.querybook.org/docs/setup_guide/connect_to_query_engines/#all-query-engines\n  Tableau: via odbc\n  Looker: https://docs.looker.com/setup-and-management/database-config/clickhouse\n  Apache Zeppelin\n  SeekTable\n  ReDash\n  Mondrian: https://altinity.com/blog/accessing-clickhouse-from-excel-using-mondrian-rolap-engine\n  Grafana\n  Cumul.io\n  ","categories":"","description":"BI Tools\n","excerpt":"BI Tools\n","ref":"/altinity-kb-integrations/bi-tools/","tags":"","title":"BI Tools"},{"body":"Here is what different statuses mean:\n parts are renamed to ‘ignored’ if they were found during ATTACH together with other, bigger parts that cover the same blocks of data, i.e. they were already merged into something else. parts are renamed to ‘broken’ if ClickHouse was not able to load data from the parts. There could be different reasons: some files are lost, checksums are not correct, etc. parts are renamed to ‘unexpected’ if they are present locally, but are not found in ZooKeeper, in case when an insert was not completed properly. The part is detached only if it’s old enough (5 minutes), otherwise CH registers this part in ZooKeeper as a new part. parts are renamed to ‘cloned’ if ClickHouse have had some parts on local disk while repairing lost replica so already existed parts being renamed and put in detached directory. Controlled by setting detach_old_local_parts_when_cloning_replica.  ‘Ignored’ parts are safe to delete. ‘Unexpected’ and ‘broken’ should be investigated, but it might not be an easy thing to do, especially for older parts. If the system.part_log table is enabled you can find some information there. Otherwise you will need to look in clickhouse-server.log for what happened when the parts were detached.\nIf there is another way you could confirm that there is no data loss in the affected tables, you could simply delete all detached parts.\nHere is a query that can help with investigations. It looks for active parts containing the same data blocks that the detached parts:\nSELECT*FROMsystem.detached_partsaALLLEFTJOIN(SELECTdatabase,table,partition_id,name,active,min_block_number,max_block_numberFROMsystem.partsWHEREactive)bUSING(database,table,partition_id)WHEREa.min_block_number\u003e=b.min_block_numberANDa.max_block_number\u003c=b.max_block_number","categories":"","description":"Can detached parts be dropped?\n","excerpt":"Can detached parts be dropped?\n","ref":"/altinity-kb-useful-queries/detached-parts/","tags":"","title":"Can detached parts be dropped?"},{"body":"{% hint style=“info” %} Article is based on feedback provided by one of Altinity clients. {% endhint %}\nCatBoost:\n It uses gradient boosting - a hard to use technique which can outperform neural networks. Gradient boosting is powerful but it’s easy to shoot yourself in the foot using it. The documentation on how to use it is quite lacking. The only good source of information on how to properly configure a model to yield good results is this video: https://www.youtube.com/watch?v=usdEWSDisS0 . We had to dig around GitHub issues to find out how to make it work with ClickHouse. CatBoost is fast. Other libraries will take ~5X to ~10X as long to do what CatBoost does. CatBoost will do preprocessing out of the box (fills nulls, apply standard scaling, encodes strings as numbers). CatBoost has all functions you’d need (metrics, plotters, feature importance)  It makes sense to split what CatBoost does into 2 parts:\n preprocessing (fills nulls, apply standard scaling, encodes strings as numbers) number crunching (convert preprocessed numbers to another number - ex: revenue of impression)  Compared to Fast.ai, CatBoost pre-processing is as simple to use and produces results that can be as good as Fast.ai.\nThe number crunching part of Fast.ai is no-config. For CatBoost you need to configure it, a lot.\nCatBoost won’t simplify or hide any complexity of the process. So you need to know data science terms and what it does (ex: if your model is underfitting you can use a smaller l2_reg parameter in the model constructor).\nIn the end both Fast.ai and CatBoost can yield comparable results.\nRegarding deploying models, CatBoost is really good. The model runs fast, it has a simple binary format which can be loaded in ClickHouse, C, or Python and it will encapsulate pre-processing with the binary file. Deploying Fast.ai models at scale/speed is impossible out of the box (we have our custom solution to do it which is not simple).\nTLDR: CatBoost is fast, produces awesome models, is super easy to deploy and it’s easy to use/train (after becoming familiar with it despite the bad documentation \u0026 if you know data science terms).\nRegarding MindsDB The project seems to be a good idea but it’s too young. I was using the GUI version and I’ve encountered some bugs, and none of those bugs have a good error message.\n  It won’t show data in preview.\n  The “download” button won’t work.\n  It’s trying to create and drop tables in ClickHouse without me asking it to.\n  Other than bugs:\n It will only use 1 core to do everything (training, analysis, download). Analysis will only run with a very small subset of data, if I use something like 1M rows it never finishes.    Training a model on 100k rows took 25 minutes - (CatBoost takes 90s to train with 1M rows)\n  The model trained on MindsDB is way worse. It had r-squared of 0.46 (CatBoost=0.58)\nTo me it seems that they are a plugin which connects ClickHouse to MySQL to run the model in Pytorch.\nIt’s too complex and hard to debug and understand. The resulting model is not good enough.\nTLDR: Easy to use (if bugs are ignored), too slow to train \u0026 produces a bad model.\n  ","categories":"","description":"CatBoost / MindsDB /  Fast.ai\n","excerpt":"CatBoost / MindsDB /  Fast.ai\n","ref":"/altinity-kb-integrations/catboost-mindsdb-fast.ai/","tags":"","title":"CatBoost / MindsDB /  Fast.ai"},{"body":" Do you have documentation on Docker deployments?\n Check\n https://hub.docker.com/r/yandex/clickhouse-server/ https://docs.altinity.com/clickhouseonkubernetes/ sources of entry point - https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh  Important things:\n use concrete version tag (avoid using latest) if possible use --network=host (due to performance reasons) you need to mount the folder /var/lib/clickhouse to have persistency. you MAY also mount the folder /var/log/clickhouse-server to have logs accessible outside of the container. Also, you may mount in some files or folders in the configuration folder:  /etc/clickhouse-server/config.d/listen_ports.xml   --ulimit nofile=262144:262144 You can also set on some linux capabilities to enable some of extra features of ClickHouse (not obligatory): SYS_PTRACE NET_ADMIN IPC_LOCK SYS_NICE you may also mount in the folder /docker-entrypoint-initdb.d/ - all SQL or bash scripts there will be executed during container startup. there are several ENV switches, see: https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh  TLDR version: use it as a starting point:\ndocker run -d \\  --name some-clickhouse-server \\  --ulimit nofile=262144:262144 \\  --volume=$(pwd)/data:/var/lib/clickhouse \\  --volume=$(pwd)/logs:/var/log/clickhouse-server \\  --volume=$(pwd)/configs/memory_adjustment.xml:/etc/clickhouse-server/config.d/memory_adjustment.xml \\  --cap-add=SYS_NICE \\  --cap-add=NET_ADMIN \\  --cap-add=IPC_LOCK \\  --cap-add=SYS_PTRACE \\  --network=host \\  yandex/clickhouse-server:21.1.7 docker exec -it some-clickhouse-server clickhouse-client docker exec -it some-clickhouse-server bash ","categories":"","description":"ClickHouse in Docker\n","excerpt":"ClickHouse in Docker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/","tags":"","title":"ClickHouse in Docker"},{"body":"ClickHouse versioning schema Example:\n21.3.10.1-lts\n 21 is the year of release. 3 indicates a Feature Release. This is an increment where features are delivered. 10 is the bugfix / maintenance version. When that version is incremented it means that some bugs was fixed comparing to 21.3.9. 1 - build number, means nothing for end users. lts - type of release. (long time support).  What is Altinity Stable version? It is one of general / public version of ClickHouse which has passed some extra testings, the upgrade path and changelog was analyzed, known issues are documented, and at least few big companies use it on production. All those things take some time, so usually that means that Altinity Stable is always a ‘behind’ the main releses.\nAltinity version - is an option for conservative users, who prefer bit older but better known things.\nUsually there is no reason to use version older than Altinity Stable. If you see that new Altinity Version arrived and you still use some older version - you should for sure consider an upgrade.\nAdditionally for Altinity client we provide extra support for those version for a longer time (and we also support newer versions).\nWhich version should I use? We recommend the following approach:\n When you start using ClickHouse and before you go on production - pick the latest stable version. If you already have ClickHouse running on production:  Check all the new queries / schemas on the staging first, especially if some new ClickHouse features are used. Do minor (bugfix) upgrades regularly: monitor new maintenance releases of the feature release you use. When considering upgrade - check Altinity Stable release docs, if you want to use newer release - analyze changelog and known issues. Check latest stable or test versions of ClickHouse on your staging environment regularly and pass the feedback to us or on the official ClickHouse github. Consider blue/green or canary upgrades.    See also: https://clickhouse.tech/docs/en/faq/operations/production/\nHow do I upgrade? {% hint style=“warning” %} Check upgrade / downgrade scenario on staging first. {% endhint %}\n check if you need to adjust some settings / to opt-out some new features you don’t need (maybe needed to to make the downgrade path possible, or to make it possible for 2 versions to work together). upgrade packages on odd replicas (if needed / depends on use case) stop ingestion into odd replicas / remove them for load-balancer etc. restart clickhouse-server service on odd replicas. once odd replicas will go back online - repeat the same procedure on the even replicas.  In some upgrade scenarios (depending from which version to which you do upgrate) when differerent replicas use different clickhouse versions you may see following issues:\n the replication don’t work at all and delays grow. errors about ‘checksum mismatch’ and traffic between replicase increase (they need to resync merge results).  Both problems will go away once all replicas will be upgraded.\nBugs?.. ClickHouse development process goes in a very high pace and has already thousands of features. CI system doing tens of thousands of tests (including tests with different sanitizers) against every commit.\nAll core features are well-tested, and very stable, and code is high-quality. But as with any other software bad things may happen. Usually the most of bugs happens in the new, freshly added functionality, and in some complex combination of several features (of course all possible combinations of features just physically can’t be tested). Usually new features are adopted by the community and stabilize quickly.\nWhat should I do if I found a bug in clickhouse?  First of all: try to upgrade to the latest bugfix release Example: if you use v21.3.5.42-lts but you know that v21.3.10.1-lts already exists - start with upgrade to that. Upgrades to latest maintenance releases are smooth and safe. Look for similar issues in github. Maybe the fix is on the way. If you can reproduce the bug: try to isolate it - remove some pieces of query one-by-one / simplify the scenario until the issue still reproduces. This way you can figure out which part is responsible for that bug, and you can try to create minimal reproducible example Once you have minimal reproducible example:  report it to github (or to Altinity Support) check if it reproduces on newer clickhouse versions    ","categories":"","description":"ClickHouse versions\n","excerpt":"ClickHouse versions\n","ref":"/altinity-kb-setup-and-maintenance/untitled/","tags":"","title":"ClickHouse versions"},{"body":"Q. How can I input multi-line SQL code? can you guys give me an example?\nA. Just run clickhouse-client with -m switch, and it starts executing only after you finish the line with a semicolon.\nQ. How can i use pager with clickhouse-client\nA. Here is an example: clickhouse-client --pager 'less -RS'\nQ. Data is returned in chunks / several tables.\nA. Data get streamed from the server in blocks, every block is formatted individually when the default PrettyCompact format is used. You can use PrettyCompactMonoBlock format instead, using one of the options:\n start clickhouse-client with an extra flag: clickhouse-client --format=PrettyCompactMonoBlock add FORMAT PrettyCompactMonoBlock at the end of your query. change clickhouse-client default format in the config. See https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42  Q. Сustomize client config\nA. you can change it globally (for all users of the workstation)\nnano /etc/clickhouse-client/conf.d/user.xml \u003cconfig\u003e \u003cuser\u003edefault1\u003c/user\u003e \u003cpassword\u003edefault1\u003c/password\u003e \u003chost\u003e\u003c/host\u003e \u003cmultiline\u003etrue\u003c/multiline\u003e \u003cmultiquery\u003etrue\u003c/multiquery\u003e \u003c/config\u003e See also https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42 or for particular users - by adjusting one of.\n./clickhouse-client.xml ~/.clickhouse-client/config.xml Also, it’s possible to have several client config files and pass one of them to clickhouse-client command explicitly\nReferences:\n https://clickhouse.tech/docs/en/interfaces/cli/  ","categories":"","description":"clickhouse-client\n","excerpt":"clickhouse-client\n","ref":"/altinity-kb-interfaces/altinity-kb-clickhouse-client/","tags":"","title":"clickhouse-client"},{"body":"The description of the utility and its parameters, as well as examples of the config files that you need to create for the copier are in the doc https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/\nThe steps to run a task:\n  Create a config file for clickhouse-copier (zookeeper.xml)\nhttps://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#format-of-zookeeper-xml\n  Create a config file for the task (task1.xml)\nhttps://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#configuration-of-copying-tasks\n  Create the task in ZooKeeper and start an instance of clickhouse-copierclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1 --task-file /opt/clickhouse-copier/task1.xml\n  If the node in ZooKeeper already exists and you want to change it, you need to add the task-upload-force parameter:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1 --task-file /opt/clickhouse-copier/task1.xml --task-upload-force 1\nIf you want to run another instance of clickhouse-copier for the same task, you need to copy the config file (zookeeper.xml) to another server, and run this command:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1\nThe number of simultaneously running instances is controlled be the max_workers parameter in your task configuration file. If you run more workers superfluous workers will sleep and log messages like this:\n\u003cDebug\u003e ClusterCopier: Too many workers (1, maximum 1). Postpone processing\n","categories":"","description":"clickhouse-copier\n","excerpt":"clickhouse-copier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/readme/","tags":"","title":"clickhouse-copier"},{"body":"Clickhouse-copier was created to move data between clusters.\nIt runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards.\nIn the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions.\nClickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe process is as follows:   Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one.  Drop the partition from the target table if it’s not empty Copy data from source shards one by one.  Check if there is data for the partition on a source shard. Check the status of the task in ZooKeeper. Create target tables on all shards of the target cluster. Insert the partition of data into the target table.   Mark the partition as completed in ZooKeeper.    If there are several workers running simultaneously, they will assign themselves to different source shards.\nIf a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table  Clickhouse-copier uses the engine from the task configuration file for these purposes:\n to create target tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to DROP existing partitions from target tables.  Clickhouse-copier does not support the old MergeTree format.\nHowever, you can create the target tables manually and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for its SELECT queries.\nHow to monitor the status of running tasks  Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers.\nHere is a list of queries that you can use to see what’s happening.\n--task-path /clickhouse/copier/task1 -- The task config select * from system.zookeeper where path='\u003ctask-path\u003e' name | ctime | mtime ----------------------------+---------------------+-------------------- description | 2019-10-18 15:40:00 | 2020-09-11 16:01:14 task_active_workers_version | 2019-10-18 16:00:09 | 2020-09-11 16:07:08 tables | 2019-10-18 16:00:25 | 2019-10-18 16:00:25 task_active_workers | 2019-10-18 16:00:09 | 2019-10-18 16:00:09 -- Running workers select * from system.zookeeper where path='\u003ctask-path\u003e/task_active_workers' -- The list of processed tables select * from system.zookeeper where path='\u003ctask-path\u003e/tables' -- The list of processed partitions select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e' name | ctime -------+-------------------- 201909 | 2019-10-18 18:24:18 -- The status of a partition select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e' name | ctime -------------------------+-------------------- shards | 2019-10-18 18:24:18 partition_active_workers | 2019-10-18 18:24:18 -- The status of source shards select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/shards' name | ctime | mtime -----+---------------------+-------------------- 1 | 2019-10-18 22:37:48 | 2019-10-18 22:49:29 ","categories":"","description":"clickhouse-copier 20.3 and earlier\n","excerpt":"clickhouse-copier 20.3 and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/","tags":"","title":"clickhouse-copier 20.3 and earlier"},{"body":"Clickhouse-copier was created to move data between clusters.\nIt runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards.\nIn the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions.\nClickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe behaviour of clickhouse-copier was changed in 20.4:\n Now clickhouse-copier inserts data into intermediate tables, and after the insert finishes successfully clickhouse-copier attaches the completed partition into the target table. This allows for incremental data copying, because the data in the target table is intact during the process. Important note: ATTACH PARTITION respects the max_partition_size_to_drop limit. Make sure the max_partition_size_to_drop limit is big enough (or set to zero) in the destination cluster. If clickhouse-copier is unable to attach a partition because of the limit, it will proceed to the next partition, and it will drop the intermediate table when the task is finished (if the intermediate table is less than the max_table_size_to_drop limit). Another important note: ATTACH PARTITION is replicated. The attached partition will need to be downloaded by the other replicas. This can create significant network traffic between ClickHouse nodes. If an attach takes a long time, clickhouse-copier will log a timeout and will proceed to the next step. Now clickhouse-copier splits the source data into chunks and copies them one by one. This is useful for big source tables, when inserting one partition of data can take hours. If there is an error during the insert clickhouse-copier has to drop the whole partition and start again. The number_of_splits parameter lets you split your data into chunks so that in case of an exception clickhouse-copier has to re-insert only one chunk of the data. Now clickhouse-copier runs OPTIMIZE target_table PARTITION ... DEDUPLICATE for non-Replicated MergeTree tables. Important note: This is a very strange feature that can do more harm than good. We recommend to disable it by configuring the engine of the target table as Replicated in the task configuration file, and create the target tables manually if they are not supposed to be replicated. Intermediate tables are always created as plain MergeTree.  The process is as follows:   Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one I’m not sure of the order since I was copying from 1 shard to 4 shards. The metadata in ZooKeeper suggests the order described here.  Copy chunks of data one by one.  Copy data from source shards one by one.  Create intermediate tables on all shards of the target cluster. Check the status of the chunk in ZooKeeper. Drop the partition from the intermediate table if the previous attempt was interrupted. Insert the chunk of data into the intermediate tables. Mark the shard as completed in ZooKeeper     Attach the chunks of the completed partition into the target table one by one  Attach a chunk into the target table. non-Replicated: Run OPTIMIZE target_table DEDUPLICATE for the partition on the target table.     Drop intermediate tables (may not succeed if the tables are bigger than max_table_size_to_drop).  If there are several workers running simultaneously, they will assign themselves to different source shards.\nIf a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table  Clickhouse-copier uses the engine from the task configuration file for these purposes:\n to create target and intermediate tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to ATTACH partitions into target tables, to DROP incomplete partitions from intermediate tables, to OPTIMIZE partitions after they are attached to the target. ORDER BY: to SELECT a chunk of data from the source table.  Here is an example of SELECT that clickhouse-copier runs to get the sixth of ten chunks of data:\nWHERE (\u003cthe PARTITION BY clause\u003e = (\u003ca value of the PARTITION BY expression\u003e AS partition_key)) AND (cityHash64(\u003cthe ORDER BY clause\u003e) % 10 = 6 ) Clickhouse-copier does not support the old MergeTree format.\nHowever, you can create the intermediate tables manually with the same engine as the target tables (otherwise ATTACH will not work), and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for SELECT, ATTACH PARTITION and DROP PARTITION queries.\nImportant note: always configure engine as Replicated to disable OPTIMIZE … DEDUPLICATE (unless you know why you need clickhouse-copier to run OPTIMIZE … DEDUPLICATE).\nHow to configure the number of chunks  The default value for number_of_splits is 10.\nYou can change this parameter in the table section of the task configuration file. We recommend setting it to 1 for smaller tables.\n\u003ccluster_push\u003etarget_cluster\u003c/cluster_push\u003e \u003cdatabase_push\u003etarget_database\u003c/database_push\u003e \u003ctable_push\u003etarget_table\u003c/table_push\u003e \u003cnumber_of_splits\u003e1\u003c/number_of_splits\u003e \u003cengine\u003eEngine=Replicated...\u003cengine\u003e How to monitor the status of running tasks  Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers.\nHere is a list of queries that you can use to see what’s happening.\n--task-path /clickhouse/copier/task1 -- The task config select * from system.zookeeper where path='\u003ctask-path\u003e' name | ctime | mtime ----------------------------+---------------------+-------------------- description | 2021-03-22 13:15:48 | 2021-03-22 13:25:28 task_active_workers_version | 2021-03-22 13:15:48 | 2021-03-22 20:32:09 tables | 2021-03-22 13:16:47 | 2021-03-22 13:16:47 task_active_workers | 2021-03-22 13:15:48 | 2021-03-22 13:15:48 -- Running workers select * from system.zookeeper where path='\u003ctask-path\u003e/task_active_workers' -- The list of processed tables select * from system.zookeeper where path='\u003ctask-path\u003e/tables' -- The list of processed partitions select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e' name | ctime -------+-------------------- 202103 | 2021-03-22 13:16:47 202102 | 2021-03-22 13:18:31 202101 | 2021-03-22 13:27:36 202012 | 2021-03-22 14:05:08 -- The status of a partition select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e' name | ctime ---------------+-------------------- piece_0 | 2021-03-22 13:18:31 attach_is_done | 2021-03-22 14:05:05 -- The status of a piece select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N' name | ctime -------------------------------+-------------------- shards | 2021-03-22 13:18:31 is_dirty | 2021-03-22 13:26:51 partition_piece_active_workers | 2021-03-22 13:26:54 clean_start | 2021-03-22 13:26:54 -- The status of source shards select * from system.zookeeper where path='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N/shards' name | ctime | mtime -----+---------------------+-------------------- 1 | 2021-03-22 13:26:54 | 2021-03-22 14:05:05 ","categories":"","description":"clickhouse-copier 20.4+\n","excerpt":"clickhouse-copier 20.4+\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4+/","tags":"","title":"clickhouse-copier 20.4+"},{"body":"In 21.3 there is already an option to run own clickhouse zookeeper implementation. It’s still experimental, and still need to be started additionally on few nodes (similar to ‘normal’ zookeeper) and speaks normal zookeeper protocol - needed to simplify A/B tests with real zookeeper.\nNo docs, for now, only PR with code \u0026 tests. Of course, if you want to play with it - you can, and early feedback is very valuable. But be prepared for a lot of tiny issues here and there, so don’t be disappointed if it will not satisfy your expectations for some reason. It’s very-very fresh :slightly_smiling_face: It’s ready for some trial runs, but not ready yet for production use cases.\nTo test that you need to run 3 instances of clickhouse-server (which will mimic zookeeper) with an extra config like that:\nhttps://github.com/ClickHouse/ClickHouse/blob/c8b1004ecb4bfc4aa581dbcbbbe3a4c72ce57123/tests/integration/test_keeper_multinode_simple/configs/enable_keeper1.xml\nhttps://github.com/ClickHouse/ClickHouse/blob/c8b1004ecb4bfc4aa581dbcbbbe3a4c72ce57123/tests/integration/test_keeper_snapshots/configs/enable_keeper.xml\nor event single instance with config like that: https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/keeper_port.xml\nhttps://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/zookeeper.xml\nAnd point all the clickhouses (zookeeper config secton) to those nodes / ports.\nLatest testing version is recommended. We will be thankful for any feedback.\n","categories":"","description":"clickhouse-keeper\n","excerpt":"clickhouse-keeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper/","tags":"","title":"clickhouse-keeper"},{"body":"ClickHouse does not start, some other unexpected behavior happening. Check clickhouse logs, they are your friends:\ntail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log | less\ntail -n 10000 /var/log/clickhouse-server/clickhouse-server.log | less\nHow Do I Restrict Memory Usage? See our knowledge base article and official documentation for more information.\nClickHouse died during big query execution Misconfigured clickhouse can try to allocate more RAM than is available on the system.\nIn that case an OS component called oomkiller can kill the clickhouse process.\nThat event leaves traces inside system logs (can be checked by running dmesg command).\nHow Do I make huge ‘Group By’ queries use less RAM? Enable on disk GROUP BY (it is slower, so is disabled by default)\nSet max_bytes_before_external_group_by to a value about 70-80% of your max_memory_usage value.\nData returned in chunks by clickhouse-client See https://kb.altinity.com/altinity-kb-interfaces/altinity-kb-clickhouse-client\nI Can’t Connect From Other Hosts. What do I do? Check the \u003clisten\u003e settings in config.xml. Verify that the connection can connect on both IPV4 and IPV6.\n","categories":"","description":"Cluster Configuration FAQ\n","excerpt":"Cluster Configuration FAQ\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/","tags":"","title":"Cluster Configuration FAQ"},{"body":"So you set up 3 nodes with zookeeper (zookeeper1, zookeeper2, zookeeper3 - How to install zookeer?), and and 4 nodes with ClickHouse (clickhouse-sh1r1,clickhouse-sh1r2,clickhouse-sh2r1,clickhouse-sh2r2 - how to install ClickHouse?). Now we need to make them work together.\nUse ansible/puppet/salt or other systems to control the servers’ configurations.\n Configure ClickHouse access to Zookeeper by adding the file zookeeper.xml in /etc/clickhouse-server/config.d/ folder. This file must be placed on all ClickHouse servers.  \u003cyandex\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003ezookeeper1\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper2\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper3\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003c/yandex\u003e  On each server put the file macros.xml in /etc/clickhouse-server/config.d/ folder.  \u003cyandex\u003e \u003c!-- That macros are defined per server, and they can be used in DDL, to make the DB schema cluster/server neutral --\u003e \u003cmacros\u003e \u003ccluster\u003eprod_cluster\u003c/cluster\u003e \u003cshard\u003e01\u003c/shard\u003e \u003creplica\u003eclickhouse-sh1r1\u003c/replica\u003e \u003c!-- better - use the same as hostname --\u003e \u003c/macros\u003e \u003c/yandex\u003e  On each server place the file cluster.xml in /etc/clickhouse-server/config.d/ folder. Before 20.10 ClickHouse will use default user to connect to other nodes (configurable, other users can be used), since 20.10 we recommend to use passwordless intercluster authentication based on common secret (HMAC auth)  \u003cyandex\u003e \u003cremote_servers\u003e \u003cprod_cluster\u003e \u003c!-- you need to give a some name for a cluster --\u003e \u003c!-- \u003csecret\u003esome_random_string, same on all cluster nodes, keep it safe\u003c/secret\u003e --\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/prod_cluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e  A good practice is to create 2 additional cluster configurations similar to prod_cluster above with the following distinction: but listing all nodes of single shard (all are replicas) and as nodes of 6 different shards (no replicas)  all-replicated: All nodes are listed as replicas in a single shard. all-sharded: All nodes are listed as separate shards with no replicas.    Once this is complete, other queries that span nodes can be performed. For example:\nCREATETABLEtest_table_localONCLUSTER'{cluster}'(idUInt8)Engine=ReplicatedMergeTree('/clickhouse/tables/{database}/{table}/{shard}','{replica}')ORDERBY(id);That will create a table on all servers in the cluster. You can insert data into this table and it will be replicated automatically to the other shards.To store the data or read the data from all shards at the same time, create a Distributed table that links to the replicatedMergeTree table.\nCREATETABLEtest_tableONCLUSTER'{cluster}'Engine=Distributed('{cluster}','default',' Hardening ClickHouse Security See \u003cstrong\u003ehttps://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\u003c/strong\u003e\nAdditional Settings See https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust\nUsers Disable or add password for the default users default and readonly if your server is accessible from non-trusted networks.\nIf you add password to the default user, you will need to adjust cluster configuration, since the other servers need to know the default user’s should know the default user’s to connect to each other.\nIf you’re inside a trusted network, you can leave default user set to nothing to allow the ClickHouse nodes to communicate with each other.\nEngines \u0026 ClickHouse building blocks For general explanations of roles of different engines - check the post Distributed vs Shard vs Replicated ahhh, help me!!!.\nZookeeper Paths Use conventions for zookeeper paths. For example, use:\nReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_name', ‘{replica}')\nfor:\nSELECT * FROM system.zookeeper WHERE path='/ …';\nConfiguration Best Practices    Attribution\nModified by a post on GitHub by Mikhail Filimonov.\n     The following are recommended Best Practices when it comes to setting up a ClickHouse Cluster with Zookeeper:\n Don’t edit/overwrite default configuration files. Sometimes a newer version of ClickHouse introduces some new settings or changes the defaults in config.xml and users.xml.  Set configurations via the extra files in conf.d directory. For example, to overwrite the interface save the file config.d/listen.xml, with the following:    \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003clisten_host replace=\"replace\"\u003e::\u003c/listen_host\u003e \u003c/yandex\u003e  The same is true for users. For example, change the default profile by putting the file in users.d/profile_default.xml:  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault replace=\"replace\"\u003e \u003cmax_memory_usage\u003e15000000000\u003c/max_memory_usage\u003e \u003cmax_bytes_before_external_group_by\u003e12000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e12000000000\u003c/max_bytes_before_external_sort\u003e \u003cdistributed_aggregation_memory_efficient\u003e1\u003c/distributed_aggregation_memory_efficient\u003e \u003cuse_uncompressed_cache\u003e0\u003c/use_uncompressed_cache\u003e \u003cload_balancing\u003erandom\u003c/load_balancing\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003cmax_execution_time\u003e600\u003c/max_execution_time\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e  Or you can create a user by putting a file users.d/user_xxx.xml (since 20.5 you can also use CREATE USER)  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cusers\u003e \u003cxxx\u003e \u003c!-- PASSWORD=$(base64 \u003c /dev/urandom | head -c8); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha256sum | tr -d '-' --\u003e \u003cpassword_sha256_hex\u003e...\u003c/password_sha256_hex\u003e \u003cnetworks incl=\"networks\" /\u003e \u003cprofile\u003ereadonly\u003c/profile\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003callow_databases incl=\"allowed_databases\" /\u003e \u003c/xxx\u003e \u003c/users\u003e \u003c/yandex\u003e  Some parts of configuration will contain repeated elements (like allowed ips for all the users). To avoid repeating that - use substitutions file. By default its /etc/metrika.xml, but you can change it for example to /etc/clickhouse-server/substitutions.xml with the \u003cinclude_from\u003e section of the main config. Put the repeated parts into substitutions file, like this:  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cnetworks\u003e \u003cip\u003e::1\u003c/ip\u003e \u003cip\u003e127.0.0.1\u003c/ip\u003e \u003cip\u003e10.42.0.0/16\u003c/ip\u003e \u003cip\u003e192.168.0.0/24\u003c/ip\u003e \u003c/networks\u003e \u003c/yandex\u003e These files can be common for all the servers inside the cluster or can be individualized per server. If you choose to use one substitutions file per cluster, not per node, you will also need to generate the file with macros, if macros are used.\nThis way you have full flexibility; you’re not limited to the settings described in the template. You can change any settings per server or data center just by assigning files with some settings to that server or server group. It becomes easy to navigate, edit, and assign files.\nOther Configuration Recommendations Other configurations that should be evaluated:\n \u003clisten\u003e in config.xml: Determines which IP addresses and ports the ClickHouse servers listen for incoming communications. \u003cmax_memory_..\u003e and \u003cmax_bytes_before_external_…\u003e in users.xml. These are part of the profile \u003cdefault\u003e. \u003cmax_execution_time\u003e \u003clog_queries\u003e  The following extra debug logs should be considered:\n part_log text_log  Understanding The Configuration ClickHouse configuration stores most of its information in two files:\n config.xml: Stores Server configuration parameters. They are server wide, some are hierarchical , and most of them can’t be changed in runtime. The list of settings to apply without a restart changes from version to version. Some settings can be verified using system tables, for example:  macros (system.macros) remote_servers (system.clusters)   users.xml: Configure users, and user level / session level settings.  Each user can change these during their session by:  Using parameter in http query By using parameter for clickhouse-client Sending query like set allow_experimental_data_skipping_indices=1.   Those settings and their current values are visible in system.settings. You can make some settings global by editing default profile in users.xml, which does not need restart. You can forbid users to change their settings by using readonly=2 for that user, or using setting constraints. Changes in users.xml are applied w/o restart.    For both config.xml and users.xml, it’s preferable to put adjustments in the config.d and users.d subfolders instead of editing config.xml and users.xml directly.\nYou can check if the config file was reread by checking /var/lib/clickhouse/preprocessed_configs/ folder.\n","categories":"","description":"Cluster Configuration Process\n","excerpt":"Cluster Configuration Process\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/","tags":"","title":"Cluster Configuration Process"},{"body":"See\n{% page-ref page=“altinity-kb-how-to-test-different-compression-codecs.md” %}\n{% embed url=“https://altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse\" %}\n{% embed url=“https://www.percona.com/sites/default/files/ple19-slides/day1-pm/clickhouse-for-timeseries.pdf\" %}\n","categories":"","description":"Codecs\n","excerpt":"Codecs\n","ref":"/altinity-kb-schema-design/codecs/","tags":"","title":"Codecs"},{"body":"{% hint style=“info” %} Supported since 20.10 (PR #15089). On older versions you will get exception:\nDB::Exception: Codec Delta is not applicable for Array(UInt64) because the data type is not of fixed size. {% endhint %}\nDROPTABLEIFEXISTSarray_codec_testSYNCcreatetablearray_codec_test(numberUInt64,arrArray(UInt64))Engine=MergeTreeORDERBYnumber;INSERTINTOarray_codec_testSELECTnumber,arrayMap(i-\u003enumber+i,range(100))fromnumbers(10000000);/**** Default LZ4 *****/OPTIMIZETABLEarray_codec_testFINAL;--- Elapsed: 3.386 sec. SELECT*FROMsystem.columnsWHERE(table='array_codec_test')AND(name='arr')/* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 173866750 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: *//****** Delta, LZ4 ******/ALTERTABLEarray_codec_testMODIFYCOLUMNarrArray(UInt64)CODEC(Delta,LZ4);OPTIMIZETABLEarray_codec_testFINAL--0 rows in set. Elapsed: 4.577 sec. SELECT*FROMsystem.columnsWHERE(table='array_codec_test')AND(name='arr')/* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 32458310 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: CODEC(Delta(8), LZ4) */","categories":"","description":"Codecs on array columns\n","excerpt":"Codecs on array columns\n","ref":"/altinity-kb-schema-design/codecs/codecs-on-array-columns/","tags":"","title":"Codecs on array columns"},{"body":"createtabletest_codec_speedengine=MergeTreeORDERBYtuple()asselectcast(now()+rand()%2000+number,'DateTime')asxfromnumbers(1000000000);option1:CODEC(LZ4)(sameasdefault)option2:CODEC(DoubleDelta)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(DoubleDelta)`);option3:CODEC(T64,LZ4)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,LZ4)`)option4:CODEC(Delta,LZ4)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Delta,LZ4)`)option5:CODEC(ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(ZSTD(1))`)option6:CODEC(T64,ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,ZSTD(1))`)option7:CODEC(Delta,ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Delta,ZSTD(1))`)option8:CODEC(T64,LZ4HC(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,LZ4HC(1))`)option9:CODEC(Gorilla)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Gorilla)`)Resultmaybenot100%reliable(checkedonmylaptop,needtoberepeatedinlabenvironment)OPTIMIZETABLEtest_codec_speedFINAL(secondrun-i.e.read+writethesamedata)1)17sec.2)30sec.3)16sec4)17sec5)29sec6)24sec7)31sec8)35sec9)19seccompressedsize1)31813768812)23337936993)18626603074)34085027575)23930782666)17655561737)21760804978)18104712479)2109640716selectmax(x)fromtest_codec_speed1)0.5972)2.756:(3)1.1684)0.7525)1.3626)1.3647)1.7528)1.2709)1.607","categories":"","description":"Codecs speed\n","excerpt":"Codecs speed\n","ref":"/altinity-kb-schema-design/codecs/codecs-speed/","tags":"","title":"Codecs speed"},{"body":"Options here are:\n UseINSERT INTO foo_replicated SELECT * FROM foo . Create table aside and attach all partition from the existing table then drop original table (uses hard links don’t require extra disk space). ALTER TABLE foo_replicated ATTACH PARTITION ID 'bar' FROM 'foo' You can easily auto generate those commands using a query like: SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID '' || partition_id || '' FROM foo' from system.parts WHERE table = 'foo' Do it ‘in place’ using some file manipulation. see the procedure described here: https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replication/#converting-from-mergetree-to-replicatedmergetree Do a backup of MergeTree and recover as ReplicatedMergeTree. https://github.com/AlexAkulov/clickhouse-backup/blob/master/Examples.md#how-to-convert-mergetree-to-replicatedmegretree Embedded command for that should be added in future.  ","categories":"","description":"Converting MergeTree to Replicated\n","excerpt":"Converting MergeTree to Replicated\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-converting-mergetree-to-replicated/","tags":"","title":"Converting MergeTree to Replicated"},{"body":"Sample data CREATETABLEevents(`ts`DateTime,`user_id`UInt32)ENGINE=Memory;INSERTINTOeventsSELECTtoDateTime('2021-04-29 10:10:10')+toIntervalHour(7*number)ASts,toDayOfWeek(ts)+(number%2)ASuser_idFROMnumbers(15);Using arrays WITHgroupArray(_ts)ASts_arr,groupArray(state)ASstate_arrSELECTarrayJoin(ts_arr)ASts,arrayReduce('uniqExactMerge',arrayFilter((x,y)-\u003e(y\u003c=ts),state_arr,ts_arr))ASuniqFROM(SELECTtoStartOfDay(ts)AS_ts,uniqExactState(user_id)ASstateFROMeventsGROUPBY_ts)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘WITHarrayJoin(range(toUInt32(_ts)ASint,least(int+toUInt32((3600*24)*5),toUInt32(toDateTime('2021-05-04 00:00:00'))),3600*24))ASts_expandedSELECTtoDateTime(ts_expanded)ASts,uniqExactMerge(state)ASuniqFROM(SELECTtoStartOfDay(ts)AS_ts,uniqExactState(user_id)ASstateFROMeventsGROUPBY_ts)GROUPBYtsORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘Using window functions (starting from Clickhouse 21.3) SELECTts,uniqExactMerge(state)OVER(ORDERBYtsASCROWSBETWEENUNBOUNDEDPRECEDINGANDCURRENTROW)ASuniqFROM(SELECTtoStartOfDay(ts)ASts,uniqExactState(user_id)ASstateFROMeventsGROUPBYts)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘Using runningAccumulate (incorrect result over blocks) SELECTts,runningAccumulate(state)ASuniqFROM(SELECTtoStartOfDay(ts)ASts,uniqExactState(user_id)ASstateFROMeventsGROUPBYtsORDERBYtsASC)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘","categories":"","description":"Cumulative Anything\n","excerpt":"Cumulative Anything\n","ref":"/altinity-kb-queries-and-syntax/cumulative-unique/","tags":"","title":"Cumulative Anything"},{"body":"Export \u0026 Import into common data formats.  Pros and cons:\nData can be inserted into any DBMS.\nDecoding \u0026 encoding of common data formats may be slower / require more CPU\nThe data size is usually bigger than ClickHouse formats.\nSome of the common data formats have limitations.\n{% hint style=“info” %} The best approach to do that is using clickhouse-client, in that case, encoding/decoding of format happens client-side, while client and server speak clickhouse Native format (columnar \u0026 compressed).\nIn contrast: when you use HTTP protocol, the server do encoding/decoding and more data is passed between client and server. {% endhint %}\nremote/remoteSecure or cluster/Distributed table  Pros and cons:\nSimple to run.\nIt’s possible to change the schema and distribution of data between shards.\nIt’s possible to copy only some subset of data\nNeeds only access to ClickHouse TCP port.\nUses CPU / RAM (mostly on the receiver side)\nSee details in:\n{% page-ref page=“remote-…-table-function.md” %}\nclickhouse-copier  Pros and cons:\nPossible to do some changes schema.\nNeeds only access to ClickHouse TCP port.\nIt’s possible to change the distribution of data between shards.\nSuitable for large clusters: many clickhouse-copier can execute the same task together.\nMay create an inconsistent result if source cluster data is changing during the process\nHard to setup.\nRequires zookeeper.\nUses CPU / RAM (mostly on the clickhouse-copier and receiver side)\n{% hint style=“info” %} Internally it works like smart INSERT INTO cluster(…) SELECT * FROM ... with some consistency checks. {% endhint %}\n{% hint style=“info” %} Run clickhouse copier on the same nodes as receiver clickhouse, to avoid doubling the network load. {% endhint %}\nSee details in:\n{% page-ref page=“altinity-kb-clickhouse-copier/” %}\nManual parts moving: freeze / rsync / attach  Pros and cons:\nLow CPU / RAM usage.\nTable schema should be the same.\nA lot of manual operations/scripting.\n{% hint style=“info” %} With some additional care and scripting, it’s possible to do cheap re-sharding on parts level. {% endhint %}\nSee details in:\n{% page-ref page=“rsync.md” %}\nclickhouse-backup Pros and cons:\nLow CPU / RAM usage.\nSuitable to recover both schema \u0026 data for all tables at once.\nTable schema should be the same.\nJust create the backup on server 1, upload it to server 2, and restore the backup.\nSee https://github.com/AlexAkulov/clickhouse-backup\n{% embed url=“https://altinity.com/blog/introduction-to-clickhouse-backups-and-clickhouse-backup\" caption=”\" %}\nFetch from zookeeper path Pros and cons:\nLow CPU / RAM usage\nTable schema should be the same.\nWorks only when the source and the destination clickhouse servers share the same zookeeper (without chroot)\nNeeds to access zookeeper and ClickHouse replication ports: (interserver_http_port or interserver_https_port)\nALTER TABLE table_name FETCH PARTITION partition_expr FROM 'path-in-zookeeper' Replication protocol  Just make one more replica in another place.\nPros and cons:\nSimple to setup\nData is consistent all the time automatically.\nLow CPU and network usage.\nNeeds to reach both zookeeper client (2181) and ClickHouse replication ports: (interserver_http_port or interserver_https_port)\nIn case of cluster migration, zookeeper need’s to be migrated too.\nReplication works both ways.\n{% page-ref page=\"../altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration.md\" %}\nSee also  Github issues: https://github.com/ClickHouse/ClickHouse/issues/10943\nhttps://github.com/ClickHouse/ClickHouse/issues/20219\nhttps://github.com/ClickHouse/ClickHouse/pull/17871\nOther links: https://habr.com/ru/company/avito/blog/500678/\n","categories":"","description":"Data Migration\n","excerpt":"Data Migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/","tags":"","title":"Data Migration"},{"body":"   DataType RAM size (=byteSize) Disk Size     String string byte length + 9 string length: 64 bit integer\nzero-byte terminator: 1 byte.\n string length prefix (varint) + string itself:\n string shorter than 128 - string byte length + 1 string shorter than 16384 - string byte length + 2 string shorter than 2097152 - string byte length + 2 string shorter than 268435456 - string byte length + 4\n   AggregateFunction(count, ...)  varint    See also https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/data_processing.pdf (slide 17-22)\n","categories":"","description":"Data types on disk and in RAM\n","excerpt":"Data types on disk and in RAM\n","ref":"/altinity-kb-queries-and-syntax/data-types-on-disk-and-in-ram/","tags":"","title":"Data types on disk and in RAM"},{"body":"Tables Table size SELECTdatabase,table,formatReadableSize(sum(data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rate,sum(rows)ASrows,count()ASpart_countFROMsystem.partsWHERE(active=1)AND(tableLIKE'%')AND(databaseLIKE'%')GROUPBYdatabase,tableORDERBYsizeDESC;Column size SELECTdatabase,table,column,formatReadableSize(sum(column_data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rateFROMsystem.parts_columnsWHERE(active=1)AND(tableLIKE'query_log')GROUPBYdatabase,table,columnORDERBYsizeDESC;Projections Projection size SELECTdatabase,table,name,formatReadableSize(sum(data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rate,sum(rows)ASrows,count()ASpart_countFROMsystem.projection_partsWHERE(table='ptest')ANDactiveGROUPBYdatabase,table,nameORDERBYsizeDESC;Projection column size SELECTdatabase,table,column,formatReadableSize(sum(column_data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rateFROMsystem.projection_parts_columnsWHERE(active=1)AND(tableLIKE'ptest')GROUPBYdatabase,table,columnORDERBYsizeDESC;","categories":"","description":"Database Size - Table - Column size\n","excerpt":"Database Size - Table - Column size\n","ref":"/altinity-kb-useful-queries/altinity-kb-database-size-table-column-size/","tags":"","title":"Database Size - Table - Column size"},{"body":"","categories":"","description":"Datasets\n","excerpt":"Datasets\n","ref":"/altinity-kb-useful-queries/altinity-kb-datasets/","tags":"","title":"Datasets"},{"body":"DDLWorker is a subprocess (thread) of clickhouse-server that executes ON CLUSTER tasks at the node.\nWhen you execute a DDL query with ON CLUSTER mycluster section the query executor at the current node reads the cluster mycluster definition (remote_servers / system.clusters) and places tasks into Zookeeper znode task_queue/ddl/... for members of the cluster mycluster.\nDDLWorker at all ClickHouse nodes constantly check this task_queue for their tasks and executes them locally and reports about a result back into task_queue.\nThe common issue is the different hostnames/IPAddresses in the cluster definition and locally.\nSo a node initiator puts tasks for a host named Host1. But the Host1 thinks about own name as localhost or xdgt634678d (internal docker hostname) and never sees tasks for the Host1 because is looking tasks for xdgt634678d. The same with internal VS external IP addresses.\nAnother issue that sometimes DDLWorker thread can crash then ClickHouse node stops to execute ON CLUSTER tasks.\nCheck that DDLWorker is alive:\n--- title: \"ps -eL|grep DDL\" linkTitle: \"ps -eL|grep DDL\" description: \u003e ps -eL|grep DDL --- 18829 18876 ? 00:00:00 DDLWorkerClnr 18829 18879 ? 00:00:00 DDLWorker --- title: \"ps -ef|grep 18829|grep -v grep\" linkTitle: \"ps -ef|grep 18829|grep -v grep\" description: \u003e ps -ef|grep 18829|grep -v grep --- clickho+ 18829 18828 1 Feb09 ? 00:55:00 /usr/bin/clickhouse-server --con... As you can see there are two threads: DDLWorker and DDLWorkerClnr.\nThe second thread – DDLWorkerCleaner cleans old tasks from task_queue. You can configure how many recent tasks to store:\nconfig.xml \u003cyandex\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/task_queue/ddl\u003c/path\u003e \u003cmax_tasks_in_queue\u003e1000\u003c/max_tasks_in_queue\u003e \u003ctask_max_lifetime\u003e604800\u003c/task_max_lifetime\u003e \u003ccleanup_delay_period\u003e60\u003c/cleanup_delay_period\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e Default values:\ncleanup_delay_period = 60 seconds – Sets how often to start cleanup to remove outdated data.\ntask_max_lifetime = 7 * 24 * 60 * 60 (in seconds = week) – Delete task if its age is greater than that.\nmax_tasks_in_queue = 1000 – How many tasks could be in the queue.\n","categories":"","description":"DDLWorker\n","excerpt":"DDLWorker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/","tags":"","title":"DDLWorker"},{"body":"CREATETABLEtest_delete(`key`UInt32,`ts`UInt32,`value_a`String,`value_b`String,`value_c`String,`is_active`UInt8DEFAULT1)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_delete(key,ts,value_a,value_b,value_c)SELECTnumber,1,concat('some_looong_string',toString(number)),concat('another_long_str',toString(number)),concat('string',toString(number))FROMnumbers(10000000);INSERTINTOtest_delete(key,ts,value_a,value_b,value_c)VALUES(400000,2,'totally different string','another totally different string','last string');SELECT*FROMtest_deleteWHEREkey=400000;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘┌────key─┬─ts─┬─value_a──────────────────┬─value_b────────────────┬─value_c──────┬─is_active─┐│400000│1│some_looong_string400000│another_long_str400000│string400000│1│└────────┴────┴──────────────────────────┴────────────────────────┴──────────────┴───────────┘SETmutations_sync=2;ALTERTABLEtest_deleteUPDATEis_active=0WHERE(key=400000)AND(ts=1);Ok.0rowsinset.Elapsed:0.058sec.SELECT*FROMtest_deleteWHERE(key=400000)ANDis_active;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ALTERTABLEtest_deleteDELETEWHERE(key=400000)AND(ts=1);Ok.0rowsinset.Elapsed:1.101sec.-- 20 times slower!!! SELECT*FROMtest_deleteWHEREkey=400000;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘-- For ReplacingMergeTree OPTIMIZETABLEtest_deleteFINAL;Ok.0rowsinset.Elapsed:2.230sec.-- 40 times slower!!! SELECT*FROMtest_deleteWHEREkey=400000┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘","categories":"","description":"DELETE via tombstone column\n","excerpt":"DELETE via tombstone column\n","ref":"/altinity-kb-queries-and-syntax/delete-via-tombstone-column/","tags":"","title":"DELETE via tombstone column"},{"body":"For more information on ClickHouse Dictionaries, see the presentation https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup34/clickhouse_integration.pdf, slides 82-95.\n","categories":"","description":"Dictionaries\n","excerpt":"Dictionaries\n","ref":"/altinity-kb-dictionaries/","tags":"","title":"Dictionaries"},{"body":"Dictionary with Clickhouse table as a source Test data DROPTABLEIFEXISTSarr_src;CREATETABLEarr_src(keyUInt64,array_intArray(Int64),array_strArray(String))ENGINE=MergeTreeorderbykey;INSERTINTOarr_srcSELECTnumber,arrayMap(i-\u003e(number*i),range(5)),arrayMap(i-\u003econcat('str',toString(number*i)),range(5))FROMnumbers(1000);Dictionary DROPDICTIONARYIFEXISTSarr_dict;CREATEDICTIONARYarr_dict(keyUInt64,array_intArray(Int64)DEFAULT[1,2,3],array_strArray(String)DEFAULT['1','2','3'])PRIMARYKEYkeySOURCE(CLICKHOUSE(DATABASE'default'TABLE'arr_src'))LIFETIME(120)LAYOUT(HASHED());SELECTdictGet('arr_dict','array_int',toUInt64(42))ASres_int,dictGetOrDefault('arr_dict','array_str',toUInt64(424242),['none'])ASres_str┌─res_int───────────┬─res_str──┐│[0,42,84,126,168]│['none']│└───────────────────┴──────────┘Dictionary with Postgresql as a source Test data in PG createuserch;createdatabasech;GRANTALLPRIVILEGESONDATABASEchTOch;ALTERUSERchWITHPASSWORD'chch';CREATETABLEarr_src(keyint,array_intinteger[],array_strtext[]);INSERTINTOarr_srcVALUES(42,'{0,42,84,126,168}','{\"str0\",\"str42\",\"str84\",\"str126\",\"str168\"}'),(66,'{0,66,132,198,264}','{\"str0\",\"str66\",\"str132\",\"str198\",\"str264\"}');Dictionary CREATEDICTIONARYpg_arr_dict(keyUInt64,array_intArray(Int64)DEFAULT[1,2,3],array_strArray(String)DEFAULT['1','2','3'])PRIMARYKEYkeySOURCE(POSTGRESQL(PORT5432HOST'pg-host'user'ch'password'chch'DATABASE'ch'TABLE'arr_src'))LIFETIME(120)LAYOUT(HASHED());select*frompg_arr_dict;┌─key─┬─array_int──────────┬─array_str───────────────────────────────────┐│66│[0,66,132,198,264]│['str0','str66','str132','str198','str264']││42│[0,42,84,126,168]│['str0','str42','str84','str126','str168']│└─────┴────────────────────┴─────────────────────────────────────────────┘SELECTdictGet('pg_arr_dict','array_int',toUInt64(42))ASres_int,dictGetOrDefault('pg_arr_dict','array_str',toUInt64(424242),['none'])ASres_str┌─res_int───────────┬─res_str──┐│[0,42,84,126,168]│['none']│└───────────────────┴──────────┘","categories":"","description":"Dictionaries \u0026 arrays\n","excerpt":"Dictionaries \u0026 arrays\n","ref":"/altinity-kb-dictionaries/dictionaries-and-arrays/","tags":"","title":"Dictionaries \u0026 arrays"},{"body":"Q. I think I’m still trying to understand how de-normalized is okay - with my relational mindset, I want to move repeated string fields into their own table, but I’m not sure to what extent this is necessary\nI will look at LowCardinality in more detail - I think it may work well here\nA. If it’s a simple repetition, which you don’t need to manipulate/change in future - LowCardinality works great, and you usually don’t need to increase the system complexity by introducing dicts.\nFor example: name of team ‘Manchester United’ will rather not be changed, and even if it will you can keep the historical records with historical name. So normalization here (with some dicts) is very optional, and de-normalized approach with LowCardinality is good \u0026 simpler alternative.\nFrom the other hand: if data can be changed in future, and that change should impact the reports, then normalization may be a big advantage.\nFor example if you need to change the used currency rare every day- it would be quite stupid to update all historical records to apply the newest exchange rate. And putting it to dict will allow to do calculations with latest exchange rate at select time.\nFor dictionary it’s possible to mark some of the attributes as injective. An attribute is called injective if different attribute values correspond to different keys. It would allow ClickHouse to replace dictGet call in GROUP BY with cheap dict key.\n","categories":"","description":"Dictionaries vs LowCardinality\n","excerpt":"Dictionaries vs LowCardinality\n","ref":"/altinity-kb-schema-design/altinity-kb-dictionaries-vs-lowcardinality/","tags":"","title":"Dictionaries vs LowCardinality"},{"body":"DROPTABLEIFEXISTSdictionary_source_en;DROPTABLEIFEXISTSdictionary_source_ru;DROPTABLEIFEXISTSdictionary_source_view;DROPDICTIONARYIFEXISTSflat_dictionary;CREATETABLEdictionary_source_en(idUInt64,valueString)ENGINE=TinyLog;INSERTINTOdictionary_source_enVALUES(1,'One'),(2,'Two'),(3,'Three');CREATETABLEdictionary_source_ru(idUInt64,valueString)ENGINE=TinyLog;INSERTINTOdictionary_source_ruVALUES(1,'Один'),(2,'Два'),(3,'Три');CREATEVIEWdictionary_source_viewASSELECTid,dictionary_source_en.valueasvalue_en,dictionary_source_ru.valueasvalue_ruFROMdictionary_source_enLEFTJOINdictionary_source_ruUSING(id);select*fromdictionary_source_view;CREATEDICTIONARYflat_dictionary(idUInt64,value_enString,value_ruString)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000USER'default'PASSWORD''TABLE'dictionary_source_view'))LIFETIME(MIN1MAX1000)LAYOUT(FLAT());SELECTdictGet(concat(currentDatabase(),'.flat_dictionary'),'value_en',number+1),dictGet(concat(currentDatabase(),'.flat_dictionary'),'value_ru',number+1)FROMnumbers(3);","categories":"","description":"Dictionary on the top of the several tables using VIEW\n","excerpt":"Dictionary on the top of the several tables using VIEW\n","ref":"/altinity-kb-dictionaries/dictionary-on-top-tables/","tags":"","title":"Dictionary on the top of the several tables using VIEW"},{"body":"RocksDB is faster than MergeTree on Key/Value queries because MergeTree primary key index is sparse. Probably it’s possible to speedup MergeTree by reducing index_granularity.\nNVMe disk is used for the tests.\nThe main feature of RocksDB is instant updates. You can update a row instantly (microseconds):\nselect*fromrocksDBwhereA=15645646;┌────────A─┬─B────────────────────┐│15645646│12517841379565221195│└──────────┴──────────────────────┘1rowsinset.Elapsed:0.001sec.insertintorocksDBvalues(15645646,'xxxx');1rowsinset.Elapsed:0.001sec.select*fromrocksDBwhereA=15645646;┌────────A─┬─B────┐│15645646│xxxx│└──────────┴──────┘1rowsinset.Elapsed:0.001sec.Let’s load 100 millions rows:\ncreatetablerocksDB(AUInt64,BString,primarykeyA)Engine=EmbeddedRocksDB();insertintorocksDBselectnumber,toString(cityHash64(number))fromnumbers(100000000);-- 0 rows in set. Elapsed: 154.559 sec. Processed 100.66 million rows, 805.28 MB (651.27 thousand rows/s., 5.21 MB/s.) -- Size on disk: 1.5GB createtablemergeTreeDB(AUInt64,BString)Engine=MergeTree()orderbyA;insertintomergeTreeDBselectnumber,toString(cityHash64(number))fromnumbers(100000000);Sizeondisk:973MBCREATEDICTIONARYtest_rocksDB(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLErocksDBDB'default'USER'default'))LAYOUT(DIRECT());CREATEDICTIONARYtest_mergeTreeDB(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(DIRECT());Direct queries to tables to request 10000 rows by a random key  selectcount()from(select*fromrocksDBwhereAin(selecttoUInt64(rand64()%100000000)fromnumbers(10000)))Elapsed:0.076sec.Processed10.00thousandrowsselectcount()from(select*frommergeTreeDBwhereAin(selecttoUInt64(rand64()%100000000)fromnumbers(10000)))Elapsed:0.202sec.Processed55.95millionrowsRocksDB as expected is much faster: 0.076 sec. VS 0.202 sec.\nRocksDB processes less rows: 10.00 thousand rows VS 55.95 million rows\ndictGet – 100.00 thousand random rows  selectcount()from(selectdictGet('default.test_rocksDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(100000))Elapsed:0.786sec.Processed100.00thousandrowsselectcount()from(selectdictGet('default.test_mergeTreeDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(100000))Elapsed:3.160sec.Processed100.00thousandrowsdictGet – 1million random rows  selectcount()from(selectdictGet('default.test_rocksDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:5.643sec.Processed1.00millionrowsselectcount()from(selectdictGet('default.test_mergeTreeDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:31.111sec.Processed1.00millionrowsdictGet – 1million random rows from Hashed  CREATEDICTIONARYtest_mergeTreeDBHashed(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(Hashed())LIFETIME(0);0rowsinset.Elapsed:46.564sec.┌─name───────────────────┬─type───┬─status─┬─element_count─┬─RAM──────┐│test_mergeTreeDBHashed│Hashed│LOADED│100000000│7.87GiB│└────────────────────────┴────────┴────────┴───────────────┴──────────┘selectcount()from(selectdictGet('default.test_mergeTreeDBHashed','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:0.079sec.Processed1.00millionrowsdictGet – 1million random rows from SparseHashed  CREATEDICTIONARYtest_mergeTreeDBSparseHashed(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(SPARSE_HASHED())LIFETIME(0);0rowsinset.Elapsed:81.404sec.┌─name─────────────────────────┬─type─────────┬─status─┬─element_count─┬─RAM──────┐│test_mergeTreeDBSparseHashed│SparseHashed│LOADED│100000000│4.24GiB│└──────────────────────────────┴──────────────┴────────┴───────────────┴──────────┘selectcount()from(selectdictGet('default.test_mergeTreeDBSparseHashed','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:0.065sec.Processed1.00millionrows","categories":"","description":"EmbeddedRocksDB \u0026 dictionary\n","excerpt":"EmbeddedRocksDB \u0026 dictionary\n","ref":"/engines/altinity-kb-embeddedrocksdb-and-dictionary/","tags":"","title":"EmbeddedRocksDB \u0026 dictionary"},{"body":"Generally: the main engine in Clickhouse is called MergeTree. It allows to store and process data on one server and feel all the advantages of Clickhouse. Basic usage of MergeTree does not require any special configuration, and you can start using it ‘out of the box’.\nBut one server and one copy of data are not fault-tolerant - something can happen with the server itself, with datacenter availability, etc. So you need to have the replica(s) - i.e. server(s) with the same data and which can ‘substitute’ the original server at any moment.\nTo have an extra copy (replica) of your data you need to use ReplicatedMergeTree engine. It can be used instead of MergeTree engine, and you can always upgrade from MergeTree to ReplicatedMergeTree (and downgrade back) if you need. To use that you need to have ZooKeeper installed and running. For tests, you can use one standalone Zookeeper instance, but for production usage, you should have zookeeper ensemble at least of 3 servers.\nWhen you use ReplicatedMergeTree then the inserted data is copied automatically to all the replicas, but all the SELECTs are executed on the single server you have connected to. So you can have 5 replicas of your data, but if you will always connect to one replica - it will not ‘share’ / ‘balance’ that traffic automatically between all the replicas, one server will be loaded and the rest will generally do nothing. If you need that balancing of load between multiple replicas - you can use the internal ‘loadbalancer’ mechanism which is provided by Distrtibuted engine of Clickhouse. As an alternative in that scenario you can work without Distribured table, but with some external load balancer that will balance the requests between several replicas according to your specific rules or preferences, or just cluster-aware client which will pick one of the servers for the query time.\nThe Distributed engine does not store any data, but it can ‘point’ to the same ReplicatedMergeTree/MergeTree table on multiple servers. To use Distributed engine you need to configure \u003ccluser\u003e settings in your ClickHouse server config file.\nSo let’s say you have 3 replicas of table my_replicated_data with ReplicatedMergeTree engine. You can create a table with Distrtibuted engine called my_distributed_replicated_data which will ‘point’ to all of that 3 servers, and when you will select from that my_distributed_replicated_data table the select will be forwarded and executed on one of the replicas. So in that scenario, each replica will get 1/3 of requests (but each request still will be fully executed on one chosen replica).\nAll that is great, and will work well while one copy of your data is fitting on a single physical server, and can be processed by the resources of one server. When you have too much data to be stored/processed on one server - you need to use sharding (it’s just a way to split the data into smaller parts). Sharding is the mechanism also provided by Distributed engine.\nWith sharding data is divided into parts (shards) according to some sharding key. You can just use random distribution, so let’s say - throw a coin to decide on each of the servers the data should be stored, or you can use some ‘smarter’ sharding scheme, to make the data connected to the same subject (let’s say to the same customer) stored on one server, and to another subject on another. So in that case all the shards should be requested at the same time and later the ‘common’ result should be calculated.\nIn ClickHouse each shard works independently and process its' part of data, inside each shard replication can work. And later to query all the shards at the same time and combine the final result - Distributed engine is used. So Distributed work as load balancer inside each shard, and can combine the data coming from different shards together to make the ‘common’ result.\nYou can use Distribured table for inserts, in that case, it will pass the data to one of the shards according to the sharding key. Or you can insert to the underlying table on one of the shards bypassing the Distributed table.\nShort summary  start with MergeTree to have several copies of data use ReplicatedMergeTree if your data is too big to fit/ to process on one server - use sharding to balance the load between replicas and to combine the result of selects from different shards - use Distributed table.  More… Official tutorial clarify that a bit: https://clickhouse.yandex/tutorial.html\nPlease check also @alex-zaitsev presentation, which also covers that subject: https://www.youtube.com/watch?v=zbjub8BQPyE\n( Slides are here: https://yadi.sk/i/iLA5ssAv3NdYGy )\nP.S. Actually you can create replication without Zookeeper and ReplicatedMergeTree, just by using the Distributed table above MergeTree and internal_replication=false cluster setting, but in that case, there will no guarantee that all the replicas will have 100% the same data, so I rather would not recommend that scenario.\n{% page-ref page=“altinity-kb-atomic-database-engine/” %}\n{% page-ref page=“altinity-kb-embeddedrocksdb-and-dictionary.md” %}\n{% page-ref page=“mergetree-table-engine-family/altinity-kb-nulls-in-order-by.md” %}\n{% page-ref page=“mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates.md” %}\nBased on my original answer on github: https://github.com/ClickHouse/ClickHouse/issues/2161\n","categories":"","description":"Engines\n","excerpt":"Engines\n","ref":"/engines/","tags":"","title":"Engines"},{"body":"Pre 21.6 There are couple options:\nCertain formats which has schema in built in them (like JSONEachRow) could silently skip any unexpected fields after enabling setting input_format_skip_unknown_fields\nIt’s also possible to skip up to N malformed messages for each block, with used setting kafka_skip_broken_messages but it’s also does not support all possible formats.\nAfter 21.6 It’s possible to stream messages which could not be parsed, this behavior could be enabled via setting: kafka_handle_error_mode='stream' and clickhouse wil write error and message from Kafka itself to two new virtual columns: _error, _raw_message.\nSo you can create another Materialized View which would collect to a separate table all errors happening while parsing with all important information like offset and content of message.\nCREATETABLEdefault.kafka_engine(`i`Int64,`s`String)ENGINE=KafkaSETTINGSkafka_broker_list='kafka:9092'kafka_topic_list='topic',kafka_group_name='clickhouse',kafka_format='JSONEachRow',kafka_handle_error_mode='stream';CREATEMATERIALIZEDVIEWdefault.kafka_errors(`topic`String,`partition`Int64,`offset`Int64,`raw`String,`error`String)ENGINE=MergeTreeORDERBY(topic,partition,offset)SETTINGSindex_granularity=8192ASSELECT_topicAStopic,_partitionASpartition,_offsetASoffset,_raw_messageASraw,_errorASerrorFROMdefault.kafka_engineWHERElength(_error)\u003e0{% embed url=“https://github.com/ClickHouse/ClickHouse/pull/20249#issuecomment-779054737” %}\n{% embed url=“https://github.com/ClickHouse/ClickHouse/pull/21850\" %}\n{% embed url=“https://altinity.com/blog/clickhouse-kafka-engine-faq\" %}\n","categories":"","description":"Error handling\n","excerpt":"Error handling\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/error-handling/","tags":"","title":"Error handling"},{"body":"EOS consumer (isolation.level=read_committed) is enabled by default since librdkafka 1.2.0, so for ClickHouse - since 20.2\nSee:\n edenhill/librdkafka@6b2a155 9de5dff  There was a report recently that it was giving some duplicates #18668 and in should be fixed in 21.2.\nBUT: while EOS semantics will guarantee you that no duplicates will happen on the Kafka side (i.e. even if you produce the same messages few times it will be consumed once), but ClickHouse as a Kafka client can currently guarantee only at-least-once. And in some corner cases (connection lost etc) you can get duplicates.\nWe need to have something like transactions on ClickHouse side to be able to avoid that. Adding something like simple transactions is in plans for Y2021.\n","categories":"","description":"Exactly once semantics\n","excerpt":"Exactly once semantics\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/","tags":"","title":"Exactly once semantics"},{"body":"CREATEDICTIONARYpostgres_dict(idUInt32,valueString)PRIMARYKEYidSOURCE(POSTGRESQL(port5432host'postgres1'user'postgres'password'mysecretpassword'db'clickhouse'table'test_schema.test_table'))LIFETIME(MIN300MAX600)LAYOUT(HASHED());and later do\nSELECTdictGetString(postgres_dict,'value',toUInt64(1))\u0000\n","categories":"","description":"Example of PostgreSQL dictionary\n","excerpt":"Example of PostgreSQL dictionary\n","ref":"/altinity-kb-dictionaries/example-of-postgresql-dictionary/","tags":"","title":"Example of PostgreSQL dictionary"},{"body":"Use cases Strong correlation between column from table ORDER BY / PARTITION BY key and other column which is regularly being used in WHERE condition. Good example is incremental ID which increasing with time.\nCREATETABLEskip_idx_corr(`key`UInt32,`id`UInt32,`ts`DateTime)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,id);INSERTINTOskip_idx_corrSELECTrand(),number,now()+intDiv(number,10)FROMnumbers(100000000);SELECTcount()FROMskip_idx_corrWHEREid=60000001rowsinset.Elapsed:0.167sec.Processed100.00millionrows,400.00MB(599.96millionrows/s.,2.40GB/s.)ALTERTABLEskip_idx_corrADDINDEXid_idxidTYPEminmaxGRANULARITY10;ALTERTABLEskip_idx_corrMATERIALIZEINDEXid_idx;SELECTcount()FROMskip_idx_corrWHEREid=60000001rowsinset.Elapsed:0.017sec.Processed6.29millionrows,25.17MB(359.78millionrows/s.,1.44GB/s.)Multiple Date/DateTime columns can be used in WHERE conditions Usually it could happen if you have separate Date and DateTime columns and different column being used in PARTITION BY expression and in WHERE condition. Another possible scenario when you have multiple DateTime columns which have pretty the same date or even time.\nCREATETABLEskip_idx_multiple(`key`UInt32,`date`Date,`time`DateTime,`created_at`DateTime,`inserted_at`DateTime)ENGINE=MergeTreePARTITIONBYtoYYYYMM(date)ORDERBY(key,time);INSERTINTOskip_idx_multipleSELECTnumber,toDate(x),now()+intDiv(number,10)ASx,x-(rand()%100),x+(rand()%100)FROMnumbers(100000000);SELECTcount()FROMskip_idx_multipleWHEREdate\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.048sec.Processed14.02millionrows,28.04MB(290.96millionrows/s.,581.92MB/s.)SELECTcount()FROMskip_idx_multipleWHEREtime\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.188sec.Processed100.00millionrows,400.00MB(530.58millionrows/s.,2.12GB/s.)SELECTcount()FROMskip_idx_multipleWHEREcreated_at\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.400sec.Processed100.00millionrows,400.00MB(250.28millionrows/s.,1.00GB/s.)ALTERTABLEskip_idx_multipleADDINDEXtime_idxtimeTYPEminmaxGRANULARITY1000;ALTERTABLEskip_idx_multipleMATERIALIZEINDEXtime_idx;SELECTcount()FROMskip_idx_multipleWHEREtime\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.036sec.Processed14.02millionrows,56.08MB(391.99millionrows/s.,1.57GB/s.)ALTERTABLEskip_idx_multipleADDINDEXcreated_at_idxcreated_atTYPEminmaxGRANULARITY1000;ALTERTABLEskip_idx_multipleMATERIALIZEINDEXcreated_at_idx;SELECTcount()FROMskip_idx_multipleWHEREcreated_at\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.076sec.Processed14.02millionrows,56.08MB(184.90millionrows/s.,739.62MB/s.)Condition in query trying to filter outlier value. CREATETABLEskip_idx_outlier(`key`UInt32,`ts`DateTime,`value`UInt32)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,ts);INSERTINTOskip_idx_outlierSELECTnumber,now(),rand()%10FROMnumbers(10000000);INSERTINTOskip_idx_outlierSELECTnumber,now(),20FROMnumbers(10);SELECTcount()FROMskip_idx_outlierWHEREvalue\u003e15;1rowsinset.Elapsed:0.059sec.Processed10.00millionrows,40.00MB(170.64millionrows/s.,682.57MB/s.)ALTERTABLEskip_idx_outlierADDINDEXvalue_idxvalueTYPEminmaxGRANULARITY10;ALTERTABLEskip_idx_outlierMATERIALIZEINDEXvalue_idx;SELECTcount()FROMskip_idx_outlierWHEREvalue\u003e15;1rowsinset.Elapsed:0.004sec.","categories":"","description":"Example: minmax\n","excerpt":"Example: minmax\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/minmax/","tags":"","title":"Example: minmax"},{"body":"https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup47/explain.pdf\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/query-profiling.pdf\n","categories":"","description":"EXPLAIN query\n","excerpt":"EXPLAIN query\n","ref":"/altinity-kb-queries-and-syntax/explain-query/","tags":"","title":"EXPLAIN query"},{"body":"CREATETABLEevent_table(`key`UInt32,`created_at`DateTime,`value_a`UInt32,`value_b`String)ENGINE=MergeTreeORDERBY(key,created_at)INSERTINTOevent_tableSELECT1ASkey,toDateTime('2020-10-11 10:10:10')+numberAScreated_at,if((number=0)OR((number%5)=1),number+1,0)ASvalue_a,if((number=0)OR((number%3)=1),toString(number),'')ASvalue_bFROMnumbers(10)SELECTmain.key,main.created_at,a.value_a,b.value_bFROMevent_tableASmainASOFINNERJOIN(SELECTkey,created_at,value_aFROMevent_tableWHEREvalue_a!=0)ASaON(main.key=a.key)AND(main.created_at\u003e=a.created_at)ASOFINNERJOIN(SELECTkey,created_at,value_bFROMevent_tableWHEREvalue_b!='')ASbON(main.key=b.key)AND(main.created_at\u003e=b.created_at)┌─main.key─┬─────main.created_at─┬─a.value_a─┬─b.value_b─┐│1│2020-10-1110:10:10│1│0││1│2020-10-1110:10:11│2│1││1│2020-10-1110:10:12│2│1││1│2020-10-1110:10:13│2│1││1│2020-10-1110:10:14│2│4││1│2020-10-1110:10:15│2│4││1│2020-10-1110:10:16│7│4││1│2020-10-1110:10:17│7│7││1│2020-10-1110:10:18│7│7││1│2020-10-1110:10:19│7│7│└──────────┴─────────────────────┴───────────┴───────────┘SELECTkey,created_at,value_a,value_bFROM(SELECTkey,groupArray(created_at)AScreated_arr,arrayFill(x-\u003e(x!=0),groupArray(value_a))ASa_arr,arrayFill(x-\u003e(x!=''),groupArray(value_b))ASb_arrFROM(SELECT*FROMevent_tableORDERBYkeyASC,created_atASC)GROUPBYkey)ARRAYJOINcreated_arrAScreated_at,a_arrASvalue_a,b_arrASvalue_b┌─key─┬──────────created_at─┬─value_a─┬─value_b─┐│1│2020-10-1110:10:10│1│0││1│2020-10-1110:10:11│2│1││1│2020-10-1110:10:12│2│1││1│2020-10-1110:10:13│2│1││1│2020-10-1110:10:14│2│4││1│2020-10-1110:10:15│2│4││1│2020-10-1110:10:16│7│4││1│2020-10-1110:10:17│7│7││1│2020-10-1110:10:18│7│7││1│2020-10-1110:10:19│7│7│└─────┴─────────────────────┴─────────┴─────────┘","categories":"","description":"Fill missing values at query time\n","excerpt":"Fill missing values at query time\n","ref":"/altinity-kb-queries-and-syntax/fill-missing-values-at-query-time/","tags":"","title":"Fill missing values at query time"},{"body":"SELECT * FROM table FINAL\n Before 20.5 - always executed in a single thread and slow. Since 20.5 - final can be parallel, see https://github.com/ClickHouse/ClickHouse/pull/10463 Since 20.10 - you can use do_not_merge_across_partitions_select_final setting.  See https://github.com/ClickHouse/ClickHouse/pull/15938 and https://github.com/ClickHouse/ClickHouse/issues/11722\nSo it can work in the following way:\n Daily partitioning After day end + some time interval during which you can get some updates - for example at 3am / 6am you do OPTIMIZE TABLE xxx PARTITION 'prev_day' FINAL In that case using that FINAL with do_not_merge_across_partitions_select_final will be cheap.  DROPTABLEIFEXISTSrepl_tbl;CREATETABLErepl_tbl(`key`UInt32,`val_1`UInt32,`val_2`String,`val_3`String,`val_4`String,`val_5`UUID,`ts`DateTime)ENGINE=ReplacingMergeTree(ts)PARTITIONBYtoDate(ts)ORDERBYkey;​INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-01 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200101'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-02 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200102'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-03 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200103'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-04 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200104'FINAL;SYSTEMSTOPMERGESrepl_tbl;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-05 00:00:00'astsFROMnumbers(10000000);​SELECTcount()FROMrepl_tblWHERENOTignore(*)┌──count()─┐│50000000│└──────────┘1rowsinset.Elapsed:1.504sec.Processed50.00millionrows,6.40GB(33.24millionrows/s.,4.26GB/s.)SELECTcount()FROMrepl_tblFINALWHERENOTignore(*)┌──count()─┐│10000000│└──────────┘1rowsinset.Elapsed:3.314sec.Processed50.00millionrows,6.40GB(15.09millionrows/s.,1.93GB/s.)/* more that 2 time slower, and will get worse once you will have more data */setdo_not_merge_across_partitions_select_final=1;SELECTcount()FROMrepl_tblFINALWHERENOTignore(*)┌──count()─┐│50000000│└──────────┘1rowsinset.Elapsed:1.850sec.Processed50.00millionrows,6.40GB(27.03millionrows/s.,3.46GB/s.)/* only 0.35 sec slower, and while partitions have about the same size that extra cost will be about constant */","categories":"","description":"FINAL clause speed\n","excerpt":"FINAL clause speed\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/","tags":"","title":"FINAL clause speed"},{"body":"It’s possible to use dictionaries for populating columns of fact table.\nCREATETABLEcustomer(`customer_id`UInt32,`first_name`String,`birth_date`Date,`sex`Enum('M'=1,'F'=2))ENGINE=MergeTreeORDERBYcustomer_idCREATETABLEorder(`order_id`UInt32,`order_date`DateTimeDEFAULTnow(),`cust_id`UInt32,`amount`Decimal(12,2))ENGINE=MergeTreePARTITIONBYtoYYYYMM(order_date)ORDERBY(order_date,cust_id,order_id)INSERTINTOcustomerVALUES(1,'Mike',now()-INTERVAL30YEAR,'M');INSERTINTOcustomerVALUES(2,'Boris',now()-INTERVAL40YEAR,'M');INSERTINTOcustomerVALUES(3,'Sofie',now()-INTERVAL24YEAR,'F');INSERTINTOorder(order_id,cust_id,amount)VALUES(50,1,15);INSERTINTOorder(order_id,cust_id,amount)VALUES(30,1,10);SELECT*EXCEPT'order_date'FROMorder┌─order_id─┬─cust_id─┬─amount─┐│30│1│10.00││50│1│15.00│└──────────┴─────────┴────────┘CREATEDICTIONARYcustomer_dict(`customer_id`UInt32,`first_name`String,`birth_date`Date,`sex`UInt8)PRIMARYKEYcustomer_idSOURCE(CLICKHOUSE(TABLE'customer'))LIFETIME(MIN0MAX300)LAYOUT(FLAT)ALTERTABLEorderADDCOLUMN`cust_first_name`StringDEFAULTdictGetString('default.customer_dict','first_name',toUInt64(cust_id)),ADDCOLUMN`cust_sex`Enum('M'=1,'F'=2)DEFAULTdictGetUInt8('default.customer_dict','sex',toUInt64(cust_id)),ADDCOLUMN`cust_birth_date`DateDEFAULTdictGetDate('default.customer_dict','birth_date',toUInt64(cust_id));INSERTINTOorder(order_id,cust_id,amount)VALUES(10,3,30);INSERTINTOorder(order_id,cust_id,amount)VALUES(20,3,60);INSERTINTOorder(order_id,cust_id,amount)VALUES(40,2,20);SELECT*EXCEPT'order_date'FROMorderFORMATPrettyCompactMonoBlock┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐│30│1│10.00│Mike│M│1991-08-05││50│1│15.00│Mike│M│1991-08-05││10│3│30.00│Sofie│F│1997-08-05││40│2│20.00│Boris│M│1981-08-05││20│3│60.00│Sofie│F│1997-08-05│└──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ALTERTABLEcustomerUPDATEbirth_date=now()-INTERVAL35YEARWHEREcustomer_id=2;SYSTEMRELOADDICTIONARYcustomer_dict;ALTERTABLEorderUPDATEcust_birth_date=dictGetDate('default.customer_dict','birth_date',toUInt64(cust_id))WHERE1-- or if you do have track of changes it's possible to lower amount of dict calls -- UPDATE cust_birth_date = dictGetDate('default.customer_dict', 'birth_date', toUInt64(cust_id)) WHERE customer_id = 2 SELECT*EXCEPT'order_date'FROMorderFORMATPrettyCompactMonoBlock┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐│30│1│10.00│Mike│M│1991-08-05││50│1│15.00│Mike│M│1991-08-05││10│3│30.00│Sofie│F│1997-08-05││40│2│20.00│Boris│M│1986-08-05││20│3│60.00│Sofie│F│1997-08-05│└──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ALTER TABLE order UPDATE would completely overwrite this column in table, so it’s not recommended to run it often.\n","categories":"","description":"Flattened table\n","excerpt":"Flattened table\n","ref":"/altinity-kb-schema-design/flattened-table/","tags":"","title":"Flattened table"},{"body":"Float arithmetics is not accurate: https://floating-point-gui.de/\nIn case you need accurate calculations you should use Decimal datatypes.\nOperations on floats are not associative select toFloat64(100000000000000000.1) + toFloat64(7.5) - toFloat64(100000000000000000.1) as res; --- title: \"0\" linkTitle: \"0\" description: \u003e 0 --- select toFloat64(100000000000000000.1) - toFloat64(100000000000000000.1) + toFloat64(7.5) as res; --- title: \"7.5\" linkTitle: \"7.5\" description: \u003e 7.5 --- --- title: \"no problem with Decimals:\" linkTitle: \"no problem with Decimals:\" description: \u003e no problem with Decimals: --- select toDecimal64(100000000000000000.1,1) + toDecimal64(7.5,1) - toDecimal64(100000000000000000.1,1) as res; --- title: \"7.5\" linkTitle: \"7.5\" description: \u003e 7.5 --- select toDecimal64(100000000000000000.1,1) - toDecimal64(100000000000000000.1,1) + toDecimal64(7.5,1) as res; --- title: \"7.5\" linkTitle: \"7.5\" description: \u003e 7.5 --- {% hint style=“warning” %} Because clickhouse uses MPP order of execution of a single query can vary on each run, and you can get slightly different results from the float column every time you run the query.\nUsually, this deviation is small, but it can be significant when some kind of arithmetic operation is performed on very large and very small numbers at the same time. {% endhint %}\nSome decimal numbers has no accurate float representation select sum(toFloat64(0.45)) from numbers(10000); --- title: \"4499.999999999948 \" linkTitle: \"4499.999999999948 \" description: \u003e 4499.999999999948 --- select toFloat32(0.6)*6; --- title: \"3.6000001430511475\" linkTitle: \"3.6000001430511475\" description: \u003e 3.6000001430511475 --- --- title: \"no problem with Decimal\" linkTitle: \"no problem with Decimal\" description: \u003e no problem with Decimal --- select sum(toDecimal64(0.45,2)) from numbers(10000); --- title: \"4500.00 \" linkTitle: \"4500.00 \" description: \u003e 4500.00 --- select toDecimal32(0.6,1)*6; --- title: \"3.6\" linkTitle: \"3.6\" description: \u003e 3.6 --- Direct comparisons of floats may be impossible The same number can have several floating-point representations and because of that you should not compare Floats directly\nselect toFloat32(0.1)*10 = toFloat32(0.01)*100; --- title: \"0\" linkTitle: \"0\" description: \u003e 0 --- SELECT sumIf(0.1, number \u003c 10) AS a, sumIf(0.01, number \u003c 100) AS b, a = b AS a_eq_b FROM numbers(100) Row 1: ────── a: 0.9999999999999999 b: 1.0000000000000007 a_eq_b: 0 See also\nhttps://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/\nhttps://stackoverflow.com/questions/4915462/how-should-i-do-floating-point-comparison\nhttps://stackoverflow.com/questions/2100490/floating-point-inaccuracy-examples\nhttps://stackoverflow.com/questions/10371857/is-floating-point-addition-and-multiplication-associative\nBut\n{% embed url=“https://github.com/ClickHouse/ClickHouse/issues/24909\" %}\n ","categories":"","description":"Floats vs Decimals\n","excerpt":"Floats vs Decimals\n","ref":"/altinity-kb-schema-design/floats-vs-decimals/","tags":"","title":"Floats vs Decimals"},{"body":" sequenceMatch  ","categories":"","description":"Functions\n","excerpt":"Functions\n","ref":"/altinity-kb-functions/","tags":"","title":"Functions"},{"body":"migrate migrate is a simple schema migration tool written in golang. No external dependencies are required (like interpreter, jre), only one platform-specific executable. golang-migrate/migrate\nmigrate supports several databases, including ClickHouse (support was introduced by @kshvakov).\nTo store information about migrations state migrate creates one additional table in target database, by default that table is called schema_migrations.\nInstall: download the migrate executable for your platform and put it to the folder listed in your %PATH.\n--- title: \"on Linux:\" linkTitle: \"on Linux:\" description: \u003e on Linux: --- #wget https://github.com/golang-migrate/migrate/releases/download/v3.2.0/migrate.linux-amd64.tar.gz wget https://github.com/golang-migrate/migrate/releases/download/v4.14.1/migrate.linux-amd64.tar.gz tar -xzf migrate.linux-amd64.tar.gz mkdir -p ~/bin mv migrate.linux-amd64 ~/bin/migrate rm migrate.linux-amd64.tar.gz Sample usage: mkdir migrations echo 'create table test(id UInt8) Engine = Memory;' \u003e migrations/000001_my_database_init.up.sql echo 'DROP TABLE test;' \u003e migrations/000001_my_database_init.down.sql --- title: \"you can also auto-create file with new migrations with automatic numbering like that:\" linkTitle: \"you can also auto-create file with new migrations with automatic numbering like that:\" description: \u003e you can also auto-create file with new migrations with automatic numbering like that: --- --- title: \"\u003e migrate create -dir migrations -seq -digits 6 -ext sql my_database_init\" linkTitle: \"\u003e migrate create -dir migrations -seq -digits 6 -ext sql my_database_init\" description: \u003e \u003e migrate create -dir migrations -seq -digits 6 -ext sql my_database_init --- --- title: \"edit migrations/000001_my_database_init.up.sql \u0026 migrations/000001_my_database_init.down.sql\" linkTitle: \"edit migrations/000001_my_database_init.up.sql \u0026 migrations/000001_my_database_init.down.sql\" description: \u003e edit migrations/000001_my_database_init.up.sql \u0026 migrations/000001_my_database_init.down.sql --- ➜ migrate -database 'clickhouse://localhost:9000' -path ./migrations up 1/u my_database_init (6.502974ms) ➜ migrate -database 'clickhouse://localhost:9000' -path ./migrations down 1/d my_database_init (2.164394ms) --- title: \"clears the database (use carefully - will not ask any confirmations)\" linkTitle: \"clears the database (use carefully - will not ask any confirmations)\" description: \u003e clears the database (use carefully - will not ask any confirmations) --- ➜ migrate -database 'clickhouse://localhost:9000' -path ./migrations drop Connection string format clickhouse://host:port?username=user\u0026password=qwerty\u0026database=clicks\n   URL Query Description     x-migrations-table Name of the migrations table   database The name of the database to connect to   username The user to sign in as   password The user’s password   host The host to connect to.   port The port to bind to.   secure to use a secure connection (for self-signed also add skip_verify=1)    Replicated / Distributed / Cluster environments. By default migrate create table schema_migrations with the following structure\nCREATETABLEschema_migrations(versionUInt32,dirtyUInt8,sequenceUInt64)ENGINE=TinyLogThat allows storing version of schema locally.\nIf you need to use migrate in some multi server environment (replicated / cluster) you should create schema_migrations manually with the same structure and with the appropriate Engine (Replicated / Distributed), otherwise, other servers will not know the version of the DB schema. As an alternative you can force the current version number on another server manually, like that:\nmigrate -database 'clickhouse://localhost:9000' -path ./migrations force 123456 # force version 123456 Known issues: could not load time location: unknown time zone Europe/Moscow in line 0:\nIt’s happens due of missing tzdata package in migrate/migrate docker image of golang-migrate.\nThere is 2 possible solutions:\n You can build your own golang-migrate image from official with tzdata package. If you using it as part of your CI you can add installing tzdata package as one of step in ci before using golang-migrate.  Related GitHub issues:\nhttps://github.com/golang-migrate/migrate/issues/494\nhttps://github.com/golang-migrate/migrate/issues/201\nUsing database name in x-migrations-table\n Creates table with database.table When running migrations migrate actually uses database from query settings and encapsulate database.table as table name: ``other_database.`database.table```  ","categories":"","description":"golang-migrate\n","excerpt":"golang-migrate\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/golang-migrate/","tags":"","title":"golang-migrate"},{"body":"GCS with the table function - seems to work correctly!\nEssentially you can follow the steps from the Migrating from Amazon S3 to Cloud Storage.\n Set up a GCS bucket. This bucket must be set as part of the default project for the account. This configuration can be found in settings -\u003e interoperability. Generate a HMAC key for the account, can be done in settings -\u003e interoperability, in the section for user account access keys. In ClickHouse, replace the S3 bucket endpoint with the GCS bucket endpoint This must be done with the path-style GCS endpoint: https://storage.googleapis.com/BUCKET\\_NAME/OBJECT\\_NAME. Replace the aws access key id and aws secret access key with the corresponding parts of the HMAC key.  ","categories":"","description":"\"Google S3 GCS\"\n","excerpt":"\"Google S3 GCS\"\n","ref":"/altinity-kb-integrations/altinity-kb-google-s3-gcs/","tags":"","title":"Google S3 (GCS)"},{"body":"ClickHouse ClickHouse will use all available hardware to maximize performance. So the more hardware - the better. As of this publication, the hardware requirements are:\n Minimum Hardware: 4-core CPU with support of SSE4.2, 16 Gb RAM, 1Tb HDD.  Recommended for development and staging environments. SSE4.2 is required, and going below 4 Gb of RAM is not recommended.   Recommended Hardware: \u003e=16-cores, \u003e=64Gb RAM, HDD-raid or SSD.  For processing up to hundreds of millions / billions of rows.    For clouds: disk throughput is the more important factor compared to IOPS. Be aware of burst / baseline disk speed difference.\nSee also: https://clickhouse.tech/benchmark/hardware/\nZookeeper Zookeeper requires separate servers from those used for ClickHouse. Zookeeper has poor performance when installed on the same node as ClickHouse.\nHardware Requirements for Zookeeper:\n Fast disk speed (ideally NVMe, 128Gb should be enough). Any modern CPU (one core, better 2) 4Gb of RAM  For clouds - be careful with burstable network disks (like gp2 on aws): you may need up to 1000 IOPs on the disk for on a long run, so gp3 with 3000 IOPs baseline is a better choice.\nThe number of Zookeeper instances depends on the environment:\n Production: 3 is an optimal number of zookeeper instances. Development and Staging: 1 zookeeper instance is sufficient.  See also:\n \u003cstrong\u003ehttps://docs.altinity.com/operationsguide/clickhouse-zookeeper/\u003c/strong\u003e \u003cstrong\u003ehttps://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup\u003c/strong\u003e \u003cstrong\u003ehttps://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring\u003c/strong\u003e  ClickHouse Hardware Configuration Configure the servers according to those recommendations the ClickHouse Usage Recommendations.\nTest Your Hardware Be sure to test the following:\n RAM speed. Network speed. Storage speed.  It’s better to find any performance issues before installing ClickHouse.\n","categories":"","description":"Hardware Requirements\n","excerpt":"Hardware Requirements\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/","tags":"","title":"Hardware Requirements"},{"body":"In general, it is a NORMAL situation for clickhouse that while processing a huge dataset it can use a lot of (or all of) the server resources. It is ‘by design’ - just to make the answers faster.\nThe main directions to reduce the CPU usage is to review the schema / queries to limit the amount of the data which need to be processed, and to plan the resources in a way when single running query will not impact the others.\nAny attempts to reduce the CPU usage will end up with slower queries!\nIf it is acceptable for you - please check the following options for limiting the CPU usage:\n1) setting max_threads: reducing the number of threads that are allowed to use one request. Fewer threads = more free cores for other requests. By default, it’s allowed to take half of the available CPU cores, adjust only when needed. So if if you have 10 cores then max_threads = 10 will work about twice faster than max_threads=5, but will take 100% or CPU. (max_threads=5 will use half of CPUs so 50%).\n2) setting os_thread_priority: increasing niceness for selected requests. In this case, the operating system, when choosing which of the running processes to allocate processor time, will prefer processes with lower niceness. 0 is the default niceness. The higher the niceness, the lower the priority of the process. The maximum niceness value is 19.\nThese are custom settings that can be tweaked in several ways:\n1) by specifying them when connecting a client, for example\nclickhouse-client --os_thread_priority = 19 -q 'SELECT max (number) from numbers (100000000)' echo 'SELECT max (number) from numbers (100000000)' | curl 'http://localhost:8123/?os_thread_priority=19' --data-binary @- 2) via dedicated API / connection parameters in client libraries\n3) using the SQL command SET (works only within the session)\nSET os_thread_priority = 19; SELECT max (number) from numbers (100000000) 4) using different profiles of settings for different users. Something like\n\u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e ... \u003c/default\u003e \u003clowcpu\u003e \u003cos_thread_priority\u003e19\u003c/os_thread_priority\u003e \u003cmax_threads\u003e4\u003c/max_threads\u003e \u003c/lowcpu\u003e \u003c/profiles\u003e \u003c!-- Users and ACL. --\u003e \u003cusers\u003e \u003c!-- If user name was not specified, 'default' user is used. --\u003e \u003climited_user\u003e \u003cpassword\u003e123\u003c/password\u003e \u003cnetworks\u003e \u003cip\u003e::/0\u003c/ip\u003e \u003c/networks\u003e \u003cprofile\u003elowcpu\u003c/profile\u003e \u003c!-- Quota for user. --\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003c/limited_user\u003e \u003c/users\u003e \u003c/yandex\u003e There are also plans to introduce a system of more flexible control over the assignment of resources to different requests.\nAlso, if these are manually created queries, then you can try to discipline users by adding quotas to them (they can be formulated as “you can read no more than 100GB of data per hour” or “no more than 10 queries”, etc.)\nIf these are automatically generated queries, it may make sense to check if there is no way to write them in a more efficient way. Still, a few minutes for KX is a lot.\n","categories":"","description":"High CPU usage\n","excerpt":"High CPU usage\n","ref":"/altinity-kb-setup-and-maintenance/high-cpu-usage/","tags":"","title":"High CPU usage"},{"body":"Zookeeper use watches to notify a client on znode changes. This article explains how to check watches set by ZooKeeper servers and how it is used.\nSolution:\nZookeeper uses the 'wchc' command to list all watches set on the Zookeeper server.\n# echo wchc | nc zookeeper 2181\nReference\nhttps://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html\nThe wchp and wchc commands are not enabled by default because of their known DOS vulnerability. For more information, see ZOOKEEPER-2693and Zookeeper 3.5.2 - Denial of Service.\nBy default those commands are disabled, they can be enabled via Java system property:\n-Dzookeeper.4lw.commands.whitelist=*\non in zookeeper config: 4lw.commands.whitelist=*\\\n","categories":"","description":"How to check the list of watches\n","excerpt":"How to check the list of watches\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-how-to-check-the-list-of-watches/","tags":"","title":"How to check the list of watches"},{"body":"The following instructions are an example on how to convert a database with the Engine type Atomic to a database with the Engine type Ordinary.\n{% hint style=“warning” %} That can be used only for simple schemas. Schemas with MATERIALIZED views will require extra manipulations. {% endhint %}\nCREATEDATABASEatomic_dbENGINE=Atomic;CREATEDATABASEordinary_dbENGINE=Ordinary;CREATETABLEatomic_db.xENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;INSERTINTOatomic_db.xSELECTnumberFROMnumbers(100000);RENAMETABLEatomic_db.xTOordinary_db.x;ls -1 /var/lib/clickhouse/data/ordinary_db/x all_1_1_0 detached format_version.txt DROPDATABASEatomic_db;DETACHDATABASEordinary_db;mv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql vi /var/lib/clickhouse/metadata/atomic_db.sql mv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db mv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db ATTACHDATABASEatomic_db;SELECTcount()FROMatomic_db.x┌─count()─┐│100000│└─────────┘SHOWCREATEDATABASEatomic_db┌─statement──────────────────────────────────┐│CREATEDATABASEatomic_dbENGINE=Ordinary│└────────────────────────────────────────────┘Schemas with Materialized VIEW DROPDATABASEIFEXISTSatomic_db;DROPDATABASEIFEXISTSordinary_db;CREATEDATABASEatomic_dbengine=Atomic;CREATEDATABASEordinary_dbengine=Ordinary;CREATETABLEatomic_db.xENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;CREATEMATERIALIZEDVIEWatomic_db.x_mvENGINE=MergeTreeORDERBYtuple()ASSELECT*FROMatomic_db.x;CREATEMATERIALIZEDVIEWatomic_db.y_mvENGINE=MergeTreeORDERBYtuple()ASSELECT*FROMatomic_db.x;CREATETABLEatomic_db.zENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;CREATEMATERIALIZEDVIEWatomic_db.z_mvTOatomic_db.zASSELECT*FROMatomic_db.x;INSERTINTOatomic_db.xSELECT*FROMnumbers(100);--- USE atomic_db; --- --- Query id: 28af886d-a339-4e9c-979c-8bdcfb32fd95 --- --- ┌─name───────────────────────────────────────────┐ --- │ .inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c │ --- │ .inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f │ --- │ x │ --- │ x_mv │ --- │ y_mv │ --- │ z │ --- │ z_mv │ --- └────────────────────────────────────────────────┘ SELECTmv_storage.database,mv_storage.name,mv.database,mv.nameFROMsystem.tablesASmv_storageLEFTJOINsystem.tablesASmvONsubstring(mv_storage.name,11)=toString(mv.uuid)WHEREmv_storage.nameLIKE'.inner_id.%'ANDmv_storage.database='atomic_db';-- ┌─database──┬─name───────────────────────────────────────────┬─mv.database─┬─mv.name─┐ -- │ atomic_db │ .inner_id.81e1a67d-3d02-4b2a-be17-84d8626d2328 │ atomic_db │ y_mv │ -- │ atomic_db │ .inner_id.e428225c-982a-4859-919b-ba5026db101d │ atomic_db │ x_mv │ -- └───────────┴────────────────────────────────────────────────┴─────────────┴─────────┘ /* STEP 1: prepare rename statements, also to rename implicit mv storage table to explicit one */SELECTif(t.nameLIKE'.inner_id.%','RENAME TABLE `'||t.database||'`.`'||t.name||'` TO `ordinary_db`.`'||mv.name||'_storage`;','RENAME TABLE `'||t.database||'`.`'||t.name||'` TO `ordinary_db`.`'||t.name||'`;')FROMsystem.tablesastLEFTJOINsystem.tablesmvON(substring(t.name,11)=toString(mv.uuid)ANDt.database=mv.database)WHEREt.database='atomic_db'ANDt.engine\u003c\u003e'MaterializedView'FORMATTSVRaw;-- RENAME TABLE `atomic_db`.`.inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c` TO `ordinary_db`.`y_mv_storage`; -- RENAME TABLE `atomic_db`.`.inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f` TO `ordinary_db`.`x_mv_storage`; -- RENAME TABLE `atomic_db`.`x` TO `ordinary_db`.`x`; -- RENAME TABLE `atomic_db`.`z` TO `ordinary_db`.`z`; /* STEP 2: prepare statements to reattach MV */-- Can be done manually: pick existing MV definition (SHOW CREATE TABLE), and change it in the following way: -- 1) add TO keyword 2) remove column names and engine settings after mv name SELECTif(t.nameLIKE'.inner_id.%',replaceRegexpOne(mv.create_table_query,'^CREATE MATERIALIZED VIEW ([^ ]+) \\\\(.*? AS ','CREATE MATERIALIZED VIEW \\\\1 TO \\\\1_storage AS '),mv.create_table_query)FROMsystem.tablesasmvLEFTJOINsystem.tablestON(substring(t.name,11)=toString(mv.uuid)ANDt.database=mv.database)WHEREmv.database='atomic_db'ANDmv.engine='MaterializedView'FORMATTSVRaw;-- CREATE MATERIALIZED VIEW atomic_db.x_mv TO atomic_db.x_mv_storage AS SELECT * FROM atomic_db.x -- CREATE MATERIALIZED VIEW atomic_db.y_mv TO atomic_db.y_mv_storage AS SELECT * FROM atomic_db.x /* STEP 3: stop inserts, fire renames statements prepared at the step 1 (hint: use clickhouse-client -mn) */RENAME.../* STEP 4: ensure that only MaterialiedView left in source db, and drop it. */SELECT*FROMsystem.tablesWHEREdatabase='atomic_db'andengine\u003c\u003e'MaterializedView';DROPDATABASEatomic_db;/* STEP 4. rename table to old name: */DETACHDATABASEordinary_db;-- rename files / folders: mv/var/lib/clickhouse/metadata/ordinary_db.sql/var/lib/clickhouse/metadata/atomic_db.sqlvi/var/lib/clickhouse/metadata/atomic_db.sqlmv/var/lib/clickhouse/metadata/ordinary_db/var/lib/clickhouse/metadata/atomic_dbmv/var/lib/clickhouse/data/ordinary_db/var/lib/clickhouse/data/atomic_db-- attach database atomic_db; ATTACHDATABASEatomic_db;/* STEP 5. restore MV using statements created on STEP 2 */","categories":"","description":"How to Convert Atomic to Ordinary\n","excerpt":"How to Convert Atomic to Ordinary\n","ref":"/engines/altinity-kb-atomic-database-engine/altinity-kb-how-to-convert-atomic-to-ordinary/","tags":"","title":"How to Convert Atomic to Ordinary"},{"body":"Example Create test_table based on the source table.\nCREATETABLEtest_tableASsource_tableENGINE=MergeTree()PARTITIONBY...;If the source table has Replicated*MergeTree engine, you would need to change it to non-replicated.\nAttach one partition with data from the source table to test_table.\nALTERTABLEtest_tableATTACHPARTITIONID'20210120'FROMsource_table;You can modify the column or create a new one based on the old column value.\nALTERTABLEtest_tableMODIFYCOLUMNcolumn_aCODEC(ZSTD(2));ALTERTABLEtest_tableADDCOLUMNcolumn_newUInt32DEFAULTtoUInt32OrZero(column_old)CODEC(T64,LZ4);After that, you would need to populate changed columns with data.\nALTERTABLEtest_tableUPDATEcolumn_a=column_a,column_new=column_newWHERE1;You can look status of mutation via the system.mutations table\nSELECT*FROMsystem.mutations;And it’s also possible to kill mutation if there are some problems with it.\nKILLMUTATIONWHERE...Useful queries SELECTdatabase,table,count()ASparts,uniqExact(partition_id)ASpartition_cnt,sum(rows),formatReadableSize(sum(data_compressed_bytes)AScomp_bytes)AScomp,formatReadableSize(sum(data_uncompressed_bytes)ASuncomp_bytes)ASuncomp,uncomp_bytes/comp_bytesASratioFROMsystem.partsWHEREactiveGROUPBYdatabase,tableORDERBYcomp_bytesDESCSELECTdatabase,table,column,type,sum(rows)ASrows,sum(column_data_compressed_bytes)AScompressed_bytes,formatReadableSize(compressed_bytes)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes))ASuncompressed,sum(column_data_uncompressed_bytes)/compressed_bytesASratio,any(compression_codec)AScodecFROMsystem.parts_columnsASpcLEFTJOINsystem.columnsAScON(pc.database=c.database)AND(c.table=pc.table)AND(c.name=pc.column)WHERE(databaseLIKE'%')AND(tableLIKE'%')ANDactiveGROUPBYdatabase,table,column,typeORDERBYdatabase,table,sum(column_data_compressed_bytes)DESC","categories":"","description":"How to test different compression codecs\n","excerpt":"How to test different compression codecs\n","ref":"/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/","tags":"","title":"How to test different compression codecs"},{"body":"https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup27/adaptive_index_granularity.pdf\n","categories":"","description":"index \u0026 column files\n","excerpt":"index \u0026 column files\n","ref":"/engines/mergetree-table-engine-family/index-and-column-files/","tags":"","title":"index \u0026 column files"},{"body":" CatBoost/MindsDB/Fast.ai Google S3 (GCS) Kafka  ","categories":"","description":"Integrations\n","excerpt":"Integrations\n","ref":"/altinity-kb-integrations/","tags":"","title":"Integrations"},{"body":" clickhouse-client JSONAsString and Mat. View as JSON parser  ","categories":"","description":"Interfaces\n","excerpt":"Interfaces\n","ref":"/altinity-kb-interfaces/","tags":"","title":"Interfaces"},{"body":"How do I Store IPv4 and IPv6 Address In One Field?  There is a clean and simple solution for that. Any IPv4 has its unique IPv6 mapping:\n IPv4 IP address: 191.239.213.197 IPv4-mapped IPv6 address: ::ffff:191.239.213.197  Find IPs matching CIDR/network mask (IPv4) WITHIPv4CIDRToRange(toIPv4('10.0.0.1'),8)asrangeSELECT*FROMvalues('ip IPv4',toIPv4('10.2.3.4'),toIPv4('192.0.2.1'),toIPv4('8.8.8.8'))WHEREipBETWEENrange.1ANDrange.2;Find IPs matching CIDR/network mask (IPv6) WITHIPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'),32)asrangeSELECT*FROMvalues('ip IPv6',toIPv6('2001:db8::8a2e:370:7334'),toIPv6('::ffff:192.0.2.1'),toIPv6('::'))WHEREipBETWEENrange.1ANDrange.2;","categories":"","description":"IPs/masks\n","excerpt":"IPs/masks\n","ref":"/altinity-kb-schema-design/how-to-store-ips/","tags":"","title":"IPs/masks"},{"body":"The main purpose of JOIN table engine is to avoid building the right table for joining on each query execution. So it’s usually used when you have a high amount of fast queries which share the same right table for joining.\nUpdates It’s possible to update rows with setting join_any_take_last_row enabled.\nCREATETABLEid_val_join(`id`UInt32,`val`UInt8)ENGINE=Join(ANY,LEFT,id)SETTINGSjoin_any_take_last_row=1Ok.INSERTINTOid_val_joinVALUES(1,21)(1,22)(3,23);Ok.SELECT*FROM(SELECTtoUInt32(number)ASidFROMnumbers(4))ASnANYLEFTJOINid_val_joinUSING(id)┌─id─┬─val─┐│0│0││1│22││2│0││3│23│└────┴─────┘INSERTINTOid_val_joinVALUES(1,40)(2,24);Ok.SELECT*FROM(SELECTtoUInt32(number)ASidFROMnumbers(4))ASnANYLEFTJOINid_val_joinUSING(id)┌─id─┬─val─┐│0│0││1│40││2│24││3│23│└────┴─────┘{% embed url=“https://clickhouse.tech/docs/en/engines/table-engines/special/join/\" %}\n","categories":"","description":"JOIN table engine\n","excerpt":"JOIN table engine\n","ref":"/altinity-kb-queries-and-syntax/joins/join-table-engine/","tags":"","title":"JOIN table engine"},{"body":"Sample data CREATETABLEtest_metrics(counter_idInt64,timestampDateTime,metricUInt64)Engine=Log;INSERTINTOtest_metricsSELECTnumber%3,toDateTime('2021-01-01 00:00:00'),1FROMnumbers(20);INSERTINTOtest_metricsSELECTnumber%3,toDateTime('2021-01-03 00:00:00'),1FROMnumbers(20);SELECTcounter_id,toDate(timestamp)dt,sum(metric)FROMtest_metricsGROUPBYcounter_id,dtORDERBYcounter_id,dt;┌─counter_id─┬─────────dt─┬─sum(metric)─┐│0│2021-01-01│7││0│2021-01-03│7││1│2021-01-01│7││1│2021-01-03│7││2│2021-01-01│6││2│2021-01-03│6│└────────────┴────────────┴─────────────┘Calendar WITHarrayMap(i-\u003e(toDate('2021-01-01')+i),range(4))ASCalendarSELECTarrayJoin(Calendar);┌─arrayJoin(Calendar)─┐│2021-01-01││2021-01-02││2021-01-03││2021-01-04│└─────────────────────┘Join with Calendar using arrayJoin SELECTcounter_id,tuple.2dt,sum(tuple.1)sumFROM(WITHarrayMap(i-\u003e(0,toDate('2021-01-01')+i),range(4))ASCalendarSELECTcounter_id,arrayJoin(arrayConcat(Calendar,[(sum,dt)]))tupleFROM(SELECTcounter_id,toDate(timestamp)dt,sum(metric)sumFROMtest_metricsGROUPBYcounter_id,dt))GROUPBYcounter_id,dtORDERBYcounter_id,dt;┌─counter_id─┬─────────dt─┬─sum─┐│0│2021-01-01│7││0│2021-01-02│0││0│2021-01-03│7││0│2021-01-04│0││1│2021-01-01│7││1│2021-01-02│0││1│2021-01-03│7││1│2021-01-04│0││2│2021-01-01│6││2│2021-01-02│0││2│2021-01-03│6││2│2021-01-04│0│└────────────┴────────────┴─────┘With fill SELECTcounter_id,toDate(timestamp)ASdt,sum(metric)ASsumFROMtest_metricsGROUPBYcounter_id,dtORDERBYcounter_idASCWITHFILL,dtASCWITHFILLFROMtoDate('2021-01-01')TOtoDate('2021-01-05');┌─counter_id─┬─────────dt─┬─sum─┐│0│2021-01-01│7││0│2021-01-02│0││0│2021-01-03│7││0│2021-01-04│0││1│2021-01-01│7││1│2021-01-02│0││1│2021-01-03│7││1│2021-01-04│0││2│2021-01-01│6││2│2021-01-02│0││2│2021-01-03│6││2│2021-01-04│0│└────────────┴────────────┴─────┘","categories":"","description":"Join with Calendar using Arrays\n","excerpt":"Join with Calendar using Arrays\n","ref":"/altinity-kb-queries-and-syntax/join-with-calendar-using-arrays/","tags":"","title":"Join with Calendar using Arrays"},{"body":"See presentation:\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/join.pdf\" %}\n","categories":"","description":"JOINs\n","excerpt":"JOINs\n","ref":"/altinity-kb-queries-and-syntax/joins/","tags":"","title":"JOINs"},{"body":"See presentation:\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/join.pdf\" %}\n","categories":"","description":"JOINs\n","excerpt":"JOINs\n","ref":"/altinity-kb-queries-and-syntax/joins/","tags":"","title":"JOINs"},{"body":"Tables with engine Null don’t store data but can be used as a source for materialized views.\nJSONAsString a special input format which allows to ingest JSONs into a String column. If the input has several JSON objects (comma separated) they will be interpreted as separate rows. JSON can be multiline.\ncreatetableentrypoint(JString)Engine=Null;createtabledatastore(aString,iInt64,fFloat64)Engine=MergeTreeorderbya;creatematerializedviewjsonConvertertodatastoreasselect(JSONExtract(J,'Tuple(String,Tuple(Int64,Float64))')asx),x.1asa,x.2.1asi,x.2.2asffromentrypoint;$echo'{\"s\": \"val1\", \"b2\": {\"i\": 42, \"f\": 0.1}}'|\\clickhouse-client-q\"insert into entrypoint format JSONAsString\"$echo'{\"s\": \"val1\",\"b2\": {\"i\": 33, \"f\": 0.2}},{\"s\": \"val1\",\"b2\": {\"i\": 34, \"f\": 0.2}}'|\\clickhouse-client-q\"insert into entrypoint format JSONAsString\"SELECT*FROMdatastore;┌─a────┬──i─┬───f─┐│val1│42│0.1│└──────┴────┴─────┘┌─a────┬──i─┬───f─┐│val1│33│0.2││val1│34│0.2│└──────┴────┴─────┘See also: https://app.gitbook.com/@altinity/s/altinitykb/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time\n","categories":"","description":"JSONAsString and Mat. View as JSON parser\n","excerpt":"JSONAsString and Mat. View as JSON parser\n","ref":"/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/","tags":"","title":"JSONAsString and Mat. View as JSON parser"},{"body":"WITHJSONExtract(json,'Tuple(name String, id String, resources Nested(description String, format String, tracking_summary Tuple(total UInt32, recent UInt32)), extras Nested(key String, value String))')ASparsed_jsonSELECTtupleElement(parsed_json,'name')ASname,tupleElement(parsed_json,'id')ASid,tupleElement(tupleElement(parsed_json,'resources'),'description')AS`resources.description`,tupleElement(tupleElement(parsed_json,'resources'),'format')AS`resources.format`,tupleElement(tupleElement(tupleElement(parsed_json,'resources'),'tracking_summary'),'total')AS`resources.tracking_summary.total`,tupleElement(tupleElement(tupleElement(parsed_json,'resources'),'tracking_summary'),'recent')AS`resources.tracking_summary.recent`FROMurl('https://raw.githubusercontent.com/jsonlines/guide/master/datagov100.json','JSONAsString','json String')","categories":"","description":"JSONExtract to parse many attributes at a time\n","excerpt":"JSONExtract to parse many attributes at a time\n","ref":"/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time/","tags":"","title":"JSONExtract to parse many attributes at a time"},{"body":"TLDR version: use fresh Java version (11 or newer), disable swap and set up (for 4 Gb node):\nJAVA_OPTS=\"-Xms3G -Xmx3G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" If you have a node with more RAM - change it accordingly, for example for 8Gb node:\nJAVA_OPTS=\"-Xms7G -Xmx7G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" Details 1) ZooKeeper runs as in JVM. Depending on version different garbage collectors are avaliable.\n2) Recent JVM versions (starting from 10) use G1 garbage collector by default (should work fine).\nOn JVM 13-14 using ZGC or Shenandoah garbage collector may reduce pauses.\nOn older JVM version (before 10) you may want to make some tuning to decrease pauses, ParNew + CMS garbage collectors (like in Yandex config) is one of the best options.\n2) One of the most important setting for JVM application is heap size. A heap size of \u003e1 GB is recommended for most use cases and monitoring heap usage to ensure no delays are caused by garbage collection. We recommend to use at least 4Gb of RAM for zookeeper nodes (8Gb is better, that will make difference only when zookeeper is heavily loaded).\nSet the Java heap size smaller than available RAM size on the node. This is very important to avoid swapping, which will seriously degrade ZooKeeper performance. Be conservative - use a maximum heap size of 3GB for a 4GB machine.\n3) Set min (Xms) and max (Xmx) heap size to same value to avoid resizing. Add XX:+AlwaysPreTouch flag as well to load the memory pages into memory at the start of the zookeeper.\n4) MaxGCPauseMillis=50 (by default 200) - the ‘target’ acceptable pause for garbage collection (milliseconds)\n4) jute.maxbuffer limits the maximum size of znode content. By default it’s 1Mb. In some usecases (lot of partitions in table) ClickHouse may need to create bigger znodes.\n5) (optional) enable GC logs: -Xloggc:/path_to/gc.log\nZookeeper configurarion used by Yandex Metrika (from 2017) The configuration used by Yandex ( https://clickhouse.tech/docs/en/operations/tips/#zookeeper ) - they use older JVM version (with UseParNewGC garbage collector), and tune GC logs heavily:\nJAVA_OPTS=\"-Xms{{ cluster.get('xms','128M') }} \\ -Xmx{{ cluster.get('xmx','1G') }} \\ -Xloggc:/var/log/$NAME/zookeeper-gc.log \\ -XX:+UseGCLogFileRotation \\ -XX:NumberOfGCLogFiles=16 \\ -XX:GCLogFileSize=16M \\ -verbose:gc \\ -XX:+PrintGCTimeStamps \\ -XX:+PrintGCDateStamps \\ -XX:+PrintGCDetails -XX:+PrintTenuringDistribution \\ -XX:+PrintGCApplicationStoppedTime \\ -XX:+PrintGCApplicationConcurrentTime \\ -XX:+PrintSafepointStatistics \\ -XX:+UseParNewGC \\ -XX:+UseConcMarkSweepGC \\ -XX:+CMSParallelRemarkEnabled\" See also  https://wikitech.wikimedia.org/wiki/JVM_Tuning#G1_for_full_gcs https://sematext.com/blog/java-garbage-collection-tuning/ https://www.oracle.com/technical-resources/articles/java/g1gc.html https://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2 https://docs.cloudera.com/runtime/7.2.7/kafka-performance-tuning/topics/kafka-tune-broker-tuning-jvm.html https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm-tune-g1gc.html https://blog.sokolenko.me/2014/11/javavm-options-production.html https://www.maknesium.de/21-most-important-java-8-vm-options-for-servers https://docs.oracle.com/javase/10/gctuning/introduction-garbage-collection-tuning.htm#JSGCT-GUID-326EB4CF-8C8C-4267-8355-21AB04F0D304 https://github.com/chewiebug/GCViewer  ","categories":"","description":"JVM sizes and garbage collector settings\n","excerpt":"JVM sizes and garbage collector settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/","tags":"","title":"JVM sizes and garbage collector settings"},{"body":"git log -- contrib/librdkafka | git name-rev --stdin    ClickHouse version librdkafka version     21.1+ (#18671) 1.6.0-RC3 + snappy fixes + boring ssl   20.13+ (#18053) 1.5.0 + msan fixes + snappy fixes + boring ssl   20.7+ (#12991) 1.5.0 + msan fixes   20.5+ (#11256) 1.4.2   20.2+ (#9000) 1.3.0   19.11+ (#5872) 1.1.0   19.5+ (#4799) 1.0.0   19.1+ (#4025) 1.0.0-RC5   v1.1.54382+ (#2276) 0.11.4    ","categories":"","description":"Kafka\n","excerpt":"Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/","tags":"","title":"Kafka"},{"body":"One of the threads from scheduled_pool (pre 20.9) / background_message_broker_schedule_pool (after 20.9) do that in infinite loop:\n Batch poll (time limit: kafka_poll_timeout_ms 500ms, messages limit: kafka_poll_max_batch_size 65536) Parse messages. If we don’t have enough data (rows limit: kafka_max_block_size 1048576) or time limit reached (kafka_flush_interval_ms 7500ms) - continue polling (goto p.1) Write a collected block of data to MV Do commit (commit after write = at-least-once).  On any error, during that process, Kafka client is restarted (leading to rebalancing - leave the group and get back in few seconds).\nImportant settings  These usually should not be adjusted:\n kafka_poll_max_batch_size = max_block_size (65536) kafka_poll_timeout_ms = stream_poll_timeout_ms (500ms)  You may want to adjust those depending on your scenario:\n kafka_flush_interval_ms = stream_poll_timeout_ms (7500ms) kafka_max_block_size = min_insert_block_size / kafka_num_consumers (for the single consumer: 1048576)  See also  https://github.com/ClickHouse/ClickHouse/pull/11388\n","categories":"","description":"Kafka main parsing loop\n","excerpt":"Kafka main parsing loop\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/","tags":"","title":"Kafka main parsing loop"},{"body":"For very large topics when you need more parallelism (especially on the insert side) you may use several tables with the same pipeline (pre 20.9) or enable kafka_thread_per_consumer (after 20.9).\nkafka_num_consumers = N, kafka_thread_per_consumer=1 Notes:\n the inserts will happen in parallel (without that setting inserts happen linearly) enough partitions are needed.  Before increasing kafka_num_consumers with keeping kafka_thread_per_consumer=0 may improve consumption \u0026 parsing speed, but flushing \u0026 committing still happens by a single thread there (so inserts are linear).\n","categories":"","description":"Kafka parallel consuming\n","excerpt":"Kafka parallel consuming\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/","tags":"","title":"Kafka parallel consuming"},{"body":"Unfortunately not all queries can be killed.\nKILL QUERY only sets a flag that must be checked by the query.\nA query pipeline is checking this flag before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed.\nIf a query does not stop, the only way to get rid of it is to restart ClickHouse.\nSee also\nhttps://github.com/ClickHouse/ClickHouse/issues/3964\nhttps://github.com/ClickHouse/ClickHouse/issues/1576\nHow to replace a running query   Q. We are trying to abort running queries when they are being replaced with a new one. We are setting the same query id for this. In some cases this error happens:\nQuery with id = e213cc8c-3077-4a6c-bc78-e8463adad35d is already running and can’t be stopped\nThe query is still being killed but the new one is not being executed. Do you know anything about this and if there is a fix or workaround for it?\n I guess you use replace_running_query + replace_running_query_max_wait_ms.\nUnfortunately it’s not always possible to kill the query at random moment of time.\nKill don’t send any signals, it just set a flag. Which gets (synchronously) checked at certain moments of query execution, mostly after finishing processing one block and starting another.\nOn certain stages (executing scalar sub-query) the query can not be killed at all. This is a known issue and requires an architectural change to fix it.\n I see. Is there a workaround?\nThis is our usecase:\nA user requests an analytics report which has a query that takes several settings, the user makes changes to the report (e.g. to filters, metrics, dimensions…). Since the user changed what he is looking for the query results from the initial query are never used and we would like to cancel it when starting the new query (edited)\n You can just use 2 commands:\nKILLQUERYWHEREquery_id=' ... 'ASYNCSELECT...newquery....in that case you don’t need to care when the original query will be stopped.\n","categories":"","description":"KILL QUERY\n","excerpt":"KILL QUERY\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-kill-query/","tags":"","title":"KILL QUERY"},{"body":"clickhouse-backup setup-example.yaml\n","categories":"","description":"Kubernetes\n","excerpt":"Kubernetes\n","ref":"/altinity-kb-kubernetes/","tags":"","title":"Kubernetes"},{"body":"Sample data CREATETABLEllexample(gInt32,aDate)ENGINE=Memory;INSERTINTOllexampleSELECTnumber%3,toDate('2020-01-01')+numberFROMnumbers(10);SELECT*FROMllexampleORDERBYg,a;┌─g─┬──────────a─┐│0│2020-01-01││0│2020-01-04││0│2020-01-07││0│2020-01-10││1│2020-01-02││1│2020-01-05││1│2020-01-08││2│2020-01-03││2│2020-01-06││2│2020-01-09│└───┴────────────┘Using arrays selectg,(arrayJoin(tuple_ll)asll).1a,ll.2prev,ll.3nextfrom(selectg,arrayMap(i,j,k-\u003e(i,j,k),arraySort(groupArray(a))asaa,arrayPopBack(arrayPushFront(aa,toDate(0))),arrayPopFront(arrayPushBack(aa,toDate(0))))tuple_llfromllexamplegroupbyg)orderbyg,a;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using window functions (starting from Clickhouse 21.3) SETallow_experimental_window_functions=1;SELECTg,a,any(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEEN1PRECEDINGAND1PRECEDING)ASprev,any(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEEN1FOLLOWINGAND1FOLLOWING)ASnextFROMllexampleORDERBYgASC,aASC;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using lagInFrame/leadInFrame (starting from ClickHouse 21.4) SELECTg,a,lagInFrame(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEENUNBOUNDEDPRECEDINGANDUNBOUNDEDFOLLOWING)ASprev,leadInFrame(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEENUNBOUNDEDPRECEDINGANDUNBOUNDEDFOLLOWING)ASnextFROMllexampleORDERBYgASC,aASC;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using neighbor (no grouping, incorrect result over blocks) SELECTg,a,neighbor(a,-1)ASprev,neighbor(a,1)ASnextFROM(SELECT*FROMllexampleORDERBYgASC,aASC);┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│2020-01-02││1│2020-01-02│2020-01-10│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│2020-01-03││2│2020-01-03│2020-01-08│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘","categories":"","description":"Lag / Lead\n","excerpt":"Lag / Lead\n","ref":"/altinity-kb-queries-and-syntax/lag-lead/","tags":"","title":"Lag / Lead"},{"body":"In general - one of the simplest option to do load balancing is to implement it on the client side.\nI.e. list serveral endpoints for clickhouse connections and add some logic to pick one of the nodes.\nMany client libraries support that.\nClickHouse native protocol (port 9000) Currently there are no protocol-aware proxies for clickhouse protocol, so the proxy / load balancer can work only on TCP level.\nOne of the best option for TCP load balancer is haproxy, also nginx can work in that mode.\nHaproxy will pick one upstream when connection is established, and after that it will keep it connected to the same server until the client or server will disconnect (or some timeout will happen).\nIt can’t send different queries coming via a single connection to different servers, as he knows nothing about clickhouse protocol and doesn’t know when one query ends and another start, it just sees the binary stream.\nSo for native protocol, there are only 3 possibilities:\n1) close connection after each query client-side\n2) close connection after each query server-side (currently there is only one setting for that - idle_connection_timeout=0, which is not exact what you need, but similar).\n3) use a clickhouse server with Distributed table as a proxy.\nHTTP protocol (port 8123) There are many more options and you can use haproxy / nginx / chproxy, etc.\nchproxy give some extra clickhouse-specific features, you can find a list of them at https://github.com/Vertamedia/chproxy\n","categories":"","description":"Load balancers\n","excerpt":"Load balancers\n","ref":"/altinity-kb-setup-and-maintenance/load-balancers/","tags":"","title":"Load balancers"},{"body":"Settings allow_suspicious_low_cardinality_types In CREATE TABLE statement allows specifying LowCardinality modifier for types of small fixed size (8 or less). Enabling this may increase merge times and memory consumption.\nlow_cardinality_max_dictionary_size\ndefault - 8192\nMaximum size (in rows) of shared global dictionary for LowCardinality type.\nlow_cardinality_use_single_dictionary_for_part\nLowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.\nlow_cardinality_allow_in_native_format\nUse LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query.\noutput_format_arrow_low_cardinality_as_dictionary\nEnable output LowCardinality type as Dictionary Arrow type\n","categories":"","description":"LowCardinality\n","excerpt":"LowCardinality\n","ref":"/altinity-kb-schema-design/lowcardinality/","tags":"","title":"LowCardinality"},{"body":"{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup31/ml.pdf\" %}\n{% page-ref page=”../altinity-kb-integrations/catboost-mindsdb-fast.ai.md\" %}\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/forecast.pdf\" %}\n","categories":"","description":"Machine learning in ClickHouse\n","excerpt":"Machine learning in ClickHouse\n","ref":"/altinity-kb-queries-and-syntax/machine-learning-in-clickhouse/","tags":"","title":"Machine learning in ClickHouse"},{"body":"{% hint style=“info” %} MATERIALIZED VIEWs in ClickHouse behave like AFTER INSERT TRIGGER to the left-most table listed in its SELECT statement. {% endhint %}\nBest practices 1. Use MATERIALIZED VIEW with TO syntax (explicit storage table) first you create the table which will store the data calculated by MV explicitly, and after that create materialized view itself with TO syntax.\nCREATE TABLE target ( ... ) Engine=ReplacingSummingMergeTree; CREATE MATERIALIZED VIEW mv_source2target TO target AS SELECT ... FROM source; That way it’s bit simpler to do schema migrations or build more complicated pipelines when one table is filled by several MV.\nWith engine=Atomic it hard to map undelying table with the MV.\n2. Avoid using POPULATE when creating MATERIALIZED VIEW on big tables Use manual backfilling (with the same query) instead.\n With POPULATE the data ingested to the source table during MV populating will not appear in MV. POPULATE doesn’t work with TO syntax. With manual backfilling, you have much better control on the process - you can do it in parts, adjust settings etc. In case of some failure ‘in the middle (for example due to timeouts), it’s hard to understand the state of the MV.  CREATE MATERIALIZED VIEW mv_source2target TO target AS SELECT ... FROM source WHERE cond \u003e ... INSERT INTO target AS SELECT ... FROM source WHERE cond \u003c ... This way you have full control backfilling process (you can backfill in smaller parts to avoid timeouts, do some cross-checks / integrity-checks, change some settings, etc.)\nFAQ Q. Can I attach MATERIALIZED VIEW to the VIEW, or engine=Merge, or engine=MySQL, etc.? Since MATERIALIZED VIEWs are updated on every INSERT to the underlying table and you can not insert anything to the usual VIEW, the materialized view update will never be triggered.\nNormally you should build MATERIALIZED VIEWs on the top of the table with MergeTree engine family.\nQ. I’ve created materialized error with some error, and since it’s it reading from Kafka I don’t understand where the error is. Server logs will help you. Also, see the next question.\nQ. How to debug misbehaving MATERIALIZED VIEW? You can also attach the same MV to some dummy table with engine=Log (or even Null) and do some manual inserts there to debug the behavior. Similar way (as the Materialized view often can contain some pieces of the business logic of the application) you can create tests for your schema.\n{% hint style=“warning” %} Always test MATERIALIZED VIEWs first on staging or testing environments {% endhint %}\nPossible test scenario:\n create a copy of the original table CREATE TABLE src_copy ... AS src create MV on that copy CREATE MATERIALIZED VIEW ... AS SELECT ... FROM src_copy check if inserts to src_copy work properly, and mv is properly filled. INSERT INTO src_copy SELECT * FROM src LIMIT 100 cleanup the temp stuff and recreate MV on real table.  Q. Can I use subqueries / joins in MV? It is possible but it is a very bad idea for most of the use cases**.**\nSo it will most probably work not as you expect and will hit insert performance significantly.\nThe MV will be attached (as AFTER INSERT TRIGGER) to the left-most table in the MV SELECT statement, and it will ‘see’ only freshly inserted rows there. It will ‘see’ the whole set of rows of other tables, and the query will be executed EVERY TIME you do the insert to the left-most table. That will impact the performance speed there significantly.\nIf you really need to update the MV with the left-most table, not impacting the performance so much you can consider using dictionary / engine=Join / engine=Set for right-hand table / subqueries (that way it will be always in memory, ready to use).\nQ. How to alter MV implicit storage (w/o TO syntax) 1) take the existing MV definition\nSHOWCREATETABLEdbname.mvname;Adjust the query in the following manner:\n replace ‘CREATE MATERIALIZED VIEW’ to ‘ATTACH MATERIALIZED VIEW’ add needed columns;  2) Detach materialized view with the command:\nDETACHTABLEdbname.mvnameONCLUSTERcluster_name;3) Add the needed column to the underlying ReplicatedAggregatingMergeTree table\n-- if the Materialized view was created without TO keyword ALTERTABLEdbname.`.inner.mvname`ONCLUSTERcluster_nameaddcolumntokensAggregateFunction(uniq,UInt64);-- othewise just alter the target table used in `CREATE MATERIALIZED VIEW ...` `TO ...` clause 4) attach MV back using the query you create at p. 1.\nATTACHMATERIALIZEDVIEWdbname.mvnameONCLUSTERcluster_name(/* ... */`tokens`AggregateFunction(uniq,UInt64))ENGINE=ReplicatedAggregatingMergeTree(...)ORDERBY...ASSELECT/* ... */uniqState(rand64())astokensFROM/* ... */GROUPBY/* ... */As you can see that operation is NOT atomic, so the safe way is to stop data ingestion during that procedure.\nIf you have version 19.16.13 or newer you can change the order of step 2 and 3 making the period when MV is detached and not working shorter (related issue https://github.com/ClickHouse/ClickHouse/issues/7878).\nSee also:\n https://github.com/ClickHouse/ClickHouse/issues/1226 https://github.com/ClickHouse/ClickHouse/pull/7533  Presentation:  https://youtu.be/ckChUkC3Pns?t=9353 https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup47/materialized_views.pdf  Webinar:\n{% embed url=“https://altinity.com/webinarspage/2019/6/26/clickhouse-and-the-magic-of-materialized-views\" %}\n","categories":"","description":"MATERIALIZED VIEWS\n","excerpt":"MATERIALIZED VIEWS\n","ref":"/altinity-kb-schema-design/materialized-views/","tags":"","title":"MATERIALIZED VIEWS"},{"body":"max_memory_usage. Single query memory usage max_memory_usage - the maximum amount of memory allowed for a single query to take. By default, it’s 10Gb. The default value is good, don’t adjust it in advance.\nThere are scenarios when you need to relax the limit for particular queries (if you hit ‘Memory limit (for query) exceeded’), or use a lower limit if you need to discipline the users or increase the number of simultaneous queries.\nServer memory usage Server memory usage = constant memory footprint (used by different caches, dictionaries, etc) + sum of memory temporary used by running queries (a theoretical limit is a number of simultaneous queries multiplied by max_memory_usage).\nSince 20.4 you can set up a global limit using the max_server_memory_usage setting. If something will hit that limit you will see ‘Memory limit (total) exceeded’ in random places.\nBy default it 90% of the physical RAM of the server.\nhttps://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#max_server_memory_usage\nhttps://github.com/ClickHouse/ClickHouse/blob/e5b96bd93b53d2c1130a249769be1049141ef386/programs/server/config.xml#L239-L250\nYou can decrease that in some scenarios (like you need to leave more free RAM for page cache or to some other software).\nHow to check what is using my RAM? {% page-ref page=“altinity-kb-who-ate-my-memory.md” %}\nMark cache https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/mark-cache.pdf\n","categories":"","description":"memory configuration settings\n","excerpt":"memory configuration settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/","tags":"","title":"memory configuration settings"},{"body":"Internals:\n{% embed url=“https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/merge_tree.pdf” %}\n{% embed url=“https://youtu.be/1UIl7FpNo2M?t=2467\" %}\n","categories":"","description":"MergeTree table engine family\n","excerpt":"MergeTree table engine family\n","ref":"/engines/mergetree-table-engine-family/readme/","tags":"","title":"MergeTree table engine family"},{"body":"Prometheus endpoint  Grafana dashboard (internal endpoint) https://grafana.com/grafana/dashboards/13500\nGrafana dashboard (clickhouse-operator) https://github.com/Altinity/clickhouse-operator/tree/master/grafana-dashboard\nPrometheus alerts (clickhouse-operator) https://github.com/Altinity/clickhouse-operator/blob/master/deploy/prometheus/prometheus-alert-rules.yaml\nClickHouse exporter  https://github.com/ClickHouse/clickhouse_exporter\nZabbix  https://github.com/Altinity/clickhouse-zabbix-template\nZooKeeper Monitoring {% page-ref page=“altinity-kb-zookeeper/zookeeper-monitoring.md” %}\n","categories":"","description":"Monitoring\n","excerpt":"Monitoring\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/","tags":"","title":"Monitoring"},{"body":"Monitoring helps to track potential issues in your cluster before they cause a critical error.\nExternal Monitoring External monitoring collects data from the ClickHouse cluster and uses it for analysis and review. Recommended external monitoring systems include:\n Prometheus: Use embedded exporter or clickhouse-exporter Graphite: Use the embedded exporter. See config.xml. InfluxDB: Use the embedded exporter, plus Telegraf. For more information, see Graphite protocol support in InfluxDB.  ClickHouse can collect the recording of metrics internally by enabling system.metric_log in config.xml.\nFor dashboard system:\n Grafana is recommended for graphs, reports, alerts, dashboard, etc. Other options are Nagios or Zabbix.  The following metrics should be collected:\n For Host Machine:  CPU Memory Network (bytes/packets) Storage (iops) Disk Space (free / used)   For ClickHouse:  Connections (count) RWLocks Read / Write / Return (bytes) Read / Write / Return (rows) Zookeeper operations (count) Absolute delay Query duration (optional) Replication parts and queue (count)   For Zookeeper:  See ZooKeeper Monitoring Integration.    The following queries are recommended to be included in monitoring:\n SELECT * FROM system.replicas  For more information, see the ClickHouse guide on System Tables   SELECT * FROM system.merges  Checks on the speed and progress of currently executed merges.   SELECT * FROM system.mutations  This is the source of information on the speed and progress of currently executed merges.    Monitor and Alerts Configure the notifications for events and thresholds based on the following table:\n Monitor and Alerts  Health Checks The following health checks should be monitored:\n   Check Name Shell or SQL command Severity     ClickHouse status $ curl 'http://localhost:8123/'Ok. Critical   Too many simultaneous queries. Maximum: 100 select value from system.metrics where metric='Query' Critical   Replication status $ curl 'http://localhost:8123/replicas_status'Ok. High   Read only replicas (reflected by replicas_status as well) select value from system.metrics where metric='ReadonlyReplica’ High   ReplicaPartialShutdown (not reflected by replicas_status, but seems to correlate with ZooKeeperHardwareExceptions) select value from system.events where event='ReplicaPartialShutdown' HighI turned this one off. It almost always correlates with ZooKeeperHardwareExceptions, and when it’s not, then there is nothing bad happening…   Some replication tasks are stuck select count()from system.replication_queuewhere num_tries \u003e 100 High   ZooKeeper is available select count() from system.zookeeper where path='/' Critical for writes   ZooKeeper exceptions select value from system.events where event='ZooKeeperHardwareExceptions' Medium   Other CH nodes are available $ for node in `echo \"select distinct host_address from system.clusters where host_name !='localhost'\" curl 'http://localhost:8123/' –silent –data-binary @-`; do curl \"http://$node:8123/\" –silent ; done   All CH clusters are available (i.e. every configured cluster has enough replicas to serve queries) for cluster in `echo \"select distinct cluster from system.clusters where host_name !='localhost'\" curl 'http://localhost:8123/' –silent –data-binary @-` ; do clickhouse-client –query=\"select '$cluster', 'OK' from cluster('$cluster', system, one)\" ; done   There are files in 'detached' folders $ find /var/lib/clickhouse/data///detached/* -type d wc -l;\n19.8+select count() from system.detached_parts\n   Too many parts:\nNumber of parts is growing;\nInserts are being delayed;\nInserts are being rejected\n select value from system.asynchronous_metrics where metric='MaxPartCountForPartition';select value from system.events/system.metrics where event/metric='DelayedInserts';\nselect value from system.events where event='RejectedInserts'\n Critical   Dictionaries: exception select concat(name,': ',last_exception) from system.dictionarieswhere last_exception != '' Medium   ClickHouse has been restarted select uptime();select value from system.asynchronous_metrics where metric='Uptime'    DistributedFilesToInsert should not be always increasing select value from system.metrics where metric='DistributedFilesToInsert' Medium   A data part was lost select value from system.events where event='ReplicatedDataLoss' High   Data parts are not the same on different replicas select value from system.events where event='DataAfterMergeDiffersFromReplica';\nselect value from system.events where event='DataAfterMutationDiffersFromReplica'\n Medium    Monitoring References  https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-monitoring https://tech.marksblogg.com/clickhouse-prometheus-grafana.html Key Metrics for Monitoring ClickHouse ClickHouse Monitoring Key Metrics to Monitor ClickHouse Monitoring Tools: Five Tools to Consider Monitoring ClickHouse Monitor ClickHouse with Datadog  ","categories":"","description":"Monitoring Considerations\n","excerpt":"Monitoring Considerations\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/monitoring-considerations/","tags":"","title":"Monitoring Considerations"},{"body":"Suppose we mount a new device at path /mnt/disk_1 and want to move table_4 to it.\n Create directory on new device for ClickHouse data. /in shell mkdir /mnt/disk_1/clickhouse Change ownership of created directory to ClickHouse user. /in shell chown -R clickhouse:clickhouse /mnt/disk_1/clickhouse Create a special storage policy which should include both disks: old and new. /in shell  nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdefault\u003e \u003cdisk\u003edefault\u003c/disk\u003e \u003cmax_data_part_size_bytes\u003e10000000\u003c/max_data_part_size_bytes\u003e \u003c/default\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e #########################################################################################  Update storage_policy setting of tables to new policy.  ALTERTABLEtable_4MODIFYSETTINGstorage_policy='move_from_default_to_disk_1'; Wait till all parts of tables change their disk_name to new disk.  SELECTname,disk_name,pathfromsystem.partsWHEREtable='table_4'andactive;SELECTdisk_name,path,sum(rows),sum(bytes_on_disk),uniq(partition),count()FROMsystem.partsWHEREtable='table_4'andactiveGROUPBYdisk_name,pathORDERBYdisk_name,path; Remove ‘default’ disk from new storage policy. In server shell:  nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e ######################################################################################### ClickHouse wouldn’t auto reload config, because we removed some disks from storage policy, so we need to restart it by hand.\n Restart ClickHouse server. Make sure that storage policy uses the right disks.  SELECT*FROMsystem.storage_policiesWHEREpolicy_name='move_from_default_to_disk_1';","categories":"","description":"Moving table to another device.\n","excerpt":"Moving table to another device.\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-moving-table-to-another-device./","tags":"","title":"Moving table to another device."},{"body":"Authorization MySQL8 used default authorization plugin caching_sha2_password. Unfortunately, libmysql which currently used (21.4-) in clickhouse is not\nYou can fix it during create custom user with mysql_native_password authentication plugin.\nCREATEUSERIFNOTEXISTS'clickhouse'@'%'IDENTIFIEDWITHmysql_native_passwordBY'clickhouse_user_password';CREATEDATABASEIFNOTEXISTStest;GRANTALLPRIVILEGESONtest.*TO'clickhouse'@'%';Table schema changes ClickHouse run SHOW TABLE STATUS LIKE 'table\\\\_name' and try to figure out was table schema changed or not from MySQL response field Update_time\nBy default for properly data loading from MySQL8 source to dictionaries, please turn off information_schema cache.\nYou can change default behavior with create /etc/mysql/conf.d/information_schema_cache.cnfwith following content:\n[mysqld] information_schema_stats_expiry=0 Or setup it via SQL query:\nSETGLOBALinformation_schema_stats_expiry=0;","categories":"","description":"MySQL8 source for dictionaries\n","excerpt":"MySQL8 source for dictionaries\n","ref":"/altinity-kb-dictionaries/mysql8-source-for-dictionaries/","tags":"","title":"MySQL8 source for dictionaries"},{"body":"Networking And Server Room Planning The network used for your ClickHouse cluster should be a fast network, ideally 10 Gbit. ClickHouse nodes generate a lot of traffic along with the Zookeeper connections and inter-Zookeeper communications.\nFor the zookeeper low latency is more important than bandwidth.\nKeep the replicas isolated on the hardware level. This allows for cluster failover from possible outages.\n For Physical Environments: Avoid placing 2 ClickHouse replicas on the same server rack. Ideally, they should be on isolated network switches and an isolated power supply. For Clouds Environments: Use different availability zones between the ClickHouse replicas when possible (but be aware of the interzone traffic costs)  These considerations are the same as the Zookeeper nodes.\nFor example:\n   Rack Server Server Server Server     Rack 1 CH_SHARD1_R1 CH_SHARD2_R1 CH_SHARD3_R1 ZOO_1   Rack 2 CH_SHARD1_R2 CH_SHARD2_R2 CH_SHARD3_R2 ZOO_2   Rack 3 ZOO3       Network Ports And Firewall ClickHouse listens the following ports:\n 9000: clickhouse-client, native clients, other clickhouse-servers connect to here. 8123: HTTP clients 9009: Other replicas will connect here to download data.  For more information, see CLICKHOUSE NETWORKING, PART 1.\nZookeeper listens the following ports:\n 2181: Client connections. 2888: Inter-ensemble connections. 3888: Leader election.  Outbound traffic from ClickHouse connects to the following ports:\n ZooKeeper: On port 2181. Other CH nodes in the cluster: On port 9000 and 9009. Dictionary sources: Depending on what was configured such as HTTP, MySQL, Mongo, etc. Kafka or Hadoop: If those integrations were enabled.  SSL For non-trusted networks enable SSL/HTTPS. If acceptable, it is better to keep interserver communications unencrypted for performance reasons.\nNaming Schema The best time to start creating a naming schema for the servers is before they’re created and configured.\nThere are a few features based on good server naming in ClickHouse:\n clickhouse-client prompts: Allows a different prompt for clickhouse-client per server hostname. Nearest hostname load balancing: For more information, see Nearest Hostname.  A good option is to use the following:\n{datacenter}-{serverroom}-{rack identifier}-{clickhouse cluster identifier}-{shard number or server number}.\nOther examples:\n rxv-olap-ch-master-sh01-r01:  rxv - location (rack#15) olap - product name ch = clickhouse master = stage sh01 = shard 1 r01 = replica 1   hetnzerde1-ch-prod-01.local:  hetnzerde1 - location (also replica id) ch = clickhouse prod = stage 01 - server number / shard number in that DC   sh01.ch-front.dev.aws-east1a.example.com:  sh01 - shard 01 ch-front - cluster name dev = stage aws = cloud provider east1a = region and availability zone    Host Name References  \u003cstrong\u003eWhat are the best practices for domain names (dev, staging, production)?\u003c/strong\u003e \u003cstrong\u003e9 Best Practices and Examples for Working with Kubernetes Labels\u003c/strong\u003e \u003cstrong\u003eThoughts On Hostname Nomenclature\u003c/strong\u003e  Additional Hostname Tips  Hostnames configured on the server should not change. If you do need to change the host name, one reference to use is How to Change Hostname on Ubuntu 18.04. The server should be accessible to other servers in the cluster via it’s hostname. Otherwise you will need to configure interserver_hostname in your config. Ensure that hostname --fqdn and getent hosts $(hostname --fqdn) return the correct name and ip.  ","categories":"","description":"Network Configuration\n","excerpt":"Network Configuration\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/","tags":"","title":"Network Configuration"},{"body":"CREATETABLEx(`a`Nullable(UInt32),`b`Nullable(UInt32),`cnt`UInt32)ENGINE=SummingMergeTreeORDERBY(a,b)SETTINGSallow_nullable_key=1;INSERTINTOxVALUES(Null,2,1),(Null,Null,1),(3,Null,1),(4,4,1);INSERTINTOxVALUES(Null,2,1),(Null,Null,1),(3,Null,1),(4,4,1);SELECT*FROMx;┌────a─┬────b─┬─cnt─┐│3│ᴺᵁᴸᴸ│2││4│4│2││ᴺᵁᴸᴸ│2│2││ᴺᵁᴸᴸ│ᴺᵁᴸᴸ│2│└──────┴──────┴─────┘","categories":"","description":"Nulls in order by\n","excerpt":"Nulls in order by\n","ref":"/engines/mergetree-table-engine-family/altinity-kb-nulls-in-order-by/","tags":"","title":"Nulls in order by"},{"body":"Q: Why do I have several active parts in a partition? Why Clickhouse does not merge them immediately?\nA: CH does not merge parts by time.\nMerge scheduler selects parts by own algorithm based on the current node workload / number of parts / size of parts.\nCH merge scheduler balances between a big number of parts and a wasting resources on merges.\nMerges are CPU/DISK IO expensive. If CH will merge every new part then all resources will be spend on merges and will no resources remain on queries (selects ).\n","categories":"","description":"Number of active parts in a partition\n","excerpt":"Number of active parts in a partition\n","ref":"/altinity-kb-useful-queries/altinity-kb-number-of-active-parts-in-a-partition/","tags":"","title":"Number of active parts in a partition"},{"body":"List of missing tables\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,table,arrayFilter(i-\u003eNOThas(groupArray(host),i),hosts)miss_tableFROM(SELECTFQDN()host,database,nametableFROMclusterAllReplicas({cluster},system,tables)WHEREengineNOTIN('Log','Memory','TinyLog'))GROUPBYdatabase,tableHAVINGmiss_table\u003c\u003e[]SETTINGSskip_unavailable_shards=1;┌─database─┬─table─┬─miss_table────────────────┐│default│test│['host366.mynetwork.net']│└──────────┴───────┴───────────────────────────┘List of inconsistent tables\nSELECTdatabase,name,engine,uniqExact(create_table_query)ASddlFROMclusterAllReplicas({cluster},system.tables)GROUPBYdatabase,name,engineHAVINGddl\u003e1List of inconsistent columns\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,table,column,arrayStringConcat(arrayMap(i-\u003ei.2||': '||i.1,(groupArray((type,host))ASg)),', ')diffFROM(SELECTFQDN()host,database,table,namecolumn,typeFROMclusterAllReplicas({cluster},system,columns))GROUPBYdatabase,table,columnHAVINGlength(arrayDistinct(g.1))\u003e1ORlength(g.1)\u003c\u003elength(hosts)SETTINGSskip_unavailable_shards=1;┌─database─┬─table───┬─column────┬─diff────────────────────────────────┐│default│z│A│ch-host22:Int64,ch-host21:String│└──────────┴─────────┴───────────┴─────────────────────────────────────┘List of inconsistent dictionaries\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,dictionary,arrayFilter(i-\u003eNOThas(groupArray(host),i),hosts)miss_dict,arrayReduce('median',(groupArray((element_count,host))ASec).1)FROM(SELECTFQDN()host,database,namedictionary,element_countFROMclusterAllReplicas({cluster},system,dictionaries))GROUPBYdatabase,dictionaryHAVINGmiss_dict\u003c\u003e[]SETTINGSskip_unavailable_shards=1;","categories":"","description":"Object consistency in a cluster\n","excerpt":"Object consistency in a cluster\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-object-consistency-in-a-cluster/","tags":"","title":"Object consistency in a cluster"},{"body":"OPTIMIZE TABLE xyz – this initiates an unscheduled merge.\nExample You have 40 parts in 3 partitions. This unscheduled merge selects some partition (i.e. February) and selects 3 small parts to merge, then merge them into a single part. You get 38 parts in the result.\nOPTIMIZE TABLE xyz FINAL – initiates a cycle of unscheduled merges.\nClickHouse merges parts in this table until will remains 1 part in each partition (if a system has enough free disk space). As a result, you get 3 parts, 1 part per partition. In this case, CH rewrites parts even if they are already merged into a single part. It creates a huge CPU / Disk load if the table ( XYZ) is huge. ClickHouse reads / uncompress / merge / compress / writes all data in the table.\nIf this table has size 1TB it could take around 3 hours to complete.\nSo we don’t recommend running OPTIMIZE TABLE xyz FINAL against tables with more than 10million rows.\n","categories":"","description":"OPTIMIZE vs OPTIMIZE FINAL\n","excerpt":"OPTIMIZE vs OPTIMIZE FINAL\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-optimize-vs-optimize-final/","tags":"","title":"OPTIMIZE vs OPTIMIZE FINAL"},{"body":"Custom settings allows to emulate parameterized views.\nYou need to enable custom settings and define any prefixes for settings.\n$ cat /etc/clickhouse-server/config.d/custom_settigs_prefix.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003ccustom_settings_prefixes\u003emy,my2\u003c/custom_settings_prefixes\u003e \u003c/yandex\u003e $ service clickhouse-server restart Now you can set settings as any other settings, and query them using getSetting() function.\nSETmy2_category='hot deals';SELECTgetSetting('my2_category');┌─getSetting('my2_category')─┐│hotdeals│└────────────────────────────┘-- you can query ClickHouse settings as well SELECTgetSetting('max_threads')┌─getSetting('max_threads')─┐│8│└───────────────────────────┘Now we can create a view\nCREATEVIEWmy_new_viewASSELECT*FROMdealsWHEREcategory_idIN(SELECTcategory_idFROMdeal_categoriesWHEREcategory=getSetting('my2_category'));And query it\nSELECT*FROMmy_new_viewSETTINGSmy2_category='hot deals';","categories":"","description":"Parameterized views\n","excerpt":"Parameterized views\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-parameterized-views/","tags":"","title":"Parameterized views"},{"body":"Clickhouse able to fetch from a source only updated rows. You need to define update_field section:\nWe have a table in an external source MySQL, PG, HTTP, ….\nLet’s use for this example Clickhouse.\nCREATETABLEcities(`polygon`Array(Tuple(Float64,Float64)),`city`String,`updated_at`DateTimeDEFAULTnow())ENGINE=MergeTreeORDERBYcityWhen you add new row and “update” some rows in this table you should update updated_at with the new timestamp.\n-- fetch updated rows every 30 seconds CREATEDICTIONARYcities_dict(polygonArray(Tuple(Float64,Float64)),cityString)PRIMARYKEYpolygonSOURCE(CLICKHOUSE(TABLEcitiesDB'default'update_field'updated_at'))LAYOUT(POLYGON())LIFETIME(MIN30MAX30)A dictionary with update_field updated_at will fetch only updated rows. A dictionary saves the current time (now) (time of the last successful update and queries the source where updated_at \u003e= previous_update - 1 (shift = 1 sec.)\nIn case of HTTP source Clickhouse will send get requests with update_field as an URL parameter \u0026updated_at=2020-01-01%2000:01:01\n","categories":"","description":"Partial updates\n","excerpt":"Partial updates\n","ref":"/altinity-kb-dictionaries/partial-updates/","tags":"","title":"Partial updates"},{"body":"In version 19.14 a serious issue was found: a race condition that can lead to server deadlock. The reason for that was quite fundamental, and a temporary workaround for that was added (“possible deadlock avoided”).\nThose locks are one of the fundamental things that the core team was actively working on in 2020.\nIn 20.3 some of the locks leading to that situation were removed as a part of huge refactoring.\nIn 20.4 more locks were removed, the check was made configurable (see lock_acquire_timeout ) so you can say how long to wait before returning that exception\nIn 20.5 heuristics of that check (“possible deadlock avoided”) was improved.\nIn 20.6 all table-level locks which were possible to remove were removed, so alters are totally lock-free.\n20.10 enables database=Atomic by default which allows running even DROP commands without locks.\nTypically issue was happening when doing some concurrent select on system.parts / system.columns / system.table with simultaneous table manipulations (doing some kind of ALTERS / TRUNCATES / DROP)I\nIf that exception happens often in your use-case: An update is recommended. In the meantime, check which queries are running (especially to system.tables / system.parts and other system tables) and check if killing them / avoiding them helps to solve the issue.\n","categories":"","description":"Possible deadlock avoided. Client should retry\n","excerpt":"Possible deadlock avoided. Client should retry\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-possible-deadlock-avoided.-client-should-retry/","tags":"","title":"Possible deadlock avoided. Client should retry"},{"body":"The biggest problem with running ClickHouse in k8s, happens when clickhouse-server can’t start for some reason and pod is falling in CrashloopBackOff, so you can’t easily get in the pod and check/fix/restart ClickHouse.\nThere is multiple possible reasons for this, some of them can be fixed without manual intervention in pod:\n Wrong configuration files Fix: Check templates which are being used for config file generation and fix them. While upgrade some backward incompatible changes prevents ClickHouse from start. Fix: Downgrade and check backward incompatible changes for all versions in between.  Next reasons would require to have manual intervention in pod/volume.\nThere is two ways, how you can get access to data:\n Change entry point of ClickHouse pod to something else, so pod wouldn’t be terminated due ClickHouse error. Attach ClickHouse data volume to some generic pod (like Ubuntu). Unclear restart which produced broken files and/or state on disk is differs too much from state in zookeeper for replicated tables. Fix: Create force_restore_data flag. Wrong file permission for ClickHouse files in pod. Fix: Use chown to set right ownership for files and directories. Errors in ClickHouse table schema prevents ClickHouse from start. Fix: Rename problematic table.sql scripts to table.sql.bak Occasional failure of distributed queries because of wrong user/password. Due nature of k8s with dynamic ip allocations, it’s possible that ClickHouse would cache wrong ip-\u003e hostname combination and disallow connections because of mismatched hostname. Fix: run SYSTEM DROP DNS CACHE; \u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml.  Caveats:\n Not all configuration/state folders are being covered by persistent volumes. (geobases) Page cache belongs to k8s node and pv are being mounted to pod, in case of fast shutdown there is possibility to loss some data(needs to be clarified) Some cloud providers (GKE) can have slow unlink command, which is important for clickhouse because it’s needed for parts management. (max_part_removal_threads setting)  Useful commands:\nkubectl logs chi-chcluster-2-1-0 -c clickhouse-pod -n chcluster --previous kubectl describe pod chi-chcluster-2-1-0 -n chcluster Q. Clickhouse is caching the Kafka pod’s IP and trying to connect to the same ip even when there is a new Kafka pod running and the old one is deprecated. Is there some setting where we could refresh the connection\n\u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml\nClickHouse init process failed. It’s due to low value for env CLICKHOUSE_INIT_TIMEOUT value. Consider increasing it up to 1 min.\nhttps://github.com/ClickHouse/ClickHouse/blob/9f5cd35a6963cc556a51218b46b0754dcac7306a/docker/server/entrypoint.sh#L120\n","categories":"","description":"Possible issues with running ClickHouse in k8s\n","excerpt":"Possible issues with running ClickHouse in k8s\n","ref":"/altinity-kb-kubernetes/altinity-kb-possible-issues-with-running-clickhouse-in-k8s/","tags":"","title":"Possible issues with running ClickHouse in k8s"},{"body":"Moving from a single ClickHouse server to a clustered format provides several benefits:\n Replication guarantees data integrity. Provides redundancy. Failover by being able to restart half of the nodes without encountering downtime.  Moving from an unsharded ClickHouse environment to a sharded cluster requires redesign of schema and queries. Starting with a sharded cluster from the beginning makes it easier in the future to scale the cluster up.\nSetting up a ClickHouse cluster for a production environment requires the following stages:\n Hardware Requirements Network Configuration Create Host Names Monitoring Considerations Configuration Steps Setting Up Backups Staging Plans Upgrading The Cluster  ","categories":"","description":"Production Cluster Configuration Guide\n","excerpt":"Production Cluster Configuration Guide\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/","tags":"","title":"Production Cluster Configuration Guide"},{"body":"Aggregating projections. createtablez(BrowserString,CountryUInt8,FFloat64)Engine=MergeTreeorderbyBrowser;insertintozselecttoString(number%9999),number%33,1fromnumbers(100000000);--Q1) selectsum(F),BrowserfromzgroupbyBrowserformatNull;Elapsed:0.205sec.Processed100.00millionrows--Q2) selectsum(F),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.381sec.Processed100.00millionrows--Q3) selectsum(F),count(),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.398sec.Processed100.00millionrowsaltertablezaddprojectionpp(selectBrowser,Country,count(),sum(F)groupbyBrowser,Country);altertablezmaterializeprojectionpp;---- 0 = don't use proj, 1 = use projection setallow_experimental_projection_optimization=1;--Q1) selectsum(F),BrowserfromzgroupbyBrowserformatNull;Elapsed:0.003sec.Processed22.43thousandrows--Q2) selectsum(F),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.004sec.Processed22.43thousandrows--Q3) selectsum(F),count(),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.005sec.Processed22.43thousandrows","categories":"","description":"Projections examples\n","excerpt":"Projections examples\n","ref":"/altinity-kb-queries-and-syntax/projections-examples/","tags":"","title":"Projections examples"},{"body":"Main docs article {% embed url=“https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/\" caption=”\" %}\nHardware requirements: TLDR version:\n1) USE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup)\n2) use 3 nodes (more nodes = slower quorum, less = no HA).\n3) low network latency between zookeeper nodes is very important (latency, not bandwidth).\n4) have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings.\n5) ensure that zookeeper will not be CPU-starved by some other processes\n6) monitor zookeeper.\nSide note:\nin many cases, the slowness of the zookeeper is actually a symptom of some issue with clickhouse schema/usage pattern (the most typical issues: an enormous number of partitions/tables/databases with real-time inserts, tiny \u0026 frequent inserts).\nSome doc about that subject:\n https://docs.confluent.io/platform/current/zookeeper/deployment.html https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.html#sc_commonProblems https://clickhouse.tech/docs/en/operations/tips/#zookeeper https://lucene.apache.org/solr/guide/7_4/setting-up-an-external-zookeeper-ensemble.html https://cwiki.apache.org/confluence/display/ZOOKEEPER/Troubleshooting  Cite from https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_commonProblems :\n Things to Avoid  Here are some common problems you can avoid by configuring ZooKeeper correctly:\n inconsistent lists of servers : The list of ZooKeeper servers used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has. Things work okay if the client list is a subset of the real list, but things will really act strange if clients have a list of ZooKeeper servers that are in different ZooKeeper clusters. Also, the server lists in each Zookeeper server configuration file should be consistent with one another. incorrect placement of transaction log : The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log. incorrect Java heap size : You should take special care to set your Java max heap size correctly. In particular, you should not create a situation in which ZooKeeper swaps to disk. The disk is death to ZooKeeper. Everything is ordered, so if processing one request swaps the disk, all other queued requests will probably do the same. the disk. DON’T SWAP. Be conservative in your estimates: if you have 4G of RAM, do not set the Java max heap size to 6G or even 4G. For example, it is more likely you would use a 3G heap for a 4G machine, as the operating system and the cache also need memory. The best and only recommend practice for estimating the heap size your system needs is to run load tests, and then make sure you are well below the usage limit that would cause the system to swap. Publicly accessible deployment : A ZooKeeper ensemble is expected to operate in a trusted computing environment. It is thus recommended to deploy ZooKeeper behind a firewall.    ","categories":"","description":"Proper setup\n","excerpt":"Proper setup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/","tags":"","title":"Proper setup"},{"body":" -SimpleStateIf or -IfState for simple aggregate functions ALTER MODIFY COLUMN is stuck, the column is inaccessible. ANSI SQL mode Coming soon on ClickHouse: Window functions Data types on disk and in RAM External dictionaries FINAL clause speed Join with Calendar using Arrays KILL QUERY Lag/Lead OPTIMIZE vs OPTIMIZE FINAL Parameterized views PIVOT/UNPIVOT Possible deadlock avoided. Client should retry Sample by  ","categories":"","description":"Queries \u0026 Syntax\n","excerpt":"Queries \u0026 Syntax\n","ref":"/altinity-kb-queries-and-syntax/","tags":"","title":"Queries \u0026 Syntax"},{"body":"DROPTABLEIFEXISTSrates;DROPDICTIONARYIFEXISTSrates_dict;CREATETABLErates(idUInt64,date_startNullable(Date),date_endNullable(Date),rateDecimal64(4))engine=Log;INSERTINTOratesVALUES(1,Null,'2021-03-13',99),(1,'2021-03-14','2021-03-16',100),(1,'2021-03-17',Null,101),(2,'2021-03-14',Null,200),(3,Null,'2021-03-14',300),(4,'2021-03-14','2021-03-14',400);CREATEDICTIONARYrates_dict(idUInt64,date_startDate,date_endDate,rateDecimal64(4))PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000USER'default'TABLE'rates'))LIFETIME(MIN1MAX1000)LAYOUT(RANGE_HASHED())RANGE(MINdate_startMAXdate_end);SELECT*FROMrates_dictorderbyid,date_start;┌─id─┬─date_start─┬───date_end─┬─────rate─┐│1│1970-01-01│2021-03-13│99.0000││1│2021-03-14│2021-03-16│100.0000││1│2021-03-17│1970-01-01│101.0000││2│2021-03-14│1970-01-01│200.0000││3│1970-01-01│2021-03-14│300.0000││4│2021-03-14│2021-03-14│400.0000│└────┴────────────┴────────────┴──────────┘WITHtoDate('2021-03-10')+INTERVALnumberDAYasdateselectdate,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(1),date)asrate1,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(2),date)asrate2,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(3),date)asrate3,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(4),date)asrate4FROMnumbers(10);┌───────date─┬────rate1─┬────rate2─┬────rate3─┬────rate4─┐│2021-03-10│99.0000│0.0000│300.0000│0.0000││2021-03-11│99.0000│0.0000│300.0000│0.0000││2021-03-12│99.0000│0.0000│300.0000│0.0000││2021-03-13│99.0000│0.0000│300.0000│0.0000││2021-03-14│100.0000│200.0000│300.0000│400.0000││2021-03-15│100.0000│200.0000│0.0000│0.0000││2021-03-16│100.0000│200.0000│0.0000│0.0000││2021-03-17│101.0000│200.0000│0.0000│0.0000││2021-03-18│101.0000│200.0000│0.0000│0.0000││2021-03-19│101.0000│200.0000│0.0000│0.0000│└────────────┴──────────┴──────────┴──────────┴──────────┘","categories":"","description":"range\\_hashed example - open intervals\n","excerpt":"range\\_hashed example - open intervals\n","ref":"/altinity-kb-dictionaries/altinity-kb-range_hashed-example-open-intervals/","tags":"","title":"range hashed example - open intervals"},{"body":"Problem  Every ClickHouse user experienced a loss of ZooKeeper one day. While the data is available and replicas respond to queries, inserts are no longer possible. ClickHouse uses ZooKeeper in order to store the reference version of the table structure and part of data, and when it is not available can not guarantee data consistency anymore. Replicated tables turn to the read-only mode. In this article we describe step-by-step instructions of how to restore ZooKeeper metadata and bring ClickHouse cluster back to normal operation.\nIn order to restore ZooKeeper we have to solve two tasks. First, we need to restore table metadata in ZooKeeper. Currently, the only way to do it is to recreate the table with the CREATE TABLE DDL statement.\nCREATETABLEtable_name...ENGINE=ReplicatedMergeTree('zookeeper_path','replica_name');The second and more difficult task is to populate zookeeper with information of clickhouse data parts. As mentioned above, ClickHouse stores the reference data about all parts of replicated tables in ZooKeeper, so we have to traverse all partitions and re-attach them to the recovered replicated table in order to fix that.\nTest case  Let’s say we have replicated table table_repl.\nCREATETABLEtable_repl(`number`UInt32)ENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl','{replica}')PARTITIONBYintDiv(number,1000)ORDERBYnumber;And populate it with some data\nSELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/cluster_1/tables/01/';INSERTINTOtable_replSELECT*FROMnumbers(1000,2000);SELECTpartition,sum(rows)ASrows,count()FROMsystem.partsWHEREtable='table_repl'ANDactiveGROUPBYpartition;Now let’s remove metadata in zookeeper using ZkCli.sh at ZooKeeper host:\ndeleteall /clickhouse/cluster_1/tables/01/table_repl And try to resync clickhouse replica state with zookeeper:\nSYSTEMRESTARTREPLICAtable_repl;If we try to insert some data in the table, error happens:\nINSERTINTOtable_replSELECTnumberASnumberFROMnumbers(1000,2000)WHEREnumber%2=0;And now we have an exception that we lost all metadata in zookeeper. It is time to recover!\nCurrent Solution    Detach replicated table.\nDETACH TABLE table_repl;   Save the table’s attach script and change engine of replicated table to non-replicated *mergetree analogue. Table definition is located in the ‘metadata’ folder, ‘/var/lib/clickhouse/metadata/default/table_repl.sql’ in our example. Please make a backup copy and modify the file as follows:\nATTACH TABLE table_repl ( `number` UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl', '{replica}') PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192 Needs to be replaced with this:\nATTACH TABLE table_repl ( `number` UInt32 ) ENGINE = MergeTree() PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192   Attach non-replicated table.\nATTACH TABLE table_repl;   Rename non-replicated table.\nRENAME TABLE table_repl TO table_repl_old;   Create a new replicated table. Take the saved attach script and replace ATTACH with CREATE, and run it.\nCREATE TABLE table_repl ( `number` UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl', '{replica}') PARTITION BY intDiv(number, 1000) ORDER BY number SETTINGS index_granularity = 8192   Attach parts from old table to new.\nALTER TABLE table_repl ATTACH PARTITION 1 FROM table_repl_old; ALTER TABLE table_repl ATTACH PARTITION 2 FROM table_repl_old;   If the table has many partitions, it may require some shell script to make it easier.\nAutomated approach For a large number of tables, you can use script https://github.com/Altinity/clickhouse-zookeeper-recovery which partially automates the above approach.\n","categories":"","description":"Recovering from complete metadata loss in ZooKeeper\n","excerpt":"Recovering from complete metadata loss in ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-recovering-from-complete-metadata-loss-in-zookeeper/","tags":"","title":"Recovering from complete metadata loss in ZooKeeper"},{"body":"Removing of empty parts is a new feature introduced in 20.12.\nEarlier versions leave empty parts (with 0 rows) if TTL removes all rows from a part (https://github.com/ClickHouse/ClickHouse/issues/5491).\nIf you set up TTL for your data it is likely that there are quite many empty parts in your system.\nThe new version notices empty parts and tries to remove all of them immediately.\nThis is a one-time operation which runs right after an upgrade.\nAfter that TTL will remove empty parts on its own.\nThere is a problem when different replicas of the same table start to remove empty parts at the same time. Because of the bug they can block each other (https://github.com/ClickHouse/ClickHouse/issues/23292).\nWhat we can do to avoid this problem during an upgrade:\n1) Drop empty partitions before upgrading to decrease the number of empty parts in the system.\nSELECTconcat('alter table ',database,'.',table,' drop partition id ''',partition_id,''';')FROMsystem.partsWHEREactiveGROUPBYdatabase,table,partition_idHAVINGcount()=countIf(rows=0)2) Upgrade/restart one replica (in a shard) at a time.\nIf only one replica is cleaning empty parts there will be no deadlock because of replicas waiting for one another.\nRestart one replica, wait for replication queue to process, then restart the next one.\nRemoving of empty parts can be disabled by adding remove_empty_parts=0 to the default profile.\n$ cat /etc/clickhouse-server/users.d/remove_empty_parts.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cremove_empty_parts\u003e0\u003c/remove_empty_parts\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e ","categories":"","description":"Removing empty parts\n","excerpt":"Removing empty parts\n","ref":"/upgrade/removing-empty-parts/","tags":"","title":"Removing empty parts"},{"body":"Last state CREATETABLErepl_tbl(`key`UInt32,`val_1`UInt32,`val_2`String,`val_3`String,`val_4`String,`val_5`UUID,`ts`DateTime)ENGINE=ReplacingMergeTree(ts)ORDERBYkeySYSTEMSTOPMERGESrepl_tbl;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);SELECTcount()FROMrepl_tbl┌──count()─┐│50000000│└──────────┘Single key -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblWHEREkey=10GROUPBYkey;1rowsinset.Elapsed:0.017sec.Processed40.96thousandrows,5.24MB(2.44millionrows/s.,312.31MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblWHEREkey=10ORDERBYtsDESCLIMIT1BYkey;1rowsinset.Elapsed:0.017sec.Processed40.96thousandrows,5.24MB(2.39millionrows/s.,305.41MB/s.)-- Subquery SELECT*FROMrepl_tblWHEREkey=10ANDts=(SELECTmax(ts)FROMrepl_tblWHEREkey=10);1rowsinset.Elapsed:0.019sec.Processed40.96thousandrows,1.18MB(2.20millionrows/s.,63.47MB/s.)-- FINAL SELECT*FROMrepl_tblFINALWHEREkey=10;1rowsinset.Elapsed:0.021sec.Processed40.96thousandrows,5.24MB(1.93millionrows/s.,247.63MB/s.)Multiple keys -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)GROUPBYkeyFORMATNull;Peakmemoryusage(forquery):2.31GiB.0rowsinset.Elapsed:3.264sec.Processed5.04millionrows,645.01MB(1.54millionrows/s.,197.60MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):1.11GiB.0rowsinset.Elapsed:1.772sec.Processed2.74millionrows,350.30MB(1.54millionrows/s.,197.73MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)ORDERBYtsDESCLIMIT1BYkeyFORMATNull;Peakmemoryusage(forquery):1.08GiB.0rowsinset.Elapsed:2.429sec.Processed5.04millionrows,645.01MB(2.07millionrows/s.,265.58MB/s.)-- Subquery SELECT*FROMrepl_tblWHERE(key,ts)IN(SELECTkey,max(ts)FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)GROUPBYkey)FORMATNull;Peakmemoryusage(forquery):432.57MiB.0rowsinset.Elapsed:0.939sec.Processed5.04millionrows,160.33MB(5.36millionrows/s.,170.69MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):202.88MiB.0rowsinset.Elapsed:0.824sec.Processed5.04millionrows,160.33MB(6.11millionrows/s.,194.58MB/s.)-- FINAL SELECT*FROMrepl_tblFINALWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)FORMATNull;Peakmemoryusage(forquery):198.32MiB.0rowsinset.Elapsed:1.211sec.Processed5.04millionrows,645.01MB(4.16millionrows/s.,532.57MB/s.)Full table -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblGROUPBYkeyFORMATNull;Peakmemoryusage(forquery):15.02GiB.0rowsinset.Elapsed:19.164sec.Processed50.00millionrows,6.40GB(2.61millionrows/s.,334.02MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):4.44GiB.0rowsinset.Elapsed:9.700sec.Processed21.03millionrows,2.69GB(2.17millionrows/s.,277.50MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblORDERBYtsDESCLIMIT1BYkeyFORMATNull;Peakmemoryusage(forquery):10.46GiB.0rowsinset.Elapsed:21.264sec.Processed50.00millionrows,6.40GB(2.35millionrows/s.,301.03MB/s.)-- Subquery SELECT*FROMrepl_tblWHERE(key,ts)IN(SELECTkey,max(ts)FROMrepl_tblGROUPBYkey)FORMATNull;Peakmemoryusage(forquery):2.52GiB.0rowsinset.Elapsed:6.891sec.Processed50.00millionrows,1.60GB(7.26millionrows/s.,232.22MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):1.05GiB.0rowsinset.Elapsed:4.427sec.Processed50.00millionrows,1.60GB(11.29millionrows/s.,361.49MB/s.)-- FINAL SELECT*FROMrepl_tblFINALFORMATNull;Peakmemoryusage(forquery):838.75MiB.0rowsinset.Elapsed:6.681sec.Processed50.00millionrows,6.40GB(7.48millionrows/s.,958.18MB/s.)FINAL Clickhouse merge parts only in scope of single partition, so if two rows with the same replacing key would land in different partitions, they would never be merged in single row. FINAL keyword works in other way, it merge all rows across all partitions. But that behavior can be changed viado_not_merge_across_partitions_select_final setting.\n{% page-ref page=\"../../../altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed.md\" %}\nCREATETABLErepl_tbl_part(`key`UInt32,`value`UInt32,`part_key`UInt32)ENGINE=ReplacingMergeTreePARTITIONBYpart_keyORDERBYkey;INSERTINTOrepl_tbl_partSELECT1ASkey,numberASvalue,number%2ASpart_keyFROMnumbers(4)SETTINGSoptimize_on_insert=0;SELECT*FROMrepl_tbl_part;┌─key─┬─value─┬─part_key─┐│1│1│1││1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│0│0││1│2│0│└─────┴───────┴──────────┘SELECT*FROMrepl_tbl_partFINAL;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘SELECT*FROMrepl_tbl_partFINALSETTINGSdo_not_merge_across_partitions_select_final=1;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│2│0│└─────┴───────┴──────────┘OPTIMIZETABLErepl_tbl_partFINAL;SELECT*FROMrepl_tbl_part;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│2│0│└─────┴───────┴──────────┘","categories":"","description":"ReplacingMergeTree\n","excerpt":"ReplacingMergeTree\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/readme/","tags":"","title":"ReplacingMergeTree"},{"body":"Hi there, I have a question about replacing merge trees. I have set up a Materialized View with ReplacingMergeTree table, but even if I call optimize on it, the parts don’t get merged. I filled that table yesterday, nothing happened since then. What should I do?\nMerges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts.\nIf the total size of input parts are greater than the maximum part size then they will never be merged.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings/#max-bytes-to-merge-at-max-space-in-pool\nhttps://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replacingmergetree/\nReplacingMergeTree is suitable for clearing out duplicate data in the background in order to save space, but it doesn’t guarantee the absence of duplicates.\n","categories":"","description":"ReplacingMergeTree does not collapse duplicates\n","excerpt":"ReplacingMergeTree does not collapse duplicates\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates/","tags":"","title":"ReplacingMergeTree does not collapse duplicates"},{"body":"SELECTdatabase,table,type,any(last_exception),any(postpone_reason),min(create_time),max(last_attempt_time),max(last_postpone_time),max(num_postponed)ASmax_postponed,max(num_tries)ASmax_tries,min(num_tries)ASmin_tries,countIf(last_exception!='')AScount_err,countIf(num_postponed\u003e0)AScount_postponed,countIf(is_currently_executing)AScount_executing,count()AScount_allFROMsystem.replication_queueGROUPBYdatabase,table,typeORDERBYcount_allDESC","categories":"","description":"Replication queue\n","excerpt":"Replication queue\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/","tags":"","title":"Replication queue"},{"body":" Step 1: Detach Kafka tables in ClickHouse Step 2: kafka-consumer-groups.sh --bootstrap-server kafka:9092 --topic topic:0,1,2 --group id1 --reset-offsets --to-latest --execute  More samples: https://gist.github.com/filimonov/1646259d18b911d7a1e8745d6411c0cc   Step: Attach Kafka tables back  See also these configuration settings:\n\u003ckafka\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003c/kafka\u003e ","categories":"","description":"Rewind / fast-forward / replay\n","excerpt":"Rewind / fast-forward / replay\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/","tags":"","title":"Rewind / fast-forward / replay"},{"body":"Short Instruction   Do FREEZE TABLE on needed table, partition. It would produce consistent snapshot of table data.\n  Run rsync command.\nrsync -ravlW --bwlimit=100000 /var/lib/clickhouse/data/shadow/N/database/table root@remote_host:/var/lib/clickhouse/data/database/table/detached --bwlimit is transfer limit in KBytes per second.\n  RunATTACH PARTITION for each partition from ./detached directory.\n  ","categories":"","description":"rsync\n","excerpt":"rsync\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/","tags":"","title":"rsync"},{"body":"The execution pipeline is embedded in the partition reading code.\nSo that works this way:\n ClickHouse does partition pruning based on WHERE conditions. For every partition, it picks a columns ranges (aka ‘marks’ / ‘granulas’) based on primary key conditions. Here the sampling logic is applied: a) in case of SAMPLE k (k in 0..1 range) it adds conditions WHERE sample_key \u003c k * max_int_of_sample_key_type b) in case of SAMPLE k OFFSET m it adds conditions WHERE sample_key BETWEEN m * max_int_of_sample_key_type AND (m + k) * max_int_of_sample_key_typec) in case of SAMPLE N (N\u003e1) if first estimates how many rows are inside the range we need to read and based on that convert it to 3a case (calculate k based on number of rows in ranges and desired number of rows) on the data returned by those other conditions are applied (so here the number of rows can be decreased here)  Source Code\nSAMPLE by  Docs\nSource Code\nSAMPLE key\nMust be:\n Included in the primary key. Uniformly distributed in the domain of its data type:  Bad: Timestamp; Good: intHash32(UserID);   Cheap to calculate:  Bad: cityHash64(URL); Good: intHash32(UserID);   Not after high granular fields in primary key:  Bad: ORDER BY (Timestamp, sample_key); Good: ORDER BY (CounterID, Date, sample_key).    Sampling is:\n Deterministic Works in a consistent way for different tables. Allows reading less amount of data from disk.  SAMPLE key, bonus SAMPLE 1/10 Select data for 1/10 of all possible sample keys; SAMPLE 1000000   Select from about (not less than) 1 000 000 rows on each shard;  You can use _sample_factor virtual column to determine the relative sample factor; SAMPLE 1/10 OFFSET 1/10   Select second 1/10 of all possible sample keys; SET max_parallel_replicas = 3 Select from multiple replicas of each shard in parallel;  ","categories":"","description":"SAMPLE by\n","excerpt":"SAMPLE by\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-sample-by/","tags":"","title":"SAMPLE by"},{"body":"","categories":"","description":"Schema design\n","excerpt":"Schema design\n","ref":"/altinity-kb-schema-design/","tags":"","title":"Schema design"},{"body":" golang-migrate tool - see golang-migrate Flyway - there are a lot of PRs introducing ClickHouse support, maintainer doesn’t merge them (maybe he will change his mind soon), but’s it’s not hard to build flyway from one of those PRs (latest at the top)  https://github.com/flyway/flyway/pull/3134 Сlickhouse support https://github.com/flyway/flyway/pull/3133 Add support clickhouse https://github.com/flyway/flyway/pull/2981 Clickhouse replicated https://github.com/flyway/flyway/pull/2640 Yet another ClickHouse support https://github.com/flyway/flyway/pull/2166 Clickhouse support (#1772) https://github.com/flyway/flyway/pull/1773 Fixed #1772: Add support for ClickHouse (https://clickhouse.yandex/)   liquibase  https://github.com/mediarithmics/liquibase-clickhouse https://johntipper.org/how-to-execute-liquibase-changesets-against-clickhouse/   custom tool for ClickHouse  https://github.com/delium/clickhouse-migrator   phpMigrations  https://github.com/smi2/phpMigrationsClickhouse https://habrahabr.ru/company/smi2/blog/317682/    know more?\n","categories":"","description":"Schema migration tools for ClickHouse\n","excerpt":"Schema migration tools for ClickHouse\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/","tags":"","title":"Schema migration tools for ClickHouse"},{"body":"Question What will happen, if we would run SELECT query from working Kafka table with MV attached? Would data showed in SELECT query appear later in MV destination table?\nAnswer  Most likely SELECT query would show nothing. If you lucky enough and something would show up, those rows wouldn’t appear in MV destination table.  So it’s not recommended to run SELECT queries on working Kafka tables.\nIn case of debug it’s possible to use another Kafka table with different consumer_group, so it wouldn’t affect your main pipeline.\n","categories":"","description":"SELECTs from engine=Kafka\n","excerpt":"SELECTs from engine=Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/","tags":"","title":"SELECTs from engine=Kafka"},{"body":"Question I expect the sequence here to only match once as a is only directly after a once - but it matches with gaps. Why is that?\nSELECTsequenceCount('(?1)(?2)')(sequence,pageILIKE'%a%',pageILIKE'%a%')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('a',2),('b',3),('b',4),('a',5),('b',6),('a',7))2#??Answer sequenceMatch just ignores the events which don’t match the condition. Check that:\nSELECTsequenceMatch('(?1)(?2)')(sequence,page='a',page='b')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));1#??SELECTsequenceMatch('(?1).(?2)')(sequence,page='a',page='b')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));0#???SELECTsequenceMatch('(?1)(?2)')(sequence,page='a',page='b',pageNOTIN('a','b'))ASsequencesfromvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));0#!SELECTsequenceMatch('(?1).(?2)')(sequence,page='a',page='b',pageNOTIN('a','b'))ASsequencesfromvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));1#So for your example - just introduce one more ‘nothing matched’ condition:\nSELECTsequenceCount('(?1)(?2)')(sequence,pageILIKE'%a%',pageILIKE'%a%',NOT(pageILIKE'%a%'))ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('a',2),('b',3),('b',4),('a',5),('b',6),('a',7))--- title:\"1\"linkTitle:\"1\"description:\u003e1--- ","categories":"","description":"sequenceMatch\n","excerpt":"sequenceMatch\n","ref":"/altinity-kb-functions/altinity-kb-sequencematch/","tags":"","title":"sequenceMatch"},{"body":" Сonfig management (recommended structure) Settings \u0026amp; restart Dictionaries incl attribute \u0026amp; metrica.xml Multiple Clickhouse instances at one host preprocessed_configs  Сonfig management (recommended structure)  Clickhouse server config consists of two parts server settings (config.xml) and users settings (users.xml).\nBy default they are stored in the folder /etc/clickhouse-server/ in two files config.xml \u0026 users.xml.\nWe suggest never change vendor config files and place your changes into separate .xml files in sub-folders. This way is easier to maintain and ease Clickhouse upgrades.\n/etc/clickhouse-server/users.d – sub-folder for user settings.\n/etc/clickhouse-server/config.d – sub-folder for server settings.\n/etc/clickhouse-server/conf.d – sub-folder for any (both) settings.\nFile names of your xml files can be arbitrary but they applied in alphabetical order.\nExamples:\n$ cat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cmacros\u003e \u003ccluster\u003etest\u003c/cluster\u003e \u003creplica\u003ehost22\u003c/replica\u003e \u003cshard\u003e0\u003c/shard\u003e \u003cserver_id\u003e41295\u003c/server_id\u003e \u003cserver_name\u003ehost22.server.com\u003c/server_name\u003e \u003c/macros\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/config.d/zoo.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/test/task_queue/ddl\u003c/path\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/users.d/enable_access_management_for_user_default.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cusers\u003e \u003cdefault\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/default\u003e \u003c/users\u003e \u003c/yandex\u003e BTW, you can define any macro in your configuration and use them in Zookeeper paths\nReplicatedMergeTree('/clickhouse/{cluster}/tables/my_table','{replica}') or in your code using function getMacro:\nCREATEORREPLACEVIEWsrv_server_infoSELECT(SELECTgetMacro('shard'))ASshard_num,(SELECTgetMacro('server_name'))ASserver_name,(SELECTgetMacro('server_id'))ASserver_keySettings can be appended to an XML tree (default behaviour) or replaced or removed.\nExample how to delete tcp_port \u0026 http_port defined on higher level in the main config.xml (it disables open tcp \u0026 http ports if you configured secure ssl):\ncat /etc/clickhouse-server/config.d/disable_open_network.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003chttp_port remove=\"1\"/\u003e \u003ctcp_port remove=\"1\"/\u003e \u003c/yandex\u003e Example how to replace remote_servers section defined on higher level in the main config.xml (it allows to remove default test clusters.\n\u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cremote_servers replace=\"1\"\u003e \u003cmycluster\u003e .... \u003c/mycluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e Settings \u0026 restart  All users settings don’t need server restart but applied on connect. User need to reconnect to Clickhouse server.\nMost of server settings applied only on a server start, except sections:\n \u003cremote_servers\u003e (cluster config) \u003cdictionaries\u003e (ext.dictionaries) \u003cmax_table_size_to_drop\u003e \u0026 \u003cmax_partition_size_to_drop\u003e  Dictionaries  We suggest to store each dictionary description in a separate (own) file in a /etc/clickhouse-server/dict sub-folder.\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003chttp\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e and add to the configuration\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cdictionaries_config\u003edict/*.xml\u003c/dictionaries_config\u003e \u003cdictionaries_lazy_load\u003etrue\u003c/dictionaries_lazy_load\u003e \u003c/yandex\u003e dict/*.xml – relative path, servers seeks files in the folder /etc/clickhouse-server/dict. More info in Multiple Clickhouse instances.\nincl attribute \u0026 metrica.xml  incl attribute allows to include some XML section from a special include file multiple times.\nBy default include file is /etc/metrika.xml. You can use many include files for each XML section.\nFor example to avoid repetition of user/password for each dictionary you can create an XML file:\n$ cat /etc/clickhouse-server/dict_sources.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cmysql_config\u003e \u003cport\u003e3306\u003c/port\u003e \u003cuser\u003euser\u003c/user\u003e \u003cpassword\u003e123\u003c/password\u003e \u003creplica\u003e \u003chost\u003emysql_host\u003c/host\u003e \u003cpriority\u003e1\u003c/priority\u003e \u003c/replica\u003e \u003cdb\u003emy_database\u003c/db\u003e \u003c/mysql_config\u003e \u003c/yandex\u003e Include this file:\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e ... \u003cinclude_from\u003e/etc/clickhouse-server/dict_sources.xml\u003c/include_from\u003e \u003c/yandex\u003e And use in dictionary descriptions (incl=“mysql_config”):\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003cmysql incl=\"mysql_config\"\u003e \u003ctable\u003emy_table\u003c/table\u003e \u003cinvalidate_query\u003eselect max(id) from my_table\u003c/invalidate_query\u003e \u003c/mysql\u003e \u003c/source\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e Multiple Clickhouse instances at one host  By default Clickhouse server configs are in /etc/clickhouse-server/ because clickhouse-server runs with a parameter –config-file /etc/clickhouse-server/config.xml\nconfig-file is defined in startup scripts:\n /etc/init.d/clickhouse-server – init-V /etc/systemd/system/clickhouse-server.service – systemd  Clickhouse uses the path from config-file parameter as base folder and seeks for other configs by relative path. All sub-folders users.d / config.d are relative.\nYou can start multiple clickhouse-server each with own –config-file.\nFor example:\n/usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node1/config.xml /etc/clickhouse-server-node1/ config.xml ... users.xml /etc/clickhouse-server-node1/config.d/disable_open_network.xml /etc/clickhouse-server-node1/users.d/.... /usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node2/config.xml /etc/clickhouse-server-node2/ config.xml ... users.xml /etc/clickhouse-server-node2/config.d/disable_open_network.xml /etc/clickhouse-server-node2/users.d/.... If you need to run multiple servers for CI purposes you can combine all settings in a single fat XML file and start ClickHouse without config folders/sub-folders.\n/usr/bin/clickhouse-server --config-file /tmp/ch1.xml /usr/bin/clickhouse-server --config-file /tmp/ch2.xml /usr/bin/clickhouse-server --config-file /tmp/ch3.xml Each ClickHouse instance must work with own data-folder and tmp-folder.\nBy default ClickHouse uses /var/lib/clickhouse/. It can be overridden in path settings\n\u003cpath\u003e/data/clickhouse-ch1/\u003c/path\u003e \u003ctmp_path\u003e/data/clickhouse-ch1/tmp/\u003c/tmp_path\u003e \u003cuser_files_path\u003e/data/clickhouse-ch1/user_files/\u003c/user_files_path\u003e \u003clocal_directory\u003e \u003cpath\u003e/data/clickhouse-ch1/access/\u003c/path\u003e \u003c/local_directory\u003e \u003cformat_schema_path\u003e/data/clickhouse-ch1/format_schemas/\u003c/format_schema_path\u003e preprocessed_configs  Clickhouse server watches config files and folders. When you change, add or remove XML files Clickhouse immediately assembles XML files into a combined file. These combined files are stored in /var/lib/clickhouse/preprocessed_configs/ folders.\nYou can verify that your changes are valid by checking /var/lib/clickhouse/preprocessed_configs/config.xml, /var/lib/clickhouse/preprocessed_configs/users.xml.\nIf something wrong with with your settings e.g. unclosed XML element or typo you can see alerts about this mistakes in /var/log/clickhouse-server/clickhouse-server.log\nIf you see your changes in preprocessed_configs it does not mean that changes are applied on running server, check Settings \u0026amp; restart\n","categories":"","description":"Server config files\n","excerpt":"Server config files\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-server-config-files/","tags":"","title":"Server config files"},{"body":"  query_log and other _log tables - set up TTL, or some other cleanup procedures.\ncat /etc/clickhouse-server/config.d/query_log.xml \u003cyandex\u003e \u003cquery_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY event_date ORDER BY (event_time) TTL event_date + interval 90 day SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/query_log\u003e \u003c/yandex\u003e   query_thread_log - typically is not useful, you can disable it (or set up TTL).\ncat /etc/clickhouse-server/config.d/disable_query_thread_log.xml \u003cyandex\u003e \u003cquery_thread_log remove=\"1\" /\u003e \u003cmetric_log remove=\"1\" /\u003e \u003casynchronous_metric_log remove=\"1\" /\u003e \u003c!-- if you have a good monitoring outside you don't need to store metrics in ClickHouse too --\u003e \u003c/yandex\u003e   part_log - may be nice, especially at the beginning / during system tuning/analyze.\ncat /etc/clickhouse-server/config.d/part_log.xml \u003cyandex\u003e \u003cpart_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003epart_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_time) TTL toStartOfMonth(event_date) + INTERVAL 3 MONTH SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/part_log\u003e \u003c/yandex\u003e   on older versions log_queries is disabled by default, it’s worth having it enabled always.\n$ cat /etc/clickhouse-server/users.d/log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e   quite often you want to have on-disk group by / order by enabled (both disabled by default).\ncat /etc/clickhouse-server/users.d/enable_on_disk_operations.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_bytes_before_external_group_by\u003e2000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e2000000000\u003c/max_bytes_before_external_sort\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e   quite often you want to create more users with different limitations.\nThe most typical is \u003cmax_execution_time\u003e\nIt’s actually also not a way to plan/share existing resources better, but it at least disciplines users.\nAlso introducing some restrictions on query complexity can be a good option to discipline users.\nYou can find the preset example here.\nAlso, force_index_by_date + force_primary_key can be a nice idea to avoid queries that ‘accidentally’ do full scans, max_concurrent_queries_for_user\n  merge_tree settings: max_bytes_to_merge_at_max_space_in_pool (may be reduced in some scenarios), fsync_* , inactive_parts_to_throw_insert - can be enabled, replicated_deduplication_window - can be extended if single insert create lot of parts , merge_with_ttl_timeout - when you use ttl\n  settings default_database_engine / insert_distributed_sync / fsync_metadata / do_not_merge_across_partitions_select_final / fsync\n  memory usage per server / query / user: memory configuration settings\n  See also:\n{% embed url=“https://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\" %}\n","categories":"","description":"Settings to adjust\n","excerpt":"Settings to adjust\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/","tags":"","title":"Settings to adjust"},{"body":"","categories":"","description":"Setup \u0026 maintenance\n","excerpt":"Setup \u0026 maintenance\n","ref":"/altinity-kb-setup-and-maintenance/","tags":"","title":"Setup \u0026 maintenance"},{"body":"It’s possible to shutdown server on fly, but that would lead to failure of some queries.\nMore safer way:\n  Remove server (which is going to be disabled) from remote_server section of config.xml on all servers.\n  Remove server from load balancer, so new queries wouldn’t hit it.\n  Wait until all already running queries would finish execution on it.\nIt’s possible to check it via query:\nSHOW PROCESSLIST;   Run sync replica query in related shard replicas via query:\nSYSTEM SYNC REPLICA db.table;   Shutdown server.\n  SYSTEM SHUTDOWN query doesn’t wait until query completion and tries to kill all queries immediately after receiving signal, even if there is setting shutdown_wait_unfinished.\nhttps://github.com/ClickHouse/ClickHouse/blob/master/programs/server/Server.cpp#L1353\n","categories":"","description":"Shutting down a node\n","excerpt":"Shutting down a node\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-shutting-down-a-node/","tags":"","title":"Shutting down a node"},{"body":"Q. What is SimpleAggregateFunction? Are there advantages to use it instead of AggregateFunction in AggregatingMergeTree? SimpleAggregateFunction can be used for those aggregations when the function state is exactly the same as the resulting function value. Typical example is max function: it only requires storing the single value which is already maximum, and no extra steps needed to get the final value. In contrast avg need to store two numbers - sum \u0026 count, which should be divided to get the final value of aggregation (done by the -Merge step at the very end).\n    SimpleAggregateFunction AggregateFunction     inserting accepts the value of underlying type OR\na value of corresponding SimpleAggregateFunction type CREATE TABLE saf_test\n( x SimpleAggregateFunction(max, UInt64) )\nENGINE=AggregatingMergeTree\nORDER BY tuple();\nINSERT INTO saf_test VALUES (1);\nINSERT INTO saf_test SELECT max(number) FROM numbers(10);\nINSERT INTO saf_test SELECT maxSimpleState(number) FROM numbers(20);  ONLY accepts the state of same aggregate function calculated using -State combinator   storing Internally store just a value of underlying type function-specific state   storage usage typically is much better due to better compression/codecs in very rare cases it can be more optimal than raw values\nadaptive granularity doesn't work for large states\n   reading raw value per row you can access it directly you need to use finalizeAgggregation function   using aggregated value just\nselect max(x) from test;  you need to use -Merge combinator select maxMerge(x) from test; \n   memory usage typically less memory needed (in some corner cases even 10 times) typically uses more memory, as every state can be quite complex   performance typically better, due to lower overhead worse    See also\nhttps://github.com/ClickHouse/ClickHouse/pull/4629\nhttps://github.com/ClickHouse/ClickHouse/issues/3852\nQ. How maxSimpleState combinator result differs from plain max? They produce the same result, but types differ (the first have SimpleAggregateFunction datatype). Both can be pushed to SimpleAggregateFunction or to the underlying type. So they are interchangeable.\n{% hint style=“info” %} -SimpleState is useful for implicit Materialized View creation, like\nCREATE MATERIALIZED VIEW mv ENGINE = AggregatingMergeTree ORDER BY date AS SELECT date, sumSimpleState(1) AS cnt, sumSimpleState(revenue) AS rev FROM table GROUP BY date {% endhint %}\n{% hint style=“warning” %} -SimpleState supported since 21.1.\nSee https://github.com/ClickHouse/ClickHouse/pull/16853/ {% endhint %}\nQ. Can I use -If combinator with SimpleAggregateFunction? Something like SimpleAggregateFunction(maxIf, UInt64, UInt8) is NOT possible. But is 100% ok to push maxIf (or maxSimpleStateIf) into SimpleAggregateFunction(max, UInt64)\nThere is one problem with that approach:\n-SimpleStateIf Would produce 0 as result in case of no-match, and it can mess up some aggregate functions state. It wouldn’t affect functions like max/argMax/sum, but could affect functions like min/argMin/any/anyLast\nSELECTminIfMerge(state_1),min(state_2)FROM(SELECTminIfState(number,number\u003e5)ASstate_1,minSimpleStateIf(number,number\u003e5)ASstate_2FROMnumbers(5)UNIONALLSELECTminIfState(toUInt64(2),2),minIf(2,2))┌─minIfMerge(state_1)─┬─min(state_2)─┐│2│0│└─────────────────────┴──────────────┘You can easily workaround that:\n Using Nullable datatype. Set result to some big number in case of no-match, which would be bigger than any possible value, so it would be safe to use. But it would work only for min/argMin  SELECTmin(state_1),min(state_2)FROM(SELECTminSimpleState(if(number\u003e5,number,1000))ASstate_1,minSimpleStateIf(toNullable(number),number\u003e5)ASstate_2FROMnumbers(5)UNIONALLSELECTminIf(2,2),minIf(2,2))┌─min(state_1)─┬─min(state_2)─┐│2│2│└──────────────┴──────────────┘Extra example WITHminIfState(number,number\u003e5)ASstate_1,minSimpleStateIf(number,number\u003e5)ASstate_2SELECTbyteSize(state_1),toTypeName(state_1),byteSize(state_2),toTypeName(state_2)FROMnumbers(10)FORMATVertical-- For UInt64 Row1:──────byteSize(state_1):24toTypeName(state_1):AggregateFunction(minIf,UInt64,UInt8)byteSize(state_2):8toTypeName(state_2):SimpleAggregateFunction(min,UInt64)-- For UInt32 ──────byteSize(state_1):16byteSize(state_2):4-- For UInt16 ──────byteSize(state_1):12byteSize(state_2):2-- For UInt8 ──────byteSize(state_1):10byteSize(state_2):1","categories":"","description":"Simple aggregate functions \u0026 combinators\n","excerpt":"Simple aggregate functions \u0026 combinators\n","ref":"/altinity-kb-queries-and-syntax/simplestateif-or-ifstate-for-simple-aggregate-functions/","tags":"","title":"Simple aggregate functions \u0026 combinators"},{"body":"{% hint style=“danger” %} When you are creating skip indexes in non-regular (Replicated)MergeTree tables over non ORDER BY columns. ClickHouse applies index condition on the first step of query execution, so it’s possible to get outdated rows. {% endhint %}\n--(1) create test table droptableifexiststest;createtabletest(versionUInt32,idUInt32,stateUInt8,INDEXstate_idx(state)typeset(0)GRANULARITY1)ENGINEReplacingMergeTree(version)ORDERBY(id);--(2) insert sample data INSERTINTOtest(version,id,state)VALUES(1,1,1);INSERTINTOtest(version,id,state)VALUES(2,1,0);INSERTINTOtest(version,id,state)VALUES(3,1,1);--(3) check the result: -- expected 3, 1, 1 selectversion,id,statefromtestfinal;┌─version─┬─id─┬─state─┐│3│1│1│└─────────┴────┴───────┘-- expected empty result selectversion,id,statefromtestfinalwherestate=0;┌─version─┬─id─┬─state─┐│2│1│0│└─────────┴────┴───────┘","categories":"","description":"Skip index\n","excerpt":"Skip index\n","ref":"/engines/mergetree-table-engine-family/skip-index/","tags":"","title":"Skip index"},{"body":"","categories":"","description":"Skip indexes\n","excerpt":"Skip indexes\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/","tags":"","title":"Skip indexes"},{"body":"Sparse_hashed layout is supposed to save memory but has some downsides. We can test how much slower SPARSE_HASHED than HASHED is with the following:\ncreatetableorders(idUInt64,priceFloat64)Engine=MergeTree()orderbyid;insertintoordersselectnumber,0fromnumbers(5000000);CREATEDICTIONARYorders_hashed(idUInt64,priceFloat64)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEordersDB'default'USER'default'))LIFETIME(MIN0MAX0)LAYOUT(HASHED());CREATEDICTIONARYorders_sparse(idUInt64,priceFloat64)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEordersDB'default'USER'default'))LIFETIME(MIN0MAX0)LAYOUT(SPARSE_HASHED());SELECTname,type,status,element_count,formatReadableSize(bytes_allocated)ASRAMFROMsystem.dictionariesWHEREnameLIKE'orders%'┌─name──────────┬─type─────────┬─status─┬─element_count─┬─RAM────────┐│orders_sparse│SparseHashed│LOADED│5000000│84.29MiB││orders_hashed│Hashed│LOADED│5000000│256.00MiB│└───────────────┴──────────────┴────────┴───────────────┴────────────┘SELECTsum(dictGet('default.orders_hashed','price',toUInt64(number)))ASresFROMnumbers(10000000)┌─res─┐│0│└─────┘1rowsinset.Elapsed:0.279sec.Processed10.02millionrows...SELECTsum(dictGet('default.orders_sparse','price',toUInt64(number)))ASresFROMnumbers(10000000)┌─res─┐│0│└─────┘1rowsinset.Elapsed:1.085sec.Processed10.02millionrows...As you can see SPARSE_HASHED is memory efficient and use about 3 times less memory (!!!) but is almost 4 times slower. But this is the ultimate case because this test does not read data from the disk (no MergeTree table involved).\nWe encourage you to test SPARSE_HASHED against your real queries, because it able to save a lot of memory and have larger (in rows) external dictionaries.\n","categories":"","description":"SPARSE\\_HASHED VS HASHED\n","excerpt":"SPARSE\\_HASHED VS HASHED\n","ref":"/altinity-kb-dictionaries/altinity-kb-sparse_hashed-vs-hashed/","tags":"","title":"SPARSE HASHED VS HASHED"},{"body":"ClickHouse doesn’t probe CA path which is default on CentOS and Amazon Linux.\nClickHouse client: cat /etc/clickhouse-client/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cclient\u003e \u003c!-- Used for connection to server's secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e ClickHouse server: cat /etc/clickhouse-server/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cserver\u003e \u003c!-- Used for https server AND secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/server\u003e \u003cclient\u003e \u003c!-- Used for connecting to https dictionary source and secured Zookeeper communication --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e {% embed url=“https://github.com/ClickHouse/ClickHouse/issues/17803\" %}\n{% embed url=“https://github.com/ClickHouse/ClickHouse/issues/18869\" %}\n","categories":"","description":"SSL connection unexpectedly closed\n","excerpt":"SSL connection unexpectedly closed\n","ref":"/altinity-kb-setup-and-maintenance/ssl-connection-unexpectedly-closed/","tags":"","title":"SSL connection unexpectedly closed"},{"body":"Nested structures. In certain conditions it could make sense to collapse one of dimensions to set of arrays. It’s usually profitable to do if this dimension is not commonly used in queries. It would reduce amount of rows in aggregated table and speed up queries which doesn’t care about this dimension in exchange of aggregation performance by collapsed dimension.\nCREATETABLEtraffic(`key1`UInt32,`key2`UInt32,`port`UInt16,`bits_in`UInt32CODEC(T64,LZ4),`bits_out`UInt32CODEC(T64,LZ4),`packets_in`UInt32CODEC(T64,LZ4),`packets_out`UInt32CODEC(T64,LZ4))ENGINE=SummingMergeTreeORDERBY(key1,key2,port);INSERTINTOtrafficSELECTnumber%1000,intDiv(number,10000),rand()%20,rand()%753,rand64()%800,rand()%140,rand64()%231FROMnumbers(100000000);CREATETABLEdefault.traffic_map(`key1`UInt32,`key2`UInt32,`bits_in`UInt32CODEC(T64,LZ4),`bits_out`UInt32CODEC(T64,LZ4),`packets_in`UInt32CODEC(T64,LZ4),`packets_out`UInt32CODEC(T64,LZ4),`portMap.port`Array(UInt16),`portMap.bits_in`Array(UInt32)CODEC(T64,LZ4),`portMap.bits_out`Array(UInt32)CODEC(T64,LZ4),`portMap.packets_in`Array(UInt32)CODEC(T64,LZ4),`portMap.packets_out`Array(UInt32)CODEC(T64,LZ4))ENGINE=SummingMergeTreeORDERBY(key1,key2);INSERTINTOtraffic_mapWITHrand()%20ASportSELECTnumber%1000ASkey1,intDiv(number,10000)ASkey2,rand()%753ASbits_in,rand64()%800ASbits_out,rand()%140ASpackets_in,rand64()%231ASpackets_out,[port],[bits_in],[bits_out],[packets_in],[packets_out]FROMnumbers(100000000);┌─table───────┬─column──────────────┬─────rows─┬─compressed─┬─uncompressed─┬──ratio─┐│traffic│bits_out│80252317│109.09MiB│306.14MiB│2.81││traffic│bits_in│80252317│108.34MiB│306.14MiB│2.83││traffic│port│80252317│99.21MiB│153.07MiB│1.54││traffic│packets_out│80252317│91.36MiB│306.14MiB│3.35││traffic│packets_in│80252317│84.61MiB│306.14MiB│3.62││traffic│key2│80252317│47.88MiB│306.14MiB│6.39││traffic│key1│80252317│1.38MiB│306.14MiB│221.42││traffic_map│portMap.bits_out│10000000│108.96MiB│306.13MiB│2.81││traffic_map│portMap.bits_in│10000000│108.32MiB│306.13MiB│2.83││traffic_map│portMap.port│10000000│92.00MiB│229.36MiB│2.49││traffic_map│portMap.packets_out│10000000│90.95MiB│306.13MiB│3.37││traffic_map│portMap.packets_in│10000000│84.19MiB│306.13MiB│3.64││traffic_map│key2│10000000│23.46MiB│38.15MiB│1.63││traffic_map│bits_in│10000000│15.59MiB│38.15MiB│2.45││traffic_map│bits_out│10000000│15.59MiB│38.15MiB│2.45││traffic_map│packets_out│10000000│13.22MiB│38.15MiB│2.89││traffic_map│packets_in│10000000│12.62MiB│38.15MiB│3.02││traffic_map│key1│10000000│180.29KiB│38.15MiB│216.66│└─────────────┴─────────────────────┴──────────┴────────────┴──────────────┴────────┘-- Queries SELECTkey1,sum(packets_in),sum(bits_out)FROMtrafficGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.488sec.Processed80.25millionrows,963.03MB(164.31millionrows/s.,1.97GB/s.)SELECTkey1,sum(packets_in),sum(bits_out)FROMtraffic_mapGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.063sec.Processed10.00millionrows,120.00MB(159.43millionrows/s.,1.91GB/s.)SELECTkey1,port,sum(packets_in),sum(bits_out)FROMtrafficGROUPBYkey1,portFORMAT`Null`0rowsinset.Elapsed:0.668sec.Processed80.25millionrows,1.12GB(120.14millionrows/s.,1.68GB/s.)WITHarrayJoin(arrayZip(untuple(sumMap(portMap.port,portMap.packets_in,portMap.bits_out))))AStplSELECTkey1,tpl.1ASport,tpl.2ASpackets_in,tpl.3ASbits_outFROMtraffic_mapGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.915sec.Processed10.00millionrows,1.08GB(10.93millionrows/s.,1.18GB/s.)","categories":"","description":"SummingMergeTree\n","excerpt":"SummingMergeTree\n","ref":"/engines/mergetree-table-engine-family/summingmergetree/","tags":"","title":"SummingMergeTree"},{"body":" Note 1: System database stores virtual tables (parts, tables, columns, etc.) and *_log tables.\nVirtual tables do not persist on disk. They reflect ClickHouse memory (c++ structures). They cannot be changed or removed.\nLog tables are named with postfix *_log and have the MergeTree engine.\nYou can drop / rename / truncate *_log tables at any time. ClickHouse will recreate them in about 7 seconds (flush period).\n  Note 2: Log tables with numeric postfixes (_1 / 2 / 3 …) query_log_1 query_thread_log_3 are results of Clickhouse upgrades. When a new version of Clickhouse starts and discovers that a system log table’s schema is incompatible with a new schema, then Clickhouse renames the old query_log table to the name with the prefix and creates a table with the new schema. You can drop such tables if you don’t need such historic data.\n You can disable all / any of them:  Do not create log tables at all (a restart is needed for these changes to take effect).\n$ cat /etc/clickhouse-server/config.d/z_log_disable.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003casynchronous_metric_log remove=\"1\"/\u003e \u003cmetric_log remove=\"1\"/\u003e \u003cpart_log remove=\"1\" /\u003e \u003cquery_log remove=\"1\" /\u003e \u003cquery_thread_log remove=\"1\" /\u003e \u003ctext_log remove=\"1\" /\u003e \u003ctrace_log remove=\"1\"/\u003e \u003c/yandex\u003e We do not recommend removing query_log and query_thread_log as queries' logging can be easily turned off without a restart through user profiles:\n$ cat /etc/clickhouse-server/users.d/z_log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e Hint: z_log_disable.xml is named with z_ in the beginning, it means this config will be applied the last and will override all other config files with these sections (config are applied in alphabetical order).\nYou can configure TTL:  Example for query_log. It drops partitions with data older than 14 days:\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cquery_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cengine\u003eENGINE = MergeTree PARTITION BY (event_date) ORDER BY (event_time) TTL event_date + INTERVAL 14 DAY DELETE SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003c/query_log\u003e \u003c/yandex\u003e After that you need to restart ClickHouse and drop or rename the existing system.query_log table, then CH creates a new table with these settings.\nRENAMETABLEsystem.query_logTOsystem.query_log_1;Important part here is a daily partitioning PARTITION BY (event_date) and ttl_only_drop_parts=1. In this case ClickHouse drops whole partitions. Dropping of partitions is very easy operation for CPU / Disk I/O.\nUsual TTL (without ttl_only_drop_parts=1) is heavy CPU / Disk I/O consuming operation which re-writes data parts without expired rows.\nYou can add TTL without ClickHouse restart (and table dropping or renaming):\nALTERTABLEsystem.query_logMODIFYTTLevent_date+INTERVAL14DAY;ALTERTABLEsystem.query_logMODIFYSETTINGttl_only_drop_parts=1;But in this case ClickHouse will drop only whole monthly partitions (will store data older than 14 days).\nOne more way to configure TTL for system tables  This way just adds TTL to a table and leaves monthly (default) partitioning (will store data older than 14 days).\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cquery_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cttl\u003eevent_date + INTERVAL 30 DAY DELETE\u003c/ttl\u003e \u003c/query_log\u003e \u003c/yandex\u003e After that you need to restart ClickHouse and drop or rename the existing system.query_log table, then CH creates a new table with this TTL setting.\nYou can disable logging on a session level or in user’s profile (for all or specific users):  But only for logs generated on session level (query_log / query_thread_log)\nIn this case a restart is not needed.\nLet’s disable query logging for all users (profile = default, all other profiles inherit it).\ncat /etc/clickhouse-server/users.d/log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e ","categories":"","description":"System tables eat my disk\n","excerpt":"System tables eat my disk\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-system-tables-eat-my-disk/","tags":"","title":"System tables eat my disk"},{"body":"Sometimes your Distributed DDL queries are being stuck, and not executing on all or subset of nodes, there are a lot of possible reasons for that kind of behavior, so it would take some time and effort to investigate.\nPossible reasons: Clickhouse node can’t recognize itself. SELECT*FROMsystem.clusters;-- check is_local column, it should have 1 for itself getent hosts clickhouse.local.net # or other name which should be local hostname --fqdn cat /etc/hosts cat /etc/hostname {% page-ref page=\"./\" %}\nDebian / Ubuntu There is an issue in Debian based images, when hostname being mapped to 127.0.1.1 address which doesn’t literally match network interface and clickhouse fails to detect this address as local.\n{% embed url=“https://github.com/ClickHouse/ClickHouse/issues/23504\" %}\nPrevious task is being executed and taking some time. It’s usually some heavy operations like merges, mutations, alter columns, so it make sense to check those tables:\nSHOWPROCESSLIST;SELECT*FROMsystem.merges;SELECT*FROMsystem.mutations;In that case, you can just wait completion of previous task.\nPrevious task is stuck because of some error. In that case, the first step is to understand which exact task is stuck and why. There are some queries which can help with that.\n-- list of all distributed ddl queries, path can be different in your installation SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/';-- information about specific task. SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/';SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/'ANDname='query-0000001000';-- How many nodes executed this task SELECTname,numChildrenassuccess_nodesFROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/'ANDname='finished';┌─name─────┬─success_nodes─┐│finished│0│└──────────┴───────────────┘-- Latest successfull executed tasks from query_log. SELECTqueryFROMsystem.query_logWHEREqueryLIKE'%ddl_entry%'ANDtype=2ORDERBYevent_timeDESCLIMIT5;-- Information about task execution from logs. grep-C40\"ddl\\_entry\"/var/log/clickhouse-server/clickhouse-server*.logIssues that can prevent the task execution: Obsolete replicas left in zookeeper.\nSELECTdatabase,table,zookeeper_path,replica_pathzookeeperFROMsystem.replicasWHEREtotal_replicas!=active_replicas;SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/cluster/tables/01/database/table/replicas';SYSTEMDROPREPLICA'replica_name';SYSTEMSTOPREPLICATIONQUEUES;SYSTEMSTARTREPLICATIONQUEUES;{% embed url=“https://clickhouse.tech/docs/en/sql-reference/statements/system/#query_language-system-drop-replica” %}\nTask were removed from DDL queue but left it Replicated*MergeTree table queue.\ngrep -C 40 \"ddl\\_entry\" /var/log/clickhouse-server/clickhouse-server*.log /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:28.956888 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053555 [ 599 ] {} \u003cError\u003e DDLWorker: ZooKeeper error: Code: 999, e.displayText() = Coordination::Exception: No node, Stack trace (when copying this message, always include the lines below): /var/log/clickhouse-server/clickhouse-server.log- /var/log/clickhouse-server/clickhouse-server.log-0. Coordination::Exception::Exception(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, Coordination::Error, int) @ 0xfb2f6b3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-1. Coordination::Exception::Exception(Coordination::Error) @ 0xfb2fb56 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:2. DB::DDLWorker::createStatusDirs(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::shared_ptr\u003czkutil::ZooKeeper\u003e const\u0026) @ 0xeb3127a in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:3. DB::DDLWorker::processTask(DB::DDLTask\u0026) @ 0xeb36c96 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:4. DB::DDLWorker::enqueueTask(std::__1::unique_ptr\u003cDB::DDLTask, std::__1::default_delete\u003cDB::DDLTask\u003e \u003e) @ 0xeb35f22 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-5. ? @ 0xeb47aed in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-6. ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::worker(std::__1::__list_iterator\u003cThreadFromGlobalPool, void*\u003e) @ 0x8633bcd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-7. ThreadFromGlobalPool::ThreadFromGlobalPool\u003cvoid ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u003e(void\u0026\u0026, void ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u0026\u0026...)::'lambda'()::operator()() @ 0x863612f in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-8. ThreadPoolImpl\u003cstd::__1::thread\u003e::worker(std::__1::__list_iterator\u003cstd::__1::thread, void*\u003e) @ 0x8630ffd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-9. ? @ 0x8634bb3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-10. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so /var/log/clickhouse-server/clickhouse-server.log-11. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so /var/log/clickhouse-server/clickhouse-server.log- (version 21.1.8.30 (official build)) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053951 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) So context of this problem is:\nConstant pressure of cheap ON CLUSTER DELETE queries.\nOne replica was down for certain amount of time.\nBecause of constant pressure on DDL queue, it purge old records due task_max_lifetime setting.\nWhen lagging replica\n","categories":"","description":"\"There are N unfinished hosts (0 of them are currently active).\"\n","excerpt":"\"There are N unfinished hosts (0 of them are currently active).\"\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/there-are-n-unfinished-hosts-0-of-them-are-currently-active/","tags":"","title":"There are N unfinished hosts (0 of them are currently active)."},{"body":"Collect thread names \u0026 counts using ps \u0026 clickhouse-local\nps H -o 'tid comm' $(pidof -s clickhouse-server) | tail -n +2 | awk '{ printf(\"%s\\t%s\\n\", $1, $2) }' | clickhouse-local -S \"threadid UInt16, name String\" -q \"SELECT name, count() FROM table GROUP BY name WITH TOTALS ORDER BY count() DESC FORMAT PrettyCompact\" Check threads used by running queries:\nSELECTquery,length(thread_ids)ASthreads_countFROMsystem.processesORDERBYthreads_count;--- title: \"cat /proc/$(pidof -s clickhouse-server)/status | grep Threads\" linkTitle: \"cat /proc/$(pidof -s clickhouse-server)/status | grep Threads\" description: \u003e cat /proc/$(pidof -s clickhouse-server)/status | grep Threads --- Threads: 103 --- title: \"ps hH $(pidof -s clickhouse-server)| wc -l\" linkTitle: \"ps hH $(pidof -s clickhouse-server)| wc -l\" description: \u003e ps hH $(pidof -s clickhouse-server) | wc -l --- 103 --- title: \"ps hH -AF | grep clickhouse | wc -l\" linkTitle: \"ps hH -AF | grep clickhouse | wc -l\" description: \u003e ps hH -AF | grep clickhouse | wc -l --- 116 Pools\nSELECTname,valueFROMsystem.settingsWHEREnameLIKE'%pool%'┌─name─────────────────────────────────────────┬─value─┐│connection_pool_max_wait_ms│0││distributed_connections_pool_size│1024││background_buffer_flush_schedule_pool_size│16││background_pool_size│16││background_move_pool_size│8││background_fetches_pool_size│8││background_schedule_pool_size│16││background_message_broker_schedule_pool_size│16││background_distributed_schedule_pool_size│16││postgresql_connection_pool_size│16││postgresql_connection_pool_wait_timeout│-1││odbc_bridge_connection_pool_size│16│└──────────────────────────────────────────────┴───────┘SELECTmetric,valueFROMsystem.metricsWHEREmetricLIKE'Background%'┌─metric──────────────────────────────────┬─value─┐│BackgroundPoolTask│0││BackgroundFetchesPoolTask│0││BackgroundMovePoolTask│0││BackgroundSchedulePoolTask│0││BackgroundBufferFlushSchedulePoolTask│0││BackgroundDistributedSchedulePoolTask│0││BackgroundMessageBrokerSchedulePoolTask│0│└─────────────────────────────────────────┴───────┘Stack traces\nSETallow_introspection_functions=1;WITHarrayMap(x-\u003edemangle(addressToSymbol(x)),trace)ASallSELECTthread_id,query_id,arrayStringConcat(all,'\\n')ASresFROMsystem.stack_traceWHEREresILIKE'%Pool%'","categories":"","description":"Threads\n","excerpt":"Threads\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-threads/","tags":"","title":"Threads"},{"body":"Important things to know:\n DateTime inside clickhouse is actually UNIX timestamp always, i.e. number of seconds since 1970-01-01 00:00:00 GMT. Conversion from that UNIX timestamp to a human-readable form and reverse can happen on the client (for native clients) and on the server (for HTTP clients, and for some type of queries, like toString(ts)) Depending on the place where that conversion happened rules of different timezones may be applied. You can check server timezone using SELECT timezone() clickhouse-client also by default tries to use server timezone (see also --use_client_time_zone flag) If you want you can store the timezone name inside the data type, in that case, timestamp \u003c-\u003e human-readable time rules of that timezone will be applied.  SELECT timezone(), toDateTime(now()) AS t, toTypeName(t), toDateTime(now(), 'UTC') AS t_utc, toTypeName(t_utc), toUnixTimestamp(t), toUnixTimestamp(t_utc) Row 1: ────── timezone(): Europe/Warsaw t: 2021-07-16 12:50:28 toTypeName(toDateTime(now())): DateTime t_utc: 2021-07-16 10:50:28 toTypeName(toDateTime(now(), 'UTC')): DateTime('UTC') toUnixTimestamp(toDateTime(now())): 1626432628 toUnixTimestamp(toDateTime(now(), 'UTC')): 1626432628 Since version 20.4 clickhouse uses embedded tzdata (see https://github.com/ClickHouse/ClickHouse/pull/10425 )\nYou get used tzdata version\nSELECT * FROM system.build_options WHERE name = 'TZDATA_VERSION' Query id: 0a9883f0-dadf-4fb1-8b42-8fe93f561430 ┌─name───────────┬─value─┐ │ TZDATA_VERSION │ 2020e │ └────────────────┴───────┘ and list of available time zones\nSELECT * FROM system.time_zones WHERE time_zone LIKE '%Anta%' Query id: 855453d7-eccd-44cb-9631-f63bb02a273c ┌─time_zone─────────────────┐ │ Antarctica/Casey │ │ Antarctica/Davis │ │ Antarctica/DumontDUrville │ │ Antarctica/Macquarie │ │ Antarctica/Mawson │ │ Antarctica/McMurdo │ │ Antarctica/Palmer │ │ Antarctica/Rothera │ │ Antarctica/South_Pole │ │ Antarctica/Syowa │ │ Antarctica/Troll │ │ Antarctica/Vostok │ │ Indian/Antananarivo │ └───────────────────────────┘ 13 rows in set. Elapsed: 0.002 sec. When the conversion using different rules happen: SELECT timezone() ┌─timezone()─┐ │ UTC │ └────────────┘ create table t_with_dt_utc ( ts DateTime64(3,'Europe/Moscow') ) engine=Log; create table x (ts String) engine=Null; create materialized view x_mv to t_with_dt_utc as select parseDateTime64BestEffort(ts) as ts from x; $ echo '2021-07-15T05:04:23.733' | clickhouse-client -q 'insert into t_with_dt_utc format CSV' -- here client checks the type of the columns, see that it's 'Europe/Moscow' and use conversion according to moscow rules $ echo '2021-07-15T05:04:23.733' | clickhouse-client -q 'insert into x format CSV' -- here client check tha type of the columns (it is string), and pass string value to the server. -- parseDateTime64BestEffort(ts) uses server default timezone (UTC in my case), and convert the value using UTC rules. -- and the result is 2 different timestamps (when i selecting from that is shows both in 'desired' timezone, forced by column type, i.e. Moscow): SELECT * FROM t_with_dt_utc ┌──────────────────────ts─┐ │ 2021-07-15 05:04:23.733 │ │ 2021-07-15 08:04:23.733 │ └─────────────────────────┘ Best practice here: use UTC timezone everywhere, OR use the same default timezone for clickhouse server as used by your data\n","categories":"","description":"Time zones\n","excerpt":"Time zones\n","ref":"/altinity-kb-queries-and-syntax/time-zones/","tags":"","title":"Time zones"},{"body":"CREATETABLEtop_with_rest(`k`String,`number`UInt64)ENGINE=Memory;INSERTINTOtop_with_restSELECTtoString(intDiv(number,10)),numberFROMnumbers_mt(10000);Using UNION ALL SELECT*FROM(SELECTk,sum(number)ASresFROMtop_with_restGROUPBYkORDERBYresDESCLIMIT10UNIONALLSELECTNULL,sum(number)ASresFROMtop_with_restWHEREkNOTIN(SELECTkFROMtop_with_restGROUPBYkORDERBYsum(number)DESCLIMIT10))ORDERBYresASC┌─k───┬───res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945│└─────┴───────┘┌─k────┬──────res─┐│ᴺᵁᴸᴸ│49000050│└──────┴──────────┘Using arrays WITHtoUInt64(sumIf(sum,isNull(k))-sumIf(sum,isNotNull(k)))AStotalSELECT(arrayJoin(arrayPushBack(groupArrayIf(10)((k,sum),isNotNull(k)),(NULL,total)))AStpl).1ASkey,tpl.2ASresFROM(SELECTtoNullable(k)ASk,sum(number)ASsumFROMtop_with_restGROUPBYkWITHCUBEORDERBYsumDESCLIMIT11)ORDERBYresASC┌─key──┬──────res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945││ᴺᵁᴸᴸ│49000050│└──────┴──────────┘Using window functions (starting from 21.1) SETallow_experimental_window_functions=1;SELECTkASkey,If(isNotNull(key),sum,toUInt64(sum-wind))ASresFROM(SELECT*,sumIf(sum,isNotNull(k))OVER()ASwindFROM(SELECTtoNullable(k)ASk,sum(number)ASsumFROMtop_with_restGROUPBYkWITHCUBEORDERBYsumDESCLIMIT11))ORDERBYresASC┌─key──┬──────res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945││ᴺᵁᴸᴸ│49000050│└──────┴──────────┘","categories":"","description":"Top N \u0026 Remain\n","excerpt":"Top N \u0026 Remain\n","ref":"/altinity-kb-queries-and-syntax/top-n-and-remain/","tags":"","title":"Top N \u0026 Remain"},{"body":"Log of query execution Controlled by session level setting send_logs_level\nPossible values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'\nCan be used with clickhouse-client in both interactive and non-interactive mode.\n$ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" [LAPTOP] 2021.04.29 00:05:31.425842 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e executeQuery: (from 127.0.0.1:42590, using production parser) SELECT sum(number) FROM numbers(1000) [LAPTOP] 2021.04.29 00:05:31.426281 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.* [LAPTOP] 2021.04.29 00:05:31.426648 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete [LAPTOP] 2021.04.29 00:05:31.427132 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e AggregatingTransform: Aggregating [LAPTOP] 2021.04.29 00:05:31.427187 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Aggregation method: without_key [LAPTOP] 2021.04.29 00:05:31.427220 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e AggregatingTransform: Aggregated. 1000 to 1 rows (from 7.81 KiB) in 0.0004469 sec. (2237637.0552696353 rows/sec., 17.07 MiB/sec.) [LAPTOP] 2021.04.29 00:05:31.427233 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Merging aggregated data [LAPTOP] 2021.04.29 00:05:31.427875 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cInformation\u003e executeQuery: Read 1000 rows, 7.81 KiB in 0.0019463 sec., 513795 rows/sec., 3.92 MiB/sec. [LAPTOP] 2021.04.29 00:05:31.427898 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B. 499500 $ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" 2\u003e ./query.log LAPTOP.localdomain:)SETsend_logs_level='trace';SETsend_logs_level='trace'Queryid:cbbffc02-283e-48ef-93e2-8b3baced6689Ok.0rowsinset.Elapsed:0.003sec.LAPTOP.localdomain:)SELECTsum(number)FROMnumbers(1000);SELECTsum(number)FROMnumbers(1000)Queryid:d3db767b-34e9-4252-9f90-348cf958f822[LAPTOP]2021.04.2900:06:51.673836[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eexecuteQuery:(from127.0.0.1:43116,usingproductionparser)SELECTsum(number)FROMnumbers(1000);[LAPTOP]2021.04.2900:06:51.674167[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eContextAccess(default):Accessgranted:CREATETEMPORARYTABLEON*.*[LAPTOP]2021.04.2900:06:51.674419[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eInterpreterSelectQuery:FetchColumns-\u003eComplete[LAPTOP]2021.04.2900:06:51.674748[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregatingTransform:Aggregating[LAPTOP]2021.04.2900:06:51.674781[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregator:Aggregationmethod:without_key[LAPTOP]2021.04.2900:06:51.674855[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eAggregatingTransform:Aggregated.1000to1rows(from7.81KiB)in0.0003299sec.(3031221.582297666rows/sec.,23.13MiB/sec.)[LAPTOP]2021.04.2900:06:51.674883[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregator:Mergingaggregateddata┌─sum(number)─┐│499500│└─────────────┘[LAPTOP]2021.04.2900:06:51.675481[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cInformation\u003eexecuteQuery:Read1000rows,7.81KiBin0.0015799sec.,632951rows/sec.,4.83MiB/sec.[LAPTOP]2021.04.2900:06:51.675508[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eMemoryTracker:Peakmemoryusage(forquery):0.00B.1rowsinset.Elapsed:0.007sec.Processed1.00thousandrows,8.00KB(136.43thousandrows/s.,1.09MB/s.)Flamegraph https://www.speedscope.app/\nWITH'95578e1c-1e93-463c-916c-a1a8cdd08198'ASquery,min(min)ASstart_value,max(max)ASend_value,groupUniqArrayArrayArray(trace_arr)ASuniq_frames,arrayMap((x,a,b)-\u003e('sampled',b,'none',start_value,end_value,arrayMap(s-\u003ereverse(arrayMap(y-\u003etoUInt32(indexOf(uniq_frames,y)-1),s)),x),a),groupArray(trace_arr),groupArray(weights),groupArray(trace_type))ASsamplesSELECTconcat('clickhouse-server@',version())ASexporter,'https://www.speedscope.app/file-format-schema.json'AS`$schema`,concat('Clickhouse query id: ',query)ASname,CAST(samples,'Array(Tuple(type String, name String, unit String, startValue UInt64, endValue UInt64, samples Array(Array(UInt32)), weights Array(UInt32)))')ASprofiles,CAST(tuple(arrayMap(x-\u003e(demangle(addressToSymbol(x)),addressToLine(x)),uniq_frames)),'Tuple(frames Array(Tuple(name String, line String)))')ASsharedFROM(SELECTmin(min_ns)ASmin,trace_type,max(max_ns)ASmax,groupArray(trace)AStrace_arr,groupArray(cnt)ASweightsFROM(SELECTmin(timestamp_ns)ASmin_ns,max(timestamp_ns)ASmax_ns,trace,trace_type,count()AScntFROMsystem.trace_logWHEREquery_id=queryGROUPBYtrace_type,trace)GROUPBYtrace_type)SETTINGSallow_introspection_functions=1,output_format_json_named_tuples_as_objects=1FORMATJSONEachRowSETTINGSoutput_format_json_named_tuples_as_objects=1","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/altinity-kb-queries-and-syntax/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"TTL\n","excerpt":"TTL\n","ref":"/altinity-kb-queries-and-syntax/ttl/","tags":"","title":"TTL"},{"body":"CREATETABLEtest_update(`key`UInt32,`value`String)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_updateSELECTnumber,concat('value ',toString(number))FROMnumbers(20);SELECT*FROMtest_update;┌─key─┬─value────┐│0│value0││1│value1││2│value2││3│value3││4│value4││5│value5││6│value6││7│value7││8│value8││9│value9││10│value10││11│value11││12│value12││13│value13││14│value14││15│value15││16│value16││17│value17││18│value18││19│value19│└─────┴──────────┘CREATETABLEtest_update_source(`key`UInt32,`value`String)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_update_sourceVALUES(1,'other value'),(10,'new value');CREATEDICTIONARYupdate_dict(`key`UInt32,`value`String)PRIMARYKEYkeySOURCE(CLICKHOUSE(TABLE'test_update_source'))LIFETIME(MIN0MAX10)LAYOUT(FLAT);SELECTdictGet('default.update_dict','value',toUInt64(1));┌─dictGet('default.update_dict','value',toUInt64(1))─┐│othervalue│└──────────────────────────────────────────────────────┘ALTERTABLEtest_updateUPDATEvalue=dictGet('default.update_dict','value',toUInt64(key))WHEREdictHas('default.update_dict',toUInt64(key));SELECT*FROMtest_update┌─key─┬─value───────┐│0│value0││1│othervalue││2│value2││3│value3││4│value4││5│value5││6│value6││7│value7││8│value8││9│value9││10│newvalue││11│value11││12│value12││13│value13││14│value14││15│value15││16│value16││17│value17││18│value18││19│value19│└─────┴─────────────┘{% hint style=“info” %} In case of Replicated installation, Dictionary should be created on all nodes and source tables should have ReplicatedMergeTree engine and be replicated across all nodes. {% endhint %}\n","categories":"","description":"UPDATE via Dictionary\n","excerpt":"UPDATE via Dictionary\n","ref":"/altinity-kb-queries-and-syntax/update-via-dictionary/","tags":"","title":"UPDATE via Dictionary"},{"body":"","categories":"","description":"Upgrade\n","excerpt":"Upgrade\n","ref":"/upgrade/","tags":"","title":"Upgrade"},{"body":" Number of active parts in a partition How to test different compression codecs. Database Size - Table - Column size Can detached parts be dropped? Datasets  ","categories":"","description":"Useful queries\n","excerpt":"Useful queries\n","ref":"/altinity-kb-useful-queries/","tags":"","title":"Useful queries"},{"body":"SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(transform(number%3,[0,1,2,3],['aa','ab','ad','af'],'a0'))1rowsinset.Elapsed:4.668sec.Processed1.00billionrows,8.00GB(214.21millionrows/s.,1.71GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(multiIf((number%3)=0,'aa',(number%3)=1,'ab',(number%3)=2,'ad',(number%3)=3,'af','a0'))1rowsinset.Elapsed:7.333sec.Processed1.00billionrows,8.00GB(136.37millionrows/s.,1.09GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(CAST(number%3ASEnum('aa'=0,'ab'=1,'ad'=2,'af'=3)')) 1 rows in set. Elapsed: 1.152 sec. Processed 1.00 billion rows, 8.00 GB (867.79 million rows/s., 6.94 GB/s.) ","categories":"","description":"Values mapping\n","excerpt":"Values mapping\n","ref":"/altinity-kb-queries-and-syntax/values-mapping/","tags":"","title":"Values mapping"},{"body":"Update itself is simple: update packages, restart clickhouse-server service afterwards.\n Check if the version you want to upgrade to is stable. We highly recommend the Altinity ClickHouse Stable Releases.  Review the changelog to ensure that no configuration changes are needed.   Update staging and test to verify all systems are working. Prepare and test downgrade procedures so the server can be returned to the previous version if necessary. Start with a “canary” update. This is one replica with one shard that is upgraded to make sure that the procedure works. Test and verify that everything works properly. Check for any errors in the log files. If everything is working well, update the rest of the cluster.  For small clusters, the BlueGreenDeployment technique is also a good option.\n ","categories":"","description":"Version Upgrades\n","excerpt":"Version Upgrades\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/version-upgrades/","tags":"","title":"Version Upgrades"},{"body":"SELECTformatReadableSize(sum(bytes_allocated))FROMsystem.dictionaries;SELECTdatabase,name,formatReadableSize(total_bytes)FROMsystem.tablesWHEREengineIN('Memory','Set','Join');SELECTformatReadableSize(sum(memory_usage))FROMsystem.merges;SELECTformatReadableSize(sum(memory_usage))FROMsystem.processes;SELECTinitial_query_id,formatReadableSize(memory_usage),formatReadableSize(peak_memory_usage),queryFROMsystem.processesORDERBYpeak_memory_usageDESCLIMIT10;SELECTmetric,formatReadableSize(value)FROMsystem.asynchronous_metricsWHEREmetricIN('UncompressedCacheBytes','MarkCacheBytes');SELECTformatReadableSize(sum(primary_key_bytes_in_memory))ASprimary_key_bytes_in_memory,formatReadableSize(sum(primary_key_bytes_in_memory_allocated))ASprimary_key_bytes_in_memory_allocatedFROMsystem.parts;SELECTinitial_query_id,formatReadableSize(memory_usage),queryFROMsystem.query_logWHERE(event_date\u003e=today())AND(event_time\u003e=(now()-7200))ORDERBYmemory_usageDESCLIMIT10;","categories":"","description":"Who ate my memory\n","excerpt":"Who ate my memory\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/","tags":"","title":"Who ate my memory"},{"body":"   Link blog.tinybird.co/2021/03/16/c…     Date Mar 26, 2021    blog.tinybird.co/2021/03/16/c…\n An exploration on what’s possible to do with the most recent experimental feature on ClickHouse - window functions, and an overview of other interesting feat…\n \u003cstrong\u003eOpen link\u003c/strong\u003e\nHow Do I Simulate Window Functions Using Arrays on older versions of clickhouse?  Group with groupArray. Calculate the needed metrics. Ungroup back using arrayJoin.  ","categories":"","description":"Window functions\n","excerpt":"Window functions\n","ref":"/altinity-kb-queries-and-syntax/window-functions/","tags":"","title":"Window functions"},{"body":"{% hint style=“danger” %} The local set of parts of table doesn’t look like the set of parts in ZooKeeper. 100.00 rows of 150.00 total rows in filesystem are suspicious. There are 1 unexpected parts with 100 rows (1 of them is not just-written with 100 rows), 0 missing parts (with 0 blocks).: Cannot attach table. {% endhint %}\nClickHouse has a registry of parts in ZooKeeper.\nAnd during the start ClickHouse compares that list of parts on a local disk is consistent with a list in ZooKeeper. If the lists are too different ClickHouse denies to start because it could be an issue with settings, wrong Shard or wrong Replica macroses. But this safe-limiter throws an exception if the difference is more 50% (in rows).\nIn your case the table is very small and the difference \u003e50% ( 100.00 vs 150.00 ) is only a single part mismatch, which can be the result of hard restart.\nSELECT*FROMsystem.merge_tree_settingsWHEREname='replicated_max_ratio_of_wrong_parts'┌─name────────────────────────────────┬─value─┬─changed─┬─description──────────────────────────────────────────────────────────────────────────┬─type──┐│replicated_max_ratio_of_wrong_parts│0.5│0│Ifratioofwrongpartstototalnumberofpartsislessthanthis-allowtostart.│Float│└─────────────────────────────────────┴───────┴─────────┴──────────────────────────────────────────────────────────────────────────────────────┴───────┘You can set another value of replicated_max_ratio_of_wrong_parts for all MergeTree tables or per table.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings\n","categories":"","description":"X rows of Y total rows in filesystem are suspicious\n","excerpt":"X rows of Y total rows in filesystem are suspicious\n","ref":"/altinity-kb-setup-and-maintenance/x-rows-of-y-total-rows-in-filesystem-are-suspicious/","tags":"","title":"X rows of Y total rows in filesystem are suspicious"},{"body":"Article on docs site:\n{% embed url=“https://docs.altinity.com/operationsguide/clickhouse-zookeeper/\" caption=”\" %}\nCheck number of followers:\necho mntr | nc zookeeper 2187 | grep foll zk_synced_followers 2 zk_synced_non_voting_followers 0 zk_avg_follower_sync_time 0.0 zk_min_follower_sync_time 0 zk_max_follower_sync_time 0 zk_cnt_follower_sync_time 0 zk_sum_follower_sync_time 0 Tools  https://github.com/apache/zookeeper/blob/master/zookeeper-docs/src/main/resources/markdown/zookeeperTools.md\nAlternatives for zkCli:   https://github.com/go-zkcli/zkcli https://github.com/outbrain/zookeepercli https://idata.co.il/2018/07/a-day-at-the-zoo-graphic-uis-for-apache-zookeeper/  Web UI:   https://github.com/elkozmon/zoonavigator-api https://github.com/tobilg/docker-zookeeper-webui https://github.com/vran-dev/PrettyZoo  ","categories":"","description":"ZooKeeper\n","excerpt":"ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/","tags":"","title":"ZooKeeper"},{"body":"You may have a question: “Do I need to backup Zookeeper Database, because it’s pretty important for ClickHouse?”\nAnswer: ZK is in memory database. All nodes of ZK has exactly the same data.\nIf you have 3 ZK servers, then you have 3 copies of (3 backups) already.\nTo backup ZK has no sense because you need to have a snapshot of ZK + last ZK logs to exactly the last ZK transaction.\nYou cannot use ZK database backed up 3 hours ago or 3 minutes ago.\nZK restored from the backup will be inconsistent with CH database.\nAnswer2: Usually, it doesn’t have too much sense. It’s very hard to take zookeeper snapshot at exactly the same state as clickhouse. (well maybe if you will turn of clickhouses, then you can take snapshots of clickhouse AND zookeepers). So for example on clouds if you can stop all nodes and take disk snapshots - it will just work.\nBut while clickhouse is working it’s almost impossible to collect the current state of zookeeper.\nYou need to restore zookeeper and clickhouse snapshots from EXACTLY THE SAME moment of time - no procedure is needed. Just start \u0026 run.\nAlso, that allows only to snapshot of clickhouse \u0026 zookeeper as a whole. You can not do partial backups then.\nIf you lose zookeeper data while having clickhouse data (or backups of clickhouse data) - you can restore the zookeeper state from clickhouse state.\nWith a couple of tables, it can be done manually.\nOn scale, you can use https://github.com/Altinity/clickhouse-zookeeper-recovery\nIn future it will be even simpler https://github.com/ClickHouse/ClickHouse/pull/13652\n","categories":"","description":"ZooKeeper backup\n","excerpt":"ZooKeeper backup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-backup/","tags":"","title":"ZooKeeper backup"},{"body":"Here is a plan for ZK 3.4.9 (no dynamic reconfiguration):\n Add the 3 new ZK nodes to the old cluster. No changes needed for the 3 old ZK nodes at this time.  Configure one of the new ZK nodes as a cluster of 4 nodes (3 old + 1 new), start it. Configure the other two new ZK nodes as a cluster of 6 nodes (3 old + 3 new), start them.   Make sure the 3 new ZK nodes connected to the old ZK cluster as followers (run echo stat | nc localhost 2181 on the 3 new ZK nodes) Confirm that the leader has 5 synced followers (run echo mntr | nc localhost 2181 on the leader, look for zk_synced_followers) Stop data ingestion in CH (this is to minimize errors when CH loses ZK). Change the zookeeper section in the configs on the CH nodes (remove the 3 old ZK servers, add the 3 new ZK servers) Make sure that there are no connections from CH to the 3 old ZK nodes (run echo stat | nc localhost 2181 on the 3 old nodes, check their Clients section). Restart all CH nodes if necessary (In some cases CH can reconnect to different ZK servers without a restart). Remove the 3 old ZK nodes from zoo.cfg on the 3 new ZK nodes. Restart the 3 new ZK nodes. They should form a cluster of 3 nodes. When CH reconnects to ZK, start data loading. Turn off the 3 old ZK nodes.  This plan works, but it is not the only way to do this, it can be changed if needed.\n","categories":"","description":"ZooKeeper cluster migration\n","excerpt":"ZooKeeper cluster migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration/","tags":"","title":"ZooKeeper cluster migration"},{"body":"/metadata Table schema.\ndate column -\u003e legacy MergeTree partition expresison. sampling expression -\u003e SAMPLE BY index granularity -\u003e index_granularity mode -\u003e type of MergeTree table sign column -\u003e sign - CollapsingMergeTree / VersionedCollapsingMergeTree primary key -\u003e ORDER BY key if PRIMARY KEY not defined. sorting key -\u003e ORDER BY key if PRIMARY KEY defined. data format version -\u003e 1 partition key -\u003e PARTITION BY granularity bytes -\u003e index_granularity_bytes types of MergeTree tables: Ordinary = 0 Collapsing = 1 Summing = 2 Aggregating = 3 Replacing = 5 Graphite = 6 VersionedCollapsing = 7 /mutations Log of latest mutations\n/columns List of columns for latest (reference) table version. Replicas would try to reach this state.\n/log Log of latest actions with table. Used mostly for debug purposes.\nRelated settings:\n┌─name────────────────────────┬─value─┬─changed─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─type───┐│max_replicated_logs_to_keep│1000│0│Howmanyrecordsmaybeinlog,ifthereisinactivereplica.Inactivereplicabecomeslostwhenwhenthisnumberexceed.│UInt64││min_replicated_logs_to_keep│10│0│KeepaboutthisnumberoflastrecordsinZooKeeperlog,eveniftheyareobsolete.Itdoesn't affect work of tables: used only to diagnose ZooKeeper log before cleaning. │ UInt64 │ └─────────────────────────────┴───────┴─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────┘ /replicas List of table replicas.\n/replicas/replica_name/ /replicas/replica_name/mutation_pointer Pointer to the latest mutation executed by replica\n/replicas/replica_name/log_pointer Pointer to the latest task from replication_queue executed by replica\n/replicas/replica_name/max_processed_insert_time /replica/replica_name/metadata Table schema of specific replica\n/replica/replica_name/columns Columns list of specific replica.\n/quorum Used for quorum inserts.\n","categories":"","description":"ZooKeeper schema\n","excerpt":"ZooKeeper schema\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-schema/","tags":"","title":"ZooKeeper schema"}]