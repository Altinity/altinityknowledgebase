



















































































































































































































[{"body":"Generally: the main engine in Clickhouse is called MergeTree. It allows to store and process data on one server and feel all the advantages of Clickhouse. Basic usage of MergeTree does not require any special configuration, and you can start using it ‘out of the box’.\nBut one server and one copy of data are not fault-tolerant - something can happen with the server itself, with datacenter availability, etc. So you need to have the replica(s) - i.e. server(s) with the same data and which can ‘substitute’ the original server at any moment.\nTo have an extra copy (replica) of your data you need to use ReplicatedMergeTree engine. It can be used instead of MergeTree engine, and you can always upgrade from MergeTree to ReplicatedMergeTree (and downgrade back) if you need. To use that you need to have ZooKeeper installed and running. For tests, you can use one standalone Zookeeper instance, but for production usage, you should have zookeeper ensemble at least of 3 servers.\nWhen you use ReplicatedMergeTree then the inserted data is copied automatically to all the replicas, but all the SELECTs are executed on the single server you have connected to. So you can have 5 replicas of your data, but if you will always connect to one replica - it will not ‘share’ / ‘balance’ that traffic automatically between all the replicas, one server will be loaded and the rest will generally do nothing. If you need that balancing of load between multiple replicas - you can use the internal ’loadbalancer’ mechanism which is provided by Distributed engine of Clickhouse. As an alternative in that scenario you can work without Distribured table, but with some external load balancer that will balance the requests between several replicas according to your specific rules or preferences, or just cluster-aware client which will pick one of the servers for the query time.\nThe Distributed engine does not store any data, but it can ‘point’ to the same ReplicatedMergeTree/MergeTree table on multiple servers. To use Distributed engine you need to configure \u003ccluser\u003e settings in your ClickHouse server config file.\nSo let’s say you have 3 replicas of table my_replicated_data with ReplicatedMergeTree engine. You can create a table with Distrtibuted engine called my_distributed_replicated_data which will ‘point’ to all of that 3 servers, and when you will select from that my_distributed_replicated_data table the select will be forwarded and executed on one of the replicas. So in that scenario, each replica will get 1/3 of requests (but each request still will be fully executed on one chosen replica).\nAll that is great, and will work well while one copy of your data is fitting on a single physical server, and can be processed by the resources of one server. When you have too much data to be stored/processed on one server - you need to use sharding (it’s just a way to split the data into smaller parts). Sharding is the mechanism also provided by Distributed engine.\nWith sharding data is divided into parts (shards) according to some sharding key. You can just use random distribution, so let’s say - throw a coin to decide on each of the servers the data should be stored, or you can use some ‘smarter’ sharding scheme, to make the data connected to the same subject (let’s say to the same customer) stored on one server, and to another subject on another. So in that case all the shards should be requested at the same time and later the ‘common’ result should be calculated.\nIn ClickHouse each shard works independently and process its’ part of data, inside each shard replication can work. And later to query all the shards at the same time and combine the final result - Distributed engine is used. So Distributed work as load balancer inside each shard, and can combine the data coming from different shards together to make the ‘common’ result.\nYou can use Distribured table for inserts, in that case, it will pass the data to one of the shards according to the sharding key. Or you can insert to the underlying table on one of the shards bypassing the Distributed table.\nShort summary  start with MergeTree to have several copies of data use ReplicatedMergeTree if your data is too big to fit/ to process on one server - use sharding to balance the load between replicas and to combine the result of selects from different shards - use Distributed table.  More Official tutorial clarify that a bit: https://clickhouse.yandex/tutorial.html\nPlease check also @alex-zaitsev presentation, which also covers that subject: https://www.youtube.com/watch?v=zbjub8BQPyE ( Slides are here: https://yadi.sk/i/iLA5ssAv3NdYGy )\nP.S. Actually you can create replication without Zookeeper and ReplicatedMergeTree, just by using the Distributed table above MergeTree and internal_replication=false cluster setting, but in that case, there will no guarantee that all the replicas will have 100% the same data, so I rather would not recommend that scenario.\naltinity-kb-atomic-database-engine/\u0026quot; \naltinity-kb-embeddedrocksdb-and-dictionary.md\u0026quot; \nmergetree-table-engine-family/altinity-kb-nulls-in-order-by.md\u0026quot; \nmergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates.md\u0026quot; \nBased on my original answer on github: https://github.com/ClickHouse/ClickHouse/issues/2161\n","categories":"","description":"Learn about ClickHouse engines, from MergeTree, Atomic Database to RocksDB.\n","excerpt":"Learn about ClickHouse engines, from MergeTree, Atomic Database to …","ref":"/engines/","tags":"","title":"Engines"},{"body":"","categories":"","description":"Learn about ClickHouse queries \u0026 syntax, including Joins \u0026 Window Functions.\n","excerpt":"Learn about ClickHouse queries \u0026 syntax, including Joins \u0026 Window …","ref":"/altinity-kb-queries-and-syntax/","tags":"","title":"Queries \u0026 Syntax"},{"body":"","categories":"","description":"Functions\n","excerpt":"Functions\n","ref":"/altinity-kb-functions/","tags":"","title":"Functions"},{"body":"","categories":"","description":"Tips and tricks for using ClickHouse with different cloud services.\n","excerpt":"Tips and tricks for using ClickHouse with different cloud services.\n","ref":"/altinity-kb-integrations/altinity-cloud/","tags":"","title":"Cloud Services"},{"body":"","categories":"","description":"Learn how you can integrate cloud services, BI tools, kafka, MySQL, Spark, MindsDB, and more with ClickHouse.\n","excerpt":"Learn how you can integrate cloud services, BI tools, kafka, MySQL, …","ref":"/altinity-kb-integrations/","tags":"","title":"Integrations"},{"body":"Organizations that want to enable administrative users in their Altinity.Cloud ClickHouse servers can do so by enabling access_management manually. This allows for administrative users to be created on the specific ClickHouse Cluster.\nWARNING Modifying the ClickHouse cluster settings manually can lead to the cluster not loading or other issues. Change settings only with full consultation with an Altinity.Cloud support team member, and be ready to remove settings if they cause any disruption of service.  To add the access_management setting to an Altinity.Cloud ClickHouse Cluster:\n  Log into your Altinity.Cloud account.\n  For the cluster to modify, select Configure -\u003e Settings.\n  Cluster setting configure     From the Settings page, select +ADD SETTING.\n  Add cluster setting     Set the following options:\n  Setting Type: Select users.d file.\n  Filename: access_management.xml\n  Contents: Enter the following to allow the clickhouse_operator that controls the cluster through the clickhouse-operator the ability to set administrative options:\n\u003cyandex\u003e  \u003cusers\u003e  \u003cadmin\u003e  \u003caccess_management\u003e1\u003c/access_management\u003e  \u003c/admin\u003e  \u003cclickhouse_operator\u003e  \u003caccess_management\u003e1\u003c/access_management\u003e  \u003c/clickhouse_operator\u003e  \u003c/users\u003e \u003c/yandex\u003e   access_management=1 means that users admin, clickhouse_operator are able to create users and grant them privileges using SQL.\n  Select OK. The cluster will restart, and users can now be created in the cluster that can be granted administrative access.\n  If you are running ClickHouse 21.9 and above you can enable storing access management in ZooKeeper. in this case it will be automatically propagated to the cluster. This requires yet another configuration file:\n  Setting Type: Select config.d file\n  Filename: user_directories.xml\n  Contents:\n\u003cyandex\u003e  \u003cuser_directories replace=\"replace\"\u003e  \u003cusers_xml\u003e  \u003cpath\u003e/etc/clickhouse-server/users.xml\u003c/path\u003e  \u003c/users_xml\u003e  \u003creplicated\u003e  \u003czookeeper_path\u003e/clickhouse/access/\u003c/zookeeper_path\u003e  \u003c/replicated\u003e  \u003c/user_directories\u003e \u003c/yandex\u003e     ","categories":"","description":"Enabling access_management for Altinity.Cloud databases.\n","excerpt":"Enabling access_management for Altinity.Cloud databases.\n","ref":"/altinity-kb-integrations/altinity-cloud/altinity-cloud-access-management/","tags":"","title":"Altinity Cloud Access Management"},{"body":"","categories":"","description":"S3 \u0026 object storage\n","excerpt":"S3 \u0026 object storage\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/","tags":"","title":"S3 \u0026 object storage"},{"body":"","categories":"","description":"Learn how to set up, deploy, monitor, and backup ClickHouse with step-by-step guides.\n","excerpt":"Learn how to set up, deploy, monitor, and backup ClickHouse with …","ref":"/altinity-kb-setup-and-maintenance/","tags":"","title":"Setup \u0026 maintenance"},{"body":"","categories":"","description":"Access useful ClickHouse queries, from finding database size, missing blocks, checking table metadata in Zookeeper, and more.\n","excerpt":"Access useful ClickHouse queries, from finding database size, missing …","ref":"/altinity-kb-useful-queries/","tags":"","title":"Useful queries"},{"body":"","categories":"","description":"All you need to know about ClickHouse schema design, including materialized view, limitations, lowcardinality, codecs.\n","excerpt":"All you need to know about ClickHouse schema design, including …","ref":"/altinity-kb-schema-design/","tags":"","title":"Schema design"},{"body":"clickhouse-backup setup-example.yaml\n","categories":"","description":"Run ClickHouse in Kubernetes without any issues.\n","excerpt":"Run ClickHouse in Kubernetes without any issues.\n","ref":"/altinity-kb-kubernetes/","tags":"","title":"Kubernetes"},{"body":"","categories":"","description":"See the frequent questions users have about clickhouse-client.\n","excerpt":"See the frequent questions users have about clickhouse-client.\n","ref":"/altinity-kb-interfaces/","tags":"","title":"Interfaces"},{"body":"Normally the upgrade procedure looks like that:\n pick the release to upgrade check the release notes/changelog between the release you use currently and the target release sometimes you may need to change some configuration settings to change the defaults (for better compatibility, etc) upgrade itself is simple:  upgrade package (it doesn’t trigger the restart of clickhouse-server automatically) restart clickhouse-server check healthchecks / logs repeat on other nodes   Mixing several versions working together in the same cluster may often lead to different degradations. Usually, it’s not recommended to have a big delay between upgrading different nodes on the same cluster. Usually, you do upgrade on the odd replicas first, and after they were back online - restart the even replicas. upgrade the dev / staging first ensure your schema/queries work properly on the staging env do the production upgrade.  ","categories":"","description":"Upgrade notes.\n","excerpt":"Upgrade notes.\n","ref":"/upgrade/","tags":"","title":"Upgrade"},{"body":"For more information on ClickHouse Dictionaries, see\nthe presentation https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup34/clickhouse_integration.pdf, slides 82-95, video https://youtu.be/728Yywcd5ys?t=10642\nWe have also couple of articles about dictionaries in our blog: https://altinity.com/blog/dictionaries-explained https://altinity.com/blog/2020/5/19/clickhouse-dictionaries-reloaded\nAnd some videos: https://www.youtube.com/watch?v=FsVrFbcyb84\nAlso there 3rd party articles on the same subj. https://prog.world/how-to-create-and-use-dictionaries-in-clickhouse/\n","categories":"","description":"All you need to know about creating and using ClickHouse dictionaries.\n","excerpt":"All you need to know about creating and using ClickHouse dictionaries. …","ref":"/altinity-kb-dictionaries/","tags":"","title":"Dictionaries"},{"body":"This Knowledge Base now supports Mermaid, a handy way to create charts from text. The following example shows a very simple chart, and the code to use.\nTo add a Mermaid chart, encase the Mermaid code between {{\u003c mermaid \u003e}}, as follows:\n{{\u003cmermaid\u003e}} graph TD;  A--\u003eB;  A--\u003eC;  B--\u003eD;  C--\u003eD; {{\u003c/mermaid\u003e}} And it renders as so:\nvar config = { startOnLoad:true, theme:'dark', align:'center' }; mermaid.initialize(config);  graph TD; A--B; A--C; B--D; C--D;  ","categories":"","description":"A short example of using the Mermaid library to add charts.\n","excerpt":"A short example of using the Mermaid library to add charts.\n","ref":"/using-this-knowledgebase/mermaid_example/","tags":"","title":"Mermaid Example"},{"body":"The Altinity Knowledge Base is built on GitHub Pages, using Hugo and Docsy. This guide provides a brief description on how to make updates and add to this knowledge base.\nPage and Section Basics The knowledge base is structured in a simple directory format, with the content of the Knowledge Base stored under the directory content/en.\nEach section is a directory, with the file _index.md that provides that sections information. For example, the Upgrade section has the following layout:\n├── upgrade │ ├── _index.md │ └── removing-empty-parts.md Each Markdown file provides the section’s information and the title that is displayed on the left navigation panel, with the file _index.md providing the top level information on the section.\nEach page is set in the following format that sets the page attributes:\n---title:\"Using This Knowledge Base\"linkTitle:\"Using This Knowledge Base\"description:\u003eHow to add pages, make updates, and expand this knowledge base.weight:11---The content of the page in Markdown format.The attributes are as follows:\n title: The title of the page displayed at the top of the page. linkTitle: The title used in the left navigation panel. description: A short description of the page listed under the title. weight: The placement of the page in the hierarchy in the left navigation panel. The higher the weight, the higher in the display order it will be. For example, the file engines/_index.md has a weight of 1, pushing its display to the top of the list.  Create Pages and Sections Create or Edit A Page To create a new page or edit an existing one in the knowledge base:\n From the page to start from:  To create a new page, select Create child page. To edit an existing page, select Edit this page.   This will open the page’s location in the GitHub repository. Update the page using Markdown. See the Docsy Formatting Options section below for tips and details.  View how the page will look Preview. The GitHub Preview is not 100% the same as the page will be displayed on the knowledge base, but it is a close enough approximation.   Saving the file will depend on your role.   For those who have been granted Knowledgebase Contributor status, select Commit New File. The changes will be automatically applied to the GitHub repository, and the additions will be displayed to the knowledge base within 1-5 minutes.\n  For those who have not been granted Knowledgebase Contributor status, they will have to fork the changes and then create a new pull request through the following process:\n  When editing is complete, select Propose New File. This will being you to the GitHub Pull Request page.\n  Verify the new file is accurate, then select Create Pull Request.\n  Name the Pull Request, then select Create pull request.\n  First time contributors will be required to review and sign the Contributor License Agreement(CLA). To signify they agree with the CLA, the following comment must be left as part of the pull request:\nI have read the CLA Document and I hereby sign the CLA   This signature will be stored as part of the GitHub repository indicating the GitHub username, the date of the agreement, and the pull request where the signer indicated their consent with the CLA.\n The Pull Request will be reviewed and if approved, the changes will be applied to the Knowledge Base.      Create a New Section To create a new section in the knowledge base, add a new directory under content/en from either the GitHub Repository or through some other GitHub related method., and add the file index.md. The same submission process will be followed as outlined in Create or Edit A Page.\nDocsy Formatting Options Docsy uses Markdown, providing a simple method of formatting documents. Refer to the Markdown documentation for how to edit pages and achieve the display results.\nThe following guide recommendations should be followed:\n Code should should be code segments, which uses three back tics to start and end a code section, with the type of code used. For example, if the code segment is regarding SQL then the section would start with ```sql` . Display text should be in bold. For example, when requesting someone click Create New Page on a page, Create New Page is in bold.  Adding Images New images and other static files are stored in the directory static, with the following categories:\n Images are stored under static/assets. Pdf files are stored under static/assets  ","categories":"","description":"Add pages, make updates, and contribute to this ClickHouse knowledge base.\n","excerpt":"Add pages, make updates, and contribute to this ClickHouse knowledge …","ref":"/using-this-knowledgebase/","tags":"","title":"Using This Knowledge Base"},{"body":"What happened After ClickHouse upgrade from version pre 21.6 to version after 21.6, count of unique UUID in AggregatingMergeTree tables nearly doubled in case of merging of data which was generated in different ClickHouse versions.\nWhy happened In pull request which changed the internal representation of big integers data types (and UUID). SipHash64 hash-function used for uniq aggregation function for UUID data type was replaced with intHash64, which leads to different result for the same UUID value across different ClickHouse versions. Therefore, it results in doubling of counts, when uniqState created by different ClickHouse versions being merged together.\nRelated issue.\nSolution You need to replace any occurrence of uniqState(uuid) in MATERIALIZED VIEWs with uniqState(sipHash64(uuid)) and change data type for already saved data from AggregateFunction(uniq, UUID) to AggregateFunction(uniq, UInt64), because result data type of sipHash64 is UInt64.\n-- On ClickHouse version 21.3 CREATETABLEuniq_state(`key`UInt32,`value`AggregateFunction(uniq,UUID))ENGINE=MergeTreeORDERBYkeyINSERTINTOuniq_stateSELECTnumber%10000ASkey,uniqState(reinterpretAsUUID(number))FROMnumbers(1000000)GROUPBYkeyOk.0rowsinset.Elapsed:0.404sec.Processed1.05millionrows,8.38MB(2.59millionrows/s.,20.74MB/s.)SELECTkey%20,uniqMerge(value)FROMuniq_stateGROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┐│0│50000││1│50000││2│50000││3│50000││4│50000││5│50000││6│49999││7│50000││8│49999││9│50000││10│50000││11│50000││12│50000││13│50000││14│50000││15│50000││16│50000││17│50000││18│50000││19│50000│└─────────────────┴──────────────────┘-- After upgrade of ClickHouse to 21.8 SELECTkey%20,uniqMerge(value)FROMuniq_stateGROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┐│0│50000││1│50000││2│50000││3│50000││4│50000││5│50000││6│49999││7│50000││8│49999││9│50000││10│50000││11│50000││12│50000││13│50000││14│50000││15│50000││16│50000││17│50000││18│50000││19│50000│└─────────────────┴──────────────────┘20rowsinset.Elapsed:0.240sec.Processed10.00thousandrows,1.16MB(41.72thousandrows/s.,4.86MB/s.)CREATETABLEuniq_state_2ENGINE=MergeTreeORDERBYkeyASSELECT*FROMuniq_stateOk.0rowsinset.Elapsed:0.128sec.Processed10.00thousandrows,1.16MB(78.30thousandrows/s.,9.12MB/s.)INSERTINTOuniq_state_2SELECTnumber%10000ASkey,uniqState(reinterpretAsUUID(number))FROMnumbers(1000000)GROUPBYkeyOk.0rowsinset.Elapsed:0.266sec.Processed1.05millionrows,8.38MB(3.93millionrows/s.,31.48MB/s.)SELECTkey%20,uniqMerge(value)FROMuniq_state_2GROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┐│0│99834│\u003c-Countofuniquevaluesnearlydoubled.│1│100219││2│100128││3│100457││4│100272││5│100279││6│99372││7│99450││8│99974││9│99632││10│99562││11│100660││12│100439││13│100252││14│100650││15│99320││16│100095││17│99632││18│99540││19│100098│└─────────────────┴──────────────────┘20rowsinset.Elapsed:0.356sec.Processed20.00thousandrows,2.33MB(56.18thousandrows/s.,6.54MB/s.)CREATETABLEuniq_state_3ENGINE=MergeTreeORDERBYkeyASSELECT*FROMuniq_state0rowsinset.Elapsed:0.126sec.Processed10.00thousandrows,1.16MB(79.33thousandrows/s.,9.24MB/s.)-- Option 1, create separate column ALTERTABLEuniq_state_3ADDCOLUMN`value_2`AggregateFunction(uniq,UInt64)DEFAULTunhex(hex(value));ALTERTABLEuniq_state_3UPDATEvalue_2=value_2WHERE1;SELECT*FROMsystem.mutationsWHEREis_done=0;Ok.0rowsinset.Elapsed:0.008sec.INSERTINTOuniq_state_3(key,value_2)SELECTnumber%10000ASkey,uniqState(sipHash64(reinterpretAsUUID(number)))FROMnumbers(1000000)GROUPBYkeyOk.0rowsinset.Elapsed:0.337sec.Processed1.05millionrows,8.38MB(3.11millionrows/s.,24.89MB/s.)SELECTkey%20,uniqMerge(value),uniqMerge(value_2)FROMuniq_state_3GROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┬─uniqMerge(value_2)─┐│0│50000│50000││1│50000│50000││2│50000│50000││3│50000│50000││4│50000│50000││5│50000│50000││6│49999│49999││7│50000│50000││8│49999│49999││9│50000│50000││10│50000│50000││11│50000│50000││12│50000│50000││13│50000│50000││14│50000│50000││15│50000│50000││16│50000│50000││17│50000│50000││18│50000│50000││19│50000│50000│└─────────────────┴──────────────────┴────────────────────┘20rowsinset.Elapsed:0.768sec.Processed20.00thousandrows,4.58MB(26.03thousandrows/s.,5.96MB/s.)-- Option 2, modify column in-place with String as intermediate data type. ALTERTABLEuniq_state_3MODIFYCOLUMN`value`StringOk.0rowsinset.Elapsed:0.280sec.ALTERTABLEuniq_state_3MODIFYCOLUMN`value`AggregateFunction(uniq,UInt64)Ok.0rowsinset.Elapsed:0.254sec.INSERTINTOuniq_state_3(key,value)SELECTnumber%10000ASkey,uniqState(sipHash64(reinterpretAsUUID(number)))FROMnumbers(1000000)GROUPBYkeyOk.0rowsinset.Elapsed:0.554sec.Processed1.05millionrows,8.38MB(1.89millionrows/s.,15.15MB/s.)SELECTkey%20,uniqMerge(value),uniqMerge(value_2)FROMuniq_state_3GROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┬─uniqMerge(value_2)─┐│0│50000│50000││1│50000│50000││2│50000│50000││3│50000│50000││4│50000│50000││5│50000│50000││6│49999│49999││7│50000│50000││8│49999│49999││9│50000│50000││10│50000│50000││11│50000│50000││12│50000│50000││13│50000│50000││14│50000│50000││15│50000│50000││16│50000│50000││17│50000│50000││18│50000│50000││19│50000│50000│└─────────────────┴──────────────────┴────────────────────┘20rowsinset.Elapsed:0.589sec.Processed30.00thousandrows,6.87MB(50.93thousandrows/s.,11.66MB/s.)SHOWCREATETABLEuniq_state_3;CREATETABLEdefault.uniq_state_3(`key`UInt32,`value`AggregateFunction(uniq,UInt64),`value_2`AggregateFunction(uniq,UInt64)DEFAULTunhex(hex(value)))ENGINE=MergeTreeORDERBYkeySETTINGSindex_granularity=8192-- Option 3, CAST uniqState(UInt64) to String. CREATETABLEuniq_state_4ENGINE=MergeTreeORDERBYkeyASSELECT*FROMuniq_stateOk.0rowsinset.Elapsed:0.146sec.Processed10.00thousandrows,1.16MB(68.50thousandrows/s.,7.98MB/s.)INSERTINTOuniq_state_4(key,value)SELECTnumber%10000ASkey,CAST(uniqState(sipHash64(reinterpretAsUUID(number))),'String')FROMnumbers(1000000)GROUPBYkeyOk.0rowsinset.Elapsed:0.476sec.Processed1.05millionrows,8.38MB(2.20millionrows/s.,17.63MB/s.)SELECTkey%20,uniqMerge(value)FROMuniq_state_4GROUPBYkey%20┌─modulo(key,20)─┬─uniqMerge(value)─┐│0│50000││1│50000││2│50000││3│50000││4│50000││5│50000││6│49999││7│50000││8│49999││9│50000││10│50000││11│50000││12│50000││13│50000││14│50000││15│50000││16│50000││17│50000││18│50000││19│50000│└─────────────────┴──────────────────┘20rowsinset.Elapsed:0.281sec.Processed20.00thousandrows,2.33MB(71.04thousandrows/s.,8.27MB/s.)SHOWCREATETABLEuniq_state_4;CREATETABLEdefault.uniq_state_4(`key`UInt32,`value`AggregateFunction(uniq,UUID))ENGINE=MergeTreeORDERBYkeySETTINGSindex_granularity=8192","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/altinity-kb-setup-and-maintenance/uniq-uuid-doubled-clickhouse-upgrade/","tags":"","title":"AggregateFunction(uniq, UUID) doubled after ClickHouse upgrade"},{"body":"Using AWS IAM — Identity and Access Management roles For EC2 instance, there is an option to configure an IAM role:\nRole shall contain a policy with permissions like:\n{  \"Version\": \"2012-10-17\",  \"Statement\": [  {  \"Sid\": \"allow-put-and-get\",  \"Effect\": \"Allow\",  \"Action\": [  \"s3:PutObject\",  \"s3:GetObject\"  ],  \"Resource\": \"arn:aws:s3:::BUCKET_NAME/test_s3_disk/*\"  }  ] } Corresponding configuration of ClickHouse:\n\u003cclickhouse\u003e  \u003cstorage_configuration\u003e  \u003cdisks\u003e  \u003cdisk_s3\u003e  \u003ctype\u003es3\u003c/type\u003e  \u003cendpoint\u003ehttp://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/\u003c/endpoint\u003e  \u003cuse_environment_credentials\u003etrue\u003c/use_environment_credentials\u003e  \u003c/disk_s3\u003e  \u003c/disks\u003e  \u003cpolicies\u003e  \u003cpolicy_s3_only\u003e  \u003cvolumes\u003e  \u003cvolume_s3\u003e  \u003cdisk\u003edisk_s3\u003c/disk\u003e  \u003c/volume_s3\u003e  \u003c/volumes\u003e  \u003c/policy_s3_only\u003e  \u003c/policies\u003e  \u003c/storage_configuration\u003e \u003c/clickhouse\u003e Small check:\nCREATETABLEtable_s3(numberInt64)ENGINE=MergeTree()ORDERBYtuple()PARTITIONBYtuple()SETTINGSstorage_policy='policy_s3_only';INSERTINTOtable_s3SELECT*FROMsystem.numbersLIMIT100000000;SELECT*FROMtable_s3;DROPTABLEtable_s3;","categories":"","description":"AWS S3 Recipes","excerpt":"AWS S3 Recipes","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/aws-s3-recipes/","tags":"","title":"AWS S3 Recipes"},{"body":"Can not connect to my ClickHouse server Errors like “Connection reset by peer, while reading from socket”\n  Ensure that the clickhouse-server is running\nsystemctl status clickhouse-server If server was restarted recently and don’t accept the connections after the restart - most probably it still just starting. During the startup sequence it need to iterate over all data folders in /var/lib/clickhouse-server In case if you have a very high number of folders there (usually caused by a wrong partitioning, or a very high number of tables / databases) that startup time can take a lot of time (same can happen if disk is very slow, for example NFS).\nYou can check that by looking for ‘Ready for connections’ line in /var/log/clickhouse-server/clickhouse-server.log (Information log level neede)\n  Ensure you use the proper port ip / interface?\nEnsure you’re not trying to connect to secure port without tls / https or vice versa.\nFor clickhouse-client - pay attention on host / port / secure flags.\nEnsure the interface you’re connecting to is the one which clickhouse listens (by default clickhouse listens only localhost).\nNote: If you uncomment line \u003clisten_host\u003e0.0.0.0\u003c/listen_host\u003e only - clickhouse will listen only ipv4 interfaces, while the localhost (used by clickhouse-client) may be resolved to ipv6 address. And clickhouse-client may be failing to connect.\nHow to check which interfaces / ports do clickhouse listen?\nsudo lsof -i -P -n | grep LISTEN  echo listen_host sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=listen_host echo tcp_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=tcp_port echo tcp_port_secure sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=tcp_port_secure echo http_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=http_port echo https_port sudo clickhouse-extract-from-config --config=/etc/clickhouse-server/config.xml --key=https_port   For secure connection:\n ensure that server uses some certificate which can be validated by the client OR disable certificate checks on the client (UNSECURE)    Check for errors in /var/log/clickhouse-server/clickhouse-server.err.log ?\n  Is clickhouse able to serve some trivial tcp / http requests from localhost?\ncurl 127.0.0.1:9200 curl 127.0.0.1:8123   Check number of sockets opened by clickhouse\nsudo lsof -i -a -p $(pidof clickhouse-server)  # or (adjust 9000 / 8123 ports if needed) netstat -tn 2\u003e/dev/null | tail -n +3 | awk '{ printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, $3, $4, $5, $6) }' | clickhouse-local -S \"Proto String, RecvQ Int64, SendQ Int64, LocalAddress String, ForeignAddress String, State LowCardinality(String)\" --query=\"SELECT * FROM table WHERE LocalAddress like '%:9000' FORMAT PrettyCompact\"  netstat -tn 2\u003e/dev/null | tail -n +3 | awk '{ printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, $3, $4, $5, $6) }' | clickhouse-local -S \"Proto String, RecvQ Int64, SendQ Int64, LocalAddress String, ForeignAddress String, State LowCardinality(String)\" --query=\"SELECT * FROM table WHERE LocalAddress like '%:8123' FORMAT PrettyCompact\" ClickHouse has a limit of number of open connections (4000 by default).\n  Check also:\n# system overall support limited number of connections it can handle netstat  # you can also be reaching of of the process ulimits (Max open files) cat /proc/$(pidof -s clickhouse-server)/limits   Check firewall / selinux rules (if used)\n  ","categories":"","description":"Can not connect to my ClickHouse server.","excerpt":"Can not connect to my ClickHouse server.","ref":"/altinity-kb-setup-and-maintenance/connection-problems/","tags":"","title":"Can not connect to my ClickHouse server"},{"body":"cgroups and kubernetes cloud providers Why my ClickHouse is slow after upgrade to version 22.2 and higher?\nThe probable reason is that ClickHouse 22.2 started to respect cgroups (Respect cgroups limits in max_threads autodetection. #33342 (JaySon).\nYou can observe that max_threads = 1\nSELECTname,valueFROMsystem.settingsWHEREname='max_threads'┌─name────────┬─value─────┐│max_threads│'auto(1)'│└─────────────┴───────────┘This makes ClickHouse to execute all queries with a single thread (normal behavior is half of available CPU cores, cores = 64, then ‘auto(32)’).\nWe observe this cgroups behavior with AWS EKS (Kubernetes) environment and Altinity ClickHouse Operator in case if requests.cpu and limits.cpu are not set for a resource.\nWorkaround We suggest to set requests.cpu = half of available CPU cores, and limits.cpu = CPU cores.\nFor example in case of 16 CPU cores:\n resources:  requests:  memory: ...  cpu: 8  limits:  memory: ....  cpu: 16 Then you should get a new result:\nSELECTname,valueFROMsystem.settingsWHEREname='max_threads'┌─name────────┬─value─────┐│max_threads│'auto(8)'│└─────────────┴───────────┘in depth For some reason AWS EKS sets cgroup kernel parameters in case of empty requests.cpu \u0026 limits.cpu into these:\n# cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us -1  # cat /sys/fs/cgroup/cpu/cpu.cfs_period_us 100000  # cat /sys/fs/cgroup/cpu/cpu.shares 2 This makes ClickHouse to set max_threads = 1 because of\ncgroup_share = /sys/fs/cgroup/cpu/cpu.shares (2) PER_CPU_SHARES = 1024 share_count = ceil( cgroup_share / PER_CPU_SHARES ) ---\u003e ceil(2 / 1024) ---\u003e 1 Fix Incorrect calculation was fixed in https://github.com/ClickHouse/ClickHouse/pull/35815 and will work correctly on newer releases.\n","categories":"","description":"cgroups and kubernetes cloud providers.","excerpt":"cgroups and kubernetes cloud providers.","ref":"/altinity-kb-setup-and-maintenance/cgroups_k8s/","tags":"","title":"cgroups and kubernetes cloud providers"},{"body":"Compare table metadata of different replicas in zookeeper  Metadata on replica is not up to date with common metadata in Zookeeper\n SELECT *, if( neighbor(name, -1) == name and name != 'is_active', neighbor(value, -1) == value , 1) as looks_good FROM ( SELECT name, path, ctime, mtime, value FROM system.zookeeper WHERE (path IN ( SELECT arrayJoin(groupUniqArray(if(path LIKE '%/replicas', concat(path, '/', name), path))) FROM system.zookeeper WHERE path IN ( SELECT arrayJoin([zookeeper_path, concat(zookeeper_path, '/replicas')]) FROM system.replicas WHERE table = 'test_repl' ) )) AND (name IN ('metadata', 'columns', 'is_active')) ORDER BY name = 'is_active', name ASC, path ASC ) vs.\nSELECT metadata_modification_time, create_table_query FROM system.tables WHERE name = 'test_repl' ","categories":"","description":"Check table metadata in zookeeper.","excerpt":"Check table metadata in zookeeper.","ref":"/altinity-kb-useful-queries/table-meta-in-zookeeper/","tags":"","title":"Check table metadata in zookeeper"},{"body":"ClickHouse operator https://github.com/Altinity/clickhouse-operator/blob/master/docs/README.md\n","categories":"","description":"ClickHouse operator","excerpt":"ClickHouse operator","ref":"/altinity-kb-setup-and-maintenance/clickhouse-operator/","tags":"","title":"ClickHouse operator"},{"body":"ClickHouse row-level deduplication. (Block level deduplication exists in Replicated tables, and is not the subject of that article).\nThere is quite common requirement to do deduplication on a record level in ClickHouse.\n Sometimes duplicates are appear naturally on collector side. Sometime they appear due the the fact that message queue system (Kafka/Rabbit/etc) offers at-least-once guarantees. Sometimes you just expect insert idempotency on row level.  For now that problem has no good solution in general case using ClickHouse only.\nThe reason in simple: to check if the row already exists you need to do some lookup (key-value) alike (ClickHouse is bad for key-value lookups), in general case - across the whole huge table (which can be terabyte/petabyte size).\nBut there many usecase when you can archive something like row-level deduplication in ClickHouse:\nApproach 0. Make deduplication before ingesting data to ClickHouse\n you have full control   extra coding and ‘moving parts’, storing some ids somewhere   clean and simple schema and selects in ClickHouse ! check if row exists in clickhouse before insert can give non-satisfing results if you use ClickHouse cluster (i.e. Replicated / Distributed tables) - due to eventual consistency.  Approach 1. Allow duplicates during ingestion. Remove them on SELECT level (by things like GROUP BY)\n simple inserts   complicate selects all selects will be significantly slower  Approach 2. Eventual deduplication using Replacing\n simple   can force you to use suboptimal primary key (which will guarantee record uniqueness) deduplication is eventual - you never know when it will happen, and you will get some duplicates if you don’t use FINAL clause selects with FINAL clause (select * from table_name FINAL) are much slower  and may require tricky manual optimization https://github.com/ClickHouse/ClickHouse/issues/31411 can work with acceptable speed in some special conditions: https://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/    Approach 3. Eventual deduplication using Collapsing\n complicated can force you to use suboptimal primary key (which will guarantee record uniqueness) you need to store previous state of the record somewhere, or extract it before ingestion from clickhouse deduplication is eventual (same as with Replacing)  you can make the proper aggregations of last state w/o FINAL (bookkeeping-alike sums, counts etc)    Approach 4. Eventual deduplication using Summing with SimpleAggregateFunction( anyLast, …), Aggregating with argMax etc.\n quite complicated can force you to use suboptimal primary key (which will guarantee record uniqueness) deduplication is eventual (same as with Replacing)  but you can finish deduplication with GROUP BY instead if FINAL (it’s faster)    Approach 5. Keep data fragment where duplicates are possible isolated. Usually you can expect the duplicates only in some time window (like 5 minutes, or one hour, or something like that). You can put that ‘dirty’ data in separate place, and put it to final MergeTree table after deduplication window timeout. For example - you insert data in some tiny tables (Engine=StripeLog) with minute suffix, and move data from tinytable older that X minutes to target MergeTree (with some external queries). In the meanwhile you can see realtime data using Engine=Merge / VIEWs etc.\n quite complicated   good control no duplicated in target table perfect ingestion speed  Approach 6. Deduplication using MV pipeline. You insert into some temporary table (even with Engine=Null) and MV do join or subselect (which will check the existence of arrived rows in some time frame of target table) and copy new only rows to destination table.\n don’t impact the select speed   complicated for clusters can be inaccurate due to eventual consistency slow down inserts significantly (every insert will need to do lookup in target table first)  In all case: due to eventual consistency of ClickHouse replication you can still get duplicates if you insert into different replicas/shards.\n","categories":"","description":"ClickHouse row-level deduplication.","excerpt":"ClickHouse row-level deduplication.","ref":"/altinity-kb-schema-design/row-level-deduplication/","tags":"","title":"ClickHouse row-level deduplication"},{"body":"CollapsingMergeTree vs ReplacingMergeTree    ReplacingMergeTree CollapsingMergeTree     + very easy to use (always replace) - more complex (accounting-alike, put ‘rollback’ records to fix something)   + you don’t need to store the previous state of the row - you need to the store (somewhere) the previous state of the row, OR extract it from the table itself (point queries is not nice for ClickHouse)   - no deletes + support deletes   - w/o FINAL - you can can always see duplicates, you need always to ‘pay’ FINAL performance penalty + properly crafted query can give correct results without final (i.e. sum(amount * sign) will be correct, no matter of you have duplicated or not)   - only uniq()-alike things can be calculated in materialied views + you can do basic counts \u0026 sums in materialized views    ","categories":"","description":"CollapsingMergeTree vs ReplacingMergeTree.","excerpt":"CollapsingMergeTree vs ReplacingMergeTree.","ref":"/engines/mergetree-table-engine-family/collapsing-vs-replacing/","tags":"","title":"CollapsingMergeTree vs ReplacingMergeTree"},{"body":"Column backfilling Sometimes you need to add a column into a huge table and backfill it with a data from another source, without reingesting all data.\nHere is an example.\ncreatedatabasetest;usetest;-- table with an existing data, we need to backfill / update S column createtablefact(key1UInt64,key2String,key3String,DDate,SString)EngineMergeTreepartitionbyDorderby(key1,key2,key3);-- example data insertintofactselectnumber,toString(number%103),toString(number%13),today(),toString(number)fromnumbers(1e9);0rowsinset.Elapsed:155.066sec.Processed1.00billionrows,8.00GB(6.45millionrows/s.,51.61MB/s.)insertintofactselectnumber,toString(number%103),toString(number%13),today()-30,toString(number)fromnumbers(1e9);0rowsinset.Elapsed:141.594sec.Processed1.00billionrows,8.00GB(7.06millionrows/s.,56.52MB/s.)insertintofactselectnumber,toString(number%103),toString(number%13),today()-60,toString(number)fromnumbers(1e10);0rowsinset.Elapsed:1585.549sec.Processed10.00billionrows,80.01GB(6.31millionrows/s.,50.46MB/s.)selectcount()fromfact;12000000000-- 12 billions rows. -- table - source of the info to update createtabledict_table(key1UInt64,key2String,key3String,SString)EngineMergeTreeorderby(key1,key2,key3);-- example data insertintodict_tableselectnumber,toString(number%103),toString(number%13),toString(number)||'xxx'fromnumbers(1e10);0rowsinset.Elapsed:1390.121sec.Processed10.00billionrows,80.01GB(7.19millionrows/s.,57.55MB/s.)-- DICTIONARY witch will be the source for update / we cannot query dict_table directly CREATEDICTIONARYitem_dict(key1UInt64,key2String,key3String,SString)PRIMARYKEYkey1,key2,key3SOURCE(CLICKHOUSE(TABLEdict_tableDB'test'USER'default'))LAYOUT(complex_key_cache(size_in_cells50000000))Lifetime(60000);-- let's test that the discionary is working selectdictGetString('item_dict','S',tuple(toUInt64(1),'1','1'));┌─dictGetString('item_dict','S',tuple(toUInt64(1),'1','1'))─┐│1xxx│└───────────────────────────────────────────────────────────────┘1rowsinset.Elapsed:0.080sec.SELECTdictGetString('item_dict','S',(toUInt64(1111111),'50','1'))┌─dictGetString('item_dict','S',tuple(toUInt64(1111111),'50','1'))─┐│1111111xxx│└──────────────────────────────────────────────────────────────────────┘1rowsinset.Elapsed:0.004sec.-- Now let's lower number of simultaneous updates/mutations selectvaluefromsystem.settingswherenamelike'%background_pool_size%';┌─value─┐│16│└───────┘altertablefactmodifysettingnumber_of_free_entries_in_pool_to_execute_mutation=15;-- only one mutation is possible per time / 16 - 15 = 1 -- the mutation itself altertabletest.factupdateS=dictGetString('test.item_dict','S',tuple(key1,key2,key3))where1;-- mutation took 26 hours and item_dict used bytes_allocated: 8187277280 select*fromsystem.mutationswherenotis_done\\GRow1:──────database:testtable:factmutation_id:mutation_11452.txtcommand:UPDATES=dictGetString('test.item_dict','S',(key1,key2,key3))WHERE1create_time:2022-01-2920:21:00block_numbers.partition_id:['']block_numbers.number:[11452]parts_to_do_names:['20220128_1_954_4','20211230_955_1148_3','20211230_1149_1320_3','20211230_1321_1525_3','20211230_1526_1718_3','20211230_1719_1823_3','20211230_1824_1859_2','20211230_1860_1895_2','20211230_1896_1900_1','20211230_1901_1906_1','20211230_1907_1907_0','20211230_1908_1908_0','20211130_2998_9023_5','20211130_9024_10177_4','20211130_10178_11416_4','20211130_11417_11445_2','20211130_11446_11446_0']parts_to_do:17is_done:0latest_failed_part:latest_fail_time:1970-01-0100:00:00latest_fail_reason:SELECTtable,(elapsed*(1/progress))-elapsed,elapsed,progress,is_mutation,formatReadableSize(total_size_bytes_compressed)ASsize,formatReadableSize(memory_usage)ASmemFROMsystem.mergesORDERBYprogressDESC┌─table────────────────────────┬─minus(multiply(elapsed,divide(1,progress)),elapsed)─┬─────────elapsed─┬────────────progress─┬─is_mutation─┬─size───────┬─mem───────┐│fact│7259.920140111059│8631.476589565│0.5431540560211632│1│1.89GiB│0.00B││fact│60929.22808705666│23985.610558929│0.28246665649246827│1│9.86GiB│4.25MiB│└──────────────────────────────┴────────────────────────────────────────────────────────┴─────────────────┴─────────────────────┴─────────────┴────────────┴───────────┘SELECT*FROMsystem.dictionariesWHEREname='item_dict'\\GRow1:──────database:testname:item_dictuuid:28fda092-260f-430f-a8fd-a092260f330fstatus:LOADEDorigin:28fda092-260f-430f-a8fd-a092260f330ftype:ComplexKeyCachekey.names:['key1','key2','key3']key.types:['UInt64','String','String']attribute.names:['S']attribute.types:['String']bytes_allocated:8187277280query_count:12000000000hit_rate:1.6666666666666666e-10found_rate:1element_count:67108864load_factor:1source:ClickHouse:test.dict_tablelifetime_min:0lifetime_max:60000loading_start_time:2022-01-2920:20:50last_successful_update_time:2022-01-2920:20:51loading_duration:0.829last_exception:-- Check that data is updated SELECT*FROMtest.factWHEREkey1=11111┌──key1─┬─key2─┬─key3─┬──────────D─┬─S────────┐│11111│90│9│2021-12-30│11111xxx││11111│90│9│2022-01-28│11111xxx││11111│90│9│2021-11-30│11111xxx│└───────┴──────┴──────┴────────────┴──────────┘In case of replicated / sharded setup need to create dict_table / item_dict at all nodes and it has to have the EXACTLY same data (probaly it’s easier to make dict_table replicated). Need to use alter table test.fact ... setting allow_nondeterministic_mutations=1\n","categories":"","description":"Column backfilling with alter/update using a dictionary","excerpt":"Column backfilling with alter/update using a dictionary","ref":"/altinity-kb-schema-design/backfill_column/","tags":"","title":"Column backfilling with alter/update using a dictionary"},{"body":"Using custom settings in config You can not use the cutsom settings in config file ‘as is’, because clickhouse don’t know which datatype should be used to parse it.\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003ccustom_data_version\u003e1\u003c/custom_data_version\u003e \u003c!-- will not work! see below --\u003e  \u003c/default\u003e  \u003c/profiles\u003e \u003c/yandex\u003e That will end up with the following error:\n2021.09.24 12:50:37.369259 [ 264905 ] {} \u003cError\u003e ConfigReloader: Error updating configuration from '/etc/clickhouse-server/users.xml' config.: Code: 536. DB::Exception: Couldn't restore Field from dump: 1: while parsing value '1' for setting 'custom_data_version'. (CANNOT_RESTORE_FROM_FIELD_DUMP), Stack trace (when copying this message, always include the lines below): 0. DB::Exception::Exception(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, int, bool) @ 0x9440eba in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 1. DB::Field::restoreFromDump(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026)::$_4::operator()() const @ 0x10449da0 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 2. DB::Field::restoreFromDump(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026) @ 0x10449bf1 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 3. DB::BaseSettings\u003cDB::SettingsTraits\u003e::stringToValueUtil(std::__1::basic_string_view\u003cchar, std::__1::char_traits\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x1042e2bf in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 4. DB::UsersConfigAccessStorage::parseFromConfig(Poco::Util::AbstractConfiguration const\u0026) @ 0x1041a097 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 5. void std::__1::__function::__policy_invoker\u003cvoid (Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e, bool)\u003e::__call_impl\u003cstd::__1::__function::__default_alloc_func\u003cDB::UsersConfigAccessStorage::load(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::function\u003cstd::__1::shared_ptr\u003czkutil::ZooKeeper\u003e ()\u003e const\u0026)::$_0, void (Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e, bool)\u003e \u003e(std::__1::__function::__policy_storage const*, Poco::AutoPtr\u003cPoco::Util::AbstractConfiguration\u003e\u0026\u0026, bool) @ 0x1042e7ff in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 6. DB::ConfigReloader::reloadIfNewer(bool, bool, bool, bool) @ 0x11caf54e in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 7. DB::ConfigReloader::run() @ 0x11cb0f8f in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 8. ThreadFromGlobalPool::ThreadFromGlobalPool\u003cvoid (DB::ConfigReloader::*)(), DB::ConfigReloader*\u003e(void (DB::ConfigReloader::*\u0026\u0026)(), DB::ConfigReloader*\u0026\u0026)::'lambda'()::operator()() @ 0x11cb19f1 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 9. ThreadPoolImpl\u003cstd::__1::thread\u003e::worker(std::__1::__list_iterator\u003cstd::__1::thread, void*\u003e) @ 0x9481f5f in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 10. void* std::__1::__thread_proxy\u003cstd::__1::tuple\u003cstd::__1::unique_ptr\u003cstd::__1::__thread_struct, std::__1::default_delete\u003cstd::__1::__thread_struct\u003e \u003e, void ThreadPoolImpl\u003cstd::__1::thread\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda0'()\u003e \u003e(void*) @ 0x9485843 in /usr/lib/debug/.build-id/ba/25f6646c3be7aa95f452ec85461e96178aa365.debug 11. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so 12. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so (version 21.10.1.8002 (official build)) 2021.09.29 11:36:07.722213 [ 2090 ] {} \u003cError\u003e Application: DB::Exception: Couldn't restore Field from dump: 1: while parsing value '1' for setting 'custom_data_version' To make it work you need to change it an the following way:\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003ccustom_data_version\u003eUInt64_1\u003c/custom_data_version\u003e  \u003c/default\u003e  \u003c/profiles\u003e \u003c/yandex\u003e or\ncat /etc/clickhouse-server/users.d/default_profile.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003ccustom_data_version\u003e'1'\u003c/custom_data_version\u003e  \u003c/default\u003e  \u003c/profiles\u003e \u003c/yandex\u003e The list of recognized prefixes is in the sources: https://github.com/ClickHouse/ClickHouse/blob/ea13a8b562edbc422c07b5b4ecce353f79b6cb63/src/Core/Field.cpp#L253-L270\n","categories":"","description":"Using custom settings","excerpt":"Using custom settings","ref":"/altinity-kb-setup-and-maintenance/custom_settings/","tags":"","title":"Custom Settings"},{"body":"Substract fractional seconds WITHtoDateTime64('2021-09-07 13:41:50.926',3)AStimeSELECTtime-1,time-0.1ASno_affect,time-toDecimal64(0.1,3)ASuncorrect_result┌──────────minus(time,1)─┬───────────────no_affect─┬────────uncorrect_result─┐│2021-09-0713:41:49.926│2021-09-0713:41:50.926│2283-11-1110:46:37.248│└─────────────────────────┴─────────────────────────┴─────────────────────────┘WITHtoDateTime64('2021-03-03 09:30:00.100',3)AStime,fromUnixTimestamp64Milli(toInt64(toUnixTimestamp64Milli(time)+(1.25*1000)))ASfirst,toDateTime64(toDecimal64(time,3)+toDecimal64('1.25',3),3)ASsecond,reinterpret(reinterpret(time,'Decimal64(3)')+toDecimal64('1.25',3),'DateTime64(3)')ASthirdSELECTfirst,second,third┌───────────────────first─┬──────────────────second─┬───────────────────third─┐│2021-03-0309:30:01.350│2021-03-0309:30:01.350│2021-03-0309:30:01.350│└─────────────────────────┴─────────────────────────┴─────────────────────────┘WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,fromUnixTimestamp64Milli(reinterpretAsInt64(toUnixTimestamp64Milli(time)+(1.25*1000)))ASfirstSELECTcount()FROMnumbers(100000000)WHERENOTignore(first)1rowsinset.Elapsed:0.927sec.Processed100.03millionrows,800.21MB(107.93millionrows/s.,863.48MB/s.)WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,fromUnixTimestamp64Milli(toUnixTimestamp64Milli(time)+toInt16(1.25*1000))ASfirstSELECTcount()FROMnumbers(100000000)WHERENOTignore(first)1rowsinset.Elapsed:0.652sec.Processed100.03millionrows,800.21MB(153.52millionrows/s.,1.23GB/s.)WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,toDateTime64(toDecimal64(time,3)+toDecimal64('1.25',3),3)ASsecondSELECTcount()FROMnumbers(100000000)WHERENOTignore(second)1rowsinset.Elapsed:6.287sec.Processed100.03millionrows,800.21MB(15.91millionrows/s.,127.28MB/s.)SETdecimal_check_overflow=0;WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,toDateTime64(toDecimal64(time,3)+toDecimal64('1.25',3),3)ASsecondSELECTcount()FROMnumbers(100000000)WHERENOTignore(second)1rowsinset.Elapsed:5.726sec.Processed100.03millionrows,800.21MB(17.47millionrows/s.,139.74MB/s.)WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,reinterpret(reinterpret(time,'Decimal64(3)')+toDecimal64('1.25',3),'DateTime64(3)')ASthirdSELECTcount()FROMnumbers(100000000)WHERENOTignore(third)1rowsinset.Elapsed:1.478sec.Processed100.03millionrows,800.21MB(67.68millionrows/s.,541.42MB/s.)SETdecimal_check_overflow=0;WITHmaterialize(toDateTime64('2021-03-03 09:30:00.100',3))AStime,reinterpret(reinterpret(time,'Decimal64(3)')+toDecimal64('1.25',3),'DateTime64(3)')ASthirdSELECTcount()FROMnumbers(100000000)WHERENOTignore(third)1rowsinset.Elapsed:0.795sec.Processed100.03millionrows,800.21MB(125.82millionrows/s.,1.01GB/s.)","categories":"","description":"DateTime64 data type","excerpt":"DateTime64 data type","ref":"/altinity-kb-queries-and-syntax/datetime64/","tags":"","title":"DateTime64"},{"body":"Debug hunging / freezing things If ClickHouse is busy with something and you don’t know what’s happeing, you can easily check the stacktraces of all the thread which are working\nSELECTarrayStringConcat(arrayMap(x-\u003edemangle(addressToSymbol(x)),trace),'\\n')AStrace_functions,count()FROMsystem.stack_traceGROUPBYtrace_functionsORDERBYcount()DESCSETTINGSallow_introspection_functions=1FORMATVertical;If you can’t start any queries, but you have access to the node, you can sent a singal\n# older versions for i in $(ls -1 /proc/$(pidof clickhouse-server)/task/); do kill -TSTP $i; done # even older versions for i in $(ls -1 /proc/$(pidof clickhouse-server)/task/); do kill -SIGPROF $i; done ","categories":"","description":"Debug hunging / freezing things","excerpt":"Debug hunging / freezing things","ref":"/altinity-kb-useful-queries/debug-hang/","tags":"","title":"Debug hunging thing"},{"body":"CompiledExpressionCacheCount -- number or compiled cached expression (if CompiledExpressionCache is enabled) jemalloc -- parameters of jemalloc allocator, they are not very useful, and not interesting MarkCacheBytes / MarkCacheFiles -- there are cache for .mrk files (default size is 5GB), you can see is it use all 5GB or not MemoryCode -- how much memory allocated for ClickHouse executable MemoryDataAndStack -- virtual memory allocated for data and stack MemoryResident -- real memory used by ClickHouse ( the same as top RES/RSS) MemoryShared -- shared memory used by ClickHouse MemoryVirtual -- virtual memory used by ClickHouse ( the same as top VIRT) NumberOfDatabases NumberOfTables ReplicasMaxAbsoluteDelay -- important parameter - replica max absolute delay in seconds ReplicasMaxRelativeDelay -- replica max relative delay (from other replicas) in seconds ReplicasMaxInsertsInQueue -- max number of parts to fetch for a single Replicated table ReplicasSumInsertsInQueue -- sum of parts to fetch for all Replicated tables ReplicasMaxMergesInQueue -- max number of merges in queue for a single Replicated table ReplicasSumMergesInQueue -- total number of merges in queue for all Replicated tables ReplicasMaxQueueSize -- max number of tasks for a single Replicated table ReplicasSumQueueSize -- total number of tasks in replication queue UncompressedCacheBytes/UncompressedCacheCells -- allocated memory for uncompressed cache (disabled by default) Uptime -- uptime seconds ","categories":"","description":"Description of asynchronous_metrics","excerpt":"Description of asynchronous_metrics","ref":"/altinity-kb-setup-and-maintenance/asynchronous_metrics_descr/","tags":"","title":"Description of asynchronous_metrics"},{"body":"Functions to count uniqs    Function Function(State) StateSize Result QPS     uniqExact uniqExactState 1600003 100000 59.23   uniq uniqState 200804 100315 85.55   uniqCombined uniqCombinedState 98505 100314 108.09   uniqCombined(12) uniqCombinedState(12) 3291 98160 151.64   uniqCombined(15) uniqCombinedState(15) 24783 100768 110.18   uniqCombined(18) uniqCombinedState(18) 196805 100332 101.56   uniqCombined(20) uniqCombinedState(20) 786621 100088 65.05   uniqCombined64(12) uniqCombined64State(12) 3291 98160 164.96   uniqCombined64(15) uniqCombined64State(15) 24783 100768 133.96   uniqCombined64(18) uniqCombined64State(18) 196805 100332 110.85   uniqCombined64(20) uniqCombined64State(20) 786621 100088 66.48   uniqHLL12 uniqHLL12State 2651 101344 177.91   uniqTheta uniqThetaState 32795 98045 144.05   uniqUpTo(100) uniqUpToState(100) 1 101 222.93    Stats collected via script below on 22.2\nfuncname=( \"uniqExact\" \"uniq\" \"uniqCombined\" \"uniqCombined(12)\" \"uniqCombined(15)\" \"uniqCombined(18)\" \"uniqCombined(20)\" \"uniqCombined64(12)\" \"uniqCombined64(15)\" \"uniqCombined64(18)\" \"uniqCombined64(20)\" \"uniqHLL12\" \"uniqTheta\" \"uniqUpTo(100)\") funcname2=( \"uniqExactState\" \"uniqState\" \"uniqCombinedState\" \"uniqCombinedState(12)\" \"uniqCombinedState(15)\" \"uniqCombinedState(18)\" \"uniqCombinedState(20)\" \"uniqCombined64State(12)\" \"uniqCombined64State(15)\" \"uniqCombined64State(18)\" \"uniqCombined64State(20)\" \"uniqHLL12State\" \"uniqThetaState\" \"uniqUpToState(100)\")  length=${#funcname[@]}   for (( j=0; j\u003clength; j++ )); do  f1=\"${funcname[$j]}\"  f2=\"${funcname2[$j]}\"  size=$( clickhouse-client -q \"select ${f2}(toString(number)) from numbers_mt(100000) FORMAT RowBinary\" | wc -c )  result=\"$( clickhouse-client -q \"select ${f1}(toString(number)) from numbers_mt(100000)\" )\"  time=$( rm /tmp/clickhouse-benchmark.json; echo \"select ${f1}(toString(number)) from numbers_mt(100000)\" | clickhouse-benchmark -i200 --json=/tmp/clickhouse-benchmark.json \u0026\u003e/dev/null; cat /tmp/clickhouse-benchmark.json | grep QPS )   printf \"|%s|%s,%s,%s,%s\\n\" \"$f1\" \"$f2\" \"$size\" \"$result\" \"$time\" done ","categories":"","description":"Functions to count uniqs.","excerpt":"Functions to count uniqs.","ref":"/altinity-kb-schema-design/uniq-functions/","tags":"","title":"Functions to count uniqs"},{"body":"The most cpu / write / read-intensive queries from query_log SELECTnormalized_query_hash,any(query),count(),sum(query_duration_ms)/1000ASQueriesDuration,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'RealTimeMicroseconds')])/1000000ASRealTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'UserTimeMicroseconds')])/1000000ASUserTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'SystemTimeMicroseconds')])/1000000ASSystemTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'DiskReadElapsedMicroseconds')])/1000000ASDiskReadTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'DiskWriteElapsedMicroseconds')])/1000000ASDiskWriteTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'NetworkSendElapsedMicroseconds')])/1000000ASNetworkSendTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'NetworkReceiveElapsedMicroseconds')])/1000000ASNetworkReceiveTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'ZooKeeperWaitMicroseconds')])/1000000ASZooKeeperWaitTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'OSIOWaitMicroseconds')])/1000000ASOSIOWaitTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'OSCPUWaitMicroseconds')])/1000000ASOSCPUWaitTime,sum(ProfileEvents.Values[indexOf(ProfileEvents.Names,'OSCPUVirtualTimeMicroseconds')])/1000000ASOSCPUVirtualTime,sum(read_rows)ASReadRows,formatReadableSize(sum(read_bytes))ASReadBytes,sum(written_rows)ASWrittenTows,formatReadableSize(sum(written_bytes))ASWrittenBytes,sum(result_rows)ASResultRows,formatReadableSize(sum(result_bytes))ASResultBytesFROMsystem.query_logWHERE(event_time\u003e(now()-3600))AND(type='QueryFinish')GROUPBYnormalized_query_hashWITHTOTALSORDERBYUserTimeDESCLIMIT30FORMATVerticalFind queries which were started but not finished at some moment in time SELECTquery_id,min(event_time)t,any(query)FROMsystem.query_logwhereevent_date=today()andevent_time\u003e'2021-11-25 02:29:12'GROUPBYquery_idHAVINGcountIf(type='QueryFinish')=0ORsum(query_duration_ms)\u003e100000orderbyt;selectquery_id,any(query)fromsystem.query_logwhereevent_timebetween'2021-09-24 07:00:00'and'2021-09-24 09:00:00'groupbyquery_idHAVINGcountIf(type=1)\u003c\u003ecountIf(type!=1)","categories":"","description":"Handy queries for a system.query_log.","excerpt":"Handy queries for a system.query_log.","ref":"/altinity-kb-useful-queries/query_log/","tags":"","title":"Handy queries for a system.query_log"},{"body":"How much is too much? In most of the cases clickhouse don’t have any hard limits. But obsiously there there are some practical limitation / barriers for different things - often they are caused by some system / network / filesystem limitation.\nSo after reaching some limits you can get different kind of problems, usually it never a failures / errors, but different kinds of degradations (slower queries / high cpu/memory usage, extra load on the network / zookeeper etc).\nWhile those numbers can vary a lot depending on your hardware \u0026 settings there is some safe ‘Goldilocks’ zone where ClickHouse work the best with default settings \u0026 usual hardware.\nNumber of tables (system-wide, across all databases)  non-replicated MergeTree-family tables = few thousands is still acceptable, if you don’t do realtime inserts in more that few dozens of them. See #32259 ReplicatedXXXMergeTree = few hundreds is still acceptable, if you don’t do realtime inserts in more that few dozens of them. Every Replicated table comes with it’s own cost (need to do housekeepeing operations, monitoing replication queues etc). See #31919 Log family table = even dozens of thousands is still ok, especially if database engine = Lazy is used.  Number of databases Fewer than number of tables (above). Dozens / hundreds is usually still acceptable.\nNumber of columns in the table Up to a few hundreds. With thousands of columns the inserts / background merges may become slower / require more of RAM. See for example https://github.com/ClickHouse/ClickHouse/issues/6943 https://github.com/ClickHouse/ClickHouse/issues/27502\nClickHouse instances on a single node / VM One is enough. Single ClickHouse can use resources of the node very efficiently, and it may require some complicated tuning to run several instances on a single node.\nNumber of parts / partitions (system-wide, across all databases) More than several dozens thousands may lead to performance degradation: slow starts (see https://github.com/ClickHouse/ClickHouse/issues/10087 ).\nNumber of tables \u0026 partitions touched by a single insert If you have realtime / frequent inserts no more than few.\nFor the inserts are rare - up to couple of dozens.\nNumber of parts in the single table More than ~ 5 thousands may lead to issues with alters in Replicated tables (caused by jute.maxbuffer overrun, see details ), and query speed degradation.\nDisk size per shard Less than 10TB of compressed data per server. Bigger disk are harder to replace / resync.\nNumber of shards Dozens is still ok. More may require having more complex (non-flat) routing.\nNumber of replicas in a single shard 2 is minimum for HA. 3 is a ‘golden standard’. Up to 6-8 is still ok. If you have more with realtime inserts - it can impact the zookeeper traffic.\nNumber of zookeeper nodes in the ensemble 3 (Three) for most of the cases is enough (you can loose one node). Using more nodes allows to scale up read throughput for zookeeper, but don’t improve writes at all.\nNumber of materialized view attached to a single table. Up to few. The less the better if the table is getting realtime inserts. (no matter if MV are chained or all are feeded from the same source table).\nThe more you have the more costy your inserts are, and the bigger risks to get some inconsitencies between some MV (inserts to MV and main table are not atomic).\nIf the table don’t have realtime inserts you can have more MV.\nNumber of projections inside a single table. Up to few. Similar to MV above.\nNumber of secondary indexes a single table. One to about a dozen. Different types of indexes has different penalty, bloom_filter is 100 times heavier than min_max index At some point your inserts will slow down. Try to create possible minimum of indexes. You can combine many columns into a single index and this index will work for any predicate but create less impact.\n","categories":"","description":"ClickHouse Limitations.","excerpt":"ClickHouse Limitations.","ref":"/altinity-kb-schema-design/how-much-is-too-much/","tags":"","title":"How much is too much?"},{"body":"Create a new table and copy data through an intermediate table. Step by step procedure. We want to add column3 to the ORDER BY in this table:\nCREATETABLEexample_table(dateDate,column1String,column2String,column3String,column4String)ENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/example_table','{replica}')PARTITIONBYtoYYYYMM(date)ORDERBY(column1,column2)  Stop publishing/INSERT into example_table.\n  Rename table example_table to example_table_old\n  Create the new table with the old name. This will preserve all dependencies like materialized views.\n  CREATETABLEexample_tableasexample_table_oldENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/example_table_new','{replica}')PARTITIONBYtoYYYYMM(date)ORDERBY(column1,column2,column3) Copy data from example_table_old into example_table_temp\na. Use this query to generate a list of INSERT statements\nselectconcat('insert into example_table_temp select * from example_table_old where toYYYYMM(date)=',partition)ascmd,database,table,partition,sum(rows),sum(bytes_on_disk),count()fromsystem.partswheredatabase='default'andtable='example_table_old'groupbydatabase,table,partitionorderbypartitionb. Create an intermediate table\nCREATETABLEexample_table_tempasexample_table_oldENGINE=MergeTreePARTITIONBYtoYYYYMM(date)ORDERBY(column1,column2,column3)c. Run the queries one by one\nAfter each query compare the number of rows in both tables. If the INSERT statement was interrupted and failed to copy data, drop the partition in example_table and repeat the INSERT. If a partition was copied successfully, proceed to the next partition.\nHere is a query to compare the tables:\nselectdatabase,table,partition,sum(rows),sum(bytes_on_disk),count()fromsystem.partswheredatabase='default'andtablelike'example_table%'groupbydatabase,table,partitionorderbypartition  Attach data from the intermediate table to example_table\na. Use this query to generate a list of ATTACH statements\nselectconcat('alter table example_table attach partition id ''',partition,''' from example_table_temp')ascmd,database,table,partition,sum(rows),sum(bytes_on_disk),count()fromsystem.partswheredatabase='default'andtable='example_table_temp'groupbydatabase,table,partitionorderbypartitionb. Run the queries one by one\nHere is a query to compare the tables:\nselecthostName(),database,table,partition,sum(rows),sum(bytes_on_disk),count()fromclusterAllReplicas('my-cluster',system.parts)wheredatabase='default'andtablelike'example_table%'groupbyhostName(),database,table,partitionorderbypartition  Drop example_table_old and example_table_temp\n  ","categories":"","description":"How to change ORDER BY.","excerpt":"How to change ORDER BY.","ref":"/altinity-kb-schema-design/change-order-by/","tags":"","title":"How to change ORDER BY"},{"body":"Example How to Convert Ordinary to Atomic createdatabasedbengine=Ordinary;createtabledb.test(AInt64)Engine=MergeTreeorderbyA;creatematerializedviewdb.test_mv(AInt64)Engine=MergeTreeorderbyAasselect*fromdb.test;insertintodb.testselect*fromnumbers(1000);createdatabasedb_tempengine=Atomic;renametabledb.testtodb_temp.test;renametabledb.test_mvtodb_temp.test_mv;dropdatabasedb;renamedatabasedb_temptodb;usedb;showtables;┌─name───────────────────────────────────────────┐│.inner_id.37db402c-fc46-421d-b7db-402cfc46921d││test││test_mv│└────────────────────────────────────────────────┘insertintodb.testselect*fromnumbers(1000);selectcount()fromtest;┌─count()─┐│2000│└─────────┘selectcount()fromtest_mv;┌─count()─┐│2000│└─────────┘showcreatedatabasedb;┌─statement─────────────────────────┐│CREATEDATABASEdbENGINE=Atomic│└───────────────────────────────────┘","categories":"","description":"Clickhouse Howto Convert Ordinary to Atomic","excerpt":"Clickhouse Howto Convert Ordinary to Atomic","ref":"/engines/altinity-kb-atomic-database-engine/how-to-convert-ordinary-to-atomic/","tags":"","title":"How to Convert Ordinary to Atomic"},{"body":"http handler example (how to disable /play) # cat /etc/clickhouse-server/config.d/play_disable.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003chttp_handlers\u003e  \u003crule\u003e  \u003curl\u003e/play\u003c/url\u003e  \u003cmethods\u003eGET\u003c/methods\u003e  \u003chandler\u003e  \u003ctype\u003estatic\u003c/type\u003e  \u003cstatus\u003e403\u003c/status\u003e  \u003ccontent_type\u003etext/plain; charset=UTF-8\u003c/content_type\u003e  \u003cresponse_content\u003e\u003c/response_content\u003e  \u003c/handler\u003e  \u003c/rule\u003e  \u003cdefaults/\u003e \u003c!-- handler to save default handlers ?query / ping --\u003e  \u003c/http_handlers\u003e \u003c/yandex\u003e ","categories":"","description":"http handler example","excerpt":"http handler example","ref":"/altinity-kb-setup-and-maintenance/http_handlers/","tags":"","title":"http handler example"},{"body":"Why inserts into materialized views are not idempotent? ClickHouse still does not have transactions. They will be implemented around 2022Q2.\nBecause of Clickhouse materialized view is a trigger. And an insert into a table and an insert into a subordinate materialized view it’s two different inserts so they are not atomic alltogether.\nAnd insert into a materialized view may fail after the succesful insert into the table. In case of any failure a client gets the error about failed insertion. You may enable insert_deduplication (it’s enabled by default for Replciated engines) and repeate the insert with an idea to achive idempotate insertion, and insertion will be skipped into the source table becase of deduplication but it will be skipped for materialized view as well because by default materialized view inherites deduplication from the source table. It’s controlled by a parameter deduplicate_blocks_in_dependent_materialized_views https://clickhouse.com/docs/en/operations/settings/settings/#settings-deduplicate-blocks-in-dependent-materialized-views\nIf your materialized view is wide enought and always have enought data for constistent deduplication then you can enable deduplicate_blocks_in_dependent_materialized_views. Or you may add information for deduplication (some unique information / insert identifier).\nExample 1. Inconsistency with deduplicate_blocks_in_dependent_materialized_views 0 createtabletest(AInt64,DDate)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}')partitionbytoYYYYMM(D)orderbyA;creatematerializedviewtest_mvEngine=ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}')partitionbyDorderbyDasselectD,count()CNTfromtestgroupbyD;setmax_partitions_per_insert_block=1;-- trick to fail insert into MV. insertintotestselectnumber,today()+number%3fromnumbers(100);DB::Exception:Receivedfromlocalhost:9000.DB::Exception:Toomanypartitionsselectcount()fromtest;┌─count()─┐│100│-- Insert was successful into the test table └─────────┘selectsum(CNT)fromtest_mv;0rowsinset.Elapsed:0.001sec.-- Insert was unsuccessful into the test_mv table (DB::Exception) -- Let's try to retry insertion setmax_partitions_per_insert_block=100;-- disable trick insertintotestselectnumber,today()+number%3fromnumbers(100);-- insert retry / No error selectcount()fromtest;┌─count()─┐│100│-- insert was deduplicated └─────────┘selectsum(CNT)fromtest_mv;0rowsinset.Elapsed:0.001sec.-- Inconsistency! Unfortunatly insert into MV was deduplicated as well Example 2. Inconsistency with deduplicate_blocks_in_dependent_materialized_views 1 createtabletest(AInt64,DDate)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}')partitionbytoYYYYMM(D)orderbyA;creatematerializedviewtest_mvEngine=ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}')partitionbyDorderbyDasselectD,count()CNTfromtestgroupbyD;setdeduplicate_blocks_in_dependent_materialized_views=1;insertintotestselectnumber,today()fromnumbers(100);-- insert 100 rows insertintotestselectnumber,today()fromnumbers(100,100);-- insert another 100 rows selectcount()fromtest;┌─count()─┐│200│-- 200 rows in the source test table └─────────┘selectsum(CNT)fromtest_mv;┌─sum(CNT)─┐│100│-- Inconsistency! The second insert was falsely deduplicated because count() was = 100 both times └──────────┘Example 3. Solution: no inconsistency with deduplicate_blocks_in_dependent_materialized_views 1 Let’s add some artificial insert_id generated by the source of inserts:\ncreate table test (A Int64, D Date, insert_id Int64) Engine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by toYYYYMM(D) order by A; create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT, any(insert_id) insert_id from test group by D; set deduplicate_blocks_in_dependent_materialized_views=1; insert into test select number, today(), 333 from numbers(100); insert into test select number, today(), 444 from numbers(100,100); select count() from test; ┌─count()─┐ │ 200 │ └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 200 │ -- no inconsistency, the second (100) was not deduplicated because 333\u003c\u003e444 └──────────┘ set max_partitions_per_insert_block=1; -- trick to fail insert into MV. insert into test select number, today()+number%3, 555 from numbers(100); DB::Exception: Too many partitions for single INSERT block (more than 1) select count() from test; ┌─count()─┐ │ 300 │ -- insert is successful into the test table └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 200 │ -- insert was unsuccessful into the test_mv table └──────────┘ set max_partitions_per_insert_block=100; insert into test select number, today()+number%3, 555 from numbers(100); -- insert retry select count() from test; ┌─count()─┐ │ 300 │ -- insert was deduplicated └─────────┘ select sum(CNT) from test_mv; ┌─sum(CNT)─┐ │ 300 │ -- No inconsistency! Insert was not deduplicated. └──────────┘ Idea how to fix it in Clickhouse source code https://github.com/ClickHouse/ClickHouse/issues/30240\nFake (unused) metric to add uniqueness. create materialized view test_mv Engine = ReplicatedSummingMergeTree('/clickhouse/{cluster}/tables/{table}','{replica}') partition by D order by D as select D, count() CNT, sum( cityHash(*) ) insert_id from test group by D; ","categories":"","description":"How to make idempotent inserts into a materialized view\".","excerpt":"How to make idempotent inserts into a materialized view\".","ref":"/altinity-kb-schema-design/materialized-views/idempotent_inserts_mv/","tags":"","title":"Idempotent inserts into a materialized view"},{"body":"Decimal SELECT9.2::Decimal64(2)ASpostgresql_cast,toDecimal64(9.2,2)ASto_function,CAST(9.2,'Decimal64(2)')AScast_float_literal,CAST('9.2','Decimal64(2)')AScast_string_literal┌─postgresql_cast─┬─to_function─┬─cast_float_literal─┬─cast_string_literal─┐│9.2│9.19│9.19│9.2│└─────────────────┴─────────────┴────────────────────┴─────────────────────┘Float64 SELECTtoFloat64(15008753.)ASto_func,toFloat64('1.5008753E7')ASto_func_scientific,CAST('1.5008753E7','Float64')AScast_scientific┌──to_func─┬─to_func_scientific─┬────cast_scientific─┐│15008753│15008753.000000002│15008753.000000002│└──────────┴────────────────────┴────────────────────┘","categories":"","description":"Imprecise parsing of literal Decimal or Float64","excerpt":"Imprecise parsing of literal Decimal or Float64","ref":"/altinity-kb-queries-and-syntax/literal-decimal-or-float/","tags":"","title":"Imprecise parsing of literal Decimal or Float64"},{"body":"Insert Deduplication Replicated tables have a special feature insert deduplication (enabled by default).\nDocumentation: Data blocks are deduplicated. For multiple writes of the same data block (data blocks of the same size containing the same rows in the same order), the block is only written once. The reason for this is in case of network failures when the client application does not know if the data was written to the DB, so the INSERT query can simply be repeated. It does not matter which replica INSERTs were sent to with identical data. INSERTs are idempotent. Deduplication parameters are controlled by merge_tree server settings.\nExample createtabletest_insert(AInt64)Engine=ReplicatedMergeTree('/clickhouse/cluster_test/tables/{table}','{replica}')orderbyA;insertintotest_insertvalues(1);insertintotest_insertvalues(1);insertintotest_insertvalues(1);insertintotest_insertvalues(1);select*fromtest_insert;┌─A─┐│1│-- only one row has been inserted, the other rows were deduplicated └───┘altertabletest_insertdeletewhere1;-- that single row was removed insertintotest_insertvalues(1);select*fromtest_insert;0rowsinset.Elapsed:0.001sec.-- the last insert was deduplicated again, -- because `alter ... delete` does not clear deduplication checksums -- only `alter table drop partition` and `truncate` clear checksums In clickhouse-server.log you may see trace messages Block with ID ... already exists locally as part ... ignoring it\n# cat /var/log/clickhouse-server/clickhouse-server.log|grep test_insert|grep Block ..17:52:45.064974.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.068979.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.072883.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. ..17:52:45.076738.. Block with ID all_7615936253566048997_747463735222236827 already exists locally as part all_0_0_0; ignoring it. Deduplication checksums are stored in Zookeeper in /blocks table’s znode for each partition separately, so when you drop partition, they could be identified and removed for this partition. (during alter table delete it’s impossible to match checksums, that’s why checksums stay in Zookeeper).\nSELECTname,valueFROMsystem.zookeeperWHEREpath='/clickhouse/cluster_test/tables/test_insert/blocks'┌─name───────────────────────────────────────┬─value─────┐│all_7615936253566048997_747463735222236827│all_0_0_0│└────────────────────────────────────────────┴───────────┘insert_deduplicate setting Insert deduplication is controled by the insert_deduplicate setting\nLet’s disable it:\nsetinsert_deduplicate=0;-- insert_deduplicate is now disabled in this session insertintotest_insertvalues(1);insertintotest_insertvalues(1);insertintotest_insertvalues(1);select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┐│1││1││1│-- all 3 insterted rows are in the table └───┘altertabletest_insertdeletewhere1;insertintotest_insertvalues(1);insertintotest_insertvalues(1);select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┐│1││1│└───┘Insert deduplication is a user-level setting, it can be disabled in a session or in a user’s profile (insert_deduplicate=0).\nclickhouse-client --insert_deduplicate=0 ....\nHow to disable insert_deduplicate by default for all queries:\n# cat /etc/clickhouse-server/users.d/insert_deduplicate.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003cinsert_deduplicate\u003e0\u003c/insert_deduplicate\u003e  \u003c/default\u003e  \u003c/profile \u003c/yandex\u003e Other related settings: replicated_deduplication_window, replicated_deduplication_window_seconds, insert_deduplication_token.\nMore info: https://github.com/ClickHouse/ClickHouse/issues/16037 https://github.com/ClickHouse/ClickHouse/issues/3322\nNon-replicated MergeTree tables By default insert deduplication is disabled for non-replicated tables (for backward compatibility).\nIt can be enabled by the merge_tree setting non_replicated_deduplication_window.\nExample:\ncreatetabletest_insert(AInt64)Engine=MergeTreeorderbyAsettingsnon_replicated_deduplication_window=100;-- 100 - how many latest checksums to store insertintotest_insertvalues(1);insertintotest_insertvalues(1);insertintotest_insertvalues(1);insertintotest_insertvalues(2);insertintotest_insertvalues(2);select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┐│2││1│└───┘In case of non-replicated tables deduplication checksums are stored in files in the table’s folder:\ncat /var/lib/clickhouse/data/default/test_insert/deduplication_logs/deduplication_log_1.txt 1\tall_1_1_0\tall_7615936253566048997_747463735222236827 1\tall_4_4_0\tall_636943575226146954_4277555262323907666 Checksums calculation Checksums are calculated not from the inserted data but from formed parts.\nInsert data is separated to parts by table’s partitioning.\nParts contain rows sorted by the table’s order by and all values of functions (i.e. now()) or Default/Materialized columns are expanded.\nExample with partial insertion because of partitioning: createtabletest_insert(AInt64,BInt64)Engine=MergeTreepartitionbyBorderbyAsettingsnon_replicated_deduplication_window=100;insertintotest_insertvalues(1,1);insertintotest_insertvalues(1,1)(1,2);select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┬─B─┐│1│1││1│2│-- the second insert was skipped for only one partition!!! └───┴───┘Example with deduplication despite the rows order: droptabletest_insert;createtabletest_insert(AInt64,BInt64)Engine=MergeTreeorderby(A,B)settingsnon_replicated_deduplication_window=100;insertintotest_insertvalues(1,1)(1,2);insertintotest_insertvalues(1,2)(1,1);-- the order of rows is not equal with the first insert select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┬─B─┐│1│1││1│2│└───┴───┘2rowsinset.Elapsed:0.001sec.-- the second insert was skipped despite the rows order Example to demonstrate how Default/Materialize columns are expanded: droptabletest_insert;createtabletest_insert(AInt64,BInt64Defaultrand())Engine=MergeTreeorderbyAsettingsnon_replicated_deduplication_window=100;insertintotest_insert(A)values(1);-- B calculated as rand() insertintotest_insert(A)values(1);-- B calculated as rand() select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┬──────────B─┐│1│3467561058││1│3981927391│└───┴────────────┘insertintotest_insert(A,B)values(1,3467561058);-- B is not calculated / will be deduplicated select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┬──────────B─┐│1│3981927391││1│3467561058│└───┴────────────┘Example to demonstrate how functions are expanded: droptabletest_insert;createtabletest_insert(AInt64,BDateTime64)Engine=MergeTreeorderbyAsettingsnon_replicated_deduplication_window=100;insertintotest_insertvalues(1,now64());....insertintotest_insertvalues(1,now64());select*fromtest_insertformatPrettyCompactMonoBlock;┌─A─┬───────────────────────B─┐│1│2022-01-3115:43:45.364││1│2022-01-3115:43:41.944│└───┴─────────────────────────┘insert_deduplication_token Since Clikhouse 22.2 there is a new setting insert_dedupplication_token. This setting allows you to define an explicit token that will be used for deduplication instead of calculating a checksum from the inserted data.\nCREATETABLEtest_table(AInt64)ENGINE=MergeTreeORDERBYASETTINGSnon_replicated_deduplication_window=100;INSERTINTOtest_tableValuesSETTINGSinsert_deduplication_token='test'(1);-- the next insert won't be deduplicated because insert_deduplication_token is different INSERTINTOtest_tableValuesSETTINGSinsert_deduplication_token='test1'(1);-- the next insert will be deduplicated because insert_deduplication_token -- is the same as one of the previous INSERTINTOtest_tableValuesSETTINGSinsert_deduplication_token='test'(2);SELECT*FROMtest_table┌─A─┐│1│└───┘┌─A─┐│1│└───┘","categories":"","description":"Insert Deduplication / Insert idempotency , insert_deduplicate setting.","excerpt":"Insert Deduplication / Insert idempotency , insert_deduplicate …","ref":"/altinity-kb-schema-design/insert_deduplication/","tags":"","title":"Insert Deduplication / Insert idempotency"},{"body":"Replication using MaterializeMySQL.  https://clickhouse.tech/docs/en/engines/database-engines/materialized-mysql/ https://translate.google.com/translate?sl=auto\u0026tl=en\u0026u=https://www.jianshu.com/p/d0d4306411b3 https://raw.githubusercontent.com/ClickHouse/clickhouse-presentations/master/meetup47/materialize_mysql.pdf  It reads mysql binlog directly and transform queries into something which clickhouse can support. Supports updates and deletes (under the hood implemented via something like ReplacingMergeTree with enforced FINAL and ‘deleted’ flag). Status is ’experimental’, there are quite a lot of known limitations and issues, but some people use it. The original author of that went to another project, and the main team don’t have a lot of resource to improve that for now (more important thing in the backlog)\nThe replication happens on the mysql database level.\nReplication using debezium + Kafka Debezium can read the binlog and transform it to Kafka messages. You can later capture the stream of message on ClickHouse side and process it as you like. Please remeber that currently Kafka engine supports only at-least-once delivery guarantees.\nIt’s used by several companies, quite nice \u0026 flexible. But initial setup may require some efforts.\nSame as above but using https://maxwells-daemon.io/ instead of debezium. Have no experience / feedback there, but should be very similar to debezium.\nReplication using clickhouse-mysql See https://altinity.com/blog/2018/6/30/realtime-mysql-clickhouse-replication-in-practice\nThat was done long time ago in altinity for one use-case, and it seem like it was never used outside of that. It’s a python application with lot of switches which can copy a schema or read binlog from mysql and put it to clickhouse. Not supported currently. But it’s just a python, so maybe can be adjusted to different needs.\nAccessing MySQL data via integration engines from inside clickhouse. MySQL table engine / table function, or MySQL database engine - clickhouse just connects to mysql server as a client, and can do normal selects.\nWe had webinar about that a year ago: https://www.youtube.com/watch?v=44kO3UzIDLI\nUsing that you can easily create some ETL script which will copy the data from mysql to clickhouse regularly, i.e. something like\nINSERTINTOclickhouse_tableSELECT*FROMmysql_tableWHEREid\u003e...Works great if you have append only table in MySQL.\nIn newer clickhouse versions you can query this was also sharded / replicated MySQL cluster - see ExternalDistributed\nMySQL dictionaries There are also MySQL dictionaries, which can be very nice alternative for storing some dimensions information in star schema.\n https://clickhouse.com/docs/en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources/#dicts-external_dicts_dict_sources-mysql https://github.com/ClickHouse/ClickHouse/blob/9f5cd35a6963cc556a51218b46b0754dcac7306a/tests/testflows/aes_encryption/tests/compatibility/mysql/dictionary.py#L35-L51  ","categories":"","description":"Integration Clickhouse with MySQL","excerpt":"Integration Clickhouse with MySQL","ref":"/altinity-kb-integrations/mysql-clickhouse/","tags":"","title":"MySQL"},{"body":"Q. I get errors:\nFile not found: /var/log/clickhouse-server/clickhouse-server.log.0. File not found: /var/log/clickhouse-server/clickhouse-server.log.8.gz.  ...   File not found: /var/log/clickhouse-server/clickhouse-server.err.log.0, Stack trace (when copying this message, always include the lines below): 0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c2b345 in /usr/bin/clickhouse 1. Poco::PurgeOneFileStrategy::purge(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026) @ 0x11c84618 in /usr/bin/clickhouse 2. Poco::FileChannel::log(Poco::Message const\u0026) @ 0x11c314cc in /usr/bin/clickhouse 3. DB::OwnFormattingChannel::logExtended(DB::ExtendedLogMessage const\u0026) @ 0x8681402 in /usr/bin/clickhouse 4. DB::OwnSplitChannel::logSplit(Poco::Message const\u0026) @ 0x8682fa8 in /usr/bin/clickhouse 5. DB::OwnSplitChannel::log(Poco::Message const\u0026) @ 0x8682e41 in /usr/bin/clickhouse A. Check if you have proper permission to a log files folder, and enough disk space (\u0026 inode numbers) on the block device used for logging.\nls -la /var/log/clickhouse-server/ df -Th df -Thi Q. How to configure logging in clickhouse?\nA. See https://github.com/ClickHouse/ClickHouse/blob/ceaf6d57b7f00e1925b85754298cf958a278289a/programs/server/config.xml#L9-L62\n","categories":"","description":"Logging configuration and issues","excerpt":"Logging configuration and issues","ref":"/altinity-kb-setup-and-maintenance/logging/","tags":"","title":"Logging"},{"body":"Alternative to doing that by minmax skip index.\nCREATETABLEpart_key_multiple_dates(`key`UInt32,`date`Date,`time`DateTime,`created_at`DateTime,`inserted_at`DateTime)ENGINE=MergeTreePARTITIONBY(toYYYYMM(date),ignore(created_at),ignore(inserted_at))ORDERBY(key,time);INSERTINTOpart_key_multiple_datesSELECTnumber,toDate(x),now()+intDiv(number,10)ASx,x-(rand()%100),x+(rand()%100)FROMnumbers(100000000);SELECTcount()FROMpart_key_multiple_datesWHEREdate\u003e(now()+toIntervalDay(105));┌─count()─┐│8434210│└─────────┘1rowsinset.Elapsed:0.022sec.Processed11.03millionrows,22.05MB(501.94millionrows/s.,1.00GB/s.)SELECTcount()FROMpart_key_multiple_datesWHEREinserted_at\u003e(now()+toIntervalDay(105));┌─count()─┐│9279818│└─────────┘1rowsinset.Elapsed:0.046sec.Processed11.03millionrows,44.10MB(237.64millionrows/s.,950.57MB/s.)SELECTcount()FROMpart_key_multiple_datesWHEREcreated_at\u003e(now()+toIntervalDay(105));┌─count()─┐│9279139│└─────────┘1rowsinset.Elapsed:0.043sec.Processed11.03millionrows,44.10MB(258.22millionrows/s.,1.03GB/s.)","categories":"","description":"How to put multiple correlated date-like columns in partition key without generating a lot of partitions in case not exact match between them.","excerpt":"How to put multiple correlated date-like columns in partition key …","ref":"/altinity-kb-queries-and-syntax/multiple-date-column-in-partition-key/","tags":"","title":"Multiple aligned date columns in PARTITION BY expression"},{"body":"ODBC Driver for ClickHouse. ODBC interface for ClickHouse RDBMS.\nLicensed under the Apache 2.0.\nInstallation and usage Windows  Download the latest release. On 64bit system you usually need both 32 bit and 64 bit drivers. Install (usually you will need ANSI driver, but better to install both versions, see below). Configure ClickHouse DSN.  Note: that install driver linked against MDAC (which is default for Windows), some non-windows native applications (cygwin / msys64 based) may require driver linked agains unixodbc. Build section below.\nMacOS  Install homebrew. Install driver  brew install https://raw.githubusercontent.com/proller/homebrew-core/chodbc/Formula/clickhouse-odbc.rb Add clickhouse DSN configuration into ~/.odbc.ini file. (sample)  Note: that install driver linked against iodbc (which is default for Mac), some homebrew applications (like python) may require unixodbc driver to work properly. In that case see Build section below.\nLinux  DEB/RPM packaging is not provided yet, please build \u0026 install the driver from sources. Add clickhouse DSN configuration into ~/.odbc.ini file. (sample)  Configuration On Linux / Max you configure DSN by adding new desctions in ~/.odbc.ini (See sample file: https://github.com/ClickHouse/clickhouse-odbc/blob/fd74398b50201ab13b535cdfab57bca86e588b37/packaging/odbc.ini.sample )\nOn Windows you can create/edit DSN using GUI tool through Control Panel.\nThe list of DSN parameters recognized by the driver is as follows:\n   Parameter Default value Description     Url empty URL that points to a running ClickHouse instance, may include username, password, port, database, etc.   Proto deduced from Url, or from Port and SSLMode: https if 443 or 8443 or SSLMode is not empty, http otherwise Protocol, one of: http, https   Server or Host deduced from Url IP or hostname of a server with a running ClickHouse instance on it   Port deduced from Url, or from Proto: 8443 if https, 8123 otherwise Port on which the ClickHouse instance is listening   Path /query Path portion of the URL   UID or Username default User name   PWD or Password empty Password   Database default Database name to connect to   Timeout 30 Connection timeout   SSLMode empty Certificate verification method (used by TLS/SSL connections, ignored in Windows), one of: allow, prefer, require, use allow to enable \u003ccode\u003eSSL_VERIFY_PEER\u003c/code\u003e TLS/SSL certificate verification mode, \u003ccode\u003eSSL_VERIFY_PEER | SSL_VERIFY_FAIL_IF_NO_PEER_CERT\u003c/code\u003e is used otherwise   PrivateKeyFile empty Path to private key file (used by TLS/SSL connections), can be empty if no private key file is used   CertificateFile empty Path to certificate file (used by TLS/SSL connections, ignored in Windows), if the private key and the certificate are stored in the same file, this can be empty if PrivateKeyFile is specified   CALocation empty Path to the file or directory containing the CA/root certificates (used by TLS/SSL connections, ignored in Windows)   DriverLog on if CMAKE_BUILD_TYPE is Debug, off otherwise Enable or disable the extended driver logging   DriverLogFile \\temp\\clickhouse-odbc-driver.log on Windows, /tmp/clickhouse-odbc-driver.log otherwise Path to the extended driver log file (used when DriverLog is on)    Troubleshooting \u0026 bug reporting If some software doesn’t work properly with that driver, but works good with other drivers - we will be appritiate if you will be able to collect debug info.\nTo debug issues with the driver, first things that need to be done are:\n enabling driver manager tracing. Links may contain some irrelevant vendor-specific details.  on Windows/MDAC: 1, 2, 3 on Mac/iODBC: 1, 2 on Linux/unixODBC: 1, 2   enabling driver logging, see DriverLog and DriverLogFile DSN parameters above making sure that the application is allowed to create and write these driver log and driver manager trace files follow the steps leading to the issue.  Collected log files will help to diagnose \u0026 solve the issue.\nDriver Managers Note, that since ODBC drivers are not used directly by a user, but rather accessed through applications, which in their turn access the driver through ODBC driver manager, user have to install the driver for the same architecture (32- or 64-bit) as the application that is going to access the driver. Moreover, both the driver and the application must be compiled for (and actually use during run-time) the same ODBC driver manager implementation (we call them “ODBC providers” here). There are three supported ODBC providers:\n ODBC driver manager associated with MDAC (Microsoft Data Access Components, sometimes referenced as WDAC, Windows Data Access Components) - the standard ODBC provider of Windows UnixODBC - the most common ODBC provider in Unix-like systems. Theoretically, could be used in Cygwin or MSYS/MinGW environments in Windows too. iODBC - less common ODBC provider, mainly used in Unix-like systems, however, it is the standard ODBC provider in macOS. Theoretically, could be used in Cygwin or MSYS/MinGW environments in Windows too.  If you don’t see a package that matches your platforms, or the version of your system is significantly different than those of the available packages, or maybe you want to try a bleeding edge version of the code that hasn’t been released yet, you can always build the driver manually from sources.\nNote, that it is always a good idea to install the driver from the corresponding native package (.msi, etc., which you can also easily create if you are building from sources), than use the binaries that were manually copied to some folder.\nBuilding from sources The general requirements for building the driver from sources are as follows:\n CMake 3.12 and later C++17 and C11 capable compiler toolchain:  Clang 4 and later GCC 7 and later Xcode 10 and later Microsoft Visual Studio 2017 and later   ODBC Driver manager (MDAC / unixodbc / iODBC) SSL library (openssl)  Generic build scenario:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git cd clickhouse-odbc mkdir build cd build cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo .. cmake --build . -C RelWithDebInfo Additional requirements exist for each platform, which also depend on whether packaging and/or testing is performed.\nLinux/macOS Execute the following in the terminal to install needed dependencies:\n# on Red Hat/CentOS (tested on CentOS 7) sudo yum groupinstall \"Development Tools\" sudo yum install centos-release-scl sudo yum install devtoolset-8 sudo yum install git cmake openssl-devel unixODBC-devel # You may use libiodbc-devel INSTEAD of unixODBC-devel scl enable devtoolset-8 -- bash # Enable Software collections for that terminal session, to use newer versions of complilers  # on Ubuntu (tested on Ubuntu 18.10, for older versions you may need to install newer c++ compiler and cmake versions) sudo apt install build-essential git cmake libpoco-dev libssl-dev unixodbc-dev # You may use libiodbc-devel INSEAD of unixODBC-devel  # MacOS:  # You will need Xcode 10 or later and Command Line Tools to be installed, as well as [Homebrew](https://brew.sh/). brew install git cmake make poco openssl libiodbc # You may use unixodbc INSTEAD of libiodbc  Note: usually on Linux you use unixODBC driver manager, and on Mac - iODBC. In some (rare) cases you may need use other driver manager, please do it only if you clearly understand the differencies. Driver should be used with the driver manager it was linked to.\nClone the repo with submodules:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git Enter the cloned source tree, create a temporary build folder, and generate a Makefile for the project in it:\ncd clickhouse-odbc mkdir build cd build  # Configuration options for the project can be specified in the next command in a form of '-Dopt=val' # For MacOS: you may also add '-G Xcode' to the next command, in order to use Xcode as a build system or IDE, and generate the solution and project files instead of Makefile. cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo .. Build the generated solution in-place:\ncmake --build . -C RelWithDebInfo cmake --build . -C RelWithDebInfo --target package …and, optionally, run tests (note, that for non-unit tests, preconfigured driver and DSN entries must exist, that point to the binaries generated in this build folder):\ncmake --build . -C RelWithDebInfo --target test For MacOS: if you configured the project with ‘-G Xcode’ initially, open the IDE and build all, package, and test targets manually from there\ncmake --open . Windows CMake bundled with the recent versions of Visual Studio can be used.\nAn SDK required for building the ODBC driver is included in Windows SDK, which in its turn is also bundled with Visual Studio.\nYou will need to install WiX toolset to be able to generate .msi packages. You can download and install it from WiX toolset home page.\nAll of the following commands have to be issued in Visual Studio Command Prompt:\n use x86 Native Tools Command Prompt for VS 2019 or equivalent for 32-bit builds use x64 Native Tools Command Prompt for VS 2019 or equivalent for 64-bit builds  Clone the repo with submodules:\ngit clone --recursive git@github.com:ClickHouse/clickhouse-odbc.git Enter the cloned source tree, create a temporary build folder, and generate the solution and project files in it:\ncd clickhouse-odbc mkdir build cd build  # Configuration options for the project can be specified in the next command in a form of '-Dopt=val'  # Use the following command for 32-bit build only. cmake -A Win32 -DCMAKE_BUILD_TYPE=RelWithDebInfo ..  # Use the following command for 64-bit build only. cmake -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo .. Build the generated solution in-place:\ncmake --build . -C RelWithDebInfo cmake --build . -C RelWithDebInfo --target package …and, optionally, run tests (note, that for non-unit tests, preconfigured driver and DSN entries must exist, that point to the binaries generated in this build folder):\ncmake --build . -C RelWithDebInfo --target test …or open the IDE and build all, package, and test targets manually from there:\ncmake --open . cmake options The list of configuration options recognized during the CMake generation step is as follows:\n   Option Default value Description     CMAKE_BUILD_TYPE RelWithDebInfo Build type, one of: Debug, Release, RelWithDebInfo   CH_ODBC_ENABLE_SSL ON Enable TLS/SSL (required for utilizing https:// interface, etc.)   CH_ODBC_ENABLE_INSTALL ON Enable install targets (required for packaging)   CH_ODBC_ENABLE_TESTING inherits value of BUILD_TESTING Enable test targets   CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES ON Prefer bundled over system variants of third party libraries   CH_ODBC_PREFER_BUNDLED_POCO inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of Poco library   CH_ODBC_PREFER_BUNDLED_SSL inherits value of CH_ODBC_PREFER_BUNDLED_POCO Prefer bundled over system variants of TLS/SSL library   CH_ODBC_PREFER_BUNDLED_GOOGLETEST inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of Google Test library   CH_ODBC_PREFER_BUNDLED_NANODBC inherits value of CH_ODBC_PREFER_BUNDLED_THIRD_PARTIES Prefer bundled over system variants of nanodbc library   CH_ODBC_RUNTIME_LINK_STATIC OFF Link with compiler and language runtime statically   CH_ODBC_THIRD_PARTY_LINK_STATIC ON Link with third party libraries statically   CH_ODBC_DEFAULT_DSN_ANSI ClickHouse DSN (ANSI) Default ANSI DSN name   CH_ODBC_DEFAULT_DSN_UNICODE ClickHouse DSN (Unicode) Default Unicode DSN name   TEST_DSN inherits value of CH_ODBC_DEFAULT_DSN_ANSI ANSI DSN name to use in tests   TEST_DSN_W inherits value of CH_ODBC_DEFAULT_DSN_UNICODE Unicode DSN name to use in tests    Packaging / redistributing the driver You can just copy the library to another computer, in that case you need to\n install run-time dependencies on target computer  Windows:  MDAC driver manager (preinstalled on all modern Windows systems) C++ Redistributable for Visual Studio 2017 or same for 2019, etc.   Linux    # CentOS / RedHat sudo yum install openssl unixODBC  # Debian/Ubuntu sudo apt install openssl unixodbc  MacOS (assuming you have Homebrew installed):  brew install poco openssl libiodbc register the driver so that the corresponding ODBC provider is able to locate it.  All this involves modifying a dedicated registry keys in case of MDAC, or editing odbcinst.ini (for driver registration) and odbc.ini (for DSN definition) files for UnixODBC or iODBC, directly or indirectly.\nThis will be done automatically using some default values if you are installing the driver using native installers.\nOtherwise, if you are configuring manually, or need to modify the default configuration created by the installer, please see the exact locations of files (or registry keys) that need to be modified.\n","categories":"","description":"ODBC Driver for ClickHouse","excerpt":"ODBC Driver for ClickHouse","ref":"/altinity-kb-integrations/clickhouse-odbc/","tags":"","title":"ODBC Driver for ClickHouse"},{"body":"Part names \u0026 multiversion concurrency control Part name format is:\n\u003cpartitionid\u003e_\u003cmin_block_number\u003e_\u003cmax_block_number\u003e_\u003clevel\u003e_\u003cdata_version\u003e system.parts contains all the information parsed.\npartitionid is quite simple (it just comes from your partitioning key).\nWhat are block_numbers?\nDROP TABLE IF EXISTS part_names; create table part_names (date Date, n UInt8, m UInt8) engine=MergeTree PARTITION BY toYYYYMM(date) ORDER BY n; insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ │ 202203_2_2_0 │ 202203 │ 2 │ 2 │ 0 │ 2 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ │ 202203_2_2_0 │ 202203 │ 2 │ 2 │ 0 │ 2 │ │ 202203_3_3_0 │ 202203 │ 3 │ 3 │ 0 │ 3 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ As you can see every insert creates a new incremental block_number which is written in part names both as \u003cmin_block_number\u003e and \u003cmin_block_number\u003e (and the level is 0 meaning that the part was never merged).\nThose block numbering works in the scope of partition (for Replicated table) or globally across all partition (for plain MergeTree table).\nClickHouse always merge only continuous blocks . And new part names always refer to the minimum and maximum block numbers.\nOPTIMIZE TABLE part_names; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1 │ 202203 │ 1 │ 3 │ 1 │ 1 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ As you can see here - three parts (with block number 1,2,3) were merged and they formed the new part with name 1_3 as min/max block size. Level get incremented.\nNow even while previous (merged) parts still exists in filesystem for a while (as inactive) clickhouse is smart enough to understand that new part ‘covers’ same range of blocks as 3 parts of the prev ‘generation’\nThere might be a fifth section in the part name, data version.\nData version gets increased when a part mutates.\nEvery mutation takes one block number:\ninsert into part_names VALUES (now(), 0, 0); insert into part_names VALUES (now(), 0, 0); insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1 │ 202203 │ 1 │ 3 │ 1 │ 1 │ │ 202203_4_4_0 │ 202203 │ 4 │ 4 │ 0 │ 4 │ │ 202203_5_5_0 │ 202203 │ 5 │ 5 │ 0 │ 5 │ │ 202203_6_6_0 │ 202203 │ 6 │ 6 │ 0 │ 6 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); alter table part_names update m=n where 1; select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name───────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_3_1_7 │ 202203 │ 1 │ 3 │ 1 │ 7 │ │ 202203_4_4_0_7 │ 202203 │ 4 │ 4 │ 0 │ 7 │ │ 202203_5_5_0_7 │ 202203 │ 5 │ 5 │ 0 │ 7 │ │ 202203_6_6_0_7 │ 202203 │ 6 │ 6 │ 0 │ 7 │ │ 202203_8_8_0 │ 202203 │ 8 │ 8 │ 0 │ 8 │ └────────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ OPTIMIZE TABLE part_names; select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = 'part_names' and active; ┌─name───────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_8_2_7 │ 202203 │ 1 │ 8 │ 2 │ 7 │ └────────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ ","categories":"","description":"Part names \u0026 multiversion concurrency control.","excerpt":"Part names \u0026 multiversion concurrency control.","ref":"/engines/mergetree-table-engine-family/part-naming-and-mvcc/","tags":"","title":"Part names \u0026 MVCC"},{"body":"Pre-Aggregation approaches: ETL vs Materialized Views vs Projections     ETL MV Projections     Realtime no yes yes   How complex queries can be used to build the preaggregaton any complex very simple   Impacts the insert speed no yes yes   Are inconsistancies possible Depends on ETL. If it process the errors properly - no. yes (no transactions / atomicity) no   Lifetime of aggregation any any Same as the raw data   Requirements need external tools/scripting is a part of database schema is a part of table schema   How complex to use in queries Depends on aggregation, usually simple, quering a separate table Depends on aggregation, sometimes quite complex, quering a separate table Very simple, quering the main table   Can work correctly with ReplacingMergeTree as a source Yes No No   Can work correctly with CollapsingMergeTree as a source Yes For simple aggregations For simple aggregations   Can be chained Yes (Usually with DAGs / special scripts) Yes (but may be not straightforward, and often is a bad idea) No   Resources needed to calculate the increment May be signigicant Usually tiny Usually tiny    ","categories":"","description":"ETL vs Materialized Views vs Projections in ClickHouse.","excerpt":"ETL vs Materialized Views vs Projections in ClickHouse.","ref":"/altinity-kb-schema-design/preaggregations/","tags":"","title":"Pre-Aggregation approaches"},{"body":"Precreate parts using clickhouse-local rm -rf /tmp/precreate_parts mkdir -p /tmp/precreate_parts/metadata/local/ cd /tmp/precreate_parts ## 1. Imagine we want to process this file: cat \u003c\u003cEOF \u003e /tmp/data.csv 1,2020-01-01,\"String\" 2,2020-02-02,\"Another string\" 3,2020-03-03,\"One more string\" 4,2020-01-02,\"String for first partition\" EOF ## 2. that is the metadata for the table we want to fill ## schema should match the schema of the table from server ## (the easiest way is just to copy it from the server) ## I've added sleepEachRow(0.5) here just to mimic slow insert cat \u003c\u003cEOF \u003e metadata/local/test.sql ATTACH TABLE local.test (id UInt64, d Date, s String, x MATERIALIZED sleepEachRow(0.5)) Engine=MergeTree ORDER BY id PARTITION BY toYYYYMM(d); EOF ## 3a. that is the metadata for the input file we want to read ## it should match the structure of source file ## use stdin to read from pipe cat \u003c\u003cEOF \u003e metadata/local/stdin.sql ATTACH TABLE local.stdin (id UInt64, d Date, s String) Engine=File(CSV, stdin); EOF ## 3b. Instead of stdin you can use file path cat \u003c\u003cEOF \u003e metadata/local/data_csv.sql ATTACH TABLE local.data_csv (id UInt64, d Date, s String) Engine=File(CSV, '/tmp/data.csv'); EOF ## All preparations done, ## the rest is simple: # option a (if 3a used) with pipe / reading stdin cat /tmp/data.csv | clickhouse-local --query \"INSERT INTO local.test SELECT * FROM local.stdin\" -- --path=. # option b (if 3b used) 0 with filepath cd /tmp/precreate_parts clickhouse-local --query \"INSERT INTO local.test SELECT * FROM local.data_csv\" -- --path=. # now you can check what was inserted (i did both options so i have doubled data) clickhouse-local --query \"SELECT _part,* FROM local.test ORDER BY id\" -- --path=. 202001_4_4_0\t1\t2020-01-01\tString 202001_1_1_0\t1\t2020-01-01\tString 202002_5_5_0\t2\t2020-02-02\tAnother string 202002_2_2_0\t2\t2020-02-02\tAnother string 202003_6_6_0\t3\t2020-03-03\tOne more string 202003_3_3_0\t3\t2020-03-03\tOne more string 202001_4_4_0\t4\t2020-01-02\tString for first partition 202001_1_1_0\t4\t2020-01-02\tString for first partition # But you can't do OPTIMIZE (local will die with coredump) :) That would be too good # clickhouse-local --query \"OPTIMIZE TABLE local.test FINAL\" -- --path=. ## now you can upload those parts to a server (in detached subfolder) and attach them. mfilimonov@laptop5591:/tmp/precreate_parts$ ls -la data/local/test/ total 40 drwxrwxr-x 9 mfilimonov mfilimonov 4096 paź 15 11:15 . drwxrwxr-x 3 mfilimonov mfilimonov 4096 paź 15 11:15 .. drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202001_1_1_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202001_4_4_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202002_2_2_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202002_5_5_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202003_3_3_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 202003_6_6_0 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 detached -rw-rw-r-- 1 mfilimonov mfilimonov 1 paź 15 11:15 format_version.txt mfilimonov@laptop5591:/tmp/precreate_parts$ ls -la data/local/test/202001_1_1_0/ total 44 drwxrwxr-x 2 mfilimonov mfilimonov 4096 paź 15 11:15 . drwxrwxr-x 9 mfilimonov mfilimonov 4096 paź 15 11:15 .. -rw-rw-r-- 1 mfilimonov mfilimonov 250 paź 15 11:15 checksums.txt -rw-rw-r-- 1 mfilimonov mfilimonov 79 paź 15 11:15 columns.txt -rw-rw-r-- 1 mfilimonov mfilimonov 1 paź 15 11:15 count.txt -rw-rw-r-- 1 mfilimonov mfilimonov 155 paź 15 11:15 data.bin -rw-rw-r-- 1 mfilimonov mfilimonov 144 paź 15 11:15 data.mrk3 -rw-rw-r-- 1 mfilimonov mfilimonov 10 paź 15 11:15 default_compression_codec.txt -rw-rw-r-- 1 mfilimonov mfilimonov 4 paź 15 11:15 minmax_d.idx -rw-rw-r-- 1 mfilimonov mfilimonov 4 paź 15 11:15 partition.dat -rw-rw-r-- 1 mfilimonov mfilimonov 16 paź 15 11:15 primary.idx ","categories":"","description":"Precreate parts using clickhouse-local.","excerpt":"Precreate parts using clickhouse-local.","ref":"/altinity-kb-setup-and-maintenance/precreate_parts_using_clickhouse_local.sh/","tags":"","title":"Precreate parts using clickhouse-local"},{"body":"How to pick an ORDER BY / PRIMARY KEY good order by usually have 3 to 5 columns, from lowest cardinal on the left (and the most important for filtering) to highest cardinal (and less important for filtering).\nPractical approach to create an good ORDER BY for a table:\n Pick the columns you use in filtering always The most important for filtering and the lowest cardinal should be the left-most. Typically it’s something like tenant_id Next column is more cardinal, less important. It can be rounded time sometimes, or site_id, or source_id, or group_id or something similar. repeat p.3 once again (or few times) if you added already all columns important for filtering and you still not addressing a single row with you pk - you can add more columns which can help to put similar records close to each other (to improve the compression) if you have something like hierarchy / tree-like relations between the columns - put there the records from ‘root’ to ’leaves’ for example (continent, country, cityname). This way clickhouse can do lookup by country / city even if continent is not specified (it will just ‘check all continents’) special variants of MergeTree may require special ORDER BY to make the record unique etc.  Some examples or good order by\nORDER BY (tenantid, site_id, utm_source, clientid, timestamp) ORDER BY (site_id, toStartOfHour(timestamp), sessionid, timestamp ) PRIMARY KEY (site_id, toStartOfHour(timestamp), sessionid) For Summing / Aggregating All dimentions go to ORDER BY, all metrics - outside of that.\nThe most important for filtering columns with the lowest cardinality should be the left most.\nIf number of dimentions is high it’s typically make sense to use a prefix of ORDER BY as a PRIMARY KEY to avoid polluting sparse index.\nExamples:\nORDER BY (tenant_id, hour, country_code, team_id, group_id, source_id) PRIMARY KEY (tenant_id, hour, country_code, team_id) For Replacing / Collapsing You need to keep all ‘mutable’ columns outside of ORDER BY, and have some unique id (a base to collapse duplicates) inside. Typically the right-most column is some row identifier. And it’s often not needed in sparse index (so PRIMARY KEY can be a prefix of ORDER BY) The rest consideration are the same.\nExamples:\nORDER BY (tenantid, site_id, eventid) -- utm_source is mutable, while tenantid, site_id is not PRIMARY KEY (tenantid, site_id) -- eventid is not used for filtering, needed only for collapsing duplicates PARTITION BY  Good size for single partition is something like 1-300Gb. For Summing/Replacing a but smaller (400Mb-40Gb) Better to avoid touching more that few dozens of partitions with typical SELECT query. Single insert should bring data to one or few partitions. The number of partitons in table - dozen or hundreds, not thousands.  The size of partitions you can check in system.parts table.\nExamples:\n-- for time-series: PARTITION BY toYYYY(timestamp) -- long retention, not too much data PARTITION BY toYYYYMM(timestamp) -- PARTITION BY toMonday(timestamp) -- PARTITION BY toDate(timestamp) -- PARTITION BY toStartOfHour(timestamp) -- short retention, lot of data -- for table with some incremental (non time-bounded) counter PARTITION BY intDiv(transaction_id, 1000000) -- for some dimention tables (always requested with WHERE userid) PARTITION BY userid % 16 For the small tables (smaller than few gigabytes) partitioning is usually not needed at all (just skip PARTITION BY expresssion when you create the table).\n","categories":"","description":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree table.","excerpt":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree …","ref":"/engines/mergetree-table-engine-family/pick-keys/","tags":"","title":"How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree-family table"},{"body":"Documentation https://clickhouse.com/docs/en/operations/access-rights/\nExample: 3 roles (dba, dashboard_ro, ingester_rw) You need to create an .xml file at each node to allow user default to manage access using SQL.\ncat /etc/clickhouse-server/users.d/access_management.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cusers\u003e  \u003cdefault\u003e  \u003caccess_management\u003e1\u003c/access_management\u003e  \u003c/default\u003e  \u003c/users\u003e \u003c/yandex\u003e createroledbaoncluster'{cluster}';grantallon*.*todbaoncluster'{cluster}';createuser`user1`identifiedby'pass1234'oncluster'{cluster}';grantdbatouser1oncluster'{cluster}';createroledashboard_rooncluster'{cluster}';grantselectondefault.*todashboard_rooncluster'{cluster}';grantdictGeton*.*todashboard_rooncluster'{cluster}';createsettingsprofileorreplaceprofile_dashboard_rooncluster'{cluster}'settingsmax_concurrent_queries_for_user=10READONLY,max_threads=16READONLY,max_memory_usage_for_user='30G'READONLY,max_memory_usage='30G'READONLY,max_execution_time=60READONLY,max_rows_to_read=1000000000READONLY,max_bytes_to_read='5000G'READONLYTOdashboard_ro;createuser`dash1`identifiedby'pass1234'oncluster'{cluster}';grantdashboard_rotodash1oncluster'{cluster}';createroleingester_rwoncluster'{cluster}';grantselect,insertondefault.*toingester_rwoncluster'{cluster}';createsettingsprofileorreplaceprofile_ingester_rwoncluster'{cluster}'settingsmax_concurrent_queries_for_user=40READONLY,-- user can run 40 queries (select, insert ...) simultaneously max_threads=10READONLY,-- each query can use up to 10 cpu (READONLY means user cannot override a value) max_memory_usage_for_user='30G'READONLY,-- all queries of the user can use up to 30G RAM max_memory_usage='25G'READONLY,-- each query can use up to 25G RAM max_execution_time=200READONLY,-- each query can executes no longer 200 seconds max_rows_to_read=1000000000READONLY,-- each query can read up to 1 billion rows max_bytes_to_read='5000G'READONLY-- each query can read up to 5 TB from a MergeTree TOingester_rw;createuser`ingester_app1`identifiedby'pass1234'oncluster'{cluster}';grantingester_rwtoingester_app1oncluster'{cluster}';check $ clickhouse-client -u dash1 --password pass1234  create table test ( A Int64) Engine=Log;  DB::Exception: dash1: Not enough privileges   $ clickhouse-client -u user1 --password pass1234  create table test ( A Int64) Engine=Log; Ok.  drop table test; Ok.   $ clickhouse-client -u ingester_app1 --password pass1234  select count() from system.numbers limit 1000000000000;  DB::Exception: Received from localhost:9000. DB::Exception: Limit for rows or bytes to read exceeded, max rows: 1.00 billion clean up showprofiles;┌─name─────────────────┐│default││profile_dashboard_ro││profile_ingester_rw││readonly│└──────────────────────┘dropprofileifexistsreadonlyoncluster'{cluster}';dropprofileifexistsprofile_dashboard_rooncluster'{cluster}';dropprofileifexistsprofile_ingester_rwoncluster'{cluster}';showroles;┌─name─────────┐│dashboard_ro││dba││ingester_rw│└──────────────┘droproleifexistsdbaoncluster'{cluster}';droproleifexistsdashboard_rooncluster'{cluster}';droproleifexistsingester_rwoncluster'{cluster}';showusers;┌─name──────────┐│dash1││default││ingester_app1││user1│└───────────────┘dropuserifexistsingester_app1oncluster'{cluster}';dropuserifexistsuser1oncluster'{cluster}';dropuserifexistsdash1oncluster'{cluster}';","categories":"","description":"Access Control and Account Management example (RBAC).","excerpt":"Access Control and Account Management example (RBAC).","ref":"/altinity-kb-setup-and-maintenance/rbac/","tags":"","title":"Access Control and Account Management example (RBAC)"},{"body":"Atomic \u0026 Ordinary databases. srv1 – good replica\nsrv2 – lost replica / we will restore it from srv1\ntest data (3 tables (atomic \u0026 ordinary databases)) srv1\ncreatedatabasetestatomiconcluster'{cluster}'engine=Atomic;createtabletestatomic.testoncluster'{cluster}'(AInt64,DDate,sString)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}')partitionbytoYYYYMM(D)orderbyA;insertintotestatomic.testselectnumber,today(),''fromnumbers(1000000);createdatabasetestordinaryoncluster'{cluster}'engine=Ordinary;createtabletestordinary.testoncluster'{cluster}'(AInt64,DDate,sString)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}')partitionbytoYYYYMM(D)orderbyA;insertintotestordinary.testselectnumber,today(),''fromnumbers(1000000);createtabledefault.testoncluster'{cluster}'(AInt64,DDate,sString)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}')partitionbytoYYYYMM(D)orderbyA;insertintodefault.testselectnumber,today(),''fromnumbers(1000000);destroy srv2 srv2\n/etc/init.d/clickhouse-server stop rm -rf /var/lib/clickhouse/* generate script to re-create databases (create_database.sql). srv1\n$cat/home/ubuntu/generate_schema.sqlSELECTconcat('CREATE DATABASE \"',name,'\" ENGINE = ',engine,' COMMENT \\'', comment, '\\';')FROMsystem.databasesWHEREnameNOTIN('INFORMATION_SCHEMA','information_schema','system','default');clickhouse-client\u003c/home/denis.zhuravlev/generate_schema.sql\u003ecreate_database.sqlcheck the result\n$ cat create_database.sql CREATE DATABASE \"testatomic\" ENGINE = Atomic COMMENT ''; CREATE DATABASE \"testordinary\" ENGINE = Ordinary COMMENT ''; transfer this create_database.sql to srv2 (scp / rsync)\nmake a copy of schema sql files (metadata_schema.tar) srv1\ncd /var/lib/clickhouse/ tar -cvhf /home/ubuntu/metadata_schema.tar metadata -h - is important! (-h, –dereference Follow symlinks; archive and dump the files they point to.)\ntransfer this metadata_schema.tar to srv2 (scp / rsync)\ncreate databases at srv2 srv2\n/etc/init.d/clickhouse-server start clickhouse-client \u003c create_database.sql /etc/init.d/clickhouse-server stop create tables at srv2 srv2\ncd /var/lib/clickhouse/ tar xkfv /home/ubuntu/metadata_schema.tar sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data /etc/init.d/clickhouse-server start tar xkfv -k is important! To save folders/symlinks created with create database ( -k, –keep-old-files Don’t replace existing files when extracting )\ncheck a recovery srv2\nSELECTcount()FROMtestatomic.test;┌─count()─┐│1000000│└─────────┘SELECTcount()FROMtestordinary.test;┌─count()─┐│1000000│└─────────┘SELECTcount()FROMdefault.test;┌─count()─┐│1000000│└─────────┘","categories":"","description":"Recovery after complete data loss ","excerpt":"Recovery after complete data loss ","ref":"/altinity-kb-setup-and-maintenance/recovery-after-complete-data-loss/","tags":"","title":"recovery-after-complete-data-loss"},{"body":"Settings \u003cclickhouse\u003e  \u003cstorage_configuration\u003e  \u003cdisks\u003e  \u003cs3\u003e  \u003ctype\u003es3\u003c/type\u003e  \u003cendpoint\u003ehttp://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/\u003c/endpoint\u003e  \u003caccess_key_id\u003eACCESS_KEY_ID\u003c/access_key_id\u003e  \u003csecret_access_key\u003eSECRET_ACCESS_KEY\u003c/secret_access_key\u003e  \u003cskip_access_check\u003etrue\u003c/skip_access_check\u003e  \u003csend_metadata\u003etrue\u003c/send_metadata\u003e  \u003c/s3\u003e \u003c/clickhouse\u003e   skip_access_check — if true, it’s possible to use read only credentials with regular MergeTree table. But you would need to disable merges (prefer_not_to_merge setting) on s3 volume as well.\n  send_metadata — if true, ClickHouse will populate s3 object with initial part \u0026 file path, which allow you to recover metadata from s3 and make debug easier.\n  Restore metadata from S3 Default Limitations:\n ClickHouse need RW access to this bucket  In order to restore metadata, you would need to create restore file in metadata_path/_s3_disk_name_ directory:\ntouch /var/lib/clickhouse/disks/_s3_disk_name_/restore In that case ClickHouse would restore to the same bucket and path and update only metadata files in s3 bucket.\nCustom Limitations:\n ClickHouse needs RO access to the old bucket and RW to the new. ClickHouse will copy objects in case of restoring to a different bucket or path.  If you would like to change bucket or path, you need to populate restore file with settings in key=value format:\ncat /var/lib/clickhouse/disks/_s3_disk_name_/restore  source_bucket=s3disk source_path=vol1/ Links https://altinity.com/blog/integrating-clickhouse-with-minio https://altinity.com/blog/clickhouse-object-storage-performance-minio-vs-aws-s3 https://altinity.com/blog/tips-for-high-performance-clickhouse-clusters-with-s3-object-storage\n","categories":"","description":"","excerpt":"Settings \u003cclickhouse\u003e  \u003cstorage_configuration\u003e  \u003cdisks\u003e  \u003cs3\u003e …","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3disk/","tags":"","title":"S3Disk"},{"body":"Symthom I see messages like: source parts size (...) is greater than the current maximum (...) in the logs and/or inside system.replication_queue\nCause Usually that means that there are already few big merges running. You can see the running merges using the query:\nSELECT * FROM system.merges That logic is needed to prevent picking a log of huge merges simultaneously (otherwise they will take all available slots and clickhouse will not be able to do smaller merges, which usally are important for keeping the number of parts stable).\nAction It is normal to see those messages on some stale replicas. And it should be resolved automatically after some time. So just wait \u0026 monitor system.merges \u0026 system.replication_queue tables, it should be resolved by it’s own.\nIf it happens often or don’t resolves by it’s own during some longer period of time, it could be caused by:\n increased insert pressure disk issues / high load (it works slow, not enought space etc.) high CPU load (not enough CPU power to catch up with merges) issue with table schemas leading to high merges pressure (high / increased number of tables / partitions / etc.)  Start from checking dmesg / system journals / clickhouse monitoring to find the anomalies.\n","categories":"","description":"source parts size (...) is greater than the current maximum (...)","excerpt":"source parts size (...) is greater than the current maximum (...)","ref":"/altinity-kb-setup-and-maintenance/source-pars-size-is-greater-than-maximum/","tags":"","title":"source parts size is greater than the current maximum"},{"body":"ClickHouse + Spark jdbc The trivial \u0026 natural way to talk to ClickHouse from Spark is using jdbc. There are 2 jdbc drivers:\n https://github.com/ClickHouse/clickhouse-jdbc/ https://github.com/housepower/ClickHouse-Native-JDBC#integration-with-spark  ClickHouse-Native-JDBC has some hints about integration with Spark even in the main README file.\n‘Official’ driver does support some conversion of complex data types (Roarring bitmaps) for Spark-Clickhouse integration: https://github.com/ClickHouse/clickhouse-jdbc/pull/596\nBut proper partitioning of the data (to spark partitions) may be tricky with jdbc.\nSome example snippets:\n https://markelic.de/how-to-access-your-clickhouse-database-with-spark-in-python/ https://stackoverflow.com/questions/60448877/how-can-i-write-spark-dataframe-to-clickhouse  Connectors  https://github.com/DmitryBe/spark-clickhouse (looks dead) https://github.com/VaBezruchko/spark-clickhouse-connector (is not actively maintained). https://github.com/housepower/spark-clickhouse-connector (actively developing connector from housepower - same guys as authors of ClickHouse-Native-JDBC)  via Kafka ClickHouse can produce / consume data from/to Kafka to exchange data with Spark.\nvia hdfs You can load data into hadoop/hdfs using sequence of statements like INSERT INTO FUNCTION hdfs(...) SELECT ... FROM clickhouse_table later process the data from hdfs by spark and do the same in reverse direction.\nvia s3 Similar to above but using s3.\nvia shell calls You can call other commands from Spark. Those commands can be clickhouse-client and/or clickhouse-local.\ndo you really need Spark? :) In many cases you can do everything inside ClickHouse without Spark help :) Arrays, Higher-order functions, machine learning, integration with lot of different things including the possibility to run some external code using executable dictionaries or UDF.\nMore info + some unordered links (mostly in Chinese / Russian)  Spark + ClickHouse: not a fight, but a symbiosis https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup28/spark_and_clickhouse.pdf (russian) Using a bunch of ClickHouse and Spark in MFI Soft https://www.youtube.com/watch?v=ID8eTnmag0s (russian) Spark read and write ClickHouse https://yerias.github.io/2020/12/08/clickhouse/9/#Jdbc%E6%93%8D%E4%BD%9Cclickhouse Spark reads and writes ClickHouse through jdbc https://blog.katastros.com/a?ID=01800-e40e1b3c-5fa4-4ea0-a3a8-f5e89ef0ce14 Spark JDBC write clickhouse operation summary https://www.jianshu.com/p/43f78c8a025b?hmsr=toutiao.io\u0026utm_campaign=toutiao.io\u0026utm_medium=toutiao.io\u0026utm_source=toutiao.io https://toutiao.io/posts/m63yw89/preview Spark-sql is based on Clickhouse’s DataSourceV2 data source extension (russian) https://www.cnblogs.com/mengyao/p/4689866.html Alibaba integration instructions https://www.alibabacloud.com/help/doc-detail/191192.htm Tencent integration instructions https://intl.cloud.tencent.com/document/product/1026/35884 Yandex DataProc demo: loading files from S3 to ClickHouse with Spark https://www.youtube.com/watch?v=N3bZW0_rRzI Clickhouse official documentation_Spark JDBC writes some pits of ClickHouse https://blog.csdn.net/weixin_39615984/article/details/111206050 ClickHouse data import (Flink, Spark, Kafka, MySQL, Hive) https://zhuanlan.zhihu.com/p/299094269 Baifendian Big Data Technical Team: Practice of ClickHouse data synchronization solutionbased on multiple Spark tasks. https://www.6aiq.com/article/1635461873075 SPARK-CLICKHOUSE-ES REAL-TIME PROJECT EIGHTH DAY-PRECISE ONE-TIME CONSUMPTION SAVE OFFSET. https://www.freesion.com/article/71421322524/ Still struggling with real-time data warehouse selection, Spark + ClickHouse makes yoamazing! https://dbaplus.cn/news-73-3806-1.html HDFS+ClickHouse+Spark: A lightweight big data analysis system from 0 to 1. https://juejin.cn/post/6850418114962653198 ClickHouse Clustering for Spark Developer http://blog.madhukaraphatak.com/clickouse-clustering-spark-developer/ «Иногда приходится заглядывать в код Spark»: Александр Морозов (SEMrush) об использовании Scala, Spark и ClickHouse. https://habr.com/ru/company/jugru/blog/341288/  ","categories":"","description":"Spark","excerpt":"Spark","ref":"/altinity-kb-integrations/spark/","tags":"","title":"ClickHouse + Spark"},{"body":"Successful ClickHouse deployment plan Stage 0. Build POC  Install single node clickhouse  https://clickhouse.com/docs/en/getting-started/tutorial/ https://clickhouse.com/docs/en/getting-started/install/ https://docs.altinity.com/altinitystablebuilds/stablequickstartguide/   Start with creating a single table (the biggest one), use MergeTree engine. Create ‘some’ schema (most probably it will be far from optimal). Prefer denormalized approach for all immutable dimensions, for mutable dimensions - consider dictionaries. Load some amount of data (at least 5 Gb, and 10 mln rows) - preferable the real one, or as close to real as possible. Usully the simplest options are either through CSV / TSV files (or insert into clickhouse_table select * FROM mysql(...) where ...) Create several representative queries. Check the columns cardinality, and appropriate types, use minimal needed type Review the partition by and order by. https://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/ Create the schema(s) with better/promising order by / partitioning, load data in. Pick the best schema. consider different improvements of particular columns (codecs / better data types etc.) https://kb.altinity.com/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/ If the performance of certain queries is not enough - consider using PREWHERE / skipping indexes Repeat 2-9 for next big table(s). Avoid scenarios when you need to join big tables. Pick the clients library for you programming language (the most mature are python / golang / java / c++), build some pipeline - for inserts (low QPS, lot of rows in singe insert, check acknowledgements \u0026 retry the same block on failures), ETLs if needed, some reporting layer (https://kb.altinity.com/altinity-kb-integrations/bi-tools/)   Stage 1. Planning the production setup  Collect more data / estimate insert speed, estimate the column sizes per day / month. Measure the speed of queries Consider improvement using materialized views / projections / dictionaries. Collect requirements (ha / number of simultaneous queries / insert pressure / ’exactly once’ etc) Do a cluster sizing estimation, plan the hardware  https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/ https://blog.cloudflare.com/clickhouse-capacity-estimation-framework/   plan the network, if needed - consider using LoadBalancers etc.  https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/   If you need sharding - consider different sharding approaches.  Stage 2. Preprod setup \u0026 developement  Install clickhouse in cluster - several nodes / VMs + zookeeper  https://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/ https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/ https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/   Create good config \u0026 automate config / os / restarts (ansible / puppet etc)  https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/ for docker: https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/ for k8, use clickhouse-operator OR https://kb.altinity.com/altinity-kb-kubernetes/altinity-kb-possible-issues-with-running-clickhouse-in-k8s/   Set up monitoring / log processing / alerts etc.  https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/#build-your-own-monitoring   Set up users.  https://kb.altinity.com/altinity-kb-setup-and-maintenance/rbac/   Think of schema management. Deploy the schema.  https://kb.altinity.com/altinity-kb-setup-and-maintenance/schema-migration-tools/   Design backup / failover strategies:  https://clickhouse.com/docs/en/operations/backup/ https://github.com/AlexAkulov/clickhouse-backup   Develop pipelines / queries, create test suite, CI/CD Do benchmark / stress tests Test configuration changes / server restarts / failovers / version upgrades Review the security topics (tls, limits / restrictions, network, passwords) Document the solution for operations  Stage 3. Production setup  Deploy the production setup (consider also canary / blue-greed deployments etc) Schedule ClickHouse upgrades every 6 to 12 months (if possible)  ","categories":"","description":"Successful ClickHouse deployment plan.","excerpt":"Successful ClickHouse deployment plan.","ref":"/altinity-kb-setup-and-maintenance/clickhouse-deployment-plan/","tags":"","title":"Successful ClickHouse deployment plan"},{"body":"Timeout exceeded ... or executing longer than distributed_ddl_task_timeout during OPTIMIZE FINAL Timeout may occur\n  due to the fact that the client reach timeout interval.\n in case of TCP / native clients - you can change send_timeout / recieve_timeout + tcp_keep_alive_timeout + driver timeout settings in case of HTTP clients - you can change http_send_timeout / http_receive_timeout + tcp_keep_alive_timeout + driver timeout settings    (in the case of ON CLUSTER queries) due to the fact that the timeout for query execution by shards ends\n see setting distributed_ddl_task_timeout    In the first case you additionally may get the misleading messages: Cancelling query. ... Query was cancelled.\nIn both cases, this does NOT stop the execution of the OPTIMIZE command. It continues to work even after the client is disconnected. You can see the progress of that in system.processes / show processlist / system.merges / system.query_log.\nThe same applies to queries like:\n INSERT ... SELECT CREATE TABLE ... AS SELECT CREATE MATERIALIZED VIEW ... POPULATE ...  It is possible to run a query with some special query_id and then poll the status from the processlist (in the case of a cluster, it can be a bit more complicated).\nSee also\n https://github.com/ClickHouse/ClickHouse/issues/6093 https://github.com/ClickHouse/ClickHouse/issues/7794 https://github.com/ClickHouse/ClickHouse/issues/28896 https://github.com/ClickHouse/ClickHouse/issues/19319  ","categories":"","description":"`Timeout exceeded ...` or `executing longer than distributed_ddl_task_timeout`  during `OPTIMIZE FINAL`.","excerpt":"`Timeout exceeded ...` or `executing longer than …","ref":"/altinity-kb-setup-and-maintenance/timeouts-during-optimize-final/","tags":"","title":"Timeouts during OPTIMIZE FINAL"},{"body":"Collecting query execution flamegraph using system.trace_log ClickHouse has embedded functionality to analyze the details of query performance.\nIt’s system.trace_log table.\nBy default it collects information only about queries when runs longer than 1 sec (and collects stacktraces every second).\nYou can adjust that per query using settings query_profiler_real_time_period_ns \u0026 query_profiler_cpu_time_period_ns.\nBoth works very similar (with desired interval dump the stacktraces of all the threads which execute the query). real timer - allows to ‘see’ the situtions when cpu was not working much, but time was spend for example on IO. cpu timer - allows to see the ‘hot’ points in calculations more accurately (skip the io time).\nTrying to collect stacktraces with a frequency higher than few KHz is usually not possible.\nTo check where most of the RAM is used you can collect stacktraces during memory allocations / deallocation, by using the setting memory_profiler_sample_probability.\nclickhouse-speedscope # install  wget https://github.com/laplab/clickhouse-speedscope/archive/refs/heads/master.tar.gz -O clickhouse-speedscope.tar.gz tar -xvzf clickhouse-speedscope.tar.gz cd clickhouse-speedscope-master/ pip3 install -r requirements.txt For debugging particular query:\nclickhouse-client SET query_profiler_cpu_time_period_ns=1000000; -- 1000 times per 'cpu' sec -- or SET query_profiler_real_time_period_ns=2000000; -- 500 times per 'real' sec. -- or SET memory_profiler_sample_probability=0.1; -- to debug the memory allocations SELECT ... \u003cyour select\u003e SYSTEM FLUSH LOGS; -- get the query_id from the clickhouse-client output or from system.query_log (also pay attention on query_id vs initial_query_id for distributed queries). Now let’s process that:\npython3 main.py \u0026 # start the proxy in background python3 main.py --query-id 908952ee-71a8-48a4-84d5-f4db92d45a5d # process the stacktraces fg # get the proxy from background Ctrl + C # stop it. To access ClickHouse with other username / password etc. - see the sources of https://github.com/laplab/clickhouse-speedscope/blob/master/main.py\nclickhouse-flamegraph Installation \u0026 usage instructions: https://github.com/Slach/clickhouse-flamegraph\npure flamegraph.pl examples git clone https://github.com/brendangregg/FlameGraph /opt/flamegraph clickhouse-client -q \"SELECT arrayStringConcat(arrayReverse(arrayMap(x -\u003e concat( addressToLine(x), '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, count() AS samples FROM system.trace_log WHERE event_time \u003e= subtractMinutes(now(),10) GROUP BY trace FORMAT TabSeparated\" | /opt/flamegraph/flamegraph.pl \u003e flamegraph.svg clickhouse-client -q \"SELECT arrayStringConcat((arrayMap(x -\u003e concat(splitByChar('/', addressToLine(x))[-1], '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, sum(abs(size)) AS samples FROM system.trace_log where trace_type = 'Memory' and event_date = today() group by trace order by samples desc FORMAT TabSeparated\" | /opt/flamegraph/flamegraph.pl \u003e allocs.svg clickhouse-client -q \"SELECT arrayStringConcat(arrayReverse(arrayMap(x -\u003e concat(splitByChar('/', addressToLine(x))[-1], '#', demangle(addressToSymbol(x)) ), trace)), ';') AS stack, count() AS samples FROM system.trace_log where trace_type = 'Memory' group by trace FORMAT TabSeparated SETTINGS allow_introspection_functions=1\" | /opt/flamegraph/flamegraph.pl \u003e ~/mem1.svg similar using perf apt-get update -y apt-get install -y linux-tools-common linux-tools-generic linux-tools-`uname -r`git apt-get install -y clickhouse-common-static-dbg clickhouse-common-dbg mkdir -p /opt/flamegraph git clone https://github.com/brendangregg/FlameGraph /opt/flamegraph perf record -F 99 -p $(pidof clickhouse) -G perf script \u003e /tmp/out.perf /opt/flamegraph/stackcollapse-perf.pl /tmp/out.perf | /opt/flamegraph/flamegraph.pl \u003e /tmp/flamegraph.svg also https://kb.altinity.com/altinity-kb-queries-and-syntax/troubleshooting/#flamegraph\nhttps://github.com/samber/grafana-flamegraph-panel/pull/2\n","categories":"","description":"Collecting query execution flamegraph using trace_log","excerpt":"Collecting query execution flamegraph using trace_log","ref":"/altinity-kb-queries-and-syntax/trace_log/","tags":"","title":"Collecting query execution flamegraphs using system.trace_log"},{"body":"Useful settings to turn on/Defaults that should be reconsidered Some setting that are not enabled by default.\n ttl_only_drop_parts  Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.\nWhen ttl_only_drop_parts is disabled (by default), the ClickHouse server only deletes expired rows according to their TTL.\nWhen ttl_only_drop_parts is enabled, the ClickHouse server drops a whole part when all rows in it are expired.\nDropping whole parts instead of partial cleaning TTL-d rows allows having shorter merge_with_ttl_timeout times and lower impact on system performance.\n join_use_nulls  Might be you not expect that join will be filled with default values for missing columns (instead of classic NULLs) during JOIN.\nSets the type of JOIN behaviour. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting.\nPossible values:\n0 — The empty cells are filled with the default value of the corresponding field type. 1 — JOIN behaves the same way as in standard SQL. The type of the corresponding field is converted to Nullable, and empty cells are filled with NULL.\n aggregate_functions_null_for_empty  Default behaviour is not compatible with ANSI SQL (ClickHouse avoids Nullable types by perfomance reasons)\nselectsum(x),avg(x)from(select1xwhere0);┌─sum(x)─┬─avg(x)─┐│0│nan│└────────┴────────┘setaggregate_functions_null_for_empty=1;selectsum(x),avg(x)from(select1xwhere0);┌─sumOrNull(x)─┬─avgOrNull(x)─┐│ᴺᵁᴸᴸ│ᴺᵁᴸᴸ│└──────────────┴──────────────┘","categories":"","description":"Useful settings to turn on.","excerpt":"Useful settings to turn on.","ref":"/altinity-kb-setup-and-maintenance/useful-setting-to-turn-on/","tags":"","title":"Useful settings to turn on/Defaults that should be reconsidered"},{"body":"Using array functions to mimic window functions alike behavior There are some usecases when you may want to mimic window functions using Arrays - as an optimization step, or to contol the memory better / use on-disk spiling, or just if you have old ClickHouse version.\nRunning difference sample DROPTABLEISEXISTStest_running_differenceCREATETABLEtest_running_differenceENGINE=LogASSELECTnumber%20ASid,toDateTime('2010-01-01 00:00:00')+(intDiv(number,20)*15)ASts,(number*round(xxHash32(number%20)/1000000))-round(rand()/1000000)ASvalFROMnumbers(100)SELECT*FROMtest_running_difference┌─id─┬──────────────────ts─┬────val─┐ │ 0 │ 2010-01-01 00:00:00 │ -1209 │ │ 1 │ 2010-01-01 00:00:00 │ 43 │ │ 2 │ 2010-01-01 00:00:00 │ 4322 │ │ 3 │ 2010-01-01 00:00:00 │ -25 │ │ 4 │ 2010-01-01 00:00:00 │ 13720 │ │ 5 │ 2010-01-01 00:00:00 │ 903 │ │ 6 │ 2010-01-01 00:00:00 │ 18062 │ │ 7 │ 2010-01-01 00:00:00 │ -2873 │ │ 8 │ 2010-01-01 00:00:00 │ 6286 │ │ 9 │ 2010-01-01 00:00:00 │ 13399 │ │ 10 │ 2010-01-01 00:00:00 │ 18320 │ │ 11 │ 2010-01-01 00:00:00 │ 11731 │ │ 12 │ 2010-01-01 00:00:00 │ 857 │ │ 13 │ 2010-01-01 00:00:00 │ 8752 │ │ 14 │ 2010-01-01 00:00:00 │ 23060 │ │ 15 │ 2010-01-01 00:00:00 │ 41902 │ │ 16 │ 2010-01-01 00:00:00 │ 39406 │ │ 17 │ 2010-01-01 00:00:00 │ 50010 │ │ 18 │ 2010-01-01 00:00:00 │ 57673 │ │ 19 │ 2010-01-01 00:00:00 │ 51389 │ │ 0 │ 2010-01-01 00:00:15 │ 66839 │ │ 1 │ 2010-01-01 00:00:15 │ 19440 │ │ 2 │ 2010-01-01 00:00:15 │ 74513 │ │ 3 │ 2010-01-01 00:00:15 │ 10542 │ │ 4 │ 2010-01-01 00:00:15 │ 94245 │ │ 5 │ 2010-01-01 00:00:15 │ 8230 │ │ 6 │ 2010-01-01 00:00:15 │ 87823 │ │ 7 │ 2010-01-01 00:00:15 │ -128 │ │ 8 │ 2010-01-01 00:00:15 │ 30101 │ │ 9 │ 2010-01-01 00:00:15 │ 54321 │ │ 10 │ 2010-01-01 00:00:15 │ 64078 │ │ 11 │ 2010-01-01 00:00:15 │ 31886 │ │ 12 │ 2010-01-01 00:00:15 │ 8749 │ │ 13 │ 2010-01-01 00:00:15 │ 28982 │ │ 14 │ 2010-01-01 00:00:15 │ 61299 │ │ 15 │ 2010-01-01 00:00:15 │ 95867 │ │ 16 │ 2010-01-01 00:00:15 │ 93667 │ │ 17 │ 2010-01-01 00:00:15 │ 114072 │ │ 18 │ 2010-01-01 00:00:15 │ 124279 │ │ 19 │ 2010-01-01 00:00:15 │ 109605 │ │ 0 │ 2010-01-01 00:00:30 │ 135082 │ │ 1 │ 2010-01-01 00:00:30 │ 37345 │ │ 2 │ 2010-01-01 00:00:30 │ 148744 │ │ 3 │ 2010-01-01 00:00:30 │ 21607 │ │ 4 │ 2010-01-01 00:00:30 │ 171744 │ │ 5 │ 2010-01-01 00:00:30 │ 14736 │ │ 6 │ 2010-01-01 00:00:30 │ 155349 │ │ 7 │ 2010-01-01 00:00:30 │ -3901 │ │ 8 │ 2010-01-01 00:00:30 │ 54303 │ │ 9 │ 2010-01-01 00:00:30 │ 89629 │ │ 10 │ 2010-01-01 00:00:30 │ 106595 │ │ 11 │ 2010-01-01 00:00:30 │ 54545 │ │ 12 │ 2010-01-01 00:00:30 │ 18903 │ │ 13 │ 2010-01-01 00:00:30 │ 48023 │ │ 14 │ 2010-01-01 00:00:30 │ 97930 │ │ 15 │ 2010-01-01 00:00:30 │ 152165 │ │ 16 │ 2010-01-01 00:00:30 │ 146130 │ │ 17 │ 2010-01-01 00:00:30 │ 174854 │ │ 18 │ 2010-01-01 00:00:30 │ 189194 │ │ 19 │ 2010-01-01 00:00:30 │ 170134 │ │ 0 │ 2010-01-01 00:00:45 │ 207471 │ │ 1 │ 2010-01-01 00:00:45 │ 54323 │ │ 2 │ 2010-01-01 00:00:45 │ 217984 │ │ 3 │ 2010-01-01 00:00:45 │ 31835 │ │ 4 │ 2010-01-01 00:00:45 │ 252709 │ │ 5 │ 2010-01-01 00:00:45 │ 21493 │ │ 6 │ 2010-01-01 00:00:45 │ 221271 │ │ 7 │ 2010-01-01 00:00:45 │ -488 │ │ 8 │ 2010-01-01 00:00:45 │ 76827 │ │ 9 │ 2010-01-01 00:00:45 │ 131066 │ │ 10 │ 2010-01-01 00:00:45 │ 149087 │ │ 11 │ 2010-01-01 00:00:45 │ 71934 │ │ 12 │ 2010-01-01 00:00:45 │ 25125 │ │ 13 │ 2010-01-01 00:00:45 │ 65274 │ │ 14 │ 2010-01-01 00:00:45 │ 135980 │ │ 15 │ 2010-01-01 00:00:45 │ 210910 │ │ 16 │ 2010-01-01 00:00:45 │ 200007 │ │ 17 │ 2010-01-01 00:00:45 │ 235872 │ │ 18 │ 2010-01-01 00:00:45 │ 256112 │ │ 19 │ 2010-01-01 00:00:45 │ 229371 │ │ 0 │ 2010-01-01 00:01:00 │ 275331 │ │ 1 │ 2010-01-01 00:01:00 │ 72668 │ │ 2 │ 2010-01-01 00:01:00 │ 290366 │ │ 3 │ 2010-01-01 00:01:00 │ 46074 │ │ 4 │ 2010-01-01 00:01:00 │ 329207 │ │ 5 │ 2010-01-01 00:01:00 │ 26770 │ │ 6 │ 2010-01-01 00:01:00 │ 287619 │ │ 7 │ 2010-01-01 00:01:00 │ -2207 │ │ 8 │ 2010-01-01 00:01:00 │ 100456 │ │ 9 │ 2010-01-01 00:01:00 │ 165688 │ │ 10 │ 2010-01-01 00:01:00 │ 194136 │ │ 11 │ 2010-01-01 00:01:00 │ 94113 │ │ 12 │ 2010-01-01 00:01:00 │ 35810 │ │ 13 │ 2010-01-01 00:01:00 │ 85081 │ │ 14 │ 2010-01-01 00:01:00 │ 170256 │ │ 15 │ 2010-01-01 00:01:00 │ 265445 │ │ 16 │ 2010-01-01 00:01:00 │ 254828 │ │ 17 │ 2010-01-01 00:01:00 │ 297238 │ │ 18 │ 2010-01-01 00:01:00 │ 323494 │ │ 19 │ 2010-01-01 00:01:00 │ 286252 │ └────┴─────────────────────┴────────┘  100 rows in set. Elapsed: 0.003 sec. runningDifference works only in blocks \u0026 require ordered data \u0026 problematic when group changes\nselectid,val,runningDifference(val)from(select*fromtest_running_differenceorderbyid,ts);┌─id─┬────val─┬─runningDifference(val)─┐ │ 0 │ -1209 │ 0 │ │ 0 │ 66839 │ 68048 │ │ 0 │ 135082 │ 68243 │ │ 0 │ 207471 │ 72389 │ │ 0 │ 275331 │ 67860 │ │ 1 │ 43 │ -275288 │ │ 1 │ 19440 │ 19397 │ │ 1 │ 37345 │ 17905 │ │ 1 │ 54323 │ 16978 │ │ 1 │ 72668 │ 18345 │ │ 2 │ 4322 │ -68346 │ │ 2 │ 74513 │ 70191 │ │ 2 │ 148744 │ 74231 │ │ 2 │ 217984 │ 69240 │ │ 2 │ 290366 │ 72382 │ │ 3 │ -25 │ -290391 │ │ 3 │ 10542 │ 10567 │ │ 3 │ 21607 │ 11065 │ │ 3 │ 31835 │ 10228 │ │ 3 │ 46074 │ 14239 │ │ 4 │ 13720 │ -32354 │ │ 4 │ 94245 │ 80525 │ │ 4 │ 171744 │ 77499 │ │ 4 │ 252709 │ 80965 │ │ 4 │ 329207 │ 76498 │ │ 5 │ 903 │ -328304 │ │ 5 │ 8230 │ 7327 │ │ 5 │ 14736 │ 6506 │ │ 5 │ 21493 │ 6757 │ │ 5 │ 26770 │ 5277 │ │ 6 │ 18062 │ -8708 │ │ 6 │ 87823 │ 69761 │ │ 6 │ 155349 │ 67526 │ │ 6 │ 221271 │ 65922 │ │ 6 │ 287619 │ 66348 │ │ 7 │ -2873 │ -290492 │ │ 7 │ -128 │ 2745 │ │ 7 │ -3901 │ -3773 │ │ 7 │ -488 │ 3413 │ │ 7 │ -2207 │ -1719 │ │ 8 │ 6286 │ 8493 │ │ 8 │ 30101 │ 23815 │ │ 8 │ 54303 │ 24202 │ │ 8 │ 76827 │ 22524 │ │ 8 │ 100456 │ 23629 │ │ 9 │ 13399 │ -87057 │ │ 9 │ 54321 │ 40922 │ │ 9 │ 89629 │ 35308 │ │ 9 │ 131066 │ 41437 │ │ 9 │ 165688 │ 34622 │ │ 10 │ 18320 │ -147368 │ │ 10 │ 64078 │ 45758 │ │ 10 │ 106595 │ 42517 │ │ 10 │ 149087 │ 42492 │ │ 10 │ 194136 │ 45049 │ │ 11 │ 11731 │ -182405 │ │ 11 │ 31886 │ 20155 │ │ 11 │ 54545 │ 22659 │ │ 11 │ 71934 │ 17389 │ │ 11 │ 94113 │ 22179 │ │ 12 │ 857 │ -93256 │ │ 12 │ 8749 │ 7892 │ │ 12 │ 18903 │ 10154 │ │ 12 │ 25125 │ 6222 │ │ 12 │ 35810 │ 10685 │ │ 13 │ 8752 │ -27058 │ │ 13 │ 28982 │ 20230 │ │ 13 │ 48023 │ 19041 │ │ 13 │ 65274 │ 17251 │ │ 13 │ 85081 │ 19807 │ │ 14 │ 23060 │ -62021 │ │ 14 │ 61299 │ 38239 │ │ 14 │ 97930 │ 36631 │ │ 14 │ 135980 │ 38050 │ │ 14 │ 170256 │ 34276 │ │ 15 │ 41902 │ -128354 │ │ 15 │ 95867 │ 53965 │ │ 15 │ 152165 │ 56298 │ │ 15 │ 210910 │ 58745 │ │ 15 │ 265445 │ 54535 │ │ 16 │ 39406 │ -226039 │ │ 16 │ 93667 │ 54261 │ │ 16 │ 146130 │ 52463 │ │ 16 │ 200007 │ 53877 │ │ 16 │ 254828 │ 54821 │ │ 17 │ 50010 │ -204818 │ │ 17 │ 114072 │ 64062 │ │ 17 │ 174854 │ 60782 │ │ 17 │ 235872 │ 61018 │ │ 17 │ 297238 │ 61366 │ │ 18 │ 57673 │ -239565 │ │ 18 │ 124279 │ 66606 │ │ 18 │ 189194 │ 64915 │ │ 18 │ 256112 │ 66918 │ │ 18 │ 323494 │ 67382 │ │ 19 │ 51389 │ -272105 │ │ 19 │ 109605 │ 58216 │ │ 19 │ 170134 │ 60529 │ │ 19 │ 229371 │ 59237 │ │ 19 │ 286252 │ 56881 │ └────┴────────┴────────────────────────┘ 100 rows in set. Elapsed: 0.005 sec. Arrays ! 1. Group \u0026 Collect the data into array you can collect several column by builing array of tuples:\nSELECT id, groupArray(tuple(ts, val)) FROM test_running_difference GROUP BY id ┌─id─┬─groupArray(tuple(ts, val))──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 0 │ [('2010-01-01 00:00:00',-1209),('2010-01-01 00:00:15',66839),('2010-01-01 00:00:30',135082),('2010-01-01 00:00:45',207471),('2010-01-01 00:01:00',275331)] │ │ 1 │ [('2010-01-01 00:00:00',43),('2010-01-01 00:00:15',19440),('2010-01-01 00:00:30',37345),('2010-01-01 00:00:45',54323),('2010-01-01 00:01:00',72668)] │ │ 2 │ [('2010-01-01 00:00:00',4322),('2010-01-01 00:00:15',74513),('2010-01-01 00:00:30',148744),('2010-01-01 00:00:45',217984),('2010-01-01 00:01:00',290366)] │ │ 3 │ [('2010-01-01 00:00:00',-25),('2010-01-01 00:00:15',10542),('2010-01-01 00:00:30',21607),('2010-01-01 00:00:45',31835),('2010-01-01 00:01:00',46074)] │ │ 4 │ [('2010-01-01 00:00:00',13720),('2010-01-01 00:00:15',94245),('2010-01-01 00:00:30',171744),('2010-01-01 00:00:45',252709),('2010-01-01 00:01:00',329207)] │ │ 5 │ [('2010-01-01 00:00:00',903),('2010-01-01 00:00:15',8230),('2010-01-01 00:00:30',14736),('2010-01-01 00:00:45',21493),('2010-01-01 00:01:00',26770)] │ │ 6 │ [('2010-01-01 00:00:00',18062),('2010-01-01 00:00:15',87823),('2010-01-01 00:00:30',155349),('2010-01-01 00:00:45',221271),('2010-01-01 00:01:00',287619)] │ │ 7 │ [('2010-01-01 00:00:00',-2873),('2010-01-01 00:00:15',-128),('2010-01-01 00:00:30',-3901),('2010-01-01 00:00:45',-488),('2010-01-01 00:01:00',-2207)] │ │ 8 │ [('2010-01-01 00:00:00',6286),('2010-01-01 00:00:15',30101),('2010-01-01 00:00:30',54303),('2010-01-01 00:00:45',76827),('2010-01-01 00:01:00',100456)] │ │ 9 │ [('2010-01-01 00:00:00',13399),('2010-01-01 00:00:15',54321),('2010-01-01 00:00:30',89629),('2010-01-01 00:00:45',131066),('2010-01-01 00:01:00',165688)] │ │ 10 │ [('2010-01-01 00:00:00',18320),('2010-01-01 00:00:15',64078),('2010-01-01 00:00:30',106595),('2010-01-01 00:00:45',149087),('2010-01-01 00:01:00',194136)] │ │ 11 │ [('2010-01-01 00:00:00',11731),('2010-01-01 00:00:15',31886),('2010-01-01 00:00:30',54545),('2010-01-01 00:00:45',71934),('2010-01-01 00:01:00',94113)] │ │ 12 │ [('2010-01-01 00:00:00',857),('2010-01-01 00:00:15',8749),('2010-01-01 00:00:30',18903),('2010-01-01 00:00:45',25125),('2010-01-01 00:01:00',35810)] │ │ 13 │ [('2010-01-01 00:00:00',8752),('2010-01-01 00:00:15',28982),('2010-01-01 00:00:30',48023),('2010-01-01 00:00:45',65274),('2010-01-01 00:01:00',85081)] │ │ 14 │ [('2010-01-01 00:00:00',23060),('2010-01-01 00:00:15',61299),('2010-01-01 00:00:30',97930),('2010-01-01 00:00:45',135980),('2010-01-01 00:01:00',170256)] │ │ 15 │ [('2010-01-01 00:00:00',41902),('2010-01-01 00:00:15',95867),('2010-01-01 00:00:30',152165),('2010-01-01 00:00:45',210910),('2010-01-01 00:01:00',265445)] │ │ 16 │ [('2010-01-01 00:00:00',39406),('2010-01-01 00:00:15',93667),('2010-01-01 00:00:30',146130),('2010-01-01 00:00:45',200007),('2010-01-01 00:01:00',254828)] │ │ 17 │ [('2010-01-01 00:00:00',50010),('2010-01-01 00:00:15',114072),('2010-01-01 00:00:30',174854),('2010-01-01 00:00:45',235872),('2010-01-01 00:01:00',297238)] │ │ 18 │ [('2010-01-01 00:00:00',57673),('2010-01-01 00:00:15',124279),('2010-01-01 00:00:30',189194),('2010-01-01 00:00:45',256112),('2010-01-01 00:01:00',323494)] │ │ 19 │ [('2010-01-01 00:00:00',51389),('2010-01-01 00:00:15',109605),('2010-01-01 00:00:30',170134),('2010-01-01 00:00:45',229371),('2010-01-01 00:01:00',286252)] │ └────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ Do needed ordering in each array For example - by second element of tuple:\nSELECT id, arraySort(x -\u003e (x.2), groupArray((ts, val))) FROM test_running_difference GROUP BY id ┌─id─┬─arraySort(lambda(tuple(x), tupleElement(x, 2)), groupArray(tuple(ts, val)))─────────────────────────────────────────────────────────────────────────────────┐ │ 0 │ [('2010-01-01 00:00:00',-1209),('2010-01-01 00:00:15',66839),('2010-01-01 00:00:30',135082),('2010-01-01 00:00:45',207471),('2010-01-01 00:01:00',275331)] │ │ 1 │ [('2010-01-01 00:00:00',43),('2010-01-01 00:00:15',19440),('2010-01-01 00:00:30',37345),('2010-01-01 00:00:45',54323),('2010-01-01 00:01:00',72668)] │ │ 2 │ [('2010-01-01 00:00:00',4322),('2010-01-01 00:00:15',74513),('2010-01-01 00:00:30',148744),('2010-01-01 00:00:45',217984),('2010-01-01 00:01:00',290366)] │ │ 3 │ [('2010-01-01 00:00:00',-25),('2010-01-01 00:00:15',10542),('2010-01-01 00:00:30',21607),('2010-01-01 00:00:45',31835),('2010-01-01 00:01:00',46074)] │ │ 4 │ [('2010-01-01 00:00:00',13720),('2010-01-01 00:00:15',94245),('2010-01-01 00:00:30',171744),('2010-01-01 00:00:45',252709),('2010-01-01 00:01:00',329207)] │ │ 5 │ [('2010-01-01 00:00:00',903),('2010-01-01 00:00:15',8230),('2010-01-01 00:00:30',14736),('2010-01-01 00:00:45',21493),('2010-01-01 00:01:00',26770)] │ │ 6 │ [('2010-01-01 00:00:00',18062),('2010-01-01 00:00:15',87823),('2010-01-01 00:00:30',155349),('2010-01-01 00:00:45',221271),('2010-01-01 00:01:00',287619)] │ │ 7 │ [('2010-01-01 00:00:30',-3901),('2010-01-01 00:00:00',-2873),('2010-01-01 00:01:00',-2207),('2010-01-01 00:00:45',-488),('2010-01-01 00:00:15',-128)] │ │ 8 │ [('2010-01-01 00:00:00',6286),('2010-01-01 00:00:15',30101),('2010-01-01 00:00:30',54303),('2010-01-01 00:00:45',76827),('2010-01-01 00:01:00',100456)] │ │ 9 │ [('2010-01-01 00:00:00',13399),('2010-01-01 00:00:15',54321),('2010-01-01 00:00:30',89629),('2010-01-01 00:00:45',131066),('2010-01-01 00:01:00',165688)] │ │ 10 │ [('2010-01-01 00:00:00',18320),('2010-01-01 00:00:15',64078),('2010-01-01 00:00:30',106595),('2010-01-01 00:00:45',149087),('2010-01-01 00:01:00',194136)] │ │ 11 │ [('2010-01-01 00:00:00',11731),('2010-01-01 00:00:15',31886),('2010-01-01 00:00:30',54545),('2010-01-01 00:00:45',71934),('2010-01-01 00:01:00',94113)] │ │ 12 │ [('2010-01-01 00:00:00',857),('2010-01-01 00:00:15',8749),('2010-01-01 00:00:30',18903),('2010-01-01 00:00:45',25125),('2010-01-01 00:01:00',35810)] │ │ 13 │ [('2010-01-01 00:00:00',8752),('2010-01-01 00:00:15',28982),('2010-01-01 00:00:30',48023),('2010-01-01 00:00:45',65274),('2010-01-01 00:01:00',85081)] │ │ 14 │ [('2010-01-01 00:00:00',23060),('2010-01-01 00:00:15',61299),('2010-01-01 00:00:30',97930),('2010-01-01 00:00:45',135980),('2010-01-01 00:01:00',170256)] │ │ 15 │ [('2010-01-01 00:00:00',41902),('2010-01-01 00:00:15',95867),('2010-01-01 00:00:30',152165),('2010-01-01 00:00:45',210910),('2010-01-01 00:01:00',265445)] │ │ 16 │ [('2010-01-01 00:00:00',39406),('2010-01-01 00:00:15',93667),('2010-01-01 00:00:30',146130),('2010-01-01 00:00:45',200007),('2010-01-01 00:01:00',254828)] │ │ 17 │ [('2010-01-01 00:00:00',50010),('2010-01-01 00:00:15',114072),('2010-01-01 00:00:30',174854),('2010-01-01 00:00:45',235872),('2010-01-01 00:01:00',297238)] │ │ 18 │ [('2010-01-01 00:00:00',57673),('2010-01-01 00:00:15',124279),('2010-01-01 00:00:30',189194),('2010-01-01 00:00:45',256112),('2010-01-01 00:01:00',323494)] │ │ 19 │ [('2010-01-01 00:00:00',51389),('2010-01-01 00:00:15',109605),('2010-01-01 00:00:30',170134),('2010-01-01 00:00:45',229371),('2010-01-01 00:01:00',286252)] │ └────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 20 rows in set. Elapsed: 0.004 sec. That can be rewritten like this:\nWITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows SELECT id, sorted_window_rows FROM test_running_difference GROUP BY id Apply needed logic arrayMap / arrayDifference etc WITH groupArray(tuple(ts, val)) as window_rows, arraySort(x -\u003e x.1, window_rows) as sorted_window_rows, arrayMap(x -\u003e x.2, sorted_window_rows) as sorted_window_rows_val_column, arrayDifference(sorted_window_rows_val_column) as sorted_window_rows_val_column_diff SELECT id, sorted_window_rows_val_column_diff FROM test_running_difference GROUP BY id ┌─id─┬─sorted_window_rows_val_column_diff─┐ │ 0 │ [0,68048,68243,72389,67860] │ │ 1 │ [0,19397,17905,16978,18345] │ │ 2 │ [0,70191,74231,69240,72382] │ │ 3 │ [0,10567,11065,10228,14239] │ │ 4 │ [0,80525,77499,80965,76498] │ │ 5 │ [0,7327,6506,6757,5277] │ │ 6 │ [0,69761,67526,65922,66348] │ │ 7 │ [0,2745,-3773,3413,-1719] │ │ 8 │ [0,23815,24202,22524,23629] │ │ 9 │ [0,40922,35308,41437,34622] │ │ 10 │ [0,45758,42517,42492,45049] │ │ 11 │ [0,20155,22659,17389,22179] │ │ 12 │ [0,7892,10154,6222,10685] │ │ 13 │ [0,20230,19041,17251,19807] │ │ 14 │ [0,38239,36631,38050,34276] │ │ 15 │ [0,53965,56298,58745,54535] │ │ 16 │ [0,54261,52463,53877,54821] │ │ 17 │ [0,64062,60782,61018,61366] │ │ 18 │ [0,66606,64915,66918,67382] │ │ 19 │ [0,58216,60529,59237,56881] │ └────┴────────────────────────────────────┘ 20 rows in set. Elapsed: 0.005 sec. You can do also a lot of magic with arrayEnumerate and accessing different values by their ids.\nNow you can return you arrays back to rows use arrayJoin\nWITHgroupArray(tuple(ts,val))aswindow_rows,arraySort(x-\u003ex.1,window_rows)assorted_window_rows,arrayMap(x-\u003ex.2,sorted_window_rows)assorted_window_rows_val_column,arrayDifference(sorted_window_rows_val_column)assorted_window_rows_val_column_diff,arrayJoin(sorted_window_rows_val_column_diff)asdiffSELECTid,diffFROMtest_running_differenceGROUPBYidor ARRAY JOIN\nSELECTid,diff,tsFROM(WITHgroupArray(tuple(ts,val))aswindow_rows,arraySort(x-\u003ex.1,window_rows)assorted_window_rows,arrayMap(x-\u003ex.2,sorted_window_rows)assorted_window_rows_val_columnSELECTid,arrayDifference(sorted_window_rows_val_column)assorted_window_rows_val_column_diff,arrayMap(x-\u003ex.1,sorted_window_rows)assorted_window_rows_ts_columnFROMtest_running_differenceGROUPBYid)ast1ARRAYJOINsorted_window_rows_val_column_diffasdiff,sorted_window_rows_ts_columnastsetc.\n","categories":"","description":"Using array functions to mimic window-functions alike behavior.","excerpt":"Using array functions to mimic window-functions alike behavior.","ref":"/altinity-kb-queries-and-syntax/array-functions-as-window/","tags":"","title":"Using array functions to mimic window-functions alike behavior"},{"body":"2022-03-15: 7 vulnerabulities in ClickHouse were published. See the details https://jfrog.com/blog/7-rce-and-dos-vulnerabilities-found-in-clickhouse-dbms/\nThose vulnerabilities were fixed by 2 PRs:\n https://github.com/ClickHouse/ClickHouse/pull/27136 https://github.com/ClickHouse/ClickHouse/pull/27743  All releases starting from v21.10.2.15 have that problem fixed.\nAlso, the fix was backported to 21.3 and 21.8 branches - versions v21.8.11.4-lts and v21.3.19.1-lts accordingly have the problem fixed (and all newer releases in those branches).\nThe latest Altinity stable releases also contain the bugfix.\n 21.8.13 21.3.20  If you use some older version we recommend upgrading.\nBefore the upgrade - please ensure that ports 9000 and 8123 are not exposed to the internet, so external clients who can try to exploit those vulnerabilities can not access your clickhouse node.\n","categories":"","description":"Vulnerabilities","excerpt":"Vulnerabilities","ref":"/upgrade/vulnerabilities/","tags":"","title":"Vulnerabilities"},{"body":"Using SHOW CREATE TABLE If you just want to see the current TTL settings on a table, you can look at the schema definition.\nSHOW CREATE TABLE events2_local FORMAT Vertical Query id: eba671e5-6b8c-4a81-a4d8-3e21e39fb76b Row 1: ────── statement: CREATE TABLE default.events2_local ( `EventDate` DateTime, `EventID` UInt32, `Value` String ) ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/default/events2_local', '{replica}') PARTITION BY toYYYYMM(EventDate) ORDER BY (EventID, EventDate) TTL EventDate + toIntervalMonth(1) SETTINGS index_granularity = 8192 This works even when there’s no data in the table. It does not tell you when the TTLs expire or anything specific to data in one or more of the table parts.\nUsing system.parts If you want to see the actually TTL values for specific data, run a query on system.parts. There are columns listing all currently applicable TTL limits for each part. (It does not work if the table is empty because there aren’t any parts yet.)\nSELECT * FROM system.parts WHERE (database = 'default') AND (table = 'events2_local') FORMAT Vertical Query id: 59106476-210f-4397-b843-9920745b6200 Row 1: ────── partition: 202203 name: 202203_0_0_0 ... database: default table: events2_local ... delete_ttl_info_min: 2022-04-27 21:26:30 delete_ttl_info_max: 2022-04-27 21:26:30 move_ttl_info.expression: [] move_ttl_info.min: [] move_ttl_info.max: [] default_compression_codec: LZ4 recompression_ttl_info.expression: [] recompression_ttl_info.min: [] recompression_ttl_info.max: [] group_by_ttl_info.expression: [] group_by_ttl_info.min: [] group_by_ttl_info.max: [] rows_where_ttl_info.expression: [] rows_where_ttl_info.min: [] rows_where_ttl_info.max: [] ","categories":"","description":"What are my TTL settings?","excerpt":"What are my TTL settings?","ref":"/altinity-kb-queries-and-syntax/ttl/what-are-my-ttls/","tags":"","title":"What are my TTL settings?"},{"body":"Reference script to install standalone Zookeeper for Ubuntu / Debian Tested on Ubuntu 20.\n# install java runtime environment sudo apt-get update sudo apt install default-jre  # prepare folders, logs folder should be on the low-latency disk. sudo mkdir -p /var/lib/zookeeper/data /var/lib/zookeeper/logs /etc/zookeeper /var/log/zookeeper /opt  # download and install files  export ZOOKEEPER_VERSION=3.6.3 wget https://dlcdn.apache.org/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -O /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz sudo tar -xvf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -C /opt rm -rf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz  # create the user  sudo groupadd -r zookeeper sudo useradd -r -g zookeeper --home-dir=/var/lib/zookeeper --shell=/bin/false zookeeper  # symlink pointing to the used version of zookeeper distibution sudo ln -s /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin /opt/zookeeper sudo chown -R zookeeper:zookeeper /var/lib/zookeeper /var/log/zookeeper /etc/zookeeper /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin sudo chown -h zookeeper:zookeeper /opt/zookeeper  # shortcuts in /usr/local/bin/ echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkCli.sh \"$@\"' | sudo tee /usr/local/bin/zkCli echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkServer.sh \"$@\"' | sudo tee /usr/local/bin/zkServer echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkCleanup.sh \"$@\"' | sudo tee /usr/local/bin/zkCleanup echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkSnapShotToolkit.sh \"$@\"' | sudo tee /usr/local/bin/zkSnapShotToolkit echo -e '#!/usr/bin/env bash\\n/opt/zookeeper/bin/zkTxnLogToolkit.sh \"$@\"' | sudo tee /usr/local/bin/zkTxnLogToolkit sudo chmod +x /usr/local/bin/zkCli /usr/local/bin/zkServer /usr/local/bin/zkCleanup /usr/local/bin/zkSnapShotToolkit /usr/local/bin/zkTxnLogToolkit  # put in the config sudo cp opt/zookeeper/conf/* /etc/zookeeper cat \u003c\u003cEOF | sudo tee /etc/zookeeper/zoo.cfg initLimit=20 syncLimit=10 maxSessionTimeout=60000000 maxClientCnxns=2000 preAllocSize=131072 snapCount=3000000 dataDir=/var/lib/zookeeper/data dataLogDir=/var/lib/zookeeper/logs # use low-latency disk! clientPort=2181 #clientPortAddress=nthk-zoo1.localdomain autopurge.snapRetainCount=10 autopurge.purgeInterval=1 4lw.commands.whitelist=* EOF sudo chown -R zookeeper:zookeeper /etc/zookeeper  # create systemd service file cat \u003c\u003cEOF | sudo tee /etc/systemd/system/zookeeper.service [Unit] Description=Zookeeper Daemon Documentation=http://zookeeper.apache.org Requires=network.target After=network.target [Service] Type=forking WorkingDirectory=/var/lib/zookeeper User=zookeeper Group=zookeeper Environment=ZK_SERVER_HEAP=1536 # in megabytes, adjust to ~ 80-90% of avaliable RAM (more than 8Gb is rather overkill) Environment=SERVER_JVMFLAGS=\"-Xms256m -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" Environment=ZOO_LOG_DIR=/var/log/zookeeper ExecStart=/opt/zookeeper/bin/zkServer.sh start /etc/zookeeper/zoo.cfg ExecStop=/opt/zookeeper/bin/zkServer.sh stop /etc/zookeeper/zoo.cfg ExecReload=/opt/zookeeper/bin/zkServer.sh restart /etc/zookeeper/zoo.cfg TimeoutSec=30 Restart=on-failure [Install] WantedBy=default.target EOF  # start zookeeper sudo systemctl daemon-reload sudo systemctl start zookeeper.service  # check status etc. echo stat | nc localhost 2181 echo ruok | nc localhost 2181 echo mntr | nc localhost 2181 ","categories":"","description":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian.","excerpt":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian.","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/","tags":"","title":"Install standalone Zookeeper for ClickHouse on Ubuntu / Debian"},{"body":" Q. I get “ZooKeeper session has expired” once. What should i do? Should I worry?\n Getting exceptions or lack of acknolegment in distributed system from time to time is a normal situation. Your client should do the retry. If that happened once and your client do retries correctly - nothing to worry about.\nIt it happens often, or with every retry - it may be a sign of some misconfiguration / issue in cluster (see below).\n Q. we see a lot of these: ZooKeeper session has expired. Switching to a new session\n A. There is a single zookeeper session per server. But there are many threads that can use zookeeper simultaneously. So the same event (we lose the single zookeeper session we had), will be reported by all the threads/queries which were using that zookeeper session.\nUsually after loosing the zookeeper session that exception is printed by all the thread which watch zookeeper replication queues, and all the threads which had some in-flight zookeeper operations (for example inserts, ON CLUSTER commands etc).\nIf you see a lot of those simultaniously - that just means you have a lot of threads talking to zookeeper simultaniously (or may be you have many replicated tables?).\nBTW: every Replicated table comes with its own cost, so you can\u0026rsquo;t scale the number of replicated tables indefinitely.\nTypically after several hundreds (sometimes thousands) of replicated tables, the clickhouse server becomes unusable: it can’t do any other work, but only keeping replication housekeeping tasks. ‘ClickHouse-way’ is to have a few (maybe dozens) of very huge tables instead of having thousands of tiny tables. (Side note: the number of not-replicated tables can be scaled much better).\nSo again if during short period of time you see lot of those exceptions and that don’t happen anymore for a while - nothing to worry about. Just ensure your client is doing retries properly.\n Q. We are wondering what is causing that session to “timeout” as the default looks like 30 seconds, and there’s certainly stuff happening much more frequently than every 30 seconds.\n Typically that has nothing with an expiration/timeout - even if you do nothing there are heartbeat events in the zookeeper protocol.\nSo internally inside clickhouse:\n we have a ‘zookeeper client’ which in practice is a single zookeeper connection (TCP socket), with 2 threads - one serving reads, the seconds serving writes, and some API around. while everything is ok zookeeper client keeps a single logical ‘zookeeper session’ (also by sending heartbeats etc). we may have hundreds of ‘users’ of that zookeeper client - those are threads that do some housekeeping, serve queries etc. zookeeper client normally have dozen ‘in-flight’ requests (asked by different threads). And if something bad happens with that (disconnect, some issue with zookeeper server, some other failure), zookeeper client needs to re-establish the connection and switch to the new session so all those ‘in-flight’ requests will be terminated with a ‘session expired’ exception.   Q. That problem happens very often (all the time, every X minutes / hours / days).\n Sometimes the real issue can be visible somewhere close to the first ‘session expired’ exception in the log. (i.e. zookeeper client thread can know \u0026 print to logs the real reason, while all ‘user’ threads just get ‘session expired’).\nAlso zookeeper logs may ofter have a clue to that was the real problem.\nKnown issues which can lead to session termination by zookeeper:\n connectivity / network issues. jute.maxbuffer overrun. If you need to pass too much data in a single zookeeper transaction. (often happens if you need to do ALTER table UPDATE or other mutation on the table with big number of parts). The fix is adjusting JVM setting: -Djute.maxbuffer=8388608. See https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/ XID overflow. XID is a transaction counter in zookeeper, if you do too many transactions the counter reaches maxint32, and to restart the counter zookeeper closes all the connections. Usually, that happens rarely, and is not avoidable in zookeeper (well in clickhouse-keeper that problem solved). There are some corner cases / some schemas which may end up with that XID overflow happening quite often. (a worst case we saw was once per 3 weeks).   Q. “ZooKeeper session has expired” happens every time I try to start the mutation / do other ALTER on Replicated table.\n During ALTERing replicated table ClickHouse need to create a record in zookeeper listing all the parts which should be mutated (that usually means = list names of all parts of the table). If the size of list of parts exceeds maximum buffer size - zookeeper drops the connection.\nParts name length can be different for different tables. In average with default jute.maxbuffer (1Mb) mutations start to fail for tables which have more than 5000 parts.\nSolutions:\n rethink partitioning, high number of parts in table is usually not recommended increase jute.maxbuffer on zookeeper side to values about 8M use IN PARITION clause for mutations (where applicable) - since 20.12 switch to clickhouse-keeper  Related issues:\n https://github.com/ClickHouse/ClickHouse/issues/16307 https://github.com/ClickHouse/ClickHouse/issues/11933 https://github.com/ClickHouse/ClickHouse/issues/32646 https://github.com/ClickHouse/ClickHouse/issues/15882  ","categories":"","description":"ZooKeeper session has expired.","excerpt":"ZooKeeper session has expired.","ref":"/altinity-kb-setup-and-maintenance/zookeeper-session-expired/","tags":"","title":"ZooKeeper session has expired"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":""},{"body":"5 categories SELECTsumResample(0,5,1)(number,number%5)ASsumFROMnumbers_mt(1000000000)┌─sum───────────────────────────────────────────────────────────────────────────────────────────┐│[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000]│└───────────────────────────────────────────────────────────────────────────────────────────────┘1rowsinset.Elapsed:1.010sec.Processed1.00billionrows,8.00GB(990.20millionrows/s.,7.92GB/s.)SELECTsumMap([number%5],[number])ASsumFROMnumbers_mt(1000000000)┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────────┐│([0,1,2,3,4],[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000])│└─────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1rowsinset.Elapsed:5.730sec.Processed1.00billionrows,8.00GB(174.51millionrows/s.,1.40GB/s.)SELECTsumIf(number,(number%5)=0)ASsum_0,sumIf(number,(number%5)=1)ASsum_1,sumIf(number,(number%5)=2)ASsum_2,sumIf(number,(number%5)=3)ASsum_3,sumIf(number,(number%5)=4)ASsum_4FROMnumbers_mt(1000000000)┌─────────────sum_0─┬─────────────sum_1─┬─────────────sum_2─┬──────────────sum_3─┬──────────────sum_4─┐│99999999500000000│99999999700000000│99999999900000000│100000000100000000│100000000300000000│└───────────────────┴───────────────────┴───────────────────┴────────────────────┴────────────────────┘1rowsinset.Elapsed:0.762sec.Processed1.00billionrows,8.00GB(1.31billionrows/s.,10.50GB/s.)SELECTsumMap([id],[sum])ASsumFROM(SELECTnumber%5ASid,sum(number)ASsumFROMnumbers_mt(1000000000)GROUPBYid)┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────────┐│([0,1,2,3,4],[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000])│└─────────────────────────────────────────────────────────────────────────────────────────────────────────────┘1rowsinset.Elapsed:0.331sec.Processed1.00billionrows,8.00GB(3.02billionrows/s.,24.15GB/s.)20 categories SELECTsumResample(0,20,1)(number,number%20)ASsumFROMnumbers_mt(1000000000)1rowsinset.Elapsed:1.056sec.Processed1.00billionrows,8.00GB(947.28millionrows/s.,7.58GB/s.)SELECTsumMap([number%20],[number])ASsumFROMnumbers_mt(1000000000)1rowsinset.Elapsed:6.410sec.Processed1.00billionrows,8.00GB(156.00millionrows/s.,1.25GB/s.)SELECTsumIf(number,(number%5)=0)ASsum_0,sumIf(number,(number%5)=1)ASsum_1,sumIf(number,(number%5)=2)ASsum_2,sumIf(number,(number%5)=3)ASsum_3,sumIf(number,(number%5)=4)ASsum_4,sumIf(number,(number%5)=5)ASsum_5,sumIf(number,(number%5)=6)ASsum_6,sumIf(number,(number%5)=7)ASsum_7,sumIf(number,(number%5)=8)ASsum_8,sumIf(number,(number%5)=9)ASsum_9,sumIf(number,(number%5)=10)ASsum_10,sumIf(number,(number%5)=11)ASsum_11,sumIf(number,(number%5)=12)ASsum_12,sumIf(number,(number%5)=13)ASsum_13,sumIf(number,(number%5)=14)ASsum_14,sumIf(number,(number%5)=15)ASsum_15,sumIf(number,(number%5)=16)ASsum_16,sumIf(number,(number%5)=17)ASsum_17,sumIf(number,(number%5)=18)ASsum_18,sumIf(number,(number%5)=19)ASsum_19FROMnumbers_mt(1000000000)1rowsinset.Elapsed:5.282sec.Processed1.00billionrows,8.00GB(189.30millionrows/s.,1.51GB/s.)SELECTsumMap([id],[sum])ASsumFROM(SELECTnumber%20ASid,sum(number)ASsumFROMnumbers_mt(1000000000)GROUPBYid)1rowsinset.Elapsed:0.362sec.Processed1.00billionrows,8.00GB(2.76billionrows/s.,22.10GB/s.)sumMapResample It’s also possible to combine them.\nSELECTday,category_id,salesFROM(SELECTsumMapResample(1,31,1)([category_id],[sales],day)ASresFROM(SELECTnumber%31ASday,100*(number%11)AScategory_id,numberASsalesFROMnumbers(10000)))ARRAYJOINres.1AScategory_id,res.2ASsales,arrayEnumerate(res.1)ASday┌─day─┬─category_id──────────────────────────────────┬─sales──────────────────────────────────────────────────────────────────────────┐│1│[0,100,200,300,400,500,600,700,800,900,1000]│[143869,148365,142970,147465,142071,146566,151155,145667,150225,144768,149295]││2│[0,100,200,300,400,500,600,700,800,900,1000]│[149325,143898,148395,142999,147494,142100,146595,151185,145696,150255,144797]││3│[0,100,200,300,400,500,600,700,800,900,1000]│[144826,149355,143927,148425,143028,147523,142129,146624,151215,145725,150285]││4│[0,100,200,300,400,500,600,700,800,900,1000]│[150315,144855,149385,143956,148455,143057,147552,142158,146653,151245,145754]││5│[0,100,200,300,400,500,600,700,800,900,1000]│[145783,150345,144884,149415,143985,148485,143086,147581,142187,146682,151275]││6│[0,100,200,300,400,500,600,700,800,900,1000]│[151305,145812,150375,144913,149445,144014,148515,143115,147610,142216,146711]││7│[0,100,200,300,400,500,600,700,800,900,1000]│[146740,151335,145841,150405,144942,149475,144043,148545,143144,147639,142245]││8│[0,100,200,300,400,500,600,700,800,900,1000]│[142274,146769,151365,145870,150435,144971,149505,144072,148575,143173,147668]││9│[0,100,200,300,400,500,600,700,800,900,1000]│[147697,142303,146798,151395,145899,150465,145000,149535,144101,148605,143202]││10│[0,100,200,300,400,500,600,700,800,900,1000]│[143231,147726,142332,146827,151425,145928,150495,145029,149565,144130,148635]││11│[0,100,200,300,400,500,600,700,800,900,1000]│[148665,143260,147755,142361,146856,151455,145957,150525,145058,149595,144159]││12│[0,100,200,300,400,500,600,700,800,900,1000]│[144188,148695,143289,147784,142390,146885,151485,145986,150555,145087,149625]││13│[0,100,200,300,400,500,600,700,800,900,1000]│[149655,144217,148725,143318,147813,142419,146914,151515,146015,150585,145116]││14│[0,100,200,300,400,500,600,700,800,900,1000]│[145145,149685,144246,148755,143347,147842,142448,146943,151545,146044,150615]││15│[0,100,200,300,400,500,600,700,800,900,1000]│[150645,145174,149715,144275,148785,143376,147871,142477,146972,151575,146073]││16│[0,100,200,300,400,500,600,700,800,900,1000]│[146102,150675,145203,149745,144304,148815,143405,147900,142506,147001,151605]││17│[0,100,200,300,400,500,600,700,800,900,1000]│[151635,146131,150705,145232,149775,144333,148845,143434,147929,142535,147030]││18│[0,100,200,300,400,500,600,700,800,900,1000]│[147059,141665,146160,150735,145261,149805,144362,148875,143463,147958,142564]││19│[0,100,200,300,400,500,600,700,800,900,1000]│[142593,147088,141694,146189,150765,145290,149835,144391,148905,143492,147987]││20│[0,100,200,300,400,500,600,700,800,900,1000]│[148016,142622,147117,141723,146218,150795,145319,149865,144420,148935,143521]││21│[0,100,200,300,400,500,600,700,800,900,1000]│[143550,148045,142651,147146,141752,146247,150825,145348,149895,144449,148965]││22│[0,100,200,300,400,500,600,700,800,900,1000]│[148995,143579,148074,142680,147175,141781,146276,150855,145377,149925,144478]││23│[0,100,200,300,400,500,600,700,800,900,1000]│[144507,149025,143608,148103,142709,147204,141810,146305,150885,145406,149955]││24│[0,100,200,300,400,500,600,700,800,900,1000]│[149985,144536,149055,143637,148132,142738,147233,141839,146334,150915,145435]││25│[0,100,200,300,400,500,600,700,800,900,1000]│[145464,150015,144565,149085,143666,148161,142767,147262,141868,146363,150945]││26│[0,100,200,300,400,500,600,700,800,900,1000]│[150975,145493,150045,144594,149115,143695,148190,142796,147291,141897,146392]││27│[0,100,200,300,400,500,600,700,800,900,1000]│[146421,151005,145522,150075,144623,149145,143724,148219,142825,147320,141926]││28│[0,100,200,300,400,500,600,700,800,900,1000]│[141955,146450,151035,145551,150105,144652,149175,143753,148248,142854,147349]││29│[0,100,200,300,400,500,600,700,800,900,1000]│[147378,141984,146479,151065,145580,150135,144681,149205,143782,148277,142883]││30│[0,100,200,300,400,500,600,700,800,900,1000]│[142912,147407,142013,146508,151095,145609,150165,144710,149235,143811,148306]│└─────┴──────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────┘","categories":"","description":"","excerpt":"5 categories …","ref":"/altinity-kb-functions/resample-vs-if-vs-map-vs-subquery/","tags":"","title":"-Resample vs -If vs -Map vs Subquery"},{"body":"-State combinator doesn’t actually store information about -If combinator, so aggregate functions with -If and without have the same serialized data.\n$clickhouse-local--query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(10) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 99$clickhouse-local--query \"SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(11) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)\" --query \"SELECT maxMerge(x), maxMerge(y) FROM table\" 910-State combinator have the same serialized data footprint regardless of parameters used in definition of aggregate function. That’s true for quantile* and sequenceMatch/sequenceCount functions.\n$clickhouse-local--query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantileTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantileTDigestWeightedMerge(0.4)(x) FROM table\" 400000$clickhouse-local--query \"SELECT quantilesTDigestIfState(0.1,0.9)(number,number % 2) FROM numbers(1000000) FORMAT RowBinary\" | clickhouse-local --input-format RowBinary --structure=\"x AggregateFunction(quantilesTDigestWeighted(0.5),UInt64,UInt8)\" --query \"SELECT quantilesTDigestWeightedMerge(0.4,0.8)(x) FROM table\" [400000,800000]SELECTquantileMerge(0.9)(x)FROM(SELECTquantileState(0.1)(number)ASxFROMnumbers(1000))┌─quantileMerge(0.9)(x)─┐│899.1│└───────────────────────┘SELECTsequenceMatchMerge('(?2)(?3)')(x)AS`2_3`,sequenceMatchMerge('(?1)(?4)')(x)AS`1_4`,sequenceMatchMerge('(?1)(?2)(?3)')(x)AS`1_2_3`FROM(SELECTsequenceMatchState('(?1)(?2)(?3)')(number,number=8,number=5,number=6,number=9)ASxFROMnumbers(10))┌─2_3─┬─1_4─┬─1_2_3─┐│1│1│0│└─────┴─────┴───────┘SELECTsequenceCountMerge('(?1)(?2)')(x)AS`2_3`,sequenceCountMerge('(?1)(?4)')(x)AS`1_4`,sequenceCountMerge('(?1)(?2)(?3)')(x)AS`1_2_3`FROM(WITHnumber%4AScondSELECTsequenceCountState('(?1)(?2)(?3)')(number,cond=1,cond=2,cond=3,cond=5)ASxFROMnumbers(11))┌─2_3─┬─1_4─┬─1_2_3─┐│3│0│2│└─────┴─────┴───────┘","categories":"","description":"-State \u0026 -Merge combinators\n","excerpt":"-State \u0026 -Merge combinators\n","ref":"/altinity-kb-queries-and-syntax/state-and-merge-combinators/","tags":"","title":"-State \u0026 -Merge combinators"},{"body":" To set rdkafka options - add to \u003ckafka\u003e section in config.xml or preferably use a separate file in config.d/:  https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md    Some random example:\n\u003ckafka\u003e  \u003cmax_poll_interval_ms\u003e60000\u003c/max_poll_interval_ms\u003e  \u003csession_timeout_ms\u003e60000\u003c/session_timeout_ms\u003e  \u003cheartbeat_interval_ms\u003e10000\u003c/heartbeat_interval_ms\u003e  \u003creconnect_backoff_ms\u003e5000\u003c/reconnect_backoff_ms\u003e  \u003creconnect_backoff_max_ms\u003e60000\u003c/reconnect_backoff_max_ms\u003e  \u003crequest_timeout_ms\u003e20000\u003c/request_timeout_ms\u003e  \u003cretry_backoff_ms\u003e500\u003c/retry_backoff_ms\u003e  \u003cmessage_max_bytes\u003e20971520\u003c/message_max_bytes\u003e  \u003cdebug\u003eall\u003c/debug\u003e\u003c!-- only to get the errors --\u003e  \u003csecurity_protocol\u003eSSL\u003c/security_protocol\u003e  \u003cssl_ca_location\u003e/etc/clickhouse-server/ssl/kafka-ca-qa.crt\u003c/ssl_ca_location\u003e  \u003cssl_certificate_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.pem\u003c/ssl_certificate_location\u003e  \u003cssl_key_location\u003e/etc/clickhouse-server/ssl/client_clickhouse_client.key\u003c/ssl_key_location\u003e  \u003cssl_key_password\u003epass\u003c/ssl_key_password\u003e \u003c/kafka\u003e Authentication / connectivity Amazon MSK \u003cyandex\u003e  \u003ckafka\u003e  \u003csecurity_protocol\u003esasl_ssl\u003c/security_protocol\u003e  \u003csasl_username\u003eroot\u003c/sasl_username\u003e  \u003csasl_password\u003etoor\u003c/sasl_password\u003e  \u003c/kafka\u003e \u003c/yandex\u003e SASL/SCRAM \u003cyandex\u003e  \u003ckafka\u003e  \u003csecurity_protocol\u003esasl_ssl\u003c/security_protocol\u003e  \u003csasl_mechanism\u003eSCRAM-SHA-512\u003c/sasl_mechanism\u003e  \u003csasl_username\u003eroot\u003c/sasl_username\u003e  \u003csasl_password\u003etoor\u003c/sasl_password\u003e  \u003c/kafka\u003e \u003c/yandex\u003e https://leftjoin.ru/all/clickhouse-as-a-consumer-to-amazon-msk/\nInline Kafka certs To connect to some Kafka cloud services you may need to use certificates.\nIf needed they can be converted to pem format and inlined into ClickHouse config.xml Example:\n\u003ckafka\u003e \u003cssl_key_pem\u003e\u003c![CDATA[ RSA Private-Key: (3072 bit, 2 primes) .... -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- ]]\u003e\u003c/ssl_key_pem\u003e \u003cssl_certificate_pem\u003e\u003c![CDATA[ -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- ]]\u003e\u003c/ssl_certificate_pem\u003e \u003c/kafka\u003e See xml\nhttps://help.aiven.io/en/articles/489572-getting-started-with-aiven-kafka\nhttps://stackoverflow.com/questions/991758/how-to-get-pem-file-from-key-and-crt-files\nAzure Event Hub See https://github.com/ClickHouse/ClickHouse/issues/12609\nKerberos  https://clickhouse.tech/docs/en/engines/table-engines/integrations/kafka/#kafka-kerberos-support https://github.com/ClickHouse/ClickHouse/blob/master/tests/integration/test_storage_kerberized_kafka/configs/kafka.xml   \u003c!-- Kerberos-aware Kafka --\u003e  \u003ckafka\u003e  \u003csecurity_protocol\u003eSASL_PLAINTEXT\u003c/security_protocol\u003e  \u003csasl_kerberos_keytab\u003e/home/kafkauser/kafkauser.keytab\u003c/sasl_kerberos_keytab\u003e  \u003csasl_kerberos_principal\u003ekafkauser/kafkahost@EXAMPLE.COM\u003c/sasl_kerberos_principal\u003e  \u003c/kafka\u003e confluent cloud  \u003cyandex\u003e  \u003ckafka\u003e  \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e  \u003csecurity_protocol\u003eSASL_SSL\u003c/security_protocol\u003e  \u003cssl_endpoint_identification_algorithm\u003ehttps\u003c/ssl_endpoint_identification_algorithm\u003e  \u003csasl_mechanism\u003ePLAIN\u003c/sasl_mechanism\u003e xml\u003csasl_username\u003eusername\u003c/sasl_username\u003e  \u003csasl_password\u003epassword\u003c/sasl_password\u003e  \u003cssl_ca_location\u003eprobe\u003c/ssl_ca_location\u003e  \u003c!-- \u003cssl_ca_location\u003e/path/to/cert.pem\u003c/ssl_ca_location\u003e --\u003e  \u003c/kafka\u003e  \u003c/yandex\u003e https://docs.confluent.io/cloud/current/client-apps/config-client.html\nHow to test connection settings Use kafkacat utility - it internally uses same library to access Kafla as clickhouse itself and allows easily to test different settings.\nkafkacat -b my_broker:9092 -C -o -10 -t my_topic \\  -X security.protocol=SASL_SSL \\  -X sasl.mechanisms=PLAIN \\  -X sasl.username=uerName \\  -X sasl.password=Password Different configurations for different tables?  Is there some more documentation how to use this multiconfiguration for Kafka ?\n The whole logic is here: https://github.com/ClickHouse/ClickHouse/blob/da4856a2be035260708fe2ba3ffb9e437d9b7fef/src/Storages/Kafka/StorageKafka.cpp#L466-L475\nSo it load the main config first, after that it load (with overwrites) the configs for all topics, listed in kafka_topic_list of the table.\nAlso since v21.12 it’s possible to use more straght-forward way using named_collections: https://github.com/ClickHouse/ClickHouse/pull/31691\nSo you can say something like\nCREATETABLEtest.kafka(keyUInt64,valueUInt64)ENGINE=Kafka(kafka1,kafka_format='CSV');And after that in configuration:\n\u003cclickhouse\u003e  \u003cnamed_collections\u003e  \u003ckafka1\u003e  \u003ckafka_broker_list\u003ekafka1:19092\u003c/kafka_broker_list\u003e  \u003ckafka_topic_list\u003econf\u003c/kafka_topic_list\u003e  \u003ckafka_group_name\u003econf\u003c/kafka_group_name\u003e  \u003c/kafka1\u003e  \u003c/named_collections\u003e \u003c/clickhouse\u003e The same fragment of code in newer versions: https://github.com/ClickHouse/ClickHouse/blob/d19e24f530c30f002488bc136da78f5fb55aedab/src/Storages/Kafka/StorageKafka.cpp#L474-L496\n","categories":"","description":"Adjusting librdkafka settings\n","excerpt":"Adjusting librdkafka settings\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/","tags":"","title":"Adjusting librdkafka settings"},{"body":"Q. What happens with columns which are nor the part of ORDER BY key, nor have the AggregateFunction type?\nA. it picks the first value met, (similar to any)\nCREATETABLEagg_test(`a`String,`b`UInt8,`c`SimpleAggregateFunction(max,UInt8))ENGINE=AggregatingMergeTreeORDERBYa;INSERTINTOagg_testVALUES('a',1,1);INSERTINTOagg_testVALUES('a',2,2);SELECT*FROMagg_testFINAL;┌─a─┬─b─┬─c─┐│a│1│2│└───┴───┴───┘INSERTINTOagg_testVALUES('a',3,3);SELECT*FROMagg_test;┌─a─┬─b─┬─c─┐│a│1│2│└───┴───┴───┘┌─a─┬─b─┬─c─┐│a│3│3│└───┴───┴───┘OPTIMIZETABLEagg_testFINAL;SELECT*FROMagg_test;┌─a─┬─b─┬─c─┐│a│1│3│└───┴───┴───┘Last non-null value for each column CREATE TABLE test_last ( `col1` Int32, `col2` SimpleAggregateFunction(anyLast, Nullable(DateTime)), `col3` SimpleAggregateFunction(anyLast, Nullable(DateTime)) ) ENGINE = AggregatingMergeTree ORDER BY col1 Ok. 0 rows in set. Elapsed: 0.003 sec. INSERT INTO test_last (col1, col2) VALUES (1, now()); Ok. 1 rows in set. Elapsed: 0.014 sec. INSERT INTO test_last (col1, col3) VALUES (1, now()) Ok. 1 rows in set. Elapsed: 0.006 sec. SELECT col1, anyLast(col2), anyLast(col3) FROM test_last GROUP BY col1 ┌─col1─┬───────anyLast(col2)─┬───────anyLast(col3)─┐ │ 1 │ 2020-01-16 20:57:46 │ 2020-01-16 20:57:51 │ └──────┴─────────────────────┴─────────────────────┘ 1 rows in set. Elapsed: 0.005 sec. SELECT * FROM test_last FINAL ┌─col1─┬────────────────col2─┬────────────────col3─┐ │ 1 │ 2020-01-16 20:57:46 │ 2020-01-16 20:57:51 │ └──────┴─────────────────────┴─────────────────────┘ 1 rows in set. Elapsed: 0.003 sec. ","categories":"","description":"AggregatingMergeTree\n","excerpt":"AggregatingMergeTree\n","ref":"/engines/mergetree-table-engine-family/aggregatingmergetree/","tags":"","title":"AggregatingMergeTree"},{"body":"Problem You have table:\nCREATETABLEmodify_column(column_nString)ENGINE=MergeTree()ORDERBYtuple();Populate it with data:\nINSERTINTOmodify_columnVALUES('key_a');INSERTINTOmodify_columnVALUES('key_b');INSERTINTOmodify_columnVALUES('key_c');Tried to apply alter table query with changing column type:\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nEnum8('key_a'=1,'key_b'=2);But it didn’t succeed and you see an error in system.mutations table:\nSELECT*FROMsystem.mutationsWHERE(table='modify_column')AND(is_done=0)FORMATVerticalRow1:──────database:defaulttable:modify_columnmutation_id:mutation_4.txtcommand:MODIFYCOLUMN`column_n`Enum8('key_a'=1,'key_b'=2)create_time:2021-03-0318:38:09block_numbers.partition_id:['']block_numbers.number:[4]parts_to_do_names:['all_3_3_0']parts_to_do:1is_done:0latest_failed_part:all_3_3_0latest_fail_time:2021-03-0318:38:59latest_fail_reason:Code:36,e.displayText()=DB::Exception:Unknownelement'key_c'fortypeEnum8('key_a'=1,'key_b'=2):whileexecuting'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)') Enum8('key_a' = 1, 'key_b' = 2) : 2':(whilereadingfrompart/var/lib/clickhouse/data/default/modify_column/all_3_3_0/):WhileexecutingMergeTree(version21.3.1.6041)And you can’t query that column anymore:\nSELECTcolumn_nFROMmodify_column┌─column_n─┐│key_a│└──────────┘┌─column_n─┐│key_b│└──────────┘↓Progress:2.00rows,2.00B(19.48rows/s.,19.48B/s.)2rowsinset.Elapsed:0.104sec.Receivedexceptionfromserver(version21.3.1):Code:36.DB::Exception:Receivedfromlocalhost:9000.DB::Exception:Unknownelement'key_c'fortypeEnum8('key_a'=1,'key_b'=2):whileexecuting'FUNCTION CAST(column_n :: 0, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)' :: 1) -\u003e cast(column_n, 'Enum8(\\'key_a\\'=1,\\'key_b\\'=2)') Enum8('key_a' = 1, 'key_b' = 2) : 2':(whilereadingfrompart/var/lib/clickhouse/data/default/modify_column/all_3_3_0/):WhileexecutingMergeTreeThread.Solution You should do the following:\nCheck which mutation is stuck and kill it:\nSELECT*FROMsystem.mutationsWHEREtable='modify_column'ANDis_done=0FORMATVertical;KILLMUTATIONWHEREtable='modify_column'ANDmutation_id='id_of_stuck_mutation';Apply reverting modify column query to convert table to previous column type:\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nString;Check if column is accessible now:\nSELECTcolumn_n,count()FROMmodify_columnGROUPBYcolumn_n;Run fixed ALTER MODIFY COLUMN query.\nALTERTABLEmodify_columnMODIFYCOLUMNcolumn_nEnum8('key_a'=1,'key_b'=2,'key_c'=3);You can monitor progress of column type change with system.mutations or system.parts_columns tables:\nSELECTcommand,parts_to_do,is_doneFROMsystem.mutationsWHEREtable='modify_column'SELECTcolumn,type,count()ASparts,sum(rows)ASrows,sum(bytes_on_disk)ASbytesFROMsystem.parts_columnsWHERE(table='modify_column')AND(column='column_n')ANDactiveGROUPBYcolumn,type","categories":"","description":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","excerpt":"ALTER MODIFY COLUMN is stuck, the column is inaccessible.\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-alter-modify-column-is-stuck-the-column-is-inaccessible/","tags":"","title":"ALTER MODIFY COLUMN is stuck, the column is inaccessible."},{"body":"Welcome to the Altinity ClickHouse Knowledge Base (KB) This knowledge base is supported by Altinity engineers to provide quick answers to common questions and issues involving ClickHouse.\nThe Altinity Knowledge Base is licensed under Apache 2.0, and available to all ClickHouse users. The information and code samples are available freely and distributed under the Apache 2.0 license.\nFor more detailed information about Altinity services support, see the following:\n Altinity: Providers of Altinity.Cloud, providing SOC-2 certified support for ClickHouse. Altinity ClickHouse Documentation: Detailed guides on installing and connecting ClickHouse to other services. Altinity Resources: News, blog posts, and webinars about ClickHouse and Altinity services.  The following sites are also useful references regarding ClickHouse:\n ClickHouse.tech documentation: From Yandex, the creators of ClickHouse ClickHouse at Stackoverflow: Community driven responses to questions regarding ClickHouse Google groups (Usenet) yes we remember it: The grandparent of all modern discussion boards.  ","categories":"","description":"Up-to-date ClickHouse knowledge base for every ClickHouse user.","excerpt":"Up-to-date ClickHouse knowledge base for every ClickHouse user.","ref":"/","tags":"","title":"Altinity Knowledge Base"},{"body":"Working with Altinity \u0026 Yandex packaging together Since version 21.1 Altinity switches to the same packaging as used by Yandex. That is needed for syncing things and introduces several improvements (like adding systemd service file).\nUnfortunately, that change leads to compatibility issues - automatic dependencies resolution gets confused by the conflicting package names: both when you update ClickHouse to the new version (the one which uses older packaging) and when you want to install older altinity packages (20.8 and older).\nInstalling old clickhouse version (with old packaging schema) When you try to install versions 20.8 or older from Altinity repo -\nversion=20.8.12.2-1.el7 yum install clickhouse-client-${version} clickhouse-server-${version} yum outputs smth like\nyum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile  * base: centos.hitme.net.pl  * extras: centos1.hti.pl  * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 --\u003e Running transaction check ---\u003e Package clickhouse-common-static.x86_64 0:20.8.12.2-1.el7 will be installed ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be installed --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common is obsoleted by clickhouse-server, but obsoleting package does not provide for requirements --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable)  Requires: clickhouse-server-common = 20.8.12.2-1.el7  Available: clickhouse-server-common-1.1.54370-2.x86_64 (clickhouse-stable)  clickhouse-server-common = 1.1.54370-2  Available: clickhouse-server-common-1.1.54378-2.x86_64 (clickhouse-stable)  clickhouse-server-common = 1.1.54378-2 ...  Available: clickhouse-server-common-20.8.11.17-1.el7.x86_64 (Altinity_clickhouse-altinity-stable)  clickhouse-server-common = 20.8.11.17-1.el7  Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable)  clickhouse-server-common = 20.8.12.2-1.el7  You could try using --skip-broken to work around the problem  You could try running: rpm -Va --nofiles --nodigest As you can see yum has an issue with resolving clickhouse-server-common dependency, which marked as obsoleted by newer packages.\nSolution with Old Packaging Scheme add --setopt=obsoletes=0 flag to the yum call.\nversion=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} --- title: \"installation succeeded\" linkTitle: \"installation succeeded\" description: \u003e  installation succeeded --- Alternatively, you can add obsoletes=0 into /etc/yum.conf.\nTo update to new ClickHouse version (from old packaging schema to new packaging schema) version=21.1.7.1-2 yum install clickhouse-client-${version} clickhouse-server-${version} Loaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile  * base: centos.hitme.net.pl  * extras: centos1.hti.pl  * updates: centos1.hti.pl Altinity_clickhouse-altinity-stable/x86_64/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable/x86_64/signature | 1.0 kB 00:00:01 !!! Altinity_clickhouse-altinity-stable-source/signature | 833 B 00:00:00 Altinity_clickhouse-altinity-stable-source/signature | 951 B 00:00:00 !!! Nothing to do It is caused by wrong dependencies resolution.\nSolution with New Package Scheme To update to the latest available version - just add clickhouse-server-common:\nyum install clickhouse-client clickhouse-server clickhouse-server-common This way the latest available version will be installed (even if you will request some other version explicitly).\nTo install some specific version remove old packages first, then install new ones.\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=21.1.7.1 yum install clickhouse-client-${version} clickhouse-server-${version} Downgrade from new version to old one version=20.8.12.2-1.el7 yum downgrade clickhouse-client-${version} clickhouse-server-${version} will not work:\nLoaded plugins: fastestmirror, ovl Loading mirror speeds from cached hostfile  * base: ftp.agh.edu.pl  * extras: ftp.agh.edu.pl  * updates: centos.wielun.net Resolving Dependencies --\u003e Running transaction check ---\u003e Package clickhouse-client.x86_64 0:20.8.12.2-1.el7 will be a downgrade ---\u003e Package clickhouse-client.noarch 0:21.1.7.1-2 will be erased ---\u003e Package clickhouse-server.x86_64 0:20.8.12.2-1.el7 will be a downgrade --\u003e Processing Dependency: clickhouse-server-common = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 Package clickhouse-server-common-20.8.12.2-1.el7.x86_64 is obsoleted by clickhouse-server-21.1.7.1-2.noarch which is already installed --\u003e Processing Dependency: clickhouse-common-static = 20.8.12.2-1.el7 for package: clickhouse-server-20.8.12.2-1.el7.x86_64 ---\u003e Package clickhouse-server.noarch 0:21.1.7.1-2 will be erased --\u003e Finished Dependency Resolution Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable)  Requires: clickhouse-common-static = 20.8.12.2-1.el7  Installed: clickhouse-common-static-21.1.7.1-2.x86_64 (@clickhouse-stable)  clickhouse-common-static = 21.1.7.1-2  Available: clickhouse-common-static-1.1.54378-2.x86_64 (clickhouse-stable)  clickhouse-common-static = 1.1.54378-2 Error: Package: clickhouse-server-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable) ...  Available: clickhouse-server-common-20.8.12.2-1.el7.x86_64 (Altinity_clickhouse-altinity-stable)  clickhouse-server-common = 20.8.12.2-1.el7  You could try using --skip-broken to work around the problem  You could try running: rpm -Va --nofiles --nodigest Solution With Downgrading Remove packages first, then install older versions:\nyum erase clickhouse-client clickhouse-server clickhouse-server-common clickhouse-common-static version=20.8.12.2-1.el7 yum install --setopt=obsoletes=0 clickhouse-client-${version} clickhouse-server-${version} ","categories":"","description":"Altinity packaging compatibility \u003e21.x and earlier\n","excerpt":"Altinity packaging compatibility \u003e21.x and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-packaging-compatibility-greater-than-21.x-and-earlier/","tags":"","title":"Altinity packaging compatibility \u003e21.x and earlier"},{"body":"It’s possible to tune some settings which would make ClickHouse more ANSI SQL compatible(and slower):\nSETjoin_use_nulls=1;-- introduced long ago SETcast_keep_nullable=1;-- introduced in 20.5 SETunion_default_mode='DISTINCT';-- introduced in 21.1 SETallow_experimental_window_functions=1;-- introduced in 21.3 SETprefer_column_name_to_alias=1;-- introduced in 21.4; ","categories":"","description":"ANSI SQL mode\n","excerpt":"ANSI SQL mode\n","ref":"/altinity-kb-queries-and-syntax/ansi-sql-mode/","tags":"","title":"ANSI SQL mode"},{"body":"arrayMap-like functions memory usage calculation. In order to calculate arrayMap or similar array* functions ClickHouse temporarily does arrayJoin-like operation, which in certain conditions can lead to huge memory usage for big arrays.\nSo for example, you have 2 columns:\nSELECT*FROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)┌─array_1─────┬─array_2─────┐│[1,2,3,4,5]│[1,2,3,4,5]│└─────────────┴─────────────┘Let’s say we want to multiply array elements at corresponding positions.\nSELECTarrayMap(x-\u003e((array_1[x])*(array_2[x])),arrayEnumerate(array_1))ASmultiFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)┌─multi─────────┐│[1,4,9,16,25]│└───────────────┘ClickHouse create temporary structure in memory like this:\nSELECTarray_1,array_2,xFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)ARRAYJOINarrayEnumerate(array_1)ASx┌─array_1─────┬─array_2─────┬─x─┐│[1,2,3,4,5]│[1,2,3,4,5]│1││[1,2,3,4,5]│[1,2,3,4,5]│2││[1,2,3,4,5]│[1,2,3,4,5]│3││[1,2,3,4,5]│[1,2,3,4,5]│4││[1,2,3,4,5]│[1,2,3,4,5]│5│└─────────────┴─────────────┴───┘We can roughly estimate memory usage by multiplying the size of columns participating in the lambda function by the size of the unnested array.\nAnd total memory usage will be 55 values (5(array size)*2(array count)*5(row count) + 5(unnested array size)), which is 5.5 times more than initial array size.\nSELECTgroupArray((array_1[x])*(array_2[x]))ASmultiFROM(SELECTarray_1,array_2,xFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)ARRAYJOINarrayEnumerate(array_1)ASx)┌─multi─────────┐│[1,4,9,16,25]│└───────────────┘But what if we write this function in a more logical way, so we wouldn’t use any unnested arrays in lambda.\nSELECTarrayMap((x,y)-\u003e(x*y),array_1,array_2)ASmultiFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)┌─multi─────────┐│[1,4,9,16,25]│└───────────────┘ClickHouse create temporary structure in memory like this:\nSELECTx,yFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)ARRAYJOINarray_1ASx,array_2ASy┌─x─┬─y─┐│1│1││2│2││3│3││4│4││5│5│└───┴───┘We have only 10 values, which is no more than what we have in initial arrays.\nSELECTgroupArray(x*y)ASmultiFROM(SELECTx,yFROM(SELECT[1,2,3,4,5]ASarray_1,[1,2,3,4,5]ASarray_2)ARRAYJOINarray_1ASx,array_2ASy)┌─multi─────────┐│[1,4,9,16,25]│└───────────────┘The same approach can be applied to other array* function with arrayMap-like capabilities to use lambda functions and ARRAY JOIN (arrayJoin).\nExamples with bigger arrays: SETmax_threads=1;SETsend_logs_level='trace';SELECTarrayMap(x-\u003e((array_1[x])*(array_2[x])),arrayEnumerate(array_1))ASmultiFROM(WITH100ASsizeSELECTmaterialize(CAST(range(size),'Array(UInt32)'))ASarray_1,materialize(CAST(range(size),'Array(UInt32)'))ASarray_2FROMnumbers(100000000))FORMAT`Null`\u003cDebug\u003eMemoryTracker:Currentmemoryusage(forquery):8.13GiB.size=100,(2*size)*size=2*(size^2)Elapsed:24.879sec.Processed524.04thousandrows,4.19MB(21.06thousandrows/s.,168.51KB/s.)SELECTarrayMap(x-\u003e((array_1[x])*(array_2[x])),arrayEnumerate(array_1))ASmultiFROM(WITH100ASsizeSELECTmaterialize(CAST(range(2*size),'Array(UInt32)'))ASarray_1,materialize(CAST(range(size),'Array(UInt32)'))ASarray_2FROMnumbers(100000000))FORMAT`Null`\u003cDebug\u003eMemoryTracker:Currentmemoryusage(forquery):24.28GiB.size=100,(3*size)*2*size=6*(size^2)Elapsed:71.547sec.Processed524.04thousandrows,4.19MB(7.32thousandrows/s.,58.60KB/s.)SELECTarrayMap(x-\u003e((array_1[x])*(array_2[x])),arrayEnumerate(array_1))ASmultiFROM(WITH100ASsizeSELECTmaterialize(CAST(range(size),'Array(UInt32)'))ASarray_1,materialize(CAST(range(2*size),'Array(UInt32)'))ASarray_2FROMnumbers(100000000))FORMAT`Null`\u003cDebug\u003eMemoryTracker:Currentmemoryusage(forquery):12.19GiB.size=100,(3*size)*size=3*(size^2)Elapsed:36.777sec.Processed524.04thousandrows,4.19MB(14.25thousandrows/s.,113.99KB/s.)Which data types we have in those arrays?\nWITH100ASsizeSELECTtoTypeName(materialize(CAST(range(size),'Array(UInt32)')))ASarray_1,toTypeName(materialize(CAST(range(2*size),'Array(UInt32)')))ASarray_2,toTypeName(arrayEnumerate(materialize(CAST(range(size),'Array(UInt32)'))))ASx┌─array_1───────┬─array_2───────┬─x─────────────┐│Array(UInt32)│Array(UInt32)│Array(UInt32)│└───────────────┴───────────────┴───────────────┘So each value use 4 bytes.\nBy default ClickHouse execute query by blocks of 65515 rows (max_block_size setting value)\nLets estimate query total memory usage given previous calculations.\nWITH100ASsize,4ASvalue_size,65515ASmax_block_sizeSELECTarray_1_multiplier,array_2_multiplier,formatReadableSize(((value_size*max_block_size)*((array_1_multiplier*size)+(array_2_multiplier*size)))*(array_1_multiplier*size)ASestimated_memory_usage_bytes)ASestimated_memory_usage,real_memory_usage,round(estimated_memory_usage_bytes/(real_memory_usage*1073741824),2)ASratioFROM(WITHarrayJoin([(1,1,8.13),(2,1,24.28),(1,2,12.19)])AStplSELECTtpl.1ASarray_1_multiplier,tpl.2ASarray_2_multiplier,tpl.3ASreal_memory_usage)┌─array_1_multiplier─┬─array_2_multiplier─┬─estimated_memory_usage─┬─real_memory_usage─┬─ratio─┐│1│1│4.88GiB│8.13│0.6││2│1│14.64GiB│24.28│0.6││1│2│7.32GiB│12.19│0.6│└────────────────────┴────────────────────┴────────────────────────┴───────────────────┴───────┘Correlation is pretty clear.\nWhat if we will reduce size of blocks used for query execution?\nSETmax_block_size='16k';SELECTarrayMap(x-\u003e((array_1[x])*(array_2[x])),arrayEnumerate(array_1))ASmultiFROM(WITH100ASsizeSELECTmaterialize(CAST(range(size),'Array(UInt32)'))ASarray_1,materialize(CAST(range(2*size),'Array(UInt32)'))ASarray_2FROMnumbers(100000000))FORMAT`Null`\u003cDebug\u003eMemoryTracker:Currentmemoryusage(forquery):3.05GiB.Elapsed:35.935sec.Processed512.00thousandrows,4.10MB(14.25thousandrows/s.,113.98KB/s.)Memory usage down in 4 times, which has strong correlation with our change: 65k -\u003e 16k ~ 4 times.\nSELECTarrayMap((x,y)-\u003e(x*y),array_1,array_2)ASmultiFROM(WITH100ASsizeSELECTmaterialize(CAST(range(size),'Array(UInt32)'))ASarray_1,materialize(CAST(range(size),'Array(UInt32)'))ASarray_2FROMnumbers(100000000))FORMAT`Null`\u003cDebug\u003eMemoryTracker:Peakmemoryusage(forquery):226.04MiB.Elapsed:5.700sec.Processed11.53millionrows,92.23MB(2.02millionrows/s.,16.18MB/s.)Almost 100 times faster than first query!\n","categories":"","description":"Why arrayMap, arrayFilter, arrayJoin use so much memory?\n","excerpt":"Why arrayMap, arrayFilter, arrayJoin use so much memory?\n","ref":"/altinity-kb-functions/array-like-memory-usage/","tags":"","title":"arrayMap, arrayJoin or ARRAY JOIN memory usage"},{"body":"assumeNotNull result is implementation specific:\nWITHCAST(NULL,'Nullable(UInt8)')AScolumnSELECTcolumn,assumeNotNull(column+999)ASx;┌─column─┬─x─┐│null│0│└────────┴───┘WITHCAST(NULL,'Nullable(UInt8)')AScolumnSELECTcolumn,assumeNotNull(materialize(column)+999)ASx;┌─column─┬───x─┐│null│999│└────────┴─────┘CREATETABLEtest_null(`key`UInt32,`value`Nullable(String))ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_nullSELECTnumber,concat('value ',toString(number))FROMnumbers(4);SELECT*FROMtest_null;┌─key─┬─value───┐│0│value0││1│value1││2│value2││3│value3│└─────┴─────────┘ALTERTABLEtest_nullUPDATEvalue=NULLWHEREkey=3;SELECT*FROMtest_null;┌─key─┬─value───┐│0│value0││1│value1││2│value2││3│null│└─────┴─────────┘SELECTkey,assumeNotNull(value)FROMtest_null;┌─key─┬─assumeNotNull(value)─┐│0│value0││1│value1││2│value2││3│value3│└─────┴──────────────────────┘WITHCAST(NULL,'Nullable(Enum8(\\'a\\' = 1, \\'b\\' = 0))')AStestSELECTassumeNotNull(test)┌─assumeNotNull(test)─┐│b│└─────────────────────┘WITHCAST(NULL,'Nullable(Enum8(\\'a\\' = 1))')AStestSELECTassumeNotNull(test)Erroronprocessingquery'with CAST(null, 'Nullable(Enum8(\\'a\\'=1))') as test select assumeNotNull(test); ;':Code:36,e.displayText()=DB::Exception:Unexpectedvalue0inenum,Stacktrace(whencopyingthismessage,alwaysincludethelinesbelow): Info Null values in ClickHouse are stored in a separate dictionary: is this value Null. And for faster dispatch of functions there is no check on Null value while function execution, so functions like plus can modify internal column value (which has default value). In normal conditions it’s not a problem because on read attempt, ClickHouse first would check the Null dictionary and return value from column itself for non-Nulls only. And assumeNotNull function just ignores this Null dictionary. So it would return only column values, and in certain cases it’s possible to have unexpected results.  If it’s possible to have Null values, it’s better to use ifNull function instead.\nSELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(ifNull(toNullable(number),0))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.705sec.Processed1.00billionrows,8.00GB(1.42billionrows/s.,11.35GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(coalesce(toNullable(number),0))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:2.383sec.Processed1.00billionrows,8.00GB(419.56millionrows/s.,3.36GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(assumeNotNull(toNullable(number)))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.051sec.Processed1.00billionrows,8.00GB(19.62billionrows/s.,156.98GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(toNullable(number))┌────count()─┐│1000000000│└────────────┘1rowsinset.Elapsed:0.050sec.Processed1.00billionrows,8.00GB(20.19billionrows/s.,161.56GB/s.) Info There is no overhead for assumeNotNull at all.  ","categories":"","description":"assumeNotNull and friends\n","excerpt":"assumeNotNull and friends\n","ref":"/altinity-kb-functions/assumenotnull-and-friends/","tags":"","title":"assumeNotNull and friends"},{"body":"In version 20.5 ClickHouse first introduced database engine=Atomic.\nSince version 20.10 it is a default database engine (before engine=Ordinary was used).\nThose 2 database engine differs in a way how they store data on a filesystem, and engine Atomic allows to resolve some of the issues existed in engine=Ordinary.\nengine=Atomic supports\n non-blocking drop table / rename table tables delete (\u0026detach) async (wait for selects finish but invisible for new selects) atomic drop table (all files / folders removed) atomic table swap (table swap by “EXCHANGE TABLES t1 AND t2;”) rename dictionary / rename database unique automatic UUID paths in FS and ZK for Replicated  FAQ Q. Data is not removed immediately A. UseDROP TABLE t SYNC;\nOr use parameter (user level) database_atomic_wait_for_drop_and_detach_synchronously:\nSETdatabase_atomic_wait_for_drop_and_detach_synchronously=1;Also, you can decrease the delay used by Atomic for real table drop (it’s 8 minutes by default)\ncat /etc/clickhouse-server/config.d/database_atomic_delay_before_drop_table.xml \u003cyandex\u003e  \u003cdatabase_atomic_delay_before_drop_table_sec\u003e1\u003c/database_atomic_delay_before_drop_table_sec\u003e \u003c/yandex\u003e Q. I cannot reuse zookeeper path after dropping the table. A. This happens because real table deletion occurs with a controlled delay. See the previous question to remove the table immediately.\nWith engine=Atomic it’s possible (and is a good practice if you do it correctly) to include UUID into zookeeper path, i.e. :\nCREATE...ONCLUSTER...ENGINE=ReplicatedMergeTree('/clickhouse/tables/{uuid}/{shard}/','{replica}')See also: https://github.com/ClickHouse/ClickHouse/issues/12135#issuecomment-653932557\nIt’s very important that the table will have the same UUID cluster-wide.\nWhen the table is created using ON CLUSTER - all tables will get the same UUID automatically. When it needs to be done manually (for example - you need to add one more replica), pick CREATE TABLE statement with UUID from one of the existing replicas.\nsetshow_table_uuid_in_table_create_qquery_if_not_nil=1;SHOWCREATETABLExxx;/* or SELECT create_table_query FROM system.tables WHERE ... */Q. Should I use Atomic or Ordinary for new setups? All things inside clickhouse itself should work smoothly with Atomic.\nBut some external tools - backup tools, things involving other kinds of direct manipulations with clickhouse files \u0026 folders may have issues with Atomic.\nOrdinary layout on the filesystem is simpler. And the issues which address Atomic (lock-free renames, drops, atomic exchange of table) are not so critical in most cases.\n    Ordinary Atomic     filesystem layout very simple more complicated   external tool support (like clickhouse-backup) good / mature limited / beta   some DDL queries (DROP / RENAME) may\nhang for a long time (waiting for some other things)\n yes 👎 no 👍   Possibility to swap 2 tables rename a to a_old, b to a,\na_old to b;\nOperation is not atomic, and can break in the middle (while chances are low).\n \nEXCHANGE TABLES t1 AND t2\nAtomic, have no intermediate states.\n   uuid in zookeeper path Not possible to use.\nThe typical pattern is to add version suffix to zookeeper path when you need to create the new version of the same table.\n You can use uuid in zookeeper paths. That requires some extra care when you expand the cluster, and makes zookeeper paths harder to map to real table.\nBut allows to to do any kind of manipulations on tables (rename, recreate with same name etc).\n   Materialized view without TO syntax\n(!we recommend using TO syntax always!)\n .inner.mv_name\nThe name is predictable, easy to match with MV.\n .inner_id.{uuid}\nThe name is unpredictable, hard to match with MV (maybe problematic for MV chains, and similar scenarios)\n    Using Ordinary by default instead of Atomic --- title: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" linkTitle: \"cat /etc/clickhouse-server/users.d/disable_atomic_database.xml \" description: \u003e  cat /etc/clickhouse-server/users.d/disable_atomic_database.xml --- \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003cdefault_database_engine\u003eOrdinary\u003c/default_database_engine\u003e  \u003c/default\u003e  \u003c/profiles\u003e \u003c/yandex\u003e Other sources Presentation https://youtu.be/1LVJ_WcLgF8?t=2744\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup46/database_engines.pdf\n","categories":"","description":"Atomic Database Engine\n","excerpt":"Atomic Database Engine\n","ref":"/engines/altinity-kb-atomic-database-engine/","tags":"","title":"Atomic Database Engine"},{"body":"An insert is atomic if it creates only one part.\nAn insert will create one part if:\n Data is inserted directly into a MergeTree table Data is inserted into a single partition. For INSERT FORMAT:  Number of rows is less than max_insert_block_size (default is 1048545) Parallel formatting is disabled (For TSV, TKSV, CSV, and JSONEachRow formats setting input_format_parallel_parsing=0 is set).   For INSERT SELECT:  Number of rows is less than max_block_size   Smaller blocks are properly squashed up to the configured block size (min_insert_block_size_rows and min_insert_block_size_bytes) The MergeTree table doesn’t have Materialized Views (there is no atomicity Table \u003c\u003e MV)  https://github.com/ClickHouse/ClickHouse/issues/9195#issuecomment-587500824 https://github.com/ClickHouse/ClickHouse/issues/5148#issuecomment-487757235\nExample how to make a large insert atomically Generate test data in Native and TSV format ( 100 millions rows ) Text formats and Native format requier different set of settings, here I want to find / demonstrate mandatory minumum of settings for any case.\nclickhouse-client -q \\  'select toInt64(number) A, toString(number) S from numbers(100000000) format Native' \u003e t.native clickhouse-client -q \\  'select toInt64(number) A, toString(number) S from numbers(100000000) format TSV' \u003e t.tsv Insert with default settings (not atomic) drop table if exists trg; create table trg(A Int64, S String) Engine=MergeTree order by A;  -- Load data in Native format clickhouse-client -q 'insert into trg format Native' \u003ct.native  -- Check how many parts is created SELECT  count(),  min(rows),  max(rows),  sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 90 │ 890935 │ 1113585 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘  --- 90 parts! was created - not atomic    drop table if exists trg; create table trg(A Int64, S String) Engine=MergeTree order by A;  -- Load data in TSV format clickhouse-client -q 'insert into trg format TSV' \u003ct.tsv  -- Check how many parts is created SELECT  count(),  min(rows),  max(rows),  sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 85 │ 898207 │ 1449610 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘  --- 85 parts! was created - not atomic Insert with adjusted settings (atomic) Atomic insert use more memory because it needs 100 millions rows in memory.\ndrop table if exists trg; create table trg(A Int64, S String) Engine=MergeTree order by A;  clickhouse-client --input_format_parallel_parsing=0 \\  --min_insert_block_size_bytes=0 \\  --min_insert_block_size_rows=1000000000 \\  --max_insert_block_size=1000000000 \\  -q 'insert into trg format Native' \u003ct.native  -- Check that only one part is created SELECT  count(),  min(rows),  max(rows),  sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 1 │ 100000000 │ 100000000 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘  -- 1 part, success.    drop table if exists trg; create table trg(A Int64, S String) Engine=MergeTree order by A;  -- Load data in TSV format clickhouse-client --input_format_parallel_parsing=0 \\  --min_insert_block_size_bytes=0 \\  --min_insert_block_size_rows=1000000000 \\  --max_insert_block_size=1000000000 \\  -q 'insert into trg format TSV' \u003ct.tsv  -- Check that only one part is created SELECT  count(),  min(rows),  max(rows),  sum(rows) FROM system.parts WHERE (level = 0) AND (table = 'trg'); ┌─count()─┬─min(rows)─┬─max(rows)─┬─sum(rows)─┐ │ 1 │ 100000000 │ 100000000 │ 100000000 │ └─────────┴───────────┴───────────┴───────────┘  -- 1 part, success. ","categories":"","description":"Atomic insert\n","excerpt":"Atomic insert\n","ref":"/altinity-kb-queries-and-syntax/atomic-insert/","tags":"","title":"Atomic insert"},{"body":"   Volume type  gp3 gp2     Max throughput per volume  1000 MiB/s 250 MiB/s   Price  $0.08/GB-month\n3,000 IOPS free and\n$0.005/provisioned IOPS-month over 3,000;\n125 MB/s free and\n$0.04/provisioned MB/s-month over 125\n $0.10/GB-month    GP2 In usual conditions ClickHouse being limited by throughput of volumes and amount of provided IOPS doesn’t make any big difference for performance starting from a certain number. So the most native choice for clickhouse is gp2 and gp3 volumes.\n‌Because gp2 volumes have a hard limit of 250 MiB/s per volume (for volumes bigger than 334 GB), it usually makes sense to split one big volume in multiple smaller volumes larger than 334GB in order to have maximum possible throughput.\n‌EC2 instances also have an EBS throughput limit, it depends on the size of the EC2 instance. That means if you would attach multiple volumes which would have high potential throughput, you would be limited by your EC2 instance, so usually there is no reason to have more than 4-5 volumes per node.\nIt’s pretty straightforward to set up a ClickHouse for using multiple EBS volumes with storage_policies.\nGP3 It’s a new type of volume, which is 20% cheaper than gp2 per GB-month and has lower free throughput: only 125 MB/s vs 250 MB/s. But you can buy additional throughput for volume and gp3 pricing became comparable with multiple gp2 volumes starting from 1000-1500GB size. It also works better if most of your queries read only one or several parts, because in that case you are not being limited by performance of a single ebs disk, as parts can be located only on one disk at once.\nFor best performance, it’s suggested to buy:\n 7000 IOPS Throughput up to the limit of your EC2 instance  https://altinity.com/blog/2019/11/27/amplifying-clickhouse-capacity-with-multi-volume-storage-part-1\nhttps://altinity.com/blog/2019/11/29/amplifying-clickhouse-capacity-with-multi-volume-storage-part-2\nhttps://calculator.aws/#/createCalculator/EBS?nc2=h_ql_pr_calc\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nhttps://aws.amazon.com/ebs/general-purpose/\n","categories":"","description":"AWS EBS\n","excerpt":"AWS EBS\n","ref":"/altinity-kb-setup-and-maintenance/aws-ebs/","tags":"","title":"AWS EBS"},{"body":"Q. How to populate MV create with TO syntax? INSERT INTO mv SELECT * FROM huge_table? Will it work if the source table has billions of rows?\nA. single huge insert ... select ... actually will work, but it will take A LOT of time, and during that time lot of bad things can happen (lack of disk space, hard restart etc). Because of that, it’s better to do such backfill in a more controlled manner and in smaller pieces.\nOne of the best options is to fill one partition at a time, and if it breaks you can drop the partition and refill it.\nIf you need to construct a single partition from several sources - then the following approach may be the best.\nCREATETABLEmv_importASmv;INSERTINTOmv_importSELECT*FROMhuge_tableWHEREtoYYYYMM(ts)=202105;/* or other partition expression*//* that insert select may take a lot of time, if something bad will happen during that - just truncate mv_import and restart the process *//* after successful loading of mv_import do*/ALTERTABLEmvATTACHPARTITIONID'202105'FROMmv_import;See also https://clickhouse.tech/docs/en/sql-reference/statements/alter/partition/#alter_attach-partition-from.\n","categories":"","description":"Backfill/populate MV in a controlled manner\n","excerpt":"Backfill/populate MV in a controlled manner\n","ref":"/altinity-kb-schema-design/materialized-views/backfill-populate-mv-in-a-controlled-manner/","tags":"","title":"Backfill/populate MV in a controlled manner"},{"body":"ClickHouse is currently at the design stage of creating some universal backup solution. Some custom backup strategies are:\n Each shard is backed up separately. FREEZE the table/partition. For more information, see Alter Freeze Partition.  This creates hard links in shadow subdirectory.   rsync that directory to a backup location, then remove that subfolder from shadow.  Cloud users are recommended to use Rclone.   Always add the full contents of the metadata subfolder that contains the current DB schema and clickhouse configs to your backup. For a second replica, it’s enough to copy metadata and configuration. Data in clickhouse is already compressed with lz4, backup can be compressed bit better, but avoid using cpu-heavy compression algorythms like gzip, use something like zstd instead.  The tool automating that process clickhouse-backup.\n","categories":"","description":"Backups\n","excerpt":"Backups\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/","tags":"","title":"Backups"},{"body":"Picking the best schema for storing many metrics registered from single source is quite a common problem.\n1 One row per metric i.e.: timestamp, sourceid, metric_name, metric_value\nPros and cons:\n Pros:  simple well normalized schema easy to extend that is quite typical pattern for timeseries databases   Cons  different metrics values stored in same columns (worse compression) to use values of different datatype you need to cast everything to string or introduce few columns for values of different types. not always nice as you need to repeat all ‘common’ fields for each row if you need to select all data for one time point you need to scan several ranges of data.    2 Each measurement (with lot of metrics) in it’s own row In that way you need to put all the metrics in one row (i.e.: timestamp, sourceid, ….) That approach is usually a source of debates about how to put all the metrics in one row.\n2a Every metric in it’s own column i.e.: timestamp, sourceid, metric1_value, … , metricN_value\nPros and cons:\n Pros  simple really easy to access / scan for rows with particular metric specialized and well adjusted datatypes for every metric. good for dense recording (each time point can have almost 100% of all the possible metrics)   Cons  adding new metric = changing the schema (adding new column). not suitable when set of metric changes dynamically not applicable when there are too many metrics (when you have more than 100-200) when each timepoint have only small subset of metrics recorded - if will create a lot of sparse filled columns. you need to store ’lack of value’ somehow (NULLs or default values) to read full row - you need to read a lot of column files.    2b Using arrays / Nested / Map i.e.: timestamp, sourceid, array_of_metric_names, array_of_metric_values\nPros and cons:\n Pros  easy to extend, you can have very dynamic / huge number of metrics. you can use Array(LowCardinality(String)) for storing metric names efficiently good for sparse recording (each time point can have only 1% of all the possible metrics)   Cons  you need to extract all metrics for row to reach a single metric not very handy / complicated non-standard syntax different metrics values stored in single array (bad compression) to use values of different datatype you need to cast everything to string or introduce few arrays for values of different types.    2c Using JSON i.e.: timestamp, sourceid, metrics_data_json\nPros and cons:\n Pros  easy to extend, you can have very dynamic / huge number of metrics. the only option to store hierarchical / complicated data structures, also with arrays etc. inside. good for sparse recording (each time point can have only 1% of all the possible metrics) ClickHouse has efficient API to work with JSON nice if your data originally came in JSON (don’t need to reformat)   Cons  uses storage non efficiently different metrics values stored in single array (bad compression) you need to extract whole JSON field to reach single metric slower than arrays    2d Using querystring-format URLs i.e.: timestamp, sourceid, metrics_querystring Same pros/cons as raw JSON, but usually bit more compact than JSON\nPros and cons:\n Pros  clickhouse has efficient API to work with URLs (extractURLParameter etc) can have sense if you data came in such format (i.e. you can store GET / POST request data directly w/o reprocessing)   Cons  slower than arrays    2e Several ‘baskets’ of arrays i.e.: timestamp, sourceid, metric_names_basket1, metric_values_basker1, …, metric_names_basketN, metric_values_basketN The same as 2b, but there are several key-value arrays (‘basket’), and metric go to one particular basket depending on metric name (and optionally by metric type)\nPros and cons:\n Pros  address some disadvantages of 2b (you need to read only single, smaller basket for reaching a value, better compression - less unrelated metrics are mixed together)   Cons  complex    2f Combined approach In real life Pareto principle is correct for many fields.\nFor that particular case: usually you need only about 20% of metrics 80% of the time. So you can pick the metrics which are used intensively, and which have a high density, and extract them into separate columns (like in option 2a), leaving the rest in a common ’trash bin’ (like in variants 2b-2e).\nWith that approach you can have as many metrics as you need and they can be very dynamic. At the same time the most used metrics are stored in special, fine-tuned columns.\nAt any time you can decide to move one more metric to a separate column ALTER TABLE ... ADD COLUMN metricX Float64 MATERIALIZED metrics.value[indexOf(metrics.names,'metricX')];\n2e Subcolumns [future] https://github.com/ClickHouse/ClickHouse/issues/23516\nWIP currently, ETA of first beta = autumn 2021\nRelated links:\nThere is one article on our blog on this subject with some benchmarks.\nSlides from Percona Live\nUber article about how they adapted combined approach\nSlides for Uber log storage approach\n","categories":"","description":"Best schema for storing many metrics registered from the single source\n","excerpt":"Best schema for storing many metrics registered from the single source …","ref":"/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/","tags":"","title":"Best schema for storing many metrics registered from the single source"},{"body":" Superset: https://superset.apache.org/docs/databases/clickhouse Metabase: https://github.com/enqueue/metabase-clickhouse-driver Querybook: https://www.querybook.org/docs/setup_guide/connect_to_query_engines/#all-query-engines Tableau: Clickhouse Tableau connector odbc Looker: https://docs.looker.com/setup-and-management/database-config/clickhouse Apache Zeppelin SeekTable ReDash Mondrian: https://altinity.com/blog/accessing-clickhouse-from-excel-using-mondrian-rolap-engine Grafana: Integrating Grafana with ClickHouse Cumul.io Tablum: https://tablum.io  ","categories":"","description":"Business Intelligence Tools\n","excerpt":"Business Intelligence Tools\n","ref":"/altinity-kb-integrations/bi-tools/","tags":"","title":"BI Tools"},{"body":"Here is what different statuses mean:\n Parts are renamed to ‘ignored’ if they were found during ATTACH together with other, bigger parts that cover the same blocks of data, i.e. they were already merged into something else. parts are renamed to ‘broken’ if ClickHouse was not able to load data from the parts. There could be different reasons: some files are lost, checksums are not correct, etc. parts are renamed to ‘unexpected’ if they are present locally, but are not found in ZooKeeper, in case when an insert was not completed properly. The part is detached only if it’s old enough (5 minutes), otherwise CH registers this part in ZooKeeper as a new part. parts are renamed to ‘cloned’ if ClickHouse have had some parts on local disk while repairing lost replica so already existed parts being renamed and put in detached directory. Controlled by setting detach_old_local_parts_when_cloning_replica.  ‘Ignored’ parts are safe to delete. ‘Unexpected’ and ‘broken’ should be investigated, but it might not be an easy thing to do, especially for older parts. If the system.part_log table is enabled you can find some information there. Otherwise you will need to look in clickhouse-server.log for what happened when the parts were detached. If there is another way you could confirm that there is no data loss in the affected tables, you could simply delete all detached parts.\nHere is a query that can help with investigations. It looks for active parts containing the same data blocks that the detached parts:\nSELECT*,concat('alter table ',database,'.',table,' drop detached part ''',a.name,''' settings allow_drop_detached=1;')asdropFROMsystem.detached_partsaALLLEFTJOIN(SELECTdatabase,table,partition_id,name,active,min_block_number,max_block_numberFROMsystem.partsWHEREactive)bUSING(database,table,partition_id)WHEREa.min_block_number\u003e=b.min_block_numberANDa.max_block_number\u003c=b.max_block_number","categories":"","description":"Can detached parts be dropped?\n","excerpt":"Can detached parts be dropped?\n","ref":"/altinity-kb-useful-queries/detached-parts/","tags":"","title":"Can detached parts be dropped?"},{"body":" Info Article is based on feedback provided by one of Altinity clients.  CatBoost:\n It uses gradient boosting - a hard to use technique which can outperform neural networks. Gradient boosting is powerful but it’s easy to shoot yourself in the foot using it. The documentation on how to use it is quite lacking. The only good source of information on how to properly configure a model to yield good results is this video: https://www.youtube.com/watch?v=usdEWSDisS0 . We had to dig around GitHub issues to find out how to make it work with ClickHouse. CatBoost is fast. Other libraries will take ~5X to ~10X as long to do what CatBoost does. CatBoost will do preprocessing out of the box (fills nulls, apply standard scaling, encodes strings as numbers). CatBoost has all functions you’d need (metrics, plotters, feature importance)  It makes sense to split what CatBoost does into 2 parts:\n preprocessing (fills nulls, apply standard scaling, encodes strings as numbers) number crunching (convert preprocessed numbers to another number - ex: revenue of impression)  Compared to Fast.ai, CatBoost pre-processing is as simple to use and produces results that can be as good as Fast.ai.\nThe number crunching part of Fast.ai is no-config. For CatBoost you need to configure it, a lot.\nCatBoost won’t simplify or hide any complexity of the process. So you need to know data science terms and what it does (ex: if your model is underfitting you can use a smaller l2_reg parameter in the model constructor).\nIn the end both Fast.ai and CatBoost can yield comparable results.\nRegarding deploying models, CatBoost is really good. The model runs fast, it has a simple binary format which can be loaded in ClickHouse, C, or Python and it will encapsulate pre-processing with the binary file. Deploying Fast.ai models at scale/speed is impossible out of the box (we have our custom solution to do it which is not simple).\nTLDR: CatBoost is fast, produces awesome models, is super easy to deploy and it’s easy to use/train (after becoming familiar with it despite the bad documentation \u0026 if you know data science terms).\nRegarding MindsDB The project seems to be a good idea but it’s too young. I was using the GUI version and I’ve encountered some bugs, and none of those bugs have a good error message.\n  It won’t show data in preview.\n  The “download” button won’t work.\n  It’s trying to create and drop tables in ClickHouse without me asking it to.\n  Other than bugs:\n It will only use 1 core to do everything (training, analysis, download). Analysis will only run with a very small subset of data, if I use something like 1M rows it never finishes.    Training a model on 100k rows took 25 minutes - (CatBoost takes 90s to train with 1M rows)\n  The model trained on MindsDB is way worse. It had r-squared of 0.46 (CatBoost=0.58)\nTo me it seems that they are a plugin which connects ClickHouse to MySQL to run the model in Pytorch.\nIt’s too complex and hard to debug and understand. The resulting model is not good enough.\nTLDR: Easy to use (if bugs are ignored), too slow to train \u0026 produces a bad model.\n  ","categories":"","description":"CatBoost / MindsDB /  Fast.ai\n","excerpt":"CatBoost / MindsDB /  Fast.ai\n","ref":"/altinity-kb-integrations/catboost-mindsdb-fast.ai/","tags":"","title":"CatBoost / MindsDB /  Fast.ai"},{"body":"Do you have documentation on Docker deployments? Check\n https://hub.docker.com/r/yandex/clickhouse-server/ https://docs.altinity.com/clickhouseonkubernetes/ sources of entry point - https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh  Important things:\n use concrete version tag (avoid using latest) if possible use --network=host (due to performance reasons) you need to mount the folder /var/lib/clickhouse to have persistency. you MAY also mount the folder /var/log/clickhouse-server to have logs accessible outside of the container. Also, you may mount in some files or folders in the configuration folder:  /etc/clickhouse-server/config.d/listen_ports.xml   --ulimit nofile=262144:262144 You can also set on some linux capabilities to enable some of extra features of ClickHouse (not obligatory): SYS_PTRACE NET_ADMIN IPC_LOCK SYS_NICE you may also mount in the folder /docker-entrypoint-initdb.d/ - all SQL or bash scripts there will be executed during container startup. if you use cgroup limits - it may misbehave https://github.com/ClickHouse/ClickHouse/issues/2261 (set up \u003cmax_server_memory_usage\u003e manually) there are several ENV switches, see: https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh  TLDR version: use it as a starting point:\ndocker run -d \\  --name some-clickhouse-server \\  --ulimit nofile=262144:262144 \\  --volume=$(pwd)/data:/var/lib/clickhouse \\  --volume=$(pwd)/logs:/var/log/clickhouse-server \\  --volume=$(pwd)/configs/memory_adjustment.xml:/etc/clickhouse-server/config.d/memory_adjustment.xml \\  --cap-add=SYS_NICE \\  --cap-add=NET_ADMIN \\  --cap-add=IPC_LOCK \\  --cap-add=SYS_PTRACE \\  --network=host \\  yandex/clickhouse-server:21.1.7  docker exec -it some-clickhouse-server clickhouse-client docker exec -it some-clickhouse-server bash ","categories":"","description":"ClickHouse in Docker\n","excerpt":"ClickHouse in Docker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/","tags":"","title":"ClickHouse in Docker"},{"body":"ClickHouse Monitoring Monitoring helps to track potential issues in your cluster before they cause a critical error.\nWhat to read / watch on subject:\n Altinity webinar “ClickHouse Monitoring 101: What to monitor and how”. recording, slides docs https://clickhouse.com/docs/en/operations/monitoring/  What should be monitored The following metrics should be collected / monitored\n  For Host Machine:\n CPU Memory Network (bytes/packets) Storage (iops) Disk Space (free / used)    For ClickHouse:\n Connections (count) RWLocks Read / Write / Return (bytes) Read / Write / Return (rows) Zookeeper operations (count) Absolute delay Query duration (optional) Replication parts and queue (count)    For Zookeeper:\n See separate article    Monitoring tools Prometheus (embedded exporter) + Grafana  Enable embedded exporter Grafana dashboards https://grafana.com/grafana/dashboards/14192 or https://grafana.com/grafana/dashboards/13500  clickhouse-operator embedded exporter  exporter is included in clickhouse-operator, and enabled automatically see instructions of Prometheus and Grafana installation (if you don’t have one) Grafana dashboard https://github.com/Altinity/clickhouse-operator/tree/master/grafana-dashboard Prometheus alerts https://github.com/Altinity/clickhouse-operator/blob/master/deploy/prometheus/prometheus-alert-rules-clickhouse.yaml  Prometheus exporter (external) + Grafana  clickhouse-exporter Dashboard: https://grafana.com/grafana/dashboards/882  (unmaintained)\nDashboards quering clickhouse directly  Overview: https://grafana.com/grafana/dashboards/13606 Queries dashboard (analyzing system.query_log) https://grafana.com/grafana/dashboards/2515  Zabbix  https://www.zabbix.com/integrations/clickhouse https://github.com/Altinity/clickhouse-zabbix-template  Graphite  Use the embedded exporter. See docs and config.xml  InfluxDB  You can use embedded exporter, plus Telegraf. For more information, see Graphite protocol support in InfluxDB.  Nagios/Icinga  https://github.com/exogroup/check_clickhouse/  Commercial solution  Datadog https://docs.datadoghq.com/integrations/clickhouse/?tab=host Sematext https://sematext.com/docs/integration/clickhouse/ Instana https://www.instana.com/supported-technologies/clickhouse-monitoring/ site24x7 https://www.site24x7.com/plugins/clickhouse-monitoring.html Acceldata Pulse https://www.acceldata.io/blog/acceldata-pulse-for-clickhouse-monitoring  “Build your own” monitoring ClickHouse allow to access lot of internals using system tables. The main tables to access monitoring data are:\n system.metrics system.asynchronous_metrics system.events  Minimum neccessary set of checks\n  Check Name  Shell or SQL command  Severity    ClickHouse status  $ curl 'http://localhost:8123/'  Ok.  Critical    Too many simultaneous queries. Maximum: 100 (by default)  select value from system.metrics   where metric='Query'  Critical    Replication status  $ curl 'http://localhost:8123/replicas_status'  Ok.  High    Read only replicas (reflected by replicas_status as well)  select value from system.metrics   where metric='ReadonlyReplica'  High    Some replication tasks are stuck  select count()  from system.replication_queue  where num_tries  100 or num_postponed  1000  High    ZooKeeper is available  select count() from system.zookeeper   where path='/'  Critical for writes    ZooKeeper exceptions  select value from system.events   where event='ZooKeeperHardwareExceptions'  Medium    Other CH nodes are available  $ for node in `echo \"select distinct host_address from system.clusters where host_name !='localhost'\" | curl 'http://localhost:8123/' --silent --data-binary @-`; do curl \"http://$node:8123/\" --silent ; done | sort -u  Ok.  High    All CH clusters are available (i.e. every configured cluster has enough replicas to serve queries)  for cluster in `echo \"select distinct cluster from system.clusters where host_name !='localhost'\" | curl 'http://localhost:8123/' --silent --data-binary @-` ; do clickhouse-client --query=\"select '$cluster', 'OK' from cluster('$cluster', system, one)\" ; done   Critical    There are files in 'detached' folders  $ find /var/lib/clickhouse/data/*/*/detached/* -type d | wc -l; \\ 19.8+  select count() from system.detached_parts  Medium    Too many parts: \\ Number of parts is growing; \\ Inserts are being delayed; \\ Inserts are being rejected  select value from system.asynchronous_metrics   where metric='MaxPartCountForPartition';  select value from system.events/system.metrics   where event/metric='DelayedInserts'; \\ select value from system.events   where event='RejectedInserts'  Critical    Dictionaries: exception  select concat(name,': ',last_exception)   from system.dictionaries  where last_exception != ''  Medium    ClickHouse has been restarted  select uptime();  select value from system.asynchronous_metrics   where metric='Uptime'      DistributedFilesToInsert should not be always increasing  select value from system.metrics   where metric='DistributedFilesToInsert'  Medium    A data part was lost  select value from system.events   where event='ReplicatedDataLoss'  High    Data parts are not the same on different replicas  select value from system.events where event='DataAfterMergeDiffersFromReplica'; \\ select value from system.events where event='DataAfterMutationDiffersFromReplica'  Medium            The following queries are recommended to be included in monitoring:\n SELECT * FROM system.replicas  For more information, see the ClickHouse guide on System Tables   SELECT * FROM system.merges  Checks on the speed and progress of currently executed merges.   SELECT * FROM system.mutations  This is the source of information on the speed and progress of currently executed merges.    Logs monitoring ClickHouse logs can be another important source of information. There are 2 logs enabled by default\n /var/log/clickhouse-server/clickhouse-server.err.log (error \u0026 warning, you may want to keep an eye on that or send it to some monitoring system) /var/log/clickhouse-server/clickhouse-server.log (trace logs, very detailed, useful for debugging, usually too verbose to monitor).  You can additionally enable system.text_log table to have an access to the logs from clickhouse sql queries (ensure that you will not expose some information to the users which should not see it).\nOpenTelemetry support See https://clickhouse.com/docs/en/operations/opentelemetry/\nOther sources  https://tech.marksblogg.com/clickhouse-prometheus-grafana.html Key Metrics for Monitoring ClickHouse ClickHouse Monitoring Key Metrics to Monitor ClickHouse Monitoring Tools: Five Tools to Consider Monitoring ClickHouse Monitor ClickHouse with Datadog Unsorted notes on monitor and Alerts https://intl.cloud.tencent.com/document/product/1026/36887 https://chowdera.com/2021/03/20210301161806704Y.html https://chowdera.com/2021/03/20210301160252465m.html#  ","categories":"","description":"ClickHouse Monitoring\n","excerpt":"ClickHouse Monitoring\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/","tags":"","title":"ClickHouse Monitoring"},{"body":"ClickHouse versioning schema Example:\n21.3.10.1-lts\n 21 is the year of release. 3 indicates a Feature Release. This is an increment where features are delivered. 10 is the bugfix / maintenance version. When that version is incremented it means that some bugs was fixed comparing to 21.3.9. 1 - build number, means nothing for end users. lts - type of release. (long time support).  What is Altinity Stable version? It is one of general / public version of ClickHouse which has passed some extra testings, the upgrade path and changelog was analyzed, known issues are documented, and at least few big companies use it on production. All those things take some time, so usually that means that Altinity Stable is always a ‘behind’ the main releases.\nAltinity version - is an option for conservative users, who prefer bit older but better known things.\nUsually there is no reason to use version older than Altinity Stable. If you see that new Altinity Version arrived and you still use some older version - you should for sure consider an upgrade.\nAdditionally for Altinity client we provide extra support for those version for a longer time (and we also support newer versions).\nWhich version should I use? We recommend the following approach:\n When you start using ClickHouse and before you go on production - pick the latest stable version. If you already have ClickHouse running on production:  Check all the new queries / schemas on the staging first, especially if some new ClickHouse features are used. Do minor (bugfix) upgrades regularly: monitor new maintenance releases of the feature release you use. When considering upgrade - check Altinity Stable release docs, if you want to use newer release - analyze changelog and known issues. Check latest stable or test versions of ClickHouse on your staging environment regularly and pass the feedback to us or on the official ClickHouse github. Consider blue/green or canary upgrades.    See also: https://clickhouse.tech/docs/en/faq/operations/production/\nHow do I upgrade? Warning Check upgrade / downgrade scenario on staging first.   check if you need to adjust some settings / to opt-out some new features you don’t need (maybe needed to to make the downgrade path possible, or to make it possible for 2 versions to work together). upgrade packages on odd replicas (if needed / depends on use case) stop ingestion into odd replicas / remove them for load-balancer etc. restart clickhouse-server service on odd replicas. once odd replicas will go back online - repeat the same procedure on the even replicas.  In some upgrade scenarios (depending from which version to which you do upgrate) when differerent replicas use different clickhouse versions you may see following issues:\n the replication don’t work at all and delays grow. errors about ‘checksum mismatch’ and traffic between replicase increase (they need to resync merge results).  Both problems will go away once all replicas will be upgraded.\nBugs? ClickHouse development process goes in a very high pace and has already thousands of features. CI system doing tens of thousands of tests (including tests with different sanitizers) against every commit.\nAll core features are well-tested, and very stable, and code is high-quality. But as with any other software bad things may happen. Usually the most of bugs happens in the new, freshly added functionality, and in some complex combination of several features (of course all possible combinations of features just physically can’t be tested). Usually new features are adopted by the community and stabilize quickly.\nWhat should I do if I found a bug in clickhouse?  First of all: try to upgrade to the latest bugfix release Example: if you use v21.3.5.42-lts but you know that v21.3.10.1-lts already exists - start with upgrade to that. Upgrades to latest maintenance releases are smooth and safe. Look for similar issues in github. Maybe the fix is on the way. If you can reproduce the bug: try to isolate it - remove some pieces of query one-by-one / simplify the scenario until the issue still reproduces. This way you can figure out which part is responsible for that bug, and you can try to create minimal reproducible example Once you have minimal reproducible example:  report it to github (or to Altinity Support) check if it reproduces on newer clickhouse versions    ","categories":"","description":"ClickHouse versions\n","excerpt":"ClickHouse versions\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-versions/","tags":"","title":"ClickHouse versions"},{"body":"Installation and configuration Download the latest clickhouse-backup.tar.gz from assets from https://github.com/AlexAkulov/clickhouse-backup/releases\nThis tar.gz contains a single binary of clickhouse-backup and an example of config file.\nBackblaze has s3 compatible API but requires empty acl parameter acl: \"\".\nhttps://www.backblaze.com/ has 15 days and free 10Gb S3 trial.\n$ mkdir clickhouse-backup$ cd clickhouse-backup$ wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/1.0.0-beta2/clickhouse-backup.tar.gz$ tar zxf clickhouse-backup.tar.gz$ rm clickhouse-backup.tar.gz$ cat config.ymlgeneral:remote_storage:s3max_file_size:1099511627776disable_progress_bar:falsebackups_to_keep_local:0backups_to_keep_remote:0log_level:infoallow_empty_backups:falseclickhouse:username:defaultpassword:\"\"host:localhostport:9000disk_mapping:{}skip_tables:- system.*timeout:5mfreeze_by_part:falsesecure:falseskip_verify:falsesync_replicated_tables:truelog_sql_queries:falses3:access_key:0****1secret_key:K****1bucket:\"mybucket\"endpoint:s3.us-west-000.backblazeb2.comregion:us-west-000acl:\"\"force_path_style:falsepath:clickhouse-backupdisable_ssl:falsepart_size:536870912compression_level:1compression_format:tarsse:\"\"disable_cert_verification:falsestorage_class:STANDARDI have a database test with table test\nselectcount()fromtest.test;┌─count()─┐│400000│└─────────┘clickhouse-backup list should work without errors (it scans local and remote (s3) folders):\n$ sudo ./clickhouse-backup list -c config.yml $ Backup  create a local backup of database test upload this backup to remote remove the local backup drop the source database  $ sudo ./clickhouse-backup create --tables='test.*' bkp01 -c config.yml 2021/05/31 23:11:13 info done backup=bkp01 operation=create table=test.test 2021/05/31 23:11:13 info done backup=bkp01 operation=create  $ sudo ./clickhouse-backup upload bkp01 -c config.yml  1.44 MiB / 1.44 MiB [=====================] 100.00% 2s 2021/05/31 23:12:13 info done backup=bkp01 operation=upload table=test.test 2021/05/31 23:12:17 info done backup=bkp01 operation=upload  $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 local bkp01 1.44MiB 31/05/2021 23:11:13 remote tar  $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:13:29 info delete 'bkp01' DROPDATABASEtest;Restore  download the remote backup restore database  $ sudo ./clickhouse-backup list -c config.yml bkp01 1.44MiB 31/05/2021 23:11:13 remote tar  $ sudo ./clickhouse-backup download bkp01 -c config.yml 2021/05/31 23:14:41 info done backup=bkp01 operation=download table=test.test  1.47 MiB / 1.47 MiB [=====================] 100.00% 0s 2021/05/31 23:14:43 info done backup=bkp01 operation=download table=test.test 2021/05/31 23:14:43 info done backup=bkp01 operation=download  $ sudo ./clickhouse-backup restore bkp01 -c config.yml 2021/05/31 23:16:04 info done backup=bkp01 operation=restore table=test.test 2021/05/31 23:16:04 info done backup=bkp01 operation=restore SELECTcount()FROMtest.test;┌─count()─┐│400000│└─────────┘Delete backups $ sudo ./clickhouse-backup delete local bkp01 -c config.yml 2021/05/31 23:17:05 info delete 'bkp01'  $ sudo ./clickhouse-backup delete remote bkp01 -c config.yml ","categories":"","description":"clickhouse-backup + backblaze\n","excerpt":"clickhouse-backup + backblaze\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-backup/","tags":"","title":"clickhouse-backup"},{"body":"Q. How can I input multi-line SQL code? can you guys give me an example?\nA. Just run clickhouse-client with -m switch, and it starts executing only after you finish the line with a semicolon.\nQ. How can i use pager with clickhouse-client\nA. Here is an example: clickhouse-client --pager 'less -RS'\nQ. Data is returned in chunks / several tables.\nA. Data get streamed from the server in blocks, every block is formatted individually when the default PrettyCompact format is used. You can use PrettyCompactMonoBlock format instead, using one of the options:\n start clickhouse-client with an extra flag: clickhouse-client --format=PrettyCompactMonoBlock add FORMAT PrettyCompactMonoBlock at the end of your query. change clickhouse-client default format in the config. See https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42  Q. Сustomize client config\nA. you can change it globally (for all users of the workstation)\nnano /etc/clickhouse-client/conf.d/user.xml \u003cconfig\u003e \u003cuser\u003edefault1\u003c/user\u003e \u003cpassword\u003edefault1\u003c/password\u003e \u003chost\u003e\u003c/host\u003e \u003cmultiline\u003etrue\u003c/multiline\u003e \u003cmultiquery\u003etrue\u003c/multiquery\u003e \u003c/config\u003e See also https://github.com/ClickHouse/ClickHouse/blob/976dbe8077f9076387528e2f40b6174f6d8a8b90/programs/client/clickhouse-client.xml#L42 or for particular users - by adjusting one of.\n./clickhouse-client.xml ~/.clickhouse-client/config.xml Also, it’s possible to have several client config files and pass one of them to clickhouse-client command explicitly\nReferences:\n https://clickhouse.tech/docs/en/interfaces/cli/  ","categories":"","description":"ClickHouse client\n","excerpt":"ClickHouse client\n","ref":"/altinity-kb-interfaces/altinity-kb-clickhouse-client/","tags":"","title":"clickhouse-client"},{"body":"The description of the utility and its parameters, as well as examples of the config files that you need to create for the copier are in the doc https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/\nThe steps to run a task:\n  Create a config file for clickhouse-copier (zookeeper.xml)\nhttps://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#format-of-zookeeper-xml\n  Create a config file for the task (task1.xml)\nhttps://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/#configuration-of-copying-tasks\n  Create the task in ZooKeeper and start an instance of clickhouse-copierclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1 --task-file /opt/clickhouse-copier/task1.xml\n  If the node in ZooKeeper already exists and you want to change it, you need to add the task-upload-force parameter:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1 --task-file /opt/clickhouse-copier/task1.xml --task-upload-force 1\nIf you want to run another instance of clickhouse-copier for the same task, you need to copy the config file (zookeeper.xml) to another server, and run this command:\nclickhouse-copier --daemon --base-dir=/opt/clickhouse-copier --config /opt/clickhouse-copier/zookeeper.xml --task-path /clickhouse/copier/task1\nThe number of simultaneously running instances is controlled be the max_workers parameter in your task configuration file. If you run more workers superfluous workers will sleep and log messages like this:\n\u003cDebug\u003e ClusterCopier: Too many workers (1, maximum 1). Postpone processing\nSee also  https://clickhouse.tech/docs/en/operations/utilities/clickhouse-copier/ Никита Михайлов. Кластер ClickHouse ctrl-с ctrl-v. HighLoad++ Весна 2021 slides 21.7 have a huge bulk of fixes / improvements. https://github.com/ClickHouse/ClickHouse/pull/23518 https://altinity.com/blog/2018/8/22/clickhouse-copier-in-practice http://www.clickhouse.com.cn/topic/601fb322b06e5e0f21ba79e1 https://github.com/getsentry/snuba/blob/master/docs/clickhouse-copier.md https://hughsite.com/post/clickhouse-copier-usage.html https://www.jianshu.com/p/c058edd664a6  ","categories":"","description":"clickhouse-copier\n","excerpt":"clickhouse-copier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/","tags":"","title":"clickhouse-copier"},{"body":"Clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe process is as follows  Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one.  Drop the partition from the target table if it’s not empty Copy data from source shards one by one.  Check if there is data for the partition on a source shard. Check the status of the task in ZooKeeper. Create target tables on all shards of the target cluster. Insert the partition of data into the target table.   Mark the partition as completed in ZooKeeper.    If there are several workers running simultaneously, they will assign themselves to different source shards. If a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table Clickhouse-copier uses the engine from the task configuration file for these purposes:\n to create target tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to DROP existing partitions from target tables.  Clickhouse-copier does not support the old MergeTree format. However, you can create the target tables manually and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for its SELECT queries.\nHow to monitor the status of running tasks Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers. Here is a list of queries that you can use to see what’s happening.\n--task-path /clickhouse/copier/task1 -- The task config select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e'name|ctime|mtime----------------------------+---------------------+-------------------- description|2019-10-1815:40:00|2020-09-1116:01:14task_active_workers_version|2019-10-1816:00:09|2020-09-1116:07:08tables|2019-10-1816:00:25|2019-10-1816:00:25task_active_workers|2019-10-1816:00:09|2019-10-1816:00:09-- Running workers select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/task_active_workers'-- The list of processed tables select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables'-- The list of processed partitions select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e'name|ctime-------+-------------------- 201909|2019-10-1818:24:18-- The status of a partition select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e'name|ctime-------------------------+-------------------- shards|2019-10-1818:24:18partition_active_workers|2019-10-1818:24:18-- The status of source shards select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/shards'name|ctime|mtime-----+---------------------+-------------------- 1|2019-10-1822:37:48|2019-10-1822:49:29","categories":"","description":"clickhouse-copier 20.3 and earlier\n","excerpt":"clickhouse-copier 20.3 and earlier\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/","tags":"","title":"clickhouse-copier 20.3 and earlier"},{"body":"Clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. Clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.\nThe behavior of clickhouse-copier was changed in 20.4:\n Now clickhouse-copier inserts data into intermediate tables, and after the insert finishes successfully clickhouse-copier attaches the completed partition into the target table. This allows for incremental data copying, because the data in the target table is intact during the process. Important note: ATTACH PARTITION respects the max_partition_size_to_drop limit. Make sure the max_partition_size_to_drop limit is big enough (or set to zero) in the destination cluster. If clickhouse-copier is unable to attach a partition because of the limit, it will proceed to the next partition, and it will drop the intermediate table when the task is finished (if the intermediate table is less than the max_table_size_to_drop limit). Another important note: ATTACH PARTITION is replicated. The attached partition will need to be downloaded by the other replicas. This can create significant network traffic between ClickHouse nodes. If an attach takes a long time, clickhouse-copier will log a timeout and will proceed to the next step. Now clickhouse-copier splits the source data into chunks and copies them one by one. This is useful for big source tables, when inserting one partition of data can take hours. If there is an error during the insert clickhouse-copier has to drop the whole partition and start again. The number_of_splits parameter lets you split your data into chunks so that in case of an exception clickhouse-copier has to re-insert only one chunk of the data. Now clickhouse-copier runs OPTIMIZE target_table PARTITION ... DEDUPLICATE for non-Replicated MergeTree tables. Important note: This is a very strange feature that can do more harm than good. We recommend to disable it by configuring the engine of the target table as Replicated in the task configuration file, and create the target tables manually if they are not supposed to be replicated. Intermediate tables are always created as plain MergeTree.  The process is as follows  Process the configuration files. Discover the list of partitions if not provided in the config. Copy partitions one by one I’m not sure of the order since I was copying from 1 shard to 4 shards. The metadata in ZooKeeper suggests the order described here.  Copy chunks of data one by one.  Copy data from source shards one by one.  Create intermediate tables on all shards of the target cluster. Check the status of the chunk in ZooKeeper. Drop the partition from the intermediate table if the previous attempt was interrupted. Insert the chunk of data into the intermediate tables. Mark the shard as completed in ZooKeeper     Attach the chunks of the completed partition into the target table one by one  Attach a chunk into the target table. non-Replicated: Run OPTIMIZE target_table DEDUPLICATE for the partition on the target table.     Drop intermediate tables (may not succeed if the tables are bigger than max_table_size_to_drop).  If there are several workers running simultaneously, they will assign themselves to different source shards. If a worker was interrupted, another worker can be started to continue the task. The next worker will drop incomplete partitions and resume the copying.\nConfiguring the engine of the target table Clickhouse-copier uses the engine from the task configuration file for these purposes:\n to create target and intermediate tables if they don’t exist. PARTITION BY: to SELECT a partition of data from the source table, to ATTACH partitions into target tables, to DROP incomplete partitions from intermediate tables, to OPTIMIZE partitions after they are attached to the target. ORDER BY: to SELECT a chunk of data from the source table.  Here is an example of SELECT that clickhouse-copier runs to get the sixth of ten chunks of data:\nWHERE(\u003cthePARTITIONBYclause\u003e=(\u003cavalueofthePARTITIONBYexpression\u003eASpartition_key))AND(cityHash64(\u003ctheORDERBYclause\u003e)%10=6)Clickhouse-copier does not support the old MergeTree format. However, you can create the intermediate tables manually with the same engine as the target tables (otherwise ATTACH will not work), and specify the engine in the task configuration file in the new format so that clickhouse-copier can parse it for SELECT, ATTACH PARTITION and DROP PARTITION queries.\nImportant note: always configure engine as Replicated to disable OPTIMIZE … DEDUPLICATE (unless you know why you need clickhouse-copier to run OPTIMIZE … DEDUPLICATE).\nHow to configure the number of chunks The default value for number_of_splits is 10. You can change this parameter in the table section of the task configuration file. We recommend setting it to 1 for smaller tables.\n\u003ccluster_push\u003etarget_cluster\u003c/cluster_push\u003e \u003cdatabase_push\u003etarget_database\u003c/database_push\u003e \u003ctable_push\u003etarget_table\u003c/table_push\u003e \u003cnumber_of_splits\u003e1\u003c/number_of_splits\u003e \u003cengine\u003eEngine=Replicated...\u003cengine\u003e How to monitor the status of running tasks Clickhouse-copier uses ZooKeeper to keep track of the progress and to communicate between workers. Here is a list of queries that you can use to see what’s happening.\n--task-path /clickhouse/copier/task1 -- The task config select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e'name|ctime|mtime----------------------------+---------------------+-------------------- description|2021-03-2213:15:48|2021-03-2213:25:28task_active_workers_version|2021-03-2213:15:48|2021-03-2220:32:09tables|2021-03-2213:16:47|2021-03-2213:16:47task_active_workers|2021-03-2213:15:48|2021-03-2213:15:48-- Running workers select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/task_active_workers'-- The list of processed tables select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables'-- The list of processed partitions select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e'name|ctime-------+-------------------- 202103|2021-03-2213:16:47202102|2021-03-2213:18:31202101|2021-03-2213:27:36202012|2021-03-2214:05:08-- The status of a partition select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e'name|ctime---------------+-------------------- piece_0|2021-03-2213:18:31attach_is_done|2021-03-2214:05:05-- The status of a piece select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N'name|ctime-------------------------------+-------------------- shards|2021-03-2213:18:31is_dirty|2021-03-2213:26:51partition_piece_active_workers|2021-03-2213:26:54clean_start|2021-03-2213:26:54-- The status of source shards select*fromsystem.zookeeperwherepath='\u003ctask-path\u003e/tables/\u003ctable\u003e/\u003cpartition\u003e/piece_N/shards'name|ctime|mtime-----+---------------------+-------------------- 1|2021-03-2213:26:54|2021-03-2214:05:05","categories":"","description":"clickhouse-copier 20.4 - 21.6\n","excerpt":"clickhouse-copier 20.4 - 21.6\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4+/","tags":"","title":"clickhouse-copier 20.4 - 21.6"},{"body":"In 21.3 there is already an option to run own clickhouse zookeeper implementation. It’s still experimental, and still need to be started additionally on few nodes (similar to ’normal’ zookeeper) and speaks normal zookeeper protocol - needed to simplify A/B tests with real zookeeper.\nNo docs, for now, only PR with code \u0026 tests. Of course, if you want to play with it - you can, and early feedback is very valuable. But be prepared for a lot of tiny issues here and there, so don’t be disappointed if it will not satisfy your expectations for some reason. It’s very-very fresh :slightly_smiling_face: It’s ready for some trial runs, but not ready yet for production use cases.\nTo test that you need to run 3 instances of clickhouse-server (which will mimic zookeeper) with an extra config like that:\nhttps://github.com/ClickHouse/ClickHouse/blob/c8b1004ecb4bfc4aa581dbcbbbe3a4c72ce57123/tests/integration/test_keeper_multinode_simple/configs/enable_keeper1.xml\nhttps://github.com/ClickHouse/ClickHouse/blob/c8b1004ecb4bfc4aa581dbcbbbe3a4c72ce57123/tests/integration/test_keeper_snapshots/configs/enable_keeper.xml\nor event single instance with config like that: https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/keeper_port.xml https://github.com/ClickHouse/ClickHouse/blob/master/tests/config/config.d/zookeeper.xml\nAnd point all the clickhouses (zookeeper config secton) to those nodes / ports.\nLatest testing version is recommended. We will be thankful for any feedback.\nExample of a simple cluster with 2 nodes of Clickhouse using built-in keeper For example you can start two Clikhouse nodes (hostname1, hostname2)\nhostname1 $ cat /etc/clickhouse-server/config.d/keeper.xml  \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003ckeeper_server\u003e  \u003ctcp_port\u003e2181\u003c/tcp_port\u003e  \u003cserver_id\u003e1\u003c/server_id\u003e  \u003clog_storage_path\u003e/var/lib/clickhouse/coordination/log\u003c/log_storage_path\u003e  \u003csnapshot_storage_path\u003e/var/lib/clickhouse/coordination/snapshots\u003c/snapshot_storage_path\u003e   \u003ccoordination_settings\u003e  \u003coperation_timeout_ms\u003e10000\u003c/operation_timeout_ms\u003e  \u003csession_timeout_ms\u003e30000\u003c/session_timeout_ms\u003e  \u003craft_logs_level\u003etrace\u003c/raft_logs_level\u003e  \u003crotate_log_storage_interval\u003e10000\u003c/rotate_log_storage_interval\u003e  \u003c/coordination_settings\u003e   \u003craft_configuration\u003e  \u003cserver\u003e  \u003cid\u003e1\u003c/id\u003e  \u003chostname\u003ehostname1\u003c/hostname\u003e  \u003cport\u003e9444\u003c/port\u003e  \u003c/server\u003e  \u003cserver\u003e  \u003cid\u003e2\u003c/id\u003e  \u003chostname\u003ehostname2\u003c/hostname\u003e  \u003cport\u003e9444\u003c/port\u003e  \u003c/server\u003e  \u003c/raft_configuration\u003e   \u003c/keeper_server\u003e   \u003czookeeper\u003e  \u003cnode\u003e  \u003chost\u003elocalhost\u003c/host\u003e  \u003cport\u003e2181\u003c/port\u003e  \u003c/node\u003e  \u003c/zookeeper\u003e   \u003cdistributed_ddl\u003e  \u003cpath\u003e/clickhouse/testcluster/task_queue/ddl\u003c/path\u003e  \u003c/distributed_ddl\u003e \u003c/yandex\u003e  $ cat /etc/clickhouse-server/config.d/macros.xml  \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003cmacros\u003e  \u003ccluster\u003etestcluster\u003c/cluster\u003e  \u003creplica\u003ereplica1\u003c/replica\u003e  \u003cshard\u003e1\u003c/shard\u003e  \u003c/macros\u003e \u003c/yandex\u003e hostname2 $ cat /etc/clickhouse-server/config.d/keeper.xml  \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003ckeeper_server\u003e  \u003ctcp_port\u003e2181\u003c/tcp_port\u003e  \u003cserver_id\u003e2\u003c/server_id\u003e  \u003clog_storage_path\u003e/var/lib/clickhouse/coordination/log\u003c/log_storage_path\u003e  \u003csnapshot_storage_path\u003e/var/lib/clickhouse/coordination/snapshots\u003c/snapshot_storage_path\u003e   \u003ccoordination_settings\u003e  \u003coperation_timeout_ms\u003e10000\u003c/operation_timeout_ms\u003e  \u003csession_timeout_ms\u003e30000\u003c/session_timeout_ms\u003e  \u003craft_logs_level\u003etrace\u003c/raft_logs_level\u003e  \u003crotate_log_storage_interval\u003e10000\u003c/rotate_log_storage_interval\u003e  \u003c/coordination_settings\u003e   \u003craft_configuration\u003e  \u003cserver\u003e  \u003cid\u003e1\u003c/id\u003e  \u003chostname\u003ehostname1\u003c/hostname\u003e  \u003cport\u003e9444\u003c/port\u003e  \u003c/server\u003e  \u003cserver\u003e  \u003cid\u003e2\u003c/id\u003e  \u003chostname\u003ehostname2\u003c/hostname\u003e  \u003cport\u003e9444\u003c/port\u003e  \u003c/server\u003e  \u003c/raft_configuration\u003e   \u003c/keeper_server\u003e   \u003czookeeper\u003e  \u003cnode\u003e  \u003chost\u003elocalhost\u003c/host\u003e  \u003cport\u003e2181\u003c/port\u003e  \u003c/node\u003e  \u003c/zookeeper\u003e   \u003cdistributed_ddl\u003e  \u003cpath\u003e/clickhouse/testcluster/task_queue/ddl\u003c/path\u003e  \u003c/distributed_ddl\u003e \u003c/yandex\u003e  $ cat /etc/clickhouse-server/config.d/macros.xml  \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003cmacros\u003e  \u003ccluster\u003etestcluster\u003c/cluster\u003e  \u003creplica\u003ereplica2\u003c/replica\u003e  \u003cshard\u003e1\u003c/shard\u003e  \u003c/macros\u003e \u003c/yandex\u003e on both $ cat /etc/clickhouse-server/config.d/clusters.xml  \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003cremote_servers\u003e  \u003ctestcluster\u003e  \u003cshard\u003e  \u003creplica\u003e  \u003chost\u003ehostname1\u003c/host\u003e  \u003cport\u003e9000\u003c/port\u003e  \u003c/replica\u003e  \u003creplica\u003e  \u003chost\u003ehostname2\u003c/host\u003e  \u003cport\u003e9000\u003c/port\u003e  \u003c/replica\u003e  \u003c/shard\u003e  \u003c/testcluster\u003e  \u003c/remote_servers\u003e \u003c/yandex\u003e Then create a table\ncreatetableteston'{cluster}'(AInt64,SString)Engine=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}','{replica}')OrderbyA;insertintotestselectnumber,''fromnumbers(100000000);-- on both nodes: selectcount()fromtest;","categories":"","description":"clickhouse-keeper\n","excerpt":"clickhouse-keeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper/","tags":"","title":"clickhouse-keeper"},{"body":"ClickHouse does not start, some other unexpected behavior happening Check clickhouse logs, they are your friends:\ntail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log | less tail -n 10000 /var/log/clickhouse-server/clickhouse-server.log | less\nHow Do I Restrict Memory Usage? See our knowledge base article and official documentation for more information.\nClickHouse died during big query execution Misconfigured clickhouse can try to allocate more RAM than is available on the system.\nIn that case an OS component called oomkiller can kill the clickhouse process.\nThat event leaves traces inside system logs (can be checked by running dmesg command).\nHow Do I make huge ‘Group By’ queries use less RAM? Enable on disk GROUP BY (it is slower, so is disabled by default)\nSet max_bytes_before_external_group_by to a value about 70-80% of your max_memory_usage value.\nData returned in chunks by clickhouse-client See altinity-kb-clickhouse-client\nI Can’t Connect From Other Hosts. What do I do? Check the  settings in config.xml. Verify that the connection can connect on both IPV4 and IPV6.\n","categories":"","description":"Cluster Configuration FAQ\n","excerpt":"Cluster Configuration FAQ\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/","tags":"","title":"Cluster Configuration FAQ"},{"body":"So you set up 3 nodes with zookeeper (zookeeper1, zookeeper2, zookeeper3 - How to install zookeer?), and and 4 nodes with ClickHouse (clickhouse-sh1r1,clickhouse-sh1r2,clickhouse-sh2r1,clickhouse-sh2r2 - how to install ClickHouse?). Now we need to make them work together.\nUse ansible/puppet/salt or other systems to control the servers’ configurations.\n Configure ClickHouse access to Zookeeper by adding the file zookeeper.xml in /etc/clickhouse-server/config.d/ folder. This file must be placed on all ClickHouse servers.  \u003cyandex\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003ezookeeper1\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper2\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003cnode\u003e \u003chost\u003ezookeeper3\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003c/yandex\u003e  On each server put the file macros.xml in /etc/clickhouse-server/config.d/ folder.  \u003cyandex\u003e \u003c!-- That macros are defined per server, and they can be used in DDL, to make the DB schema cluster/server neutral --\u003e \u003cmacros\u003e \u003ccluster\u003eprod_cluster\u003c/cluster\u003e \u003cshard\u003e01\u003c/shard\u003e \u003creplica\u003eclickhouse-sh1r1\u003c/replica\u003e \u003c!-- better - use the same as hostname --\u003e \u003c/macros\u003e \u003c/yandex\u003e  On each server place the file cluster.xml in /etc/clickhouse-server/config.d/ folder. Before 20.10 ClickHouse will use default user to connect to other nodes (configurable, other users can be used), since 20.10 we recommend to use passwordless intercluster authentication based on common secret (HMAC auth)  \u003cyandex\u003e \u003cremote_servers\u003e \u003cprod_cluster\u003e \u003c!-- you need to give a some name for a cluster --\u003e \u003c!-- \u003csecret\u003esome_random_string, same on all cluster nodes, keep it safe\u003c/secret\u003e --\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh1r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003cshard\u003e \u003cinternal_replication\u003etrue\u003c/internal_replication\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r1\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003creplica\u003e \u003chost\u003eclickhouse-sh2r2\u003c/host\u003e \u003cport\u003e9000\u003c/port\u003e \u003c/replica\u003e \u003c/shard\u003e \u003c/prod_cluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e  A good practice is to create 2 additional cluster configurations similar to prod_cluster above with the following distinction: but listing all nodes of single shard (all are replicas) and as nodes of 6 different shards (no replicas)  all-replicated: All nodes are listed as replicas in a single shard. all-sharded: All nodes are listed as separate shards with no replicas.    Once this is complete, other queries that span nodes can be performed. For example:\nCREATETABLEtest_table_localONCLUSTER'{cluster}'(idUInt8)Engine=ReplicatedMergeTree('/clickhouse/tables/{database}/{table}/{shard}','{replica}')ORDERBY(id);That will create a table on all servers in the cluster. You can insert data into this table and it will be replicated automatically to the other shards.To store the data or read the data from all shards at the same time, create a Distributed table that links to the replicatedMergeTree table.\nCREATETABLEtest_tableONCLUSTER'{cluster}'Engine=Distributed('{cluster}','default',' Hardening ClickHouse Security See https://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\nAdditional Settings See altinity-kb-settings-to-adjust\nUsers Disable or add password for the default users default and readonly if your server is accessible from non-trusted networks.\nIf you add password to the default user, you will need to adjust cluster configuration, since the other servers need to know the default user’s should know the default user’s to connect to each other.\nIf you’re inside a trusted network, you can leave default user set to nothing to allow the ClickHouse nodes to communicate with each other.\nEngines \u0026 ClickHouse building blocks For general explanations of roles of different engines - check the post Distributed vs Shard vs Replicated ahhh, help me!!!.\nZookeeper Paths Use conventions for zookeeper paths. For example, use:\nReplicatedMergeTree(’/clickhouse/{cluster}/tables/{shard}/table_name’, ‘{replica}’)\nfor:\nSELECT * FROM system.zookeeper WHERE path=’/ …';\nConfiguration Best Practices    Attribution\nModified by a post [on GitHub by Mikhail Filimonov](https://github.com/ClickHouse/ClickHouse/issues/3607#issuecomment-440235298).\n     The following are recommended Best Practices when it comes to setting up a ClickHouse Cluster with Zookeeper:\n Don’t edit/overwrite default configuration files. Sometimes a newer version of ClickHouse introduces some new settings or changes the defaults in config.xml and users.xml.  Set configurations via the extra files in conf.d directory. For example, to overwrite the interface save the file config.d/listen.xml, with the following:    \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003clisten_host replace=\"replace\"\u003e::\u003c/listen_host\u003e \u003c/yandex\u003e  The same is true for users. For example, change the default profile by putting the file in users.d/profile_default.xml:  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault replace=\"replace\"\u003e \u003cmax_memory_usage\u003e15000000000\u003c/max_memory_usage\u003e \u003cmax_bytes_before_external_group_by\u003e12000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e12000000000\u003c/max_bytes_before_external_sort\u003e \u003cdistributed_aggregation_memory_efficient\u003e1\u003c/distributed_aggregation_memory_efficient\u003e \u003cuse_uncompressed_cache\u003e0\u003c/use_uncompressed_cache\u003e \u003cload_balancing\u003erandom\u003c/load_balancing\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003cmax_execution_time\u003e600\u003c/max_execution_time\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e  Or you can create a user by putting a file users.d/user_xxx.xml (since 20.5 you can also use CREATE USER)  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cusers\u003e \u003cxxx\u003e \u003c!-- PASSWORD=$(base64 \u003c /dev/urandom | head -c8); echo \"$PASSWORD\"; echo -n \"$PASSWORD\" | sha256sum | tr -d '-' --\u003e \u003cpassword_sha256_hex\u003e...\u003c/password_sha256_hex\u003e \u003cnetworks incl=\"networks\" /\u003e \u003cprofile\u003ereadonly\u003c/profile\u003e \u003cquota\u003edefault\u003c/quota\u003e \u003callow_databases incl=\"allowed_databases\" /\u003e \u003c/xxx\u003e \u003c/users\u003e \u003c/yandex\u003e  Some parts of configuration will contain repeated elements (like allowed ips for all the users). To avoid repeating that - use substitutions file. By default its /etc/metrika.xml, but you can change it for example to /etc/clickhouse-server/substitutions.xml with the \u003cinclude_from\u003e section of the main config. Put the repeated parts into substitutions file, like this:  \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cnetworks\u003e \u003cip\u003e::1\u003c/ip\u003e \u003cip\u003e127.0.0.1\u003c/ip\u003e \u003cip\u003e10.42.0.0/16\u003c/ip\u003e \u003cip\u003e192.168.0.0/24\u003c/ip\u003e \u003c/networks\u003e \u003c/yandex\u003e These files can be common for all the servers inside the cluster or can be individualized per server. If you choose to use one substitutions file per cluster, not per node, you will also need to generate the file with macros, if macros are used.\nThis way you have full flexibility; you’re not limited to the settings described in the template. You can change any settings per server or data center just by assigning files with some settings to that server or server group. It becomes easy to navigate, edit, and assign files.\nOther Configuration Recommendations Other configurations that should be evaluated:\n  in config.xml: Determines which IP addresses and ports the ClickHouse servers listen for incoming communications. \u003cmax_memory_..\u003e and \u003cmax_bytes_before_external_…\u003e in users.xml. These are part of the profile . \u003cmax_execution_time\u003e \u003clog_queries\u003e  The following extra debug logs should be considered:\n part_log text_log  Understanding The Configuration ClickHouse configuration stores most of its information in two files:\n config.xml: Stores Server configuration parameters. They are server wide, some are hierarchical , and most of them can’t be changed in runtime. The list of settings to apply without a restart changes from version to version. Some settings can be verified using system tables, for example:  macros (system.macros) remote_servers (system.clusters)   users.xml: Configure users, and user level / session level settings.  Each user can change these during their session by:  Using parameter in http query By using parameter for clickhouse-client Sending query like set allow_experimental_data_skipping_indices=1.   Those settings and their current values are visible in system.settings. You can make some settings global by editing default profile in users.xml, which does not need restart. You can forbid users to change their settings by using readonly=2 for that user, or using setting constraints. Changes in users.xml are applied w/o restart.    For both config.xml and users.xml, it’s preferable to put adjustments in the config.d and users.d subfolders instead of editing config.xml and users.xml directly.\nYou can check if the config file was reread by checking /var/lib/clickhouse/preprocessed_configs/ folder.\n","categories":"","description":"Cluster Configuration Process\n","excerpt":"Cluster Configuration Process\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/","tags":"","title":"Cluster Configuration Process"},{"body":"See\nHow to test different compression codecs\nhttps://altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse\nhttps://www.percona.com/sites/default/files/ple19-slides/day1-pm/clickhouse-for-timeseries.pdf\n","categories":"","description":"Codecs\n","excerpt":"Codecs\n","ref":"/altinity-kb-schema-design/codecs/","tags":"","title":"Codecs"},{"body":" Info Supported since 20.10 (PR #15089). On older versions you will get exception: DB::Exception: Codec Delta is not applicable for Array(UInt64) because the data type is not of fixed size.  DROPTABLEIFEXISTSarray_codec_testSYNCcreatetablearray_codec_test(numberUInt64,arrArray(UInt64))Engine=MergeTreeORDERBYnumber;INSERTINTOarray_codec_testSELECTnumber,arrayMap(i-\u003enumber+i,range(100))fromnumbers(10000000);/**** Default LZ4 *****/OPTIMIZETABLEarray_codec_testFINAL;--- Elapsed: 3.386 sec. SELECT*FROMsystem.columnsWHERE(table='array_codec_test')AND(name='arr')/* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 173866750 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: *//****** Delta, LZ4 ******/ALTERTABLEarray_codec_testMODIFYCOLUMNarrArray(UInt64)CODEC(Delta,LZ4);OPTIMIZETABLEarray_codec_testFINAL--0 rows in set. Elapsed: 4.577 sec. SELECT*FROMsystem.columnsWHERE(table='array_codec_test')AND(name='arr')/* Row 1: ────── database: default table: array_codec_test name: arr type: Array(UInt64) position: 2 default_kind: default_expression: data_compressed_bytes: 32458310 data_uncompressed_bytes: 8080000000 marks_bytes: 58656 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: CODEC(Delta(8), LZ4) */","categories":"","description":"Codecs on array columns\n","excerpt":"Codecs on array columns\n","ref":"/altinity-kb-schema-design/codecs/codecs-on-array-columns/","tags":"","title":"Codecs on array columns"},{"body":"createtabletest_codec_speedengine=MergeTreeORDERBYtuple()asselectcast(now()+rand()%2000+number,'DateTime')asxfromnumbers(1000000000);option1:CODEC(LZ4)(sameasdefault)option2:CODEC(DoubleDelta)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(DoubleDelta)`);option3:CODEC(T64,LZ4)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,LZ4)`)option4:CODEC(Delta,LZ4)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Delta,LZ4)`)option5:CODEC(ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(ZSTD(1))`)option6:CODEC(T64,ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,ZSTD(1))`)option7:CODEC(Delta,ZSTD(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Delta,ZSTD(1))`)option8:CODEC(T64,LZ4HC(1))(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(T64,LZ4HC(1))`)option9:CODEC(Gorilla)(`altertabletest_codec_speedmodifycolumnxDateTimeCODEC(Gorilla)`)Resultmaybenot100%reliable(checkedonmylaptop,needtoberepeatedinlabenvironment)OPTIMIZETABLEtest_codec_speedFINAL(secondrun-i.e.read+writethesamedata)1)17sec.2)30sec.3)16sec4)17sec5)29sec6)24sec7)31sec8)35sec9)19seccompressedsize1)31813768812)23337936993)18626603074)34085027575)23930782666)17655561737)21760804978)18104712479)2109640716selectmax(x)fromtest_codec_speed1)0.5972)2.756:(3)1.1684)0.7525)1.3626)1.3647)1.7528)1.2709)1.607","categories":"","description":"Codecs speed\n","excerpt":"Codecs speed\n","ref":"/altinity-kb-schema-design/codecs/codecs-speed/","tags":"","title":"Codecs speed"},{"body":"Options here are:\n UseINSERT INTO foo_replicated SELECT * FROM foo . Create table aside and attach all partition from the existing table then drop original table (uses hard links don’t require extra disk space). ALTER TABLE foo_replicated ATTACH PARTITION ID 'bar' FROM 'foo' You can easily auto generate those commands using a query like: SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;' from system.parts WHERE table = 'foo'; Do it ‘in place’ using some file manipulation. see the procedure described here: https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replication/#converting-from-mergetree-to-replicatedmergetree Do a backup of MergeTree and recover as ReplicatedMergeTree. https://github.com/AlexAkulov/clickhouse-backup/blob/master/Examples.md#how-to-convert-mergetree-to-replicatedmegretree Embedded command for that should be added in future.  example for option 2 Note: ATTACH PARTITION ID ‘bar’ FROM ‘foo’` is practically free from compute and disk space perpespective. This feature utilizes filesysem hard-links and the fact that files are immutable in Clickhouse ( it’s the core of the Clickhouse design, filesysem hard-links and such file manipulations are widely used ).\ncreatetablefoo(AInt64,DDate,SString)EngineMergeTreepartitionbytoYYYYMM(D)orderbyA;insertintofooselectnumber,today(),''fromnumbers(1e8);insertintofooselectnumber,today()-60,''fromnumbers(1e8);selectcount()fromfoo;┌───count()─┐│200000000│└───────────┘createtablefoo_replicatedasfooEngineReplicatedMergeTree('/clickhouse/{cluster}/tables/{database}/{table}/{shard}','{replica}')partitionbytoYYYYMM(D)orderbyA;SYSTEMSTOPMERGES;SELECTDISTINCT'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;'fromsystem.partsWHEREtable='foo'ANDactive;┌─concat('ALTER TABLE foo_replicated ATTACH PARTITION ID \\'', partition_id, '\\' FROM foo;')─┐│ALTERTABLEfoo_replicatedATTACHPARTITIONID'202111'FROMfoo;││ALTERTABLEfoo_replicatedATTACHPARTITIONID'202201'FROMfoo;│└───────────────────────────────────────────────────────────────────────────────────────────┘clickhouse-client-q\"SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \\'' || partition_id || '\\' FROM foo;' from system.parts WHERE table = 'foo' format TabSeparatedRaw\"|clickhouse-client-mnSYSTEMSTARTMERGES;SELECTcount()FROMfoo_replicated;┌───count()─┐│200000000│└───────────┘renametablefootofoo_old,foo_replicatedtofoo;-- you can drop foo_old any time later, it's kinda a cheap backup, -- it cost nothing until you insert a lot of additional data into foo_replicated ","categories":"","description":"Converting MergeTree to Replicated\n","excerpt":"Converting MergeTree to Replicated\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-converting-mergetree-to-replicated/","tags":"","title":"Converting MergeTree to Replicated"},{"body":"Sample data CREATETABLEevents(`ts`DateTime,`user_id`UInt32)ENGINE=Memory;INSERTINTOeventsSELECTtoDateTime('2021-04-29 10:10:10')+toIntervalHour(7*number)ASts,toDayOfWeek(ts)+(number%2)ASuser_idFROMnumbers(15);Using arrays WITHgroupArray(_ts)ASts_arr,groupArray(state)ASstate_arrSELECTarrayJoin(ts_arr)ASts,arrayReduce('uniqExactMerge',arrayFilter((x,y)-\u003e(y\u003c=ts),state_arr,ts_arr))ASuniqFROM(SELECTtoStartOfDay(ts)AS_ts,uniqExactState(user_id)ASstateFROMeventsGROUPBY_ts)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘WITHarrayJoin(range(toUInt32(_ts)ASint,least(int+toUInt32((3600*24)*5),toUInt32(toDateTime('2021-05-04 00:00:00'))),3600*24))ASts_expandedSELECTtoDateTime(ts_expanded)ASts,uniqExactMerge(state)ASuniqFROM(SELECTtoStartOfDay(ts)AS_ts,uniqExactState(user_id)ASstateFROMeventsGROUPBY_ts)GROUPBYtsORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘Using window functions (starting from Clickhouse 21.3) SELECTts,uniqExactMerge(state)OVER(ORDERBYtsASCROWSBETWEENUNBOUNDEDPRECEDINGANDCURRENTROW)ASuniqFROM(SELECTtoStartOfDay(ts)ASts,uniqExactState(user_id)ASstateFROMeventsGROUPBYts)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘Using runningAccumulate (incorrect result over blocks) SELECTts,runningAccumulate(state)ASuniqFROM(SELECTtoStartOfDay(ts)ASts,uniqExactState(user_id)ASstateFROMeventsGROUPBYtsORDERBYtsASC)ORDERBYtsASC┌──────────────────ts─┬─uniq─┐│2021-04-2900:00:00│2││2021-04-3000:00:00│3││2021-05-0100:00:00│4││2021-05-0200:00:00│5││2021-05-0300:00:00│7│└─────────────────────┴──────┘","categories":"","description":"Cumulative Anything\n","excerpt":"Cumulative Anything\n","ref":"/altinity-kb-queries-and-syntax/cumulative-unique/","tags":"","title":"Cumulative Anything"},{"body":"Export \u0026 Import into common data formats Pros:\n Data can be inserted into any DBMS.  Cons:\n Decoding \u0026 encoding of common data formats may be slower / require more CPU The data size is usually bigger than ClickHouse formats. Some of the common data formats have limitations.  Info The best approach to do that is using clickhouse-client, in that case, encoding/decoding of format happens client-side, while client and server speak clickhouse Native format (columnar \u0026 compressed).\nIn contrast: when you use HTTP protocol, the server do encoding/decoding and more data is passed between client and server.\n remote/remoteSecure or cluster/Distributed table Pros:\n Simple to run. It’s possible to change the schema and distribution of data between shards. It’s possible to copy only some subset of data. Needs only access to ClickHouse TCP port.  Cons:\n Uses CPU / RAM (mostly on the receiver side)  See details in:\nremote-table-function.md\nclickhouse-copier Pros:\n Possible to do some changes in schema. Needs only access to ClickHouse TCP port. It’s possible to change the distribution of data between shards. Suitable for large clusters: many clickhouse-copier can execute the same task together.  Cons:\n May create an inconsistent result if source cluster data is changing during the process. Hard to setup. Requires zookeeper. Uses CPU / RAM (mostly on the clickhouse-copier and receiver side)  Info Internally it works like smart INSERT INTO cluster(…) SELECT * FROM ... with some consistency checks.  Info Run clickhouse copier on the same nodes as receiver clickhouse, to avoid doubling the network load.  See details in:\naltinity-kb-clickhouse-copier\nManual parts moving: freeze / rsync / attach Pros:\n Low CPU / RAM usage.  Cons:\n Table schema should be the same. A lot of manual operations/scripting.  Info With some additional care and scripting, it’s possible to do cheap re-sharding on parts level.  See details in:\nrsync.md\nclickhouse-backup Pros:\n Low CPU / RAM usage. Suitable to recover both schema \u0026 data for all tables at once.  Cons:\n Table schema should be the same.  Just create the backup on server 1, upload it to server 2, and restore the backup.\nSee https://github.com/AlexAkulov/clickhouse-backup\nhttps://altinity.com/blog/introduction-to-clickhouse-backups-and-clickhouse-backup\nFetch from zookeeper path Pros:\n Low CPU / RAM usage.  Cons:\n Table schema should be the same. Works only when the source and the destination clickhouse servers share the same zookeeper (without chroot) Needs to access zookeeper and ClickHouse replication ports: (interserver_http_port or interserver_https_port)  ALTERTABLEtable_nameFETCHPARTITIONpartition_exprFROM'path-in-zookeeper'Replication protocol Just make one more replica in another place.\nPros:\n Simple to setup Data is consistent all the time automatically. Low CPU and network usage.  Cons:\n Needs to reach both zookeeper client (2181) and ClickHouse replication ports: (interserver_http_port or interserver_https_port) In case of cluster migration, zookeeper need’s to be migrated too. Replication works both ways.  ../altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration.md\nSee also Github issues https://github.com/ClickHouse/ClickHouse/issues/10943 https://github.com/ClickHouse/ClickHouse/issues/20219 https://github.com/ClickHouse/ClickHouse/pull/17871\nOther links https://habr.com/ru/company/avito/blog/500678/\n","categories":"","description":"Data Migration\n","excerpt":"Data Migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/","tags":"","title":"Data Migration"},{"body":"   DataType RAM size (=byteSize) Disk Size     String string byte length + 9 string length: 64 bit integer\nzero-byte terminator: 1 byte.\n string length prefix (varint) + string itself:\n string shorter than 128 - string byte length + 1 string shorter than 16384 - string byte length + 2 string shorter than 2097152 - string byte length + 2 string shorter than 268435456 - string byte length + 4\n   AggregateFunction(count, ...)  varint    See also https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/data_processing.pdf (slide 17-22)\n","categories":"","description":"Data types on disk and in RAM\n","excerpt":"Data types on disk and in RAM\n","ref":"/altinity-kb-queries-and-syntax/data-types-on-disk-and-in-ram/","tags":"","title":"Data types on disk and in RAM"},{"body":"Tables Table size SELECTdatabase,table,formatReadableSize(sum(data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rate,sum(rows)ASrows,count()ASpart_countFROMsystem.partsWHERE(active=1)AND(tableLIKE'%')AND(databaseLIKE'%')GROUPBYdatabase,tableORDERBYsizeDESC;Column size SELECTdatabase,table,column,formatReadableSize(sum(column_data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rate,sum(rows)rows_cnt,round(sum(column_data_uncompressed_bytes)/sum(rows),2)avg_row_sizeFROMsystem.parts_columnsWHERE(active=1)AND(tableLIKE'query_log')GROUPBYdatabase,table,columnORDERBYsizeDESC;Projections Projection size SELECTdatabase,table,name,formatReadableSize(sum(data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rate,sum(rows)ASrows,count()ASpart_countFROMsystem.projection_partsWHERE(table='ptest')ANDactiveGROUPBYdatabase,table,nameORDERBYsizeDESC;Projection column size SELECTdatabase,table,column,formatReadableSize(sum(column_data_compressed_bytes)ASsize)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes)ASusize)ASuncompressed,round(usize/size,2)AScompr_rateFROMsystem.projection_parts_columnsWHERE(active=1)AND(tableLIKE'ptest')GROUPBYdatabase,table,columnORDERBYsizeDESC;Understanding the columns data properties: SELECTcount(),*APPLY(uniq),*APPLY(max),*APPLY(min),*APPLY(topK(5))FROMtable_nameFORMATVertical;-- also you can add * APPLY (entropy) to show entropy (i.e. 'randomness' of the column). -- if the table is huge add some WHERE condition to slice some 'representative' data range, for example single month / week / day of data. Understanding the ingest pattern: SELECTdatabase,table,median(rows),median(bytes_on_disk),sum(rows),max(bytes_on_disk),min(bytes_on_disk),round(quantile(0.95)(bytes_on_disk),0),sum(bytes_on_disk),count(),countIf(NOTactive),uniqExact(partition)FROMsystem.partsWHERE(modification_time\u003e(now()-480))AND(level=0)GROUPBYdatabase,tableORDERBYcount()DESCpart_log WITH30*60ASframe_sizeSELECTtoStartOfInterval(event_time,toIntervalSecond(frame_size))ASm,database,table,ROUND(countIf(event_type='NewPart')/frame_size,2)ASnew,ROUND(countIf(event_type='MergeParts')/frame_size,2)ASmerge,ROUND(countIf(event_type='DownloadPart')/frame_size,2)ASdl,ROUND(countIf(event_type='RemovePart')/frame_size,2)ASrm,ROUND(countIf(event_type='MutatePart')/frame_size,2)ASmut,ROUND(countIf(event_type='MovePart')/frame_size,2)ASmvFROMsystem.part_logWHEREevent_time\u003e(now()-toIntervalHour(24))GROUPBYm,database,tableORDERBYdatabaseASC,tableASC,mASCWITH30*60ASframe_sizeSELECTtoStartOfInterval(event_time,toIntervalSecond(frame_size))ASm,database,table,ROUND(countIf(event_type='NewPart')/frame_size,2)ASinserts_per_sec,ROUND(sumIf(rows,event_type='NewPart')/frame_size,2)ASrows_per_sec,ROUND(sumIf(size_in_bytes,event_type='NewPart')/frame_size,2)ASbytes_per_secFROMsystem.part_logWHEREevent_time\u003e(now()-toIntervalHour(24))GROUPBYm,database,tableORDERBYdatabaseASC,tableASC,mASCUnderstanding the partitioning SELECTdatabase,table,count(),topK(5)(partition),COLUMNS('metric.*')APPLY(quantiles(0.005,0.05,0.10,0.25,0.5,0.75,0.9,0.95,0.995))FROM(SELECTdatabase,table,partition,sum(bytes_on_disk)ASmetric_bytes,sum(data_uncompressed_bytes)ASmetric_uncompressed_bytes,sum(rows)ASmetric_rows,sum(primary_key_bytes_in_memory)ASmetric_pk_size,count()ASmetric_count,countIf(part_type='Wide')ASmetric_wide_count,countIf(part_type='Compact')ASmetric_compact_count,countIf(part_type='Memory')ASmetric_memory_countFROMsystem.partsGROUPBYdatabase,table,partition)GROUPBYdatabase,tableFORMATVertical","categories":"","description":"Database Size - Table - Column size\n","excerpt":"Database Size - Table - Column size\n","ref":"/altinity-kb-useful-queries/altinity-kb-database-size-table-column-size/","tags":"","title":"Database Size - Table - Column size"},{"body":"","categories":"","description":"Datasets\n","excerpt":"Datasets\n","ref":"/altinity-kb-useful-queries/altinity-kb-datasets/","tags":"","title":"Datasets"},{"body":"DDLWorker is a subprocess (thread) of clickhouse-server that executes ON CLUSTER tasks at the node.\nWhen you execute a DDL query with ON CLUSTER mycluster section the query executor at the current node reads the cluster mycluster definition (remote_servers / system.clusters) and places tasks into Zookeeper znode task_queue/ddl/... for members of the cluster mycluster.\nDDLWorker at all ClickHouse nodes constantly check this task_queue for their tasks and executes them locally and reports about a result back into task_queue.\nThe common issue is the different hostnames/IPAddresses in the cluster definition and locally.\nSo a node initiator puts tasks for a host named Host1. But the Host1 thinks about own name as localhost or xdgt634678d (internal docker hostname) and never sees tasks for the Host1 because is looking tasks for xdgt634678d. The same with internal VS external IP addresses.\nAnother issue that sometimes DDLWorker thread can crash then ClickHouse node stops to execute ON CLUSTER tasks.\nCheck that DDLWorker is alive:\nps -eL|grep DDL 18829 18876 ? 00:00:00 DDLWorkerClnr 18829 18879 ? 00:00:00 DDLWorker  ps -ef|grep 18829|grep -v grep clickho+ 18829 18828 1 Feb09 ? 00:55:00 /usr/bin/clickhouse-server --con... As you can see there are two threads: DDLWorker and DDLWorkerClnr.\nThe second thread – DDLWorkerCleaner cleans old tasks from task_queue. You can configure how many recent tasks to store:\nconfig.xml \u003cyandex\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/task_queue/ddl\u003c/path\u003e \u003cmax_tasks_in_queue\u003e1000\u003c/max_tasks_in_queue\u003e \u003ctask_max_lifetime\u003e604800\u003c/task_max_lifetime\u003e \u003ccleanup_delay_period\u003e60\u003c/cleanup_delay_period\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e Default values:\ncleanup_delay_period = 60 seconds – Sets how often to start cleanup to remove outdated data.\ntask_max_lifetime = 7 * 24 * 60 * 60 (in seconds = week) – Delete task if its age is greater than that.\nmax_tasks_in_queue = 1000 – How many tasks could be in the queue.\n","categories":"","description":"DDLWorker\n","excerpt":"DDLWorker\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/","tags":"","title":"DDLWorker"},{"body":"CREATETABLEtest_delete(`key`UInt32,`ts`UInt32,`value_a`String,`value_b`String,`value_c`String,`is_active`UInt8DEFAULT1)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_delete(key,ts,value_a,value_b,value_c)SELECTnumber,1,concat('some_looong_string',toString(number)),concat('another_long_str',toString(number)),concat('string',toString(number))FROMnumbers(10000000);INSERTINTOtest_delete(key,ts,value_a,value_b,value_c)VALUES(400000,2,'totally different string','another totally different string','last string');SELECT*FROMtest_deleteWHEREkey=400000;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘┌────key─┬─ts─┬─value_a──────────────────┬─value_b────────────────┬─value_c──────┬─is_active─┐│400000│1│some_looong_string400000│another_long_str400000│string400000│1│└────────┴────┴──────────────────────────┴────────────────────────┴──────────────┴───────────┘SETmutations_sync=2;ALTERTABLEtest_deleteUPDATEis_active=0WHERE(key=400000)AND(ts=1);Ok.0rowsinset.Elapsed:0.058sec.SELECT*FROMtest_deleteWHERE(key=400000)ANDis_active;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ALTERTABLEtest_deleteDELETEWHERE(key=400000)AND(ts=1);Ok.0rowsinset.Elapsed:1.101sec.-- 20 times slower!!! SELECT*FROMtest_deleteWHEREkey=400000;┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘-- For ReplacingMergeTree OPTIMIZETABLEtest_deleteFINAL;Ok.0rowsinset.Elapsed:2.230sec.-- 40 times slower!!! SELECT*FROMtest_deleteWHEREkey=400000┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐│400000│2│totallydifferentstring│anothertotallydifferentstring│laststring│1│└────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘","categories":"","description":"DELETE via tombstone column\n","excerpt":"DELETE via tombstone column\n","ref":"/altinity-kb-queries-and-syntax/delete-via-tombstone-column/","tags":"","title":"DELETE via tombstone column"},{"body":"Dictionary with Clickhouse table as a source Test data DROPTABLEIFEXISTSarr_src;CREATETABLEarr_src(keyUInt64,array_intArray(Int64),array_strArray(String))ENGINE=MergeTreeorderbykey;INSERTINTOarr_srcSELECTnumber,arrayMap(i-\u003e(number*i),range(5)),arrayMap(i-\u003econcat('str',toString(number*i)),range(5))FROMnumbers(1000);Dictionary DROPDICTIONARYIFEXISTSarr_dict;CREATEDICTIONARYarr_dict(keyUInt64,array_intArray(Int64)DEFAULT[1,2,3],array_strArray(String)DEFAULT['1','2','3'])PRIMARYKEYkeySOURCE(CLICKHOUSE(DATABASE'default'TABLE'arr_src'))LIFETIME(120)LAYOUT(HASHED());SELECTdictGet('arr_dict','array_int',toUInt64(42))ASres_int,dictGetOrDefault('arr_dict','array_str',toUInt64(424242),['none'])ASres_str┌─res_int───────────┬─res_str──┐│[0,42,84,126,168]│['none']│└───────────────────┴──────────┘Dictionary with PostgreSQL as a source Test data in PG createuserch;createdatabasech;GRANTALLPRIVILEGESONDATABASEchTOch;ALTERUSERchWITHPASSWORD'chch';CREATETABLEarr_src(keyint,array_intinteger[],array_strtext[]);INSERTINTOarr_srcVALUES(42,'{0,42,84,126,168}','{\"str0\",\"str42\",\"str84\",\"str126\",\"str168\"}'),(66,'{0,66,132,198,264}','{\"str0\",\"str66\",\"str132\",\"str198\",\"str264\"}');Dictionary Example CREATEDICTIONARYpg_arr_dict(keyUInt64,array_intArray(Int64)DEFAULT[1,2,3],array_strArray(String)DEFAULT['1','2','3'])PRIMARYKEYkeySOURCE(POSTGRESQL(PORT5432HOST'pg-host'user'ch'password'chch'DATABASE'ch'TABLE'arr_src'))LIFETIME(120)LAYOUT(HASHED());select*frompg_arr_dict;┌─key─┬─array_int──────────┬─array_str───────────────────────────────────┐│66│[0,66,132,198,264]│['str0','str66','str132','str198','str264']││42│[0,42,84,126,168]│['str0','str42','str84','str126','str168']│└─────┴────────────────────┴─────────────────────────────────────────────┘SELECTdictGet('pg_arr_dict','array_int',toUInt64(42))ASres_int,dictGetOrDefault('pg_arr_dict','array_str',toUInt64(424242),['none'])ASres_str┌─res_int───────────┬─res_str──┐│[0,42,84,126,168]│['none']│└───────────────────┴──────────┘Dictionary with MySQL as a source Test data in MySQL -- casted into CH Arrays createtablearr_src(_keybigint(20)NOTNULL,_array_inttext,_array_strtext,PRIMARYKEY(_key));INSERTINTOarr_srcVALUES(42,'[0,42,84,126,168]','[''str0'',''str42'',''str84'',''str126'',''str168'']'),(66,'[0,66,132,198,264]','[''str0'',''str66'',''str132'',''str198'',''str264'']');Dictionary in MySQL -- supporting table to cast data CREATETABLEarr_src(`_key`UInt8,`_array_int`String,`array_int`Array(Int32)ALIAScast(_array_int,'Array(Int32)'),`_array_str`String,`array_str`Array(String)ALIAScast(_array_str,'Array(String)'))ENGINE=MySQL('mysql_host','ch','arr_src','ch','pass');-- dictionary fetches data from the supporting table CREATEDICTIONARYmysql_arr_dict(_keyUInt64,array_intArray(Int64)DEFAULT[1,2,3],array_strArray(String)DEFAULT['1','2','3'])PRIMARYKEY_keySOURCE(CLICKHOUSE(DATABASE'default'TABLE'arr_src'))LIFETIME(120)LAYOUT(HASHED());select*frommysql_arr_dict;┌─_key─┬─array_int──────────┬─array_str───────────────────────────────────┐│66│[0,66,132,198,264]│['str0','str66','str132','str198','str264']││42│[0,42,84,126,168]│['str0','str42','str84','str126','str168']│└──────┴────────────────────┴─────────────────────────────────────────────┘SELECTdictGet('mysql_arr_dict','array_int',toUInt64(42))ASres_int,dictGetOrDefault('mysql_arr_dict','array_str',toUInt64(424242),['none'])ASres_str┌─res_int───────────┬─res_str──┐│[0,42,84,126,168]│['none']│└───────────────────┴──────────┘SELECTdictGet('mysql_arr_dict','array_int',toUInt64(66))ASres_int,dictGetOrDefault('mysql_arr_dict','array_str',toUInt64(66),['none'])ASres_str┌─res_int────────────┬─res_str─────────────────────────────────────┐│[0,66,132,198,264]│['str0','str66','str132','str198','str264']│└────────────────────┴─────────────────────────────────────────────┘","categories":"","description":"Dictionaries \u0026 arrays\n","excerpt":"Dictionaries \u0026 arrays\n","ref":"/altinity-kb-dictionaries/dictionaries-and-arrays/","tags":"","title":"Dictionaries \u0026 arrays"},{"body":"Q. I think I’m still trying to understand how de-normalized is okay - with my relational mindset, I want to move repeated string fields into their own table, but I’m not sure to what extent this is necessary\nI will look at LowCardinality in more detail - I think it may work well here\nA. If it’s a simple repetition, which you don’t need to manipulate/change in future - LowCardinality works great, and you usually don’t need to increase the system complexity by introducing dicts.\nFor example: name of team ‘Manchester United’ will rather not be changed, and even if it will you can keep the historical records with historical name. So normalization here (with some dicts) is very optional, and de-normalized approach with LowCardinality is good \u0026 simpler alternative.\nFrom the other hand: if data can be changed in future, and that change should impact the reports, then normalization may be a big advantage.\nFor example if you need to change the used currency rare every day- it would be quite stupid to update all historical records to apply the newest exchange rate. And putting it to dict will allow to do calculations with latest exchange rate at select time.\nFor dictionary it’s possible to mark some of the attributes as injective. An attribute is called injective if different attribute values correspond to different keys. It would allow ClickHouse to replace dictGet call in GROUP BY with cheap dict key.\n","categories":"","description":"Dictionaries vs LowCardinality\n","excerpt":"Dictionaries vs LowCardinality\n","ref":"/altinity-kb-schema-design/altinity-kb-dictionaries-vs-lowcardinality/","tags":"","title":"Dictionaries vs LowCardinality"},{"body":"DROPTABLEIFEXISTSdictionary_source_en;DROPTABLEIFEXISTSdictionary_source_ru;DROPTABLEIFEXISTSdictionary_source_view;DROPDICTIONARYIFEXISTSflat_dictionary;CREATETABLEdictionary_source_en(idUInt64,valueString)ENGINE=TinyLog;INSERTINTOdictionary_source_enVALUES(1,'One'),(2,'Two'),(3,'Three');CREATETABLEdictionary_source_ru(idUInt64,valueString)ENGINE=TinyLog;INSERTINTOdictionary_source_ruVALUES(1,'Один'),(2,'Два'),(3,'Три');CREATEVIEWdictionary_source_viewASSELECTid,dictionary_source_en.valueasvalue_en,dictionary_source_ru.valueasvalue_ruFROMdictionary_source_enLEFTJOINdictionary_source_ruUSING(id);select*fromdictionary_source_view;CREATEDICTIONARYflat_dictionary(idUInt64,value_enString,value_ruString)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000USER'default'PASSWORD''TABLE'dictionary_source_view'))LIFETIME(MIN1MAX1000)LAYOUT(FLAT());SELECTdictGet(concat(currentDatabase(),'.flat_dictionary'),'value_en',number+1),dictGet(concat(currentDatabase(),'.flat_dictionary'),'value_ru',number+1)FROMnumbers(3);","categories":"","description":"Dictionary on the top of the several tables using VIEW\n","excerpt":"Dictionary on the top of the several tables using VIEW\n","ref":"/altinity-kb-dictionaries/dictionary-on-top-tables/","tags":"","title":"Dictionary on the top of the several tables using VIEW"},{"body":"differential backups using clickhouse-backup  Download the latest clickhouse-backup for your platform https://github.com/AlexAkulov/clickhouse-backup/releases  # ubuntu / debian  wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup_1.0.0_amd64.deb sudo dpkg -i clickhouse-backup_1.0.0_amd64.deb  # centos / redhat / fedora   sudo yum install https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup-1.0.0-1.x86_64.rpm  # other platforms wget https://github.com/AlexAkulov/clickhouse-backup/releases/download/v1.0.0/clickhouse-backup.tar.gz sudo mkdir /etc/clickhouse-backup/ sudo mv clickhouse-backup/config.yml /etc/clickhouse-backup/config.yml.example sudo mv clickhouse-backup/clickhouse-backup /usr/bin/ rm -rf clickhouse-backup clickhouse-backup.tar.gz Create a runner script for the crontab  mkdir /opt/clickhouse-backup-diff/  cat \u003c\u003c 'END' \u003e /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh #!/bin/bash set +x command_line_argument=$1 backup_name=$(date +%Y-%M-%d-%H-%M-%S) echo \"Creating local backup '${backup_name}' (full, using hardlinks)...\" clickhouse-backup create \"${backup_name}\" if [[ \"run_diff\" == \"${command_line_argument}\" \u0026\u0026 \"2\" -le \"$(clickhouse-backup list local | wc -l)\" ]]; then prev_backup_name=\"$(clickhouse-backup list local | tail -n 2 | head -n 1 | cut -d \" \" -f 1)\" echo \"Uploading the backup '${backup_name}' as diff from the previous backup ('${prev_backup_name}')\" clickhouse-backup upload --diff-from \"${prev_backup_name}\" \"${backup_name}\" elif [[ \"\" == \"${command_line_argument}\" ]]; then echo \"Uploading the backup '${backup_name}, and removing old unneeded backups\" KEEP_BACKUPS_LOCAL=1 KEEP_BACKUPS_REMOTE=1 clickhouse-backup upload \"${backup_name}\" fi END  chmod +x /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh Create confuguration for clickhouse-backup  # Check the example: /etc/clickhouse-backup/config.yml.example vim /etc/clickhouse-backup/config.yml Edit the crontab  crontab -e # full backup at 0:00 Monday 0 0 * * 1 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh # differential backup every hour (except of 00:00) Monday 0 1-23 * * 1 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh run_diff # differential backup every hour Sunday, Tuesday-Saturday 0 */1 * * 0,2-6 clickhouse /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh run_diff Recover the last backup:  last_remote_backup=\"$(clickhouse-backup list remote | tail -n 1 | cut -d \" \" -f 1)\" clickhouse-backup download \"${last_remote_backup}\" clickhouse-backup restore --rm \"${last_remote_backup}\" ","categories":"","description":"differential backups using clickhouse-backup\n","excerpt":"differential backups using clickhouse-backup\n","ref":"/altinity-kb-setup-and-maintenance/clickhouse-backup-diff/","tags":"","title":"differential backups using clickhouse-backup"},{"body":"RocksDB is faster than MergeTree on Key/Value queries because MergeTree primary key index is sparse. Probably it’s possible to speedup MergeTree by reducing index_granularity.\nNVMe disk is used for the tests.\nThe main feature of RocksDB is instant updates. You can update a row instantly (microseconds):\nselect*fromrocksDBwhereA=15645646;┌────────A─┬─B────────────────────┐│15645646│12517841379565221195│└──────────┴──────────────────────┘1rowsinset.Elapsed:0.001sec.insertintorocksDBvalues(15645646,'xxxx');1rowsinset.Elapsed:0.001sec.select*fromrocksDBwhereA=15645646;┌────────A─┬─B────┐│15645646│xxxx│└──────────┴──────┘1rowsinset.Elapsed:0.001sec.Let’s load 100 millions rows:\ncreatetablerocksDB(AUInt64,BString,primarykeyA)Engine=EmbeddedRocksDB();insertintorocksDBselectnumber,toString(cityHash64(number))fromnumbers(100000000);-- 0 rows in set. Elapsed: 154.559 sec. Processed 100.66 million rows, 805.28 MB (651.27 thousand rows/s., 5.21 MB/s.) -- Size on disk: 1.5GB createtablemergeTreeDB(AUInt64,BString)Engine=MergeTree()orderbyA;insertintomergeTreeDBselectnumber,toString(cityHash64(number))fromnumbers(100000000);Sizeondisk:973MBCREATEDICTIONARYtest_rocksDB(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLErocksDBDB'default'USER'default'))LAYOUT(DIRECT());CREATEDICTIONARYtest_mergeTreeDB(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(DIRECT());Direct queries to tables to request 10000 rows by a random key selectcount()from(select*fromrocksDBwhereAin(selecttoUInt64(rand64()%100000000)fromnumbers(10000)))Elapsed:0.076sec.Processed10.00thousandrowsselectcount()from(select*frommergeTreeDBwhereAin(selecttoUInt64(rand64()%100000000)fromnumbers(10000)))Elapsed:0.202sec.Processed55.95millionrowsRocksDB as expected is much faster: 0.076 sec. VS 0.202 sec.\nRocksDB processes less rows: 10.00 thousand rows VS 55.95 million rows\ndictGet – 100.00 thousand random rows selectcount()from(selectdictGet('default.test_rocksDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(100000))Elapsed:0.786sec.Processed100.00thousandrowsselectcount()from(selectdictGet('default.test_mergeTreeDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(100000))Elapsed:3.160sec.Processed100.00thousandrowsdictGet – 1million random rows selectcount()from(selectdictGet('default.test_rocksDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:5.643sec.Processed1.00millionrowsselectcount()from(selectdictGet('default.test_mergeTreeDB','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:31.111sec.Processed1.00millionrowsdictGet – 1million random rows from Hashed CREATEDICTIONARYtest_mergeTreeDBHashed(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(Hashed())LIFETIME(0);0rowsinset.Elapsed:46.564sec.┌─name───────────────────┬─type───┬─status─┬─element_count─┬─RAM──────┐│test_mergeTreeDBHashed│Hashed│LOADED│100000000│7.87GiB│└────────────────────────┴────────┴────────┴───────────────┴──────────┘selectcount()from(selectdictGet('default.test_mergeTreeDBHashed','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:0.079sec.Processed1.00millionrowsdictGet – 1million random rows from SparseHashed CREATEDICTIONARYtest_mergeTreeDBSparseHashed(AUInt64,BString)PRIMARYKEYASOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEmergeTreeDBDB'default'USER'default'))LAYOUT(SPARSE_HASHED())LIFETIME(0);0rowsinset.Elapsed:81.404sec.┌─name─────────────────────────┬─type─────────┬─status─┬─element_count─┬─RAM──────┐│test_mergeTreeDBSparseHashed│SparseHashed│LOADED│100000000│4.24GiB│└──────────────────────────────┴──────────────┴────────┴───────────────┴──────────┘selectcount()from(selectdictGet('default.test_mergeTreeDBSparseHashed','B',toUInt64(rand64()%100000000))fromnumbers_mt(1000000))Elapsed:0.065sec.Processed1.00millionrows","categories":"","description":"EmbeddedRocksDB \u0026 dictionary\n","excerpt":"EmbeddedRocksDB \u0026 dictionary\n","ref":"/engines/altinity-kb-embeddedrocksdb-and-dictionary/","tags":"","title":"EmbeddedRocksDB \u0026 dictionary"},{"body":"WHERE over encrypted column CREATETABLEencrypt(`key`UInt32,`value`FixedString(4))ENGINE=MergeTreeORDERBYkey;INSERTINTOencryptSELECTnumber,encrypt('aes-256-ctr',reinterpretAsString(number+0.3),'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx','xxxxxxxxxxxxxxxx')FROMnumbers(100000000);SETmax_threads=1;SELECTcount()FROMencryptWHEREvalueINencrypt('aes-256-ctr',reinterpretAsString(toFloat32(1.3)),'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx','xxxxxxxxxxxxxxxx')┌─count()─┐│1│└─────────┘1rowsinset.Elapsed:0.666sec.Processed100.00millionrows,400.01MB(150.23millionrows/s.,600.93MB/s.)SELECTcount()FROMencryptWHEREreinterpretAsFloat32(encrypt('aes-256-ctr',value,'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx','xxxxxxxxxxxxxxxx'))INtoFloat32(1.3)┌─count()─┐│1│└─────────┘1rowsinset.Elapsed:8.395sec.Processed100.00millionrows,400.01MB(11.91millionrows/s.,47.65MB/s.) Info Because encryption and decryption can be expensive due re-initialization of keys and iv, usually it make sense to use those functions over literal values instead of table column.  ","categories":"","description":"","excerpt":"WHERE over encrypted column …","ref":"/altinity-kb-functions/encrypt/","tags":"","title":"Encrypt"},{"body":"Pre 21.6 There are couple options:\nCertain formats which has schema in built in them (like JSONEachRow) could silently skip any unexpected fields after enabling setting input_format_skip_unknown_fields\nIt’s also possible to skip up to N malformed messages for each block, with used setting kafka_skip_broken_messages but it’s also does not support all possible formats.\nAfter 21.6 It’s possible to stream messages which could not be parsed, this behavior could be enabled via setting: kafka_handle_error_mode='stream' and clickhouse wil write error and message from Kafka itself to two new virtual columns: _error, _raw_message.\nSo you can create another Materialized View which would collect to a separate table all errors happening while parsing with all important information like offset and content of message.\nCREATETABLEdefault.kafka_engine(`i`Int64,`s`String)ENGINE=KafkaSETTINGSkafka_broker_list='kafka:9092'kafka_topic_list='topic',kafka_group_name='clickhouse',kafka_format='JSONEachRow',kafka_handle_error_mode='stream';CREATEMATERIALIZEDVIEWdefault.kafka_errors(`topic`String,`partition`Int64,`offset`Int64,`raw`String,`error`String)ENGINE=MergeTreeORDERBY(topic,partition,offset)SETTINGSindex_granularity=8192ASSELECT_topicAStopic,_partitionASpartition,_offsetASoffset,_raw_messageASraw,_errorASerrorFROMdefault.kafka_engineWHERElength(_error)\u003e0https://github.com/ClickHouse/ClickHouse/pull/20249#issuecomment-779054737\nhttps://github.com/ClickHouse/ClickHouse/pull/21850\nhttps://altinity.com/blog/clickhouse-kafka-engine-faq\n","categories":"","description":"Error handling\n","excerpt":"Error handling\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/error-handling/","tags":"","title":"Error handling"},{"body":"EOS consumer (isolation.level=read_committed) is enabled by default since librdkafka 1.2.0, so for ClickHouse - since 20.2\nSee:\n edenhill/librdkafka@6b2a155 9de5dff  BUT: while EOS semantics will guarantee you that no duplicates will happen on the Kafka side (i.e. even if you produce the same messages few times it will be consumed once), but ClickHouse as a Kafka client can currently guarantee only at-least-once. And in some corner cases (connection lost etc) you can get duplicates.\nWe need to have something like transactions on ClickHouse side to be able to avoid that. Adding something like simple transactions is in plans for Y2022.\nblock-aggregator by eBay Block Aggregator is a data loader that subscribes to Kafka topics, aggregates the Kafka messages into blocks that follow the Clickhouse’s table schemas, and then inserts the blocks into ClickHouse. Block Aggregator provides exactly-once delivery guarantee to load data from Kafka to ClickHouse. Block Aggregator utilizes Kafka’s metadata to keep track of blocks that are intended to send to ClickHouse, and later uses this metadata information to deterministically re-produce ClickHouse blocks for re-tries in case of failures. The identical blocks are guaranteed to be deduplicated by ClickHouse.\neBay/block-aggregator\n","categories":"","description":"Exactly once semantics\n","excerpt":"Exactly once semantics\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/","tags":"","title":"Exactly once semantics"},{"body":"CREATEDICTIONARYpostgres_dict(idUInt32,valueString)PRIMARYKEYidSOURCE(POSTGRESQL(port5432host'postgres1'user'postgres'password'mysecretpassword'db'clickhouse'table'test_schema.test_table'))LIFETIME(MIN300MAX600)LAYOUT(HASHED());and later do\nSELECTdictGetString(postgres_dict,'value',toUInt64(1))","categories":"","description":"Example of PostgreSQL dictionary\n","excerpt":"Example of PostgreSQL dictionary\n","ref":"/altinity-kb-dictionaries/example-of-postgresql-dictionary/","tags":"","title":"Example of PostgreSQL dictionary"},{"body":"Use cases Strong correlation between column from table ORDER BY / PARTITION BY key and other column which is regularly being used in WHERE condition Good example is incremental ID which increasing with time.\nCREATETABLEskip_idx_corr(`key`UInt32,`id`UInt32,`ts`DateTime)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,id);INSERTINTOskip_idx_corrSELECTrand(),number,now()+intDiv(number,10)FROMnumbers(100000000);SELECTcount()FROMskip_idx_corrWHEREid=60000001rowsinset.Elapsed:0.167sec.Processed100.00millionrows,400.00MB(599.96millionrows/s.,2.40GB/s.)ALTERTABLEskip_idx_corrADDINDEXid_idxidTYPEminmaxGRANULARITY10;ALTERTABLEskip_idx_corrMATERIALIZEINDEXid_idx;SELECTcount()FROMskip_idx_corrWHEREid=60000001rowsinset.Elapsed:0.017sec.Processed6.29millionrows,25.17MB(359.78millionrows/s.,1.44GB/s.)Multiple Date/DateTime columns can be used in WHERE conditions Usually it could happen if you have separate Date and DateTime columns and different column being used in PARTITION BY expression and in WHERE condition. Another possible scenario when you have multiple DateTime columns which have pretty the same date or even time.\nCREATETABLEskip_idx_multiple(`key`UInt32,`date`Date,`time`DateTime,`created_at`DateTime,`inserted_at`DateTime)ENGINE=MergeTreePARTITIONBYtoYYYYMM(date)ORDERBY(key,time);INSERTINTOskip_idx_multipleSELECTnumber,toDate(x),now()+intDiv(number,10)ASx,x-(rand()%100),x+(rand()%100)FROMnumbers(100000000);SELECTcount()FROMskip_idx_multipleWHEREdate\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.048sec.Processed14.02millionrows,28.04MB(290.96millionrows/s.,581.92MB/s.)SELECTcount()FROMskip_idx_multipleWHEREtime\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.188sec.Processed100.00millionrows,400.00MB(530.58millionrows/s.,2.12GB/s.)SELECTcount()FROMskip_idx_multipleWHEREcreated_at\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.400sec.Processed100.00millionrows,400.00MB(250.28millionrows/s.,1.00GB/s.)ALTERTABLEskip_idx_multipleADDINDEXtime_idxtimeTYPEminmaxGRANULARITY1000;ALTERTABLEskip_idx_multipleMATERIALIZEINDEXtime_idx;SELECTcount()FROMskip_idx_multipleWHEREtime\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.036sec.Processed14.02millionrows,56.08MB(391.99millionrows/s.,1.57GB/s.)ALTERTABLEskip_idx_multipleADDINDEXcreated_at_idxcreated_atTYPEminmaxGRANULARITY1000;ALTERTABLEskip_idx_multipleMATERIALIZEINDEXcreated_at_idx;SELECTcount()FROMskip_idx_multipleWHEREcreated_at\u003e(now()+toIntervalDay(105));1rowsinset.Elapsed:0.076sec.Processed14.02millionrows,56.08MB(184.90millionrows/s.,739.62MB/s.)Condition in query trying to filter outlier value CREATETABLEskip_idx_outlier(`key`UInt32,`ts`DateTime,`value`UInt32)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,ts);INSERTINTOskip_idx_outlierSELECTnumber,now(),rand()%10FROMnumbers(10000000);INSERTINTOskip_idx_outlierSELECTnumber,now(),20FROMnumbers(10);SELECTcount()FROMskip_idx_outlierWHEREvalue\u003e15;1rowsinset.Elapsed:0.059sec.Processed10.00millionrows,40.00MB(170.64millionrows/s.,682.57MB/s.)ALTERTABLEskip_idx_outlierADDINDEXvalue_idxvalueTYPEminmaxGRANULARITY10;ALTERTABLEskip_idx_outlierMATERIALIZEINDEXvalue_idx;SELECTcount()FROMskip_idx_outlierWHEREvalue\u003e15;1rowsinset.Elapsed:0.004sec.","categories":"","description":"Example: minmax\n","excerpt":"Example: minmax\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/minmax/","tags":"","title":"Example: minmax"},{"body":"EXPLAIN types EXPLAINASTSYNTAXPLANheader=0,description=1,actions=0,optimize=1PIPELINEheader=0,graph=0,compact=1ESTIMATESELECT... AST - abstract syntax tree SYNTAX - query text after AST-level optimizations PLAN - query execution plan PIPELINE - query execution pipeline ESTIMATE - https://github.com/ClickHouse/ClickHouse/pull/26131 (since 21.9)  References\n https://clickhouse.com/docs/en/sql-reference/statements/explain/ Nikolai Kochetov from Yandeх. EXPLAIN query in ClickHouse. slides, video https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/query-profiling.pdf https://github.com/ClickHouse/ClickHouse/issues/28847  ","categories":"","description":"EXPLAIN query\n","excerpt":"EXPLAIN query\n","ref":"/altinity-kb-queries-and-syntax/explain-query/","tags":"","title":"EXPLAIN query"},{"body":"CREATETABLEevent_table(`key`UInt32,`created_at`DateTime,`value_a`UInt32,`value_b`String)ENGINE=MergeTreeORDERBY(key,created_at)INSERTINTOevent_tableSELECT1ASkey,toDateTime('2020-10-11 10:10:10')+numberAScreated_at,if((number=0)OR((number%5)=1),number+1,0)ASvalue_a,if((number=0)OR((number%3)=1),toString(number),'')ASvalue_bFROMnumbers(10)SELECTmain.key,main.created_at,a.value_a,b.value_bFROMevent_tableASmainASOFINNERJOIN(SELECTkey,created_at,value_aFROMevent_tableWHEREvalue_a!=0)ASaON(main.key=a.key)AND(main.created_at\u003e=a.created_at)ASOFINNERJOIN(SELECTkey,created_at,value_bFROMevent_tableWHEREvalue_b!='')ASbON(main.key=b.key)AND(main.created_at\u003e=b.created_at)┌─main.key─┬─────main.created_at─┬─a.value_a─┬─b.value_b─┐│1│2020-10-1110:10:10│1│0││1│2020-10-1110:10:11│2│1││1│2020-10-1110:10:12│2│1││1│2020-10-1110:10:13│2│1││1│2020-10-1110:10:14│2│4││1│2020-10-1110:10:15│2│4││1│2020-10-1110:10:16│7│4││1│2020-10-1110:10:17│7│7││1│2020-10-1110:10:18│7│7││1│2020-10-1110:10:19│7│7│└──────────┴─────────────────────┴───────────┴───────────┘SELECTkey,created_at,value_a,value_bFROM(SELECTkey,groupArray(created_at)AScreated_arr,arrayFill(x-\u003e(x!=0),groupArray(value_a))ASa_arr,arrayFill(x-\u003e(x!=''),groupArray(value_b))ASb_arrFROM(SELECT*FROMevent_tableORDERBYkeyASC,created_atASC)GROUPBYkey)ARRAYJOINcreated_arrAScreated_at,a_arrASvalue_a,b_arrASvalue_b┌─key─┬──────────created_at─┬─value_a─┬─value_b─┐│1│2020-10-1110:10:10│1│0││1│2020-10-1110:10:11│2│1││1│2020-10-1110:10:12│2│1││1│2020-10-1110:10:13│2│1││1│2020-10-1110:10:14│2│4││1│2020-10-1110:10:15│2│4││1│2020-10-1110:10:16│7│4││1│2020-10-1110:10:17│7│7││1│2020-10-1110:10:18│7│7││1│2020-10-1110:10:19│7│7│└─────┴─────────────────────┴─────────┴─────────┘","categories":"","description":"Fill missing values at query time\n","excerpt":"Fill missing values at query time\n","ref":"/altinity-kb-queries-and-syntax/fill-missing-values-at-query-time/","tags":"","title":"Fill missing values at query time"},{"body":"SELECT * FROM table FINAL\n Before 20.5 - always executed in a single thread and slow. Since 20.5 - final can be parallel, see https://github.com/ClickHouse/ClickHouse/pull/10463 Since 20.10 - you can use do_not_merge_across_partitions_select_final setting.  See https://github.com/ClickHouse/ClickHouse/pull/15938 and https://github.com/ClickHouse/ClickHouse/issues/11722\nSo it can work in the following way:\n Daily partitioning After day end + some time interval during which you can get some updates - for example at 3am / 6am you do OPTIMIZE TABLE xxx PARTITION 'prev_day' FINAL In that case using that FINAL with do_not_merge_across_partitions_select_final will be cheap.  DROPTABLEIFEXISTSrepl_tbl;CREATETABLErepl_tbl(`key`UInt32,`val_1`UInt32,`val_2`String,`val_3`String,`val_4`String,`val_5`UUID,`ts`DateTime)ENGINE=ReplacingMergeTree(ts)PARTITIONBYtoDate(ts)ORDERBYkey;​INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-01 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200101'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-02 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200102'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-03 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200103'FINAL;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-04 00:00:00'astsFROMnumbers(10000000);OPTIMIZETABLErepl_tblPARTITIONID'20200104'FINAL;SYSTEMSTOPMERGESrepl_tbl;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,'2020-01-05 00:00:00'astsFROMnumbers(10000000);​SELECTcount()FROMrepl_tblWHERENOTignore(*)┌──count()─┐│50000000│└──────────┘1rowsinset.Elapsed:1.504sec.Processed50.00millionrows,6.40GB(33.24millionrows/s.,4.26GB/s.)SELECTcount()FROMrepl_tblFINALWHERENOTignore(*)┌──count()─┐│10000000│└──────────┘1rowsinset.Elapsed:3.314sec.Processed50.00millionrows,6.40GB(15.09millionrows/s.,1.93GB/s.)/* more that 2 time slower, and will get worse once you will have more data */setdo_not_merge_across_partitions_select_final=1;SELECTcount()FROMrepl_tblFINALWHERENOTignore(*)┌──count()─┐│50000000│└──────────┘1rowsinset.Elapsed:1.850sec.Processed50.00millionrows,6.40GB(27.03millionrows/s.,3.46GB/s.)/* only 0.35 sec slower, and while partitions have about the same size that extra cost will be about constant */","categories":"","description":"FINAL clause speed\n","excerpt":"FINAL clause speed\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/","tags":"","title":"FINAL clause speed"},{"body":"It’s possible to use dictionaries for populating columns of fact table.\nCREATETABLEcustomer(`customer_id`UInt32,`first_name`String,`birth_date`Date,`sex`Enum('M'=1,'F'=2))ENGINE=MergeTreeORDERBYcustomer_idCREATETABLEorder(`order_id`UInt32,`order_date`DateTimeDEFAULTnow(),`cust_id`UInt32,`amount`Decimal(12,2))ENGINE=MergeTreePARTITIONBYtoYYYYMM(order_date)ORDERBY(order_date,cust_id,order_id)INSERTINTOcustomerVALUES(1,'Mike',now()-INTERVAL30YEAR,'M');INSERTINTOcustomerVALUES(2,'Boris',now()-INTERVAL40YEAR,'M');INSERTINTOcustomerVALUES(3,'Sofie',now()-INTERVAL24YEAR,'F');INSERTINTOorder(order_id,cust_id,amount)VALUES(50,1,15);INSERTINTOorder(order_id,cust_id,amount)VALUES(30,1,10);SELECT*EXCEPT'order_date'FROMorder┌─order_id─┬─cust_id─┬─amount─┐│30│1│10.00││50│1│15.00│└──────────┴─────────┴────────┘CREATEDICTIONARYcustomer_dict(`customer_id`UInt32,`first_name`String,`birth_date`Date,`sex`UInt8)PRIMARYKEYcustomer_idSOURCE(CLICKHOUSE(TABLE'customer'))LIFETIME(MIN0MAX300)LAYOUT(FLAT)ALTERTABLEorderADDCOLUMN`cust_first_name`StringDEFAULTdictGetString('default.customer_dict','first_name',toUInt64(cust_id)),ADDCOLUMN`cust_sex`Enum('M'=1,'F'=2)DEFAULTdictGetUInt8('default.customer_dict','sex',toUInt64(cust_id)),ADDCOLUMN`cust_birth_date`DateDEFAULTdictGetDate('default.customer_dict','birth_date',toUInt64(cust_id));INSERTINTOorder(order_id,cust_id,amount)VALUES(10,3,30);INSERTINTOorder(order_id,cust_id,amount)VALUES(20,3,60);INSERTINTOorder(order_id,cust_id,amount)VALUES(40,2,20);SELECT*EXCEPT'order_date'FROMorderFORMATPrettyCompactMonoBlock┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐│30│1│10.00│Mike│M│1991-08-05││50│1│15.00│Mike│M│1991-08-05││10│3│30.00│Sofie│F│1997-08-05││40│2│20.00│Boris│M│1981-08-05││20│3│60.00│Sofie│F│1997-08-05│└──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ALTERTABLEcustomerUPDATEbirth_date=now()-INTERVAL35YEARWHEREcustomer_id=2;SYSTEMRELOADDICTIONARYcustomer_dict;ALTERTABLEorderUPDATEcust_birth_date=dictGetDate('default.customer_dict','birth_date',toUInt64(cust_id))WHERE1-- or if you do have track of changes it's possible to lower amount of dict calls -- UPDATE cust_birth_date = dictGetDate('default.customer_dict', 'birth_date', toUInt64(cust_id)) WHERE customer_id = 2 SELECT*EXCEPT'order_date'FROMorderFORMATPrettyCompactMonoBlock┌─order_id─┬─cust_id─┬─amount─┬─cust_first_name─┬─cust_sex─┬─cust_birth_date─┐│30│1│10.00│Mike│M│1991-08-05││50│1│15.00│Mike│M│1991-08-05││10│3│30.00│Sofie│F│1997-08-05││40│2│20.00│Boris│M│1986-08-05││20│3│60.00│Sofie│F│1997-08-05│└──────────┴─────────┴────────┴─────────────────┴──────────┴─────────────────┘ALTER TABLE order UPDATE would completely overwrite this column in table, so it’s not recommended to run it often.\n","categories":"","description":"Flattened table\n","excerpt":"Flattened table\n","ref":"/altinity-kb-schema-design/flattened-table/","tags":"","title":"Flattened table"},{"body":"Float arithmetics is not accurate: https://floating-point-gui.de/\nIn case you need accurate calculations you should use Decimal datatypes.\nOperations on floats are not associative SELECT(toFloat64(100000000000000000.)+toFloat64(7.5))-toFloat64(100000000000000000.)ASres┌─res─┐│0│└─────┘SELECT(toFloat64(100000000000000000.)-toFloat64(100000000000000000.))+toFloat64(7.5)ASres┌─res─┐│7.5│└─────┘No problem with Decimals: SELECT(toDecimal64(100000000000000000.,1)+toDecimal64(7.5,1))-toDecimal64(100000000000000000.,1)ASres┌─res─┐│7.5│└─────┘SELECT(toDecimal64(100000000000000000.,1)-toDecimal64(100000000000000000.,1))+toDecimal64(7.5,1)ASres┌─res─┐│7.5│└─────┘ Warning Because clickhouse uses MPP order of execution of a single query can vary on each run, and you can get slightly different results from the float column every time you run the query.\nUsually, this deviation is small, but it can be significant when some kind of arithmetic operation is performed on very large and very small numbers at the same time.\n Some decimal numbers has no accurate float representation SELECTsum(toFloat64(0.45))ASresFROMnumbers(10000)┌───────────────res─┐│4499.999999999948│└───────────────────┘SELECTsumKahan(toFloat64(0.45))ASresFROMnumbers(10000)┌──res─┐│4500│└──────┘SELECTtoFloat32(0.6)*6ASres┌────────────────res─┐│3.6000001430511475│└────────────────────┘No problem with Decimal: SELECTsum(toDecimal64(0.45,2))ASresFROMnumbers(10000)┌──res─┐│4500│└──────┘SELECTtoDecimal32(0.6,1)*6ASres┌─res─┐│3.6│└─────┘Direct comparisons of floats may be impossible The same number can have several floating-point representations and because of that you should not compare Floats directly\nSELECT(toFloat32(0.1)*10)=(toFloat32(0.01)*100)ASres┌─res─┐│0│└─────┘SELECTsumIf(0.1,number\u003c10)ASa,sumIf(0.01,number\u003c100)ASb,a=bASa_eq_bFROMnumbers(100)┌──────────────────a─┬──────────────────b─┬─a_eq_b─┐│0.9999999999999999│1.0000000000000004│0│└────────────────────┴────────────────────┴────────┘See also\nhttps://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/ https://stackoverflow.com/questions/4915462/how-should-i-do-floating-point-comparison https://stackoverflow.com/questions/2100490/floating-point-inaccuracy-examples https://stackoverflow.com/questions/10371857/is-floating-point-addition-and-multiplication-associative\nBut:\nhttps://github.com/ClickHouse/ClickHouse/issues/24909\n","categories":"","description":"Floats vs Decimals\n","excerpt":"Floats vs Decimals\n","ref":"/altinity-kb-schema-design/floats-vs-decimals/","tags":"","title":"Floats vs Decimals"},{"body":"migrate migrate is a simple schema migration tool written in golang. No external dependencies are required (like interpreter, jre), only one platform-specific executable. golang-migrate/migrate\nmigrate supports several databases, including ClickHouse (support was introduced by @kshvakov).\nTo store information about migrations state migrate creates one additional table in target database, by default that table is called schema_migrations.\nInstall download the migrate executable for your platform and put it to the folder listed in your %PATH.\n#wget https://github.com/golang-migrate/migrate/releases/download/v3.2.0/migrate.linux-amd64.tar.gz wget https://github.com/golang-migrate/migrate/releases/download/v4.14.1/migrate.linux-amd64.tar.gz tar -xzf migrate.linux-amd64.tar.gz mkdir -p ~/bin mv migrate.linux-amd64 ~/bin/migrate rm migrate.linux-amd64.tar.gz Sample usage mkdir migrations echo 'create table test(id UInt8) Engine = Memory;' \u003e migrations/000001_my_database_init.up.sql echo 'DROP TABLE test;' \u003e migrations/000001_my_database_init.down.sql  # you can also auto-create file with new migrations with automatic numbering like that: migrate create -dir migrations -seq -digits 6 -ext sql my_database_init  edit migrations/000001_my_database_init.up.sql \u0026 migrations/000001_my_database_init.down.sql  migrate -database 'clickhouse://localhost:9000' -path ./migrations up 1/u my_database_init (6.502974ms)  migrate -database 'clickhouse://localhost:9000' -path ./migrations down 1/d my_database_init (2.164394ms)  # clears the database (use carefully - will not ask any confirmations) ➜ migrate -database 'clickhouse://localhost:9000' -path ./migrations drop Connection string format clickhouse://host:port?username=user\u0026password=qwerty\u0026database=clicks\n   URL Query Description     x-migrations-table Name of the migrations table   database The name of the database to connect to   username The user to sign in as   password The user’s password   host The host to connect to.   port The port to bind to.   secure to use a secure connection (for self-signed also add skip_verify=1)    Replicated / Distributed / Cluster environments By default migrate create table schema_migrations with the following structure\nCREATETABLEschema_migrations(versionUInt32,dirtyUInt8,sequenceUInt64)ENGINE=TinyLogThat allows storing version of schema locally.\nIf you need to use migrate in some multi server environment (replicated / cluster) you should create schema_migrations manually with the same structure and with the appropriate Engine (Replicated / Distributed), otherwise, other servers will not know the version of the DB schema. As an alternative you can force the current version number on another server manually, like that:\nmigrate -database 'clickhouse://localhost:9000' -path ./migrations force 123456 # force version 123456 Known issues could not load time location: unknown time zone Europe/Moscow in line 0:\nIt’s happens due of missing tzdata package in migrate/migrate docker image of golang-migrate. There is 2 possible solutions:\n You can build your own golang-migrate image from official with tzdata package. If you using it as part of your CI you can add installing tzdata package as one of step in CI before using golang-migrate.  Related GitHub issues: https://github.com/golang-migrate/migrate/issues/494 https://github.com/golang-migrate/migrate/issues/201\nUsing database name in x-migrations-table\n Creates table with database.table When running migrations migrate actually uses database from query settings and encapsulate database.table as table name: ``other_database.`database.table```  ","categories":"","description":"golang-migrate\n","excerpt":"golang-migrate\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/golang-migrate/","tags":"","title":"golang-migrate"},{"body":"GCS with the table function - seems to work correctly for simple scenarios.\nEssentially you can follow the steps from the Migrating from Amazon S3 to Cloud Storage.\n Set up a GCS bucket. This bucket must be set as part of the default project for the account. This configuration can be found in settings -\u003e interoperability. Generate a HMAC key for the account, can be done in settings -\u003e interoperability, in the section for user account access keys. In ClickHouse, replace the S3 bucket endpoint with the GCS bucket endpoint This must be done with the path-style GCS endpoint: https://storage.googleapis.com/BUCKET_NAME/OBJECT_NAME. Replace the aws access key id and aws secret access key with the corresponding parts of the HMAC key.  s3 Disk on the top of GCS and writing to GSC may be NOT working because GCS don’t support some of bulk S3 API calls, see https://github.com/ClickHouse/ClickHouse/issues/24246\n","categories":"","description":"\"Google S3 GCS\"\n","excerpt":"\"Google S3 GCS\"\n","ref":"/altinity-kb-integrations/altinity-kb-google-s3-gcs/","tags":"","title":"Google S3 (GCS)"},{"body":"ClickHouse ClickHouse will use all available hardware to maximize performance. So the more hardware - the better. As of this publication, the hardware requirements are:\n Minimum Hardware: 4-core CPU with support of SSE4.2, 16 Gb RAM, 1Tb HDD.  Recommended for development and staging environments. SSE4.2 is required, and going below 4 Gb of RAM is not recommended.   Recommended Hardware: \u003e=16-cores, \u003e=64Gb RAM, HDD-raid or SSD.  For processing up to hundreds of millions / billions of rows.    For clouds: disk throughput is the more important factor compared to IOPS. Be aware of burst / baseline disk speed difference.\nSee also: https://clickhouse.tech/benchmark/hardware/\nZookeeper Zookeeper requires separate servers from those used for ClickHouse. Zookeeper has poor performance when installed on the same node as ClickHouse.\nHardware Requirements for Zookeeper:\n Fast disk speed (ideally NVMe, 128Gb should be enough). Any modern CPU (one core, better 2) 4Gb of RAM  For clouds - be careful with burstable network disks (like gp2 on aws): you may need up to 1000 IOPs on the disk for on a long run, so gp3 with 3000 IOPs baseline is a better choice.\nThe number of Zookeeper instances depends on the environment:\n Production: 3 is an optimal number of zookeeper instances. Development and Staging: 1 zookeeper instance is sufficient.  See also:\n https://docs.altinity.com/operationsguide/clickhouse-zookeeper/ altinity-kb-proper-setup zookeeper-monitoring  ClickHouse Hardware Configuration Configure the servers according to those recommendations the ClickHouse Usage Recommendations.\nTest Your Hardware Be sure to test the following:\n RAM speed. Network speed. Storage speed.  It’s better to find any performance issues before installing ClickHouse.\n","categories":"","description":"Hardware Requirements\n","excerpt":"Hardware Requirements\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/","tags":"","title":"Hardware Requirements"},{"body":"In general, it is a NORMAL situation for clickhouse that while processing a huge dataset it can use a lot of (or all of) the server resources. It is ‘by design’ - just to make the answers faster.\nThe main directions to reduce the CPU usage is to review the schema / queries to limit the amount of the data which need to be processed, and to plan the resources in a way when single running query will not impact the others.\nAny attempts to reduce the CPU usage will end up with slower queries!\nHow to slow down queries to reduce the CPU usage If it is acceptable for you - please check the following options for limiting the CPU usage:\n  setting max_threads: reducing the number of threads that are allowed to use one request. Fewer threads = more free cores for other requests. By default, it’s allowed to take half of the available CPU cores, adjust only when needed. So if if you have 10 cores then max_threads = 10 will work about twice faster than max_threads=5, but will take 100% or CPU. (max_threads=5 will use half of CPUs so 50%).\n  setting os_thread_priority: increasing niceness for selected requests. In this case, the operating system, when choosing which of the running processes to allocate processor time, will prefer processes with lower niceness. 0 is the default niceness. The higher the niceness, the lower the priority of the process. The maximum niceness value is 19.\n  These are custom settings that can be tweaked in several ways:\n  by specifying them when connecting a client, for example\nclickhouse-client --os_thread_priority=19 -q 'SELECT max (number) from numbers (100000000)'  echo 'SELECT max(number) from numbers(100000000)' | curl 'http://localhost:8123/?os_thread_priority=19' --data-binary @-   via dedicated API / connection parameters in client libraries\n  using the SQL command SET (works only within the session)\nSETos_thread_priority=19;SELECTmax(number)fromnumbers(100000000)  using different profiles of settings for different users. Something like\n\u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  ...  \u003c/default\u003e   \u003clowcpu\u003e  \u003cos_thread_priority\u003e19\u003c/os_thread_priority\u003e  \u003cmax_threads\u003e4\u003c/max_threads\u003e  \u003c/lowcpu\u003e  \u003c/profiles\u003e   \u003c!-- Users and ACL. --\u003e  \u003cusers\u003e  \u003c!-- If user name was not specified, 'default' user is used. --\u003e  \u003climited_user\u003e  \u003cpassword\u003e123\u003c/password\u003e  \u003cnetworks\u003e  \u003cip\u003e::/0\u003c/ip\u003e  \u003c/networks\u003e  \u003cprofile\u003elowcpu\u003c/profile\u003e   \u003c!-- Quota for user. --\u003e  \u003cquota\u003edefault\u003c/quota\u003e  \u003c/limited_user\u003e  \u003c/users\u003e  \u003c/yandex\u003e   There are also plans to introduce a system of more flexible control over the assignment of resources to different requests.\nAlso, if these are manually created queries, then you can try to discipline users by adding quotas to them (they can be formulated as “you can read no more than 100GB of data per hour” or “no more than 10 queries”, etc.)\nIf these are automatically generated queries, it may make sense to check if there is no way to write them in a more efficient way.\n","categories":"","description":"High CPU usage\n","excerpt":"High CPU usage\n","ref":"/altinity-kb-setup-and-maintenance/high-cpu-usage/","tags":"","title":"High CPU usage"},{"body":"Zookeeper use watches to notify a client on znode changes. This article explains how to check watches set by ZooKeeper servers and how it is used.\nSolution:\nZookeeper uses the 'wchc' command to list all watches set on the Zookeeper server.\n# echo wchc | nc zookeeper 2181\nReference\nhttps://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html\nThe wchp and wchc commands are not enabled by default because of their known DOS vulnerability. For more information, see ZOOKEEPER-2693and Zookeeper 3.5.2 - Denial of Service.\nBy default those commands are disabled, they can be enabled via Java system property:\n-Dzookeeper.4lw.commands.whitelist=*\non in zookeeper config: 4lw.commands.whitelist=*\\\n","categories":"","description":"How to check the list of watches\n","excerpt":"How to check the list of watches\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-how-to-check-the-list-of-watches/","tags":"","title":"How to check the list of watches"},{"body":"The following instructions are an example on how to convert a database with the Engine type Atomic to a database with the Engine type Ordinary.\nWarning That can be used only for simple schemas. Schemas with MATERIALIZED views will require extra manipulations.  CREATEDATABASEatomic_dbENGINE=Atomic;CREATEDATABASEordinary_dbENGINE=Ordinary;CREATETABLEatomic_db.xENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;INSERTINTOatomic_db.xSELECTnumberFROMnumbers(100000);RENAMETABLEatomic_db.xTOordinary_db.x;ls -1 /var/lib/clickhouse/data/ordinary_db/x all_1_1_0 detached format_version.txt DROPDATABASEatomic_db;DETACHDATABASEordinary_db;mv /var/lib/clickhouse/metadata/ordinary_db.sql /var/lib/clickhouse/metadata/atomic_db.sql vi /var/lib/clickhouse/metadata/atomic_db.sql mv /var/lib/clickhouse/metadata/ordinary_db /var/lib/clickhouse/metadata/atomic_db mv /var/lib/clickhouse/data/ordinary_db /var/lib/clickhouse/data/atomic_db ATTACHDATABASEatomic_db;SELECTcount()FROMatomic_db.x┌─count()─┐│100000│└─────────┘SHOWCREATEDATABASEatomic_db┌─statement──────────────────────────────────┐│CREATEDATABASEatomic_dbENGINE=Ordinary│└────────────────────────────────────────────┘Schemas with Materialized VIEW DROPDATABASEIFEXISTSatomic_db;DROPDATABASEIFEXISTSordinary_db;CREATEDATABASEatomic_dbengine=Atomic;CREATEDATABASEordinary_dbengine=Ordinary;CREATETABLEatomic_db.xENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;CREATEMATERIALIZEDVIEWatomic_db.x_mvENGINE=MergeTreeORDERBYtuple()ASSELECT*FROMatomic_db.x;CREATEMATERIALIZEDVIEWatomic_db.y_mvENGINE=MergeTreeORDERBYtuple()ASSELECT*FROMatomic_db.x;CREATETABLEatomic_db.zENGINE=MergeTreeORDERBYtuple()ASsystem.numbers;CREATEMATERIALIZEDVIEWatomic_db.z_mvTOatomic_db.zASSELECT*FROMatomic_db.x;INSERTINTOatomic_db.xSELECT*FROMnumbers(100);--- USE atomic_db; --- --- Query id: 28af886d-a339-4e9c-979c-8bdcfb32fd95 --- --- ┌─name───────────────────────────────────────────┐ --- │ .inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c │ --- │ .inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f │ --- │ x │ --- │ x_mv │ --- │ y_mv │ --- │ z │ --- │ z_mv │ --- └────────────────────────────────────────────────┘ SELECTmv_storage.database,mv_storage.name,mv.database,mv.nameFROMsystem.tablesASmv_storageLEFTJOINsystem.tablesASmvONsubstring(mv_storage.name,11)=toString(mv.uuid)WHEREmv_storage.nameLIKE'.inner_id.%'ANDmv_storage.database='atomic_db';-- ┌─database──┬─name───────────────────────────────────────────┬─mv.database─┬─mv.name─┐ -- │ atomic_db │ .inner_id.81e1a67d-3d02-4b2a-be17-84d8626d2328 │ atomic_db │ y_mv │ -- │ atomic_db │ .inner_id.e428225c-982a-4859-919b-ba5026db101d │ atomic_db │ x_mv │ -- └───────────┴────────────────────────────────────────────────┴─────────────┴─────────┘ /* STEP 1: prepare rename statements, also to rename implicit mv storage table to explicit one */SELECTif(t.nameLIKE'.inner_id.%','RENAME TABLE `'||t.database||'`.`'||t.name||'` TO `ordinary_db`.`'||mv.name||'_storage`;','RENAME TABLE `'||t.database||'`.`'||t.name||'` TO `ordinary_db`.`'||t.name||'`;')FROMsystem.tablesastLEFTJOINsystem.tablesmvON(substring(t.name,11)=toString(mv.uuid)ANDt.database=mv.database)WHEREt.database='atomic_db'ANDt.engine\u003c\u003e'MaterializedView'FORMATTSVRaw;-- RENAME TABLE `atomic_db`.`.inner_id.b7906fec-f4b2-455b-bf9b-2b18ca64842c` TO `ordinary_db`.`y_mv_storage`; -- RENAME TABLE `atomic_db`.`.inner_id.bd32d79b-272d-4710-b5ad-bca78d09782f` TO `ordinary_db`.`x_mv_storage`; -- RENAME TABLE `atomic_db`.`x` TO `ordinary_db`.`x`; -- RENAME TABLE `atomic_db`.`z` TO `ordinary_db`.`z`; /* STEP 2: prepare statements to reattach MV */-- Can be done manually: pick existing MV definition (SHOW CREATE TABLE), and change it in the following way: -- 1) add TO keyword 2) remove column names and engine settings after mv name SELECTif(t.nameLIKE'.inner_id.%',replaceRegexpOne(mv.create_table_query,'^CREATE MATERIALIZED VIEW ([^ ]+) (.*? AS ','CREATE MATERIALIZED VIEW \\\\1 TO \\\\1_storage AS '),mv.create_table_query)FROMsystem.tablesasmvLEFTJOINsystem.tablestON(substring(t.name,11)=toString(mv.uuid)ANDt.database=mv.database)WHEREmv.database='atomic_db'ANDmv.engine='MaterializedView'FORMATTSVRaw;-- CREATE MATERIALIZED VIEW atomic_db.x_mv TO atomic_db.x_mv_storage AS SELECT * FROM atomic_db.x -- CREATE MATERIALIZED VIEW atomic_db.y_mv TO atomic_db.y_mv_storage AS SELECT * FROM atomic_db.x /* STEP 3: stop inserts, fire renames statements prepared at the step 1 (hint: use clickhouse-client -mn) */RENAME.../* STEP 4: ensure that only MaterializedView left in source db, and drop it. */SELECT*FROMsystem.tablesWHEREdatabase='atomic_db'andengine\u003c\u003e'MaterializedView';DROPDATABASEatomic_db;/* STEP 4. rename table to old name: */DETACHDATABASEordinary_db;-- rename files / folders: mv/var/lib/clickhouse/metadata/ordinary_db.sql/var/lib/clickhouse/metadata/atomic_db.sqlvi/var/lib/clickhouse/metadata/atomic_db.sqlmv/var/lib/clickhouse/metadata/ordinary_db/var/lib/clickhouse/metadata/atomic_dbmv/var/lib/clickhouse/data/ordinary_db/var/lib/clickhouse/data/atomic_db-- attach database atomic_db; ATTACHDATABASEatomic_db;/* STEP 5. restore MV using statements created on STEP 2 */","categories":"","description":"How to Convert Atomic to Ordinary\n","excerpt":"How to Convert Atomic to Ordinary\n","ref":"/engines/altinity-kb-atomic-database-engine/altinity-kb-how-to-convert-atomic-to-ordinary/","tags":"","title":"How to Convert Atomic to Ordinary"},{"body":"Example Create test_table based on the source table.\nCREATETABLEtest_tableASsource_tableENGINE=MergeTree()PARTITIONBY...;If the source table has Replicated*MergeTree engine, you would need to change it to non-replicated.\nAttach one partition with data from the source table to test_table.\nALTERTABLEtest_tableATTACHPARTITIONID'20210120'FROMsource_table;You can modify the column or create a new one based on the old column value.\nALTERTABLEtest_tableMODIFYCOLUMNcolumn_aCODEC(ZSTD(2));ALTERTABLEtest_tableADDCOLUMNcolumn_newUInt32DEFAULTtoUInt32OrZero(column_old)CODEC(T64,LZ4);After that, you would need to populate changed columns with data.\nALTERTABLEtest_tableUPDATEcolumn_a=column_a,column_new=column_newWHERE1;You can look status of mutation via the system.mutations table\nSELECT*FROMsystem.mutations;And it’s also possible to kill mutation if there are some problems with it.\nKILLMUTATIONWHERE...Useful queries SELECTdatabase,table,count()ASparts,uniqExact(partition_id)ASpartition_cnt,sum(rows),formatReadableSize(sum(data_compressed_bytes)AScomp_bytes)AScomp,formatReadableSize(sum(data_uncompressed_bytes)ASuncomp_bytes)ASuncomp,uncomp_bytes/comp_bytesASratioFROMsystem.partsWHEREactiveGROUPBYdatabase,tableORDERBYcomp_bytesDESCSELECTdatabase,table,column,type,sum(rows)ASrows,sum(column_data_compressed_bytes)AScompressed_bytes,formatReadableSize(compressed_bytes)AScompressed,formatReadableSize(sum(column_data_uncompressed_bytes))ASuncompressed,sum(column_data_uncompressed_bytes)/compressed_bytesASratio,any(compression_codec)AScodecFROMsystem.parts_columnsASpcLEFTJOINsystem.columnsAScON(pc.database=c.database)AND(c.table=pc.table)AND(c.name=pc.column)WHERE(databaseLIKE'%')AND(tableLIKE'%')ANDactiveGROUPBYdatabase,table,column,typeORDERBYdatabase,table,sum(column_data_compressed_bytes)DESC","categories":"","description":"How to test different compression codecs\n","excerpt":"How to test different compression codecs\n","ref":"/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/","tags":"","title":"How to test different compression codecs"},{"body":"https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup27/adaptive_index_granularity.pdf\n","categories":"","description":"index \u0026 column files\n","excerpt":"index \u0026 column files\n","ref":"/engines/mergetree-table-engine-family/index-and-column-files/","tags":"","title":"index \u0026 column files"},{"body":"clickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format TSV'\u003espeed.tsvclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format RowBinary'\u003espeed.RowBinaryclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format Native'\u003espeed.Nativeclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format CSV'\u003espeed.csvclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format JSONEachRow'\u003espeed.JSONEachRowclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format Parquet'\u003espeed.parquetclickhouse-client-q'select toString(number) s, number n, number/1000 f from numbers(100000000) format Avro'\u003espeed.avro-- Engine=Null does not have I/O / sorting overhead -- we test only formats parsing performance. createtablen(sString,nUInt64,fFloat64)Engine=Null-- clickhouse-client parses formats itself -- it allows to see user CPU time -- time is used in a multithreaded application -- another option is to disable parallelism `--input_format_parallel_parsing=0` -- real -- wall / clock time. timeclickhouse-client-t-q'insert into n format TSV'\u003cspeed.tsv2.693real0m2.728suser0m14.066stimeclickhouse-client-t-q'insert into n format RowBinary'\u003cspeed.RowBinary3.744real0m3.773suser0m4.245stimeclickhouse-client-t-q'insert into n format Native'\u003cspeed.Native2.359real0m2.382suser0m1.945stimeclickhouse-client-t-q'insert into n format CSV'\u003cspeed.csv3.296real0m3.328suser0m18.145stimeclickhouse-client-t-q'insert into n format JSONEachRow'\u003cspeed.JSONEachRow8.872real0m8.899suser0m30.235stimeclickhouse-client-t-q'insert into n format Parquet'\u003cspeed.parquet4.905real0m4.929suser0m5.478stimeclickhouse-client-t-q'insert into n format Avro'\u003cspeed.avro11.491real0m11.519suser0m12.166sAs you can see the JSONEachRow is the worst format (user 0m30.235s) for this synthetic dataset. Native is the best (user 0m1.945s). TSV / CSV are good in wall time but spend a lot of CPU (user time).\n","categories":"","description":"","excerpt":"clickhouse-client-q'select toString(number) s, number n, number/1000 f …","ref":"/altinity-kb-schema-design/ingestion-performance-and-formats/","tags":"","title":"Ingestion performance and formats"},{"body":"How do I Store IPv4 and IPv6 Address In One Field? There is a clean and simple solution for that. Any IPv4 has its unique IPv6 mapping:\n IPv4 IP address: 191.239.213.197 IPv4-mapped IPv6 address: ::ffff:191.239.213.197  Find IPs matching CIDR/network mask (IPv4) WITHIPv4CIDRToRange(toIPv4('10.0.0.1'),8)asrangeSELECT*FROMvalues('ip IPv4',toIPv4('10.2.3.4'),toIPv4('192.0.2.1'),toIPv4('8.8.8.8'))WHEREipBETWEENrange.1ANDrange.2;Find IPs matching CIDR/network mask (IPv6) WITHIPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'),32)asrangeSELECT*FROMvalues('ip IPv6',toIPv6('2001:db8::8a2e:370:7334'),toIPv6('::ffff:192.0.2.1'),toIPv6('::'))WHEREipBETWEENrange.1ANDrange.2;","categories":"","description":"IPs/masks\n","excerpt":"IPs/masks\n","ref":"/altinity-kb-schema-design/how-to-store-ips/","tags":"","title":"IPs/masks"},{"body":"The main purpose of JOIN table engine is to avoid building the right table for joining on each query execution. So it’s usually used when you have a high amount of fast queries which share the same right table for joining.\nUpdates It’s possible to update rows with setting join_any_take_last_row enabled.\nCREATETABLEid_val_join(`id`UInt32,`val`UInt8)ENGINE=Join(ANY,LEFT,id)SETTINGSjoin_any_take_last_row=1Ok.INSERTINTOid_val_joinVALUES(1,21)(1,22)(3,23);Ok.SELECT*FROM(SELECTtoUInt32(number)ASidFROMnumbers(4))ASnANYLEFTJOINid_val_joinUSING(id)┌─id─┬─val─┐│0│0││1│22││2│0││3│23│└────┴─────┘INSERTINTOid_val_joinVALUES(1,40)(2,24);Ok.SELECT*FROM(SELECTtoUInt32(number)ASidFROMnumbers(4))ASnANYLEFTJOINid_val_joinUSING(id)┌─id─┬─val─┐│0│0││1│40││2│24││3│23│└────┴─────┘https://clickhouse.tech/docs/en/engines/table-engines/special/join/\n","categories":"","description":"JOIN table engine\n","excerpt":"JOIN table engine\n","ref":"/altinity-kb-queries-and-syntax/joins/join-table-engine/","tags":"","title":"JOIN table engine"},{"body":"Sample data CREATETABLEtest_metrics(counter_idInt64,timestampDateTime,metricUInt64)Engine=Log;INSERTINTOtest_metricsSELECTnumber%3,toDateTime('2021-01-01 00:00:00'),1FROMnumbers(20);INSERTINTOtest_metricsSELECTnumber%3,toDateTime('2021-01-03 00:00:00'),1FROMnumbers(20);SELECTcounter_id,toDate(timestamp)dt,sum(metric)FROMtest_metricsGROUPBYcounter_id,dtORDERBYcounter_id,dt;┌─counter_id─┬─────────dt─┬─sum(metric)─┐│0│2021-01-01│7││0│2021-01-03│7││1│2021-01-01│7││1│2021-01-03│7││2│2021-01-01│6││2│2021-01-03│6│└────────────┴────────────┴─────────────┘Calendar WITHarrayMap(i-\u003e(toDate('2021-01-01')+i),range(4))ASCalendarSELECTarrayJoin(Calendar);┌─arrayJoin(Calendar)─┐│2021-01-01││2021-01-02││2021-01-03││2021-01-04│└─────────────────────┘Join with Calendar using arrayJoin SELECTcounter_id,tuple.2dt,sum(tuple.1)sumFROM(WITHarrayMap(i-\u003e(0,toDate('2021-01-01')+i),range(4))ASCalendarSELECTcounter_id,arrayJoin(arrayConcat(Calendar,[(sum,dt)]))tupleFROM(SELECTcounter_id,toDate(timestamp)dt,sum(metric)sumFROMtest_metricsGROUPBYcounter_id,dt))GROUPBYcounter_id,dtORDERBYcounter_id,dt;┌─counter_id─┬─────────dt─┬─sum─┐│0│2021-01-01│7││0│2021-01-02│0││0│2021-01-03│7││0│2021-01-04│0││1│2021-01-01│7││1│2021-01-02│0││1│2021-01-03│7││1│2021-01-04│0││2│2021-01-01│6││2│2021-01-02│0││2│2021-01-03│6││2│2021-01-04│0│└────────────┴────────────┴─────┘With fill SELECTcounter_id,toDate(timestamp)ASdt,sum(metric)ASsumFROMtest_metricsGROUPBYcounter_id,dtORDERBYcounter_idASCWITHFILL,dtASCWITHFILLFROMtoDate('2021-01-01')TOtoDate('2021-01-05');┌─counter_id─┬─────────dt─┬─sum─┐│0│2021-01-01│7││0│2021-01-02│0││0│2021-01-03│7││0│2021-01-04│0││1│2021-01-01│7││1│2021-01-02│0││1│2021-01-03│7││1│2021-01-04│0││2│2021-01-01│6││2│2021-01-02│0││2│2021-01-03│6││2│2021-01-04│0│└────────────┴────────────┴─────┘","categories":"","description":"Join with Calendar using Arrays\n","excerpt":"Join with Calendar using Arrays\n","ref":"/altinity-kb-queries-and-syntax/join-with-calendar-using-arrays/","tags":"","title":"Join with Calendar using Arrays"},{"body":"See presentation:\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/join.pdf\n","categories":"","description":"JOINs\n","excerpt":"JOINs\n","ref":"/altinity-kb-queries-and-syntax/joins/","tags":"","title":"JOINs"},{"body":"Tables with engine Null don’t store data but can be used as a source for materialized views.\nJSONAsString a special input format which allows to ingest JSONs into a String column. If the input has several JSON objects (comma separated) they will be interpreted as separate rows. JSON can be multiline.\ncreatetableentrypoint(JString)Engine=Null;createtabledatastore(aString,iInt64,fFloat64)Engine=MergeTreeorderbya;creatematerializedviewjsonConvertertodatastoreasselect(JSONExtract(J,'Tuple(String,Tuple(Int64,Float64))')asx),x.1asa,x.2.1asi,x.2.2asffromentrypoint;$echo'{\"s\": \"val1\", \"b2\": {\"i\": 42, \"f\": 0.1}}'|\\clickhouse-client-q\"insert into entrypoint format JSONAsString\"$echo'{\"s\": \"val1\",\"b2\": {\"i\": 33, \"f\": 0.2}},{\"s\": \"val1\",\"b2\": {\"i\": 34, \"f\": 0.2}}'|\\clickhouse-client-q\"insert into entrypoint format JSONAsString\"SELECT*FROMdatastore;┌─a────┬──i─┬───f─┐│val1│42│0.1│└──────┴────┴─────┘┌─a────┬──i─┬───f─┐│val1│33│0.2││val1│34│0.2│└──────┴────┴─────┘See also: JSONExtract to parse many attributes at a time\n","categories":"","description":"JSONAsString and Mat. View as JSON parser\n","excerpt":"JSONAsString and Mat. View as JSON parser\n","ref":"/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/","tags":"","title":"JSONAsString and Mat. View as JSON parser"},{"body":"WITHJSONExtract(json,'Tuple(name String, id String, resources Nested(description String, format String, tracking_summary Tuple(total UInt32, recent UInt32)), extras Nested(key String, value String))')ASparsed_jsonSELECTtupleElement(parsed_json,'name')ASname,tupleElement(parsed_json,'id')ASid,tupleElement(tupleElement(parsed_json,'resources'),'description')AS`resources.description`,tupleElement(tupleElement(parsed_json,'resources'),'format')AS`resources.format`,tupleElement(tupleElement(tupleElement(parsed_json,'resources'),'tracking_summary'),'total')AS`resources.tracking_summary.total`,tupleElement(tupleElement(tupleElement(parsed_json,'resources'),'tracking_summary'),'recent')AS`resources.tracking_summary.recent`FROMurl('https://raw.githubusercontent.com/jsonlines/guide/master/datagov100.json','JSONAsString','json String')","categories":"","description":"JSONExtract to parse many attributes at a time\n","excerpt":"JSONExtract to parse many attributes at a time\n","ref":"/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time/","tags":"","title":"JSONExtract to parse many attributes at a time"},{"body":"TLDR version use fresh Java version (11 or newer), disable swap and set up (for 4 Gb node):\nJAVA_OPTS=\"-Xms512m -Xmx3G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" If you have a node with more RAM - change it accordingly, for example for 8Gb node:\nJAVA_OPTS=\"-Xms512m -Xmx7G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50\" Details   ZooKeeper runs as in JVM. Depending on version different garbage collectors are available.\n  Recent JVM versions (starting from 10) use G1 garbage collector by default (should work fine). On JVM 13-14 using ZGC or Shenandoah garbage collector may reduce pauses. On older JVM version (before 10) you may want to make some tuning to decrease pauses, ParNew + CMS garbage collectors (like in Yandex config) is one of the best options.\n  One of the most important setting for JVM application is heap size. A heap size of \u003e1 GB is recommended for most use cases and monitoring heap usage to ensure no delays are caused by garbage collection. We recommend to use at least 4Gb of RAM for zookeeper nodes (8Gb is better, that will make difference only when zookeeper is heavily loaded).\n  Set the Java heap size smaller than available RAM size on the node. This is very important to avoid swapping, which will seriously degrade ZooKeeper performance. Be conservative - use a maximum heap size of 3GB for a 4GB machine.\n  Add XX:+AlwaysPreTouch flag as well to load the memory pages into memory at the start of the zookeeper.\n  Set min (Xms) heap size to the values like 512Mb, or even to the same value as max (Xmx) to avoid resizing and returning the RAM to OS. Add XX:+AlwaysPreTouch flag as well to load the memory pages into memory at the start of the zookeeper.\n  MaxGCPauseMillis=50 (by default 200) - the ’target’ acceptable pause for garbage collection (milliseconds)\n  jute.maxbuffer limits the maximum size of znode content. By default it’s 1Mb. In some usecases (lot of partitions in table) ClickHouse may need to create bigger znodes.\n  (optional) enable GC logs: -Xloggc:/path_to/gc.log\n  Zookeeper configurarion used by Yandex Metrika (from 2017) The configuration used by Yandex ( https://clickhouse.tech/docs/en/operations/tips/#zookeeper ) - they use older JVM version (with UseParNewGC garbage collector), and tune GC logs heavily:\nJAVA_OPTS=\"-Xms{{ cluster.get('xms','128M') }} \\ -Xmx{{ cluster.get('xmx','1G') }} \\ -Xloggc:/var/log/$NAME/zookeeper-gc.log \\ -XX:+UseGCLogFileRotation \\ -XX:NumberOfGCLogFiles=16 \\ -XX:GCLogFileSize=16M \\ -verbose:gc \\ -XX:+PrintGCTimeStamps \\ -XX:+PrintGCDateStamps \\ -XX:+PrintGCDetails -XX:+PrintTenuringDistribution \\ -XX:+PrintGCApplicationStoppedTime \\ -XX:+PrintGCApplicationConcurrentTime \\ -XX:+PrintSafepointStatistics \\ -XX:+UseParNewGC \\ -XX:+UseConcMarkSweepGC \\ -XX:+CMSParallelRemarkEnabled\" See also  https://wikitech.wikimedia.org/wiki/JVM_Tuning#G1_for_full_gcs https://sematext.com/blog/java-garbage-collection-tuning/ https://www.oracle.com/technical-resources/articles/java/g1gc.html https://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2 https://docs.cloudera.com/runtime/7.2.7/kafka-performance-tuning/topics/kafka-tune-broker-tuning-jvm.html https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cm-tune-g1gc.html https://blog.sokolenko.me/2014/11/javavm-options-production.html https://www.maknesium.de/21-most-important-java-8-vm-options-for-servers https://docs.oracle.com/javase/10/gctuning/introduction-garbage-collection-tuning.htm#JSGCT-GUID-326EB4CF-8C8C-4267-8355-21AB04F0D304 https://github.com/chewiebug/GCViewer  ","categories":"","description":"JVM sizes and garbage collector settings\n","excerpt":"JVM sizes and garbage collector settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/","tags":"","title":"JVM sizes and garbage collector settings"},{"body":"git log -- contrib/librdkafka | git name-rev --stdin    ClickHouse version librdkafka version     21.10+ (#27883) 1.6.1 + snappy fixes + boring ssl + illumos_build fixes + edenhill#3279 fix   21.6+ (#23874) 1.6.1 + snappy fixes + boring ssl + illumos_build fixes   21.1+ (#18671) 1.6.0-RC3 + snappy fixes + boring ssl   20.13+ (#18053) 1.5.0 + msan fixes + snappy fixes + boring ssl   20.7+ (#12991) 1.5.0 + msan fixes   20.5+ (#11256) 1.4.2   20.2+ (#9000) 1.3.0   19.11+ (#5872) 1.1.0   19.5+ (#4799) 1.0.0   19.1+ (#4025) 1.0.0-RC5   v1.1.54382+ (#2276) 0.11.4    ","categories":"","description":"Kafka\n","excerpt":"Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/","tags":"","title":"Kafka"},{"body":"One of the threads from scheduled_pool (pre 20.9) / background_message_broker_schedule_pool (after 20.9) do that in infinite loop:\n Batch poll (time limit: kafka_poll_timeout_ms 500ms, messages limit: kafka_poll_max_batch_size 65536) Parse messages. If we don’t have enough data (rows limit: kafka_max_block_size 1048576) or time limit reached (kafka_flush_interval_ms 7500ms) - continue polling (goto p.1) Write a collected block of data to MV Do commit (commit after write = at-least-once).  On any error, during that process, Kafka client is restarted (leading to rebalancing - leave the group and get back in few seconds).\nImportant settings These usually should not be adjusted:\n kafka_poll_max_batch_size = max_block_size (65536) kafka_poll_timeout_ms = stream_poll_timeout_ms (500ms)  You may want to adjust those depending on your scenario:\n kafka_flush_interval_ms = stream_poll_timeout_ms (7500ms) kafka_max_block_size = min_insert_block_size / kafka_num_consumers (for the single consumer: 1048576)  See also https://github.com/ClickHouse/ClickHouse/pull/11388\n","categories":"","description":"Kafka main parsing loop\n","excerpt":"Kafka main parsing loop\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/","tags":"","title":"Kafka main parsing loop"},{"body":"For very large topics when you need more parallelism (especially on the insert side) you may use several tables with the same pipeline (pre 20.9) or enable kafka_thread_per_consumer (after 20.9).\nkafka_num_consumers = N, kafka_thread_per_consumer=1 Notes:\n the inserts will happen in parallel (without that setting inserts happen linearly) enough partitions are needed.  Before increasing kafka_num_consumers with keeping kafka_thread_per_consumer=0 may improve consumption \u0026 parsing speed, but flushing \u0026 committing still happens by a single thread there (so inserts are linear).\n","categories":"","description":"Kafka parallel consuming\n","excerpt":"Kafka parallel consuming\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/","tags":"","title":"Kafka parallel consuming"},{"body":"Unfortunately not all queries can be killed. KILL QUERY only sets a flag that must be checked by the query. A query pipeline is checking this flag before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed. If a query does not stop, the only way to get rid of it is to restart ClickHouse.\nSee also\nhttps://github.com/ClickHouse/ClickHouse/issues/3964 https://github.com/ClickHouse/ClickHouse/issues/1576\nHow to replace a running query  Q. We are trying to abort running queries when they are being replaced with a new one. We are setting the same query id for this. In some cases this error happens:\nQuery with id = e213cc8c-3077-4a6c-bc78-e8463adad35d is already running and can’t be stopped\nThe query is still being killed but the new one is not being executed. Do you know anything about this and if there is a fix or workaround for it?\n I guess you use replace_running_query + replace_running_query_max_wait_ms.\nUnfortunately it’s not always possible to kill the query at random moment of time.\nKill don’t send any signals, it just set a flag. Which gets (synchronously) checked at certain moments of query execution, mostly after finishing processing one block and starting another.\nOn certain stages (executing scalar sub-query) the query can not be killed at all. This is a known issue and requires an architectural change to fix it.\n I see. Is there a workaround?\nThis is our use case:\nA user requests an analytics report which has a query that takes several settings, the user makes changes to the report (e.g. to filters, metrics, dimensions…). Since the user changed what he is looking for the query results from the initial query are never used and we would like to cancel it when starting the new query (edited)\n You can just use 2 commands:\nKILLQUERYWHEREquery_id=' ... 'ASYNCSELECT...newquery....in that case you don’t need to care when the original query will be stopped.\n","categories":"","description":"KILL QUERY\n","excerpt":"KILL QUERY\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-kill-query/","tags":"","title":"KILL QUERY"},{"body":"Sample data CREATETABLEllexample(gInt32,aDate)ENGINE=Memory;INSERTINTOllexampleSELECTnumber%3,toDate('2020-01-01')+numberFROMnumbers(10);SELECT*FROMllexampleORDERBYg,a;┌─g─┬──────────a─┐│0│2020-01-01││0│2020-01-04││0│2020-01-07││0│2020-01-10││1│2020-01-02││1│2020-01-05││1│2020-01-08││2│2020-01-03││2│2020-01-06││2│2020-01-09│└───┴────────────┘Using arrays selectg,(arrayJoin(tuple_ll)asll).1a,ll.2prev,ll.3nextfrom(selectg,arrayZip(arraySort(groupArray(a))asaa,arrayPopBack(arrayPushFront(aa,toDate(0))),arrayPopFront(arrayPushBack(aa,toDate(0))))tuple_llfromllexamplegroupbyg)orderbyg,a;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using window functions (starting from Clickhouse 21.3) SETallow_experimental_window_functions=1;SELECTg,a,any(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEEN1PRECEDINGAND1PRECEDING)ASprev,any(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEEN1FOLLOWINGAND1FOLLOWING)ASnextFROMllexampleORDERBYgASC,aASC;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using lagInFrame/leadInFrame (starting from ClickHouse 21.4) SELECTg,a,lagInFrame(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEENUNBOUNDEDPRECEDINGANDUNBOUNDEDFOLLOWING)ASprev,leadInFrame(a)OVER(PARTITIONBYgORDERBYaASCROWSBETWEENUNBOUNDEDPRECEDINGANDUNBOUNDEDFOLLOWING)ASnextFROMllexampleORDERBYgASC,aASC;┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│1970-01-01││1│2020-01-02│1970-01-01│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│1970-01-01││2│2020-01-03│1970-01-01│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘Using neighbor (no grouping, incorrect result over blocks) SELECTg,a,neighbor(a,-1)ASprev,neighbor(a,1)ASnextFROM(SELECT*FROMllexampleORDERBYgASC,aASC);┌─g─┬──────────a─┬───────prev─┬───────next─┐│0│2020-01-01│1970-01-01│2020-01-04││0│2020-01-04│2020-01-01│2020-01-07││0│2020-01-07│2020-01-04│2020-01-10││0│2020-01-10│2020-01-07│2020-01-02││1│2020-01-02│2020-01-10│2020-01-05││1│2020-01-05│2020-01-02│2020-01-08││1│2020-01-08│2020-01-05│2020-01-03││2│2020-01-03│2020-01-08│2020-01-06││2│2020-01-06│2020-01-03│2020-01-09││2│2020-01-09│2020-01-06│1970-01-01│└───┴────────────┴────────────┴────────────┘","categories":"","description":"Lag / Lead\n","excerpt":"Lag / Lead\n","ref":"/altinity-kb-queries-and-syntax/lag-lead/","tags":"","title":"Lag / Lead"},{"body":"In general - one of the simplest option to do load balancing is to implement it on the client side.\nI.e. list several endpoints for clickhouse connections and add some logic to pick one of the nodes.\nMany client libraries support that.\nClickHouse native protocol (port 9000) Currently there are no protocol-aware proxies for clickhouse protocol, so the proxy / load balancer can work only on TCP level.\nOne of the best option for TCP load balancer is haproxy, also nginx can work in that mode.\nHaproxy will pick one upstream when connection is established, and after that it will keep it connected to the same server until the client or server will disconnect (or some timeout will happen).\nIt can’t send different queries coming via a single connection to different servers, as he knows nothing about clickhouse protocol and doesn’t know when one query ends and another start, it just sees the binary stream.\nSo for native protocol, there are only 3 possibilities:\n close connection after each query client-side close connection after each query server-side (currently there is only one setting for that - idle_connection_timeout=0, which is not exact what you need, but similar). use a clickhouse server with Distributed table as a proxy.  HTTP protocol (port 8123) There are many more options and you can use haproxy / nginx / chproxy, etc. chproxy give some extra clickhouse-specific features, you can find a list of them at https://github.com/Vertamedia/chproxy\n","categories":"","description":"Load balancers\n","excerpt":"Load balancers\n","ref":"/altinity-kb-setup-and-maintenance/load-balancers/","tags":"","title":"Load balancers"},{"body":"Settings allow_suspicious_low_cardinality_types In CREATE TABLE statement allows specifying LowCardinality modifier for types of small fixed size (8 or less). Enabling this may increase merge times and memory consumption.\nlow_cardinality_max_dictionary_size default - 8192\nMaximum size (in rows) of shared global dictionary for LowCardinality type.\nlow_cardinality_use_single_dictionary_for_part LowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.\nlow_cardinality_allow_in_native_format Use LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query.\noutput_format_arrow_low_cardinality_as_dictionary Enable output LowCardinality type as Dictionary Arrow type\n","categories":"","description":"LowCardinality\n","excerpt":"LowCardinality\n","ref":"/altinity-kb-schema-design/lowcardinality/","tags":"","title":"LowCardinality"},{"body":"https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup31/ml.pdf\nCatBoost / MindsDB / Fast.ai\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup38/forecast.pdf\n","categories":"","description":"Machine learning in ClickHouse\n","excerpt":"Machine learning in ClickHouse\n","ref":"/altinity-kb-queries-and-syntax/machine-learning-in-clickhouse/","tags":"","title":"Machine learning in ClickHouse"},{"body":" Info MATERIALIZED VIEWs in ClickHouse behave like AFTER INSERT TRIGGER to the left-most table listed in its SELECT statement.  MATERIALIZED VIEWS  Clickhouse and the magic of materialized views. Basics explained with examples: webinar recording Everything you should know about materialized views. Very detailed information about internals: video, annotated presentation, presentation  Best practices   Use MATERIALIZED VIEW with TO syntax (explicit storage table)\nFirst you create the table which will store the data calculated by MV explicitly, and after that create materialized view itself with TO syntax.\nCREATETABLEtarget(...)Engine=ReplacingSummingMergeTree;CREATEMATERIALIZEDVIEWmv_source2targetTOtargetASSELECT...FROMsource;That way it’s bit simpler to do schema migrations or build more complicated pipelines when one table is filled by several MV.\nWith engine=Atomic it hard to map undelying table with the MV.\n  Avoid using POPULATE when creating MATERIALIZED VIEW on big tables.\nUse manual backfilling (with the same query) instead.\n With POPULATE the data ingested to the source table during MV populating will not appear in MV. POPULATE doesn’t work with TO syntax. With manual backfilling, you have much better control on the process - you can do it in parts, adjust settings etc. In case of some failure ‘in the middle (for example due to timeouts), it’s hard to understand the state of the MV.  CREATEMATERIALIZEDVIEWmv_source2targetTOtargetASSELECT...FROMsourceWHEREcond\u003e...INSERTINTOtargetSELECT...FROMsourceWHEREcond\u003c...This way you have full control backfilling process (you can backfill in smaller parts to avoid timeouts, do some cross-checks / integrity-checks, change some settings, etc.)\n  FAQ Q. Can I attach MATERIALIZED VIEW to the VIEW, or engine=Merge, or engine=MySQL, etc.? Since MATERIALIZED VIEWs are updated on every INSERT to the underlying table and you can not insert anything to the usual VIEW, the materialized view update will never be triggered.\nNormally you should build MATERIALIZED VIEWs on the top of the table with MergeTree engine family.\nQ. I’ve created materialized error with some error, and since it’s it reading from Kafka I don’t understand where the error is Server logs will help you. Also, see the next question.\nQ. How to debug misbehaving MATERIALIZED VIEW? You can also attach the same MV to some dummy table with engine=Log (or even Null) and do some manual inserts there to debug the behavior. Similar way (as the Materialized view often can contain some pieces of the business logic of the application) you can create tests for your schema.\nWarning Always test MATERIALIZED VIEWs first on staging or testing environments  Possible test scenario:\n create a copy of the original table CREATE TABLE src_copy ... AS src create MV on that copy CREATE MATERIALIZED VIEW ... AS SELECT ... FROM src_copy check if inserts to src_copy work properly, and mv is properly filled. INSERT INTO src_copy SELECT * FROM src LIMIT 100 cleanup the temp stuff and recreate MV on real table.  Q. Can I use subqueries / joins in MV? It is possible but it is a very bad idea for most of the use cases**.**\nSo it will most probably work not as you expect and will hit insert performance significantly.\nThe MV will be attached (as AFTER INSERT TRIGGER) to the left-most table in the MV SELECT statement, and it will ‘see’ only freshly inserted rows there. It will ‘see’ the whole set of rows of other tables, and the query will be executed EVERY TIME you do the insert to the left-most table. That will impact the performance speed there significantly. If you really need to update the MV with the left-most table, not impacting the performance so much you can consider using dictionary / engine=Join / engine=Set for right-hand table / subqueries (that way it will be always in memory, ready to use).\nQ. How to alter MV implicit storage (w/o TO syntax)   take the existing MV definition\nSHOWCREATETABLEdbname.mvname;Adjust the query in the following manner:\n replace ‘CREATE MATERIALIZED VIEW’ to ‘ATTACH MATERIALIZED VIEW’ add needed columns;    Detach materialized view with the command:\nDETACHTABLEdbname.mvnameONCLUSTERcluster_name;  Add the needed column to the underlying ReplicatedAggregatingMergeTree table\n-- if the Materialized view was created without TO keyword ALTERTABLEdbname.`.inner.mvname`ONCLUSTERcluster_nameaddcolumntokensAggregateFunction(uniq,UInt64);-- othewise just alter the target table used in `CREATE MATERIALIZED VIEW ...` `TO ...` clause   attach MV back using the query you create at p. 1.\nATTACHMATERIALIZEDVIEWdbname.mvnameONCLUSTERcluster_name(/* ... */`tokens`AggregateFunction(uniq,UInt64))ENGINE=ReplicatedAggregatingMergeTree(...)ORDERBY...ASSELECT/* ... */uniqState(rand64())astokensFROM/* ... */GROUPBY/* ... */  As you can see that operation is NOT atomic, so the safe way is to stop data ingestion during that procedure.\nIf you have version 19.16.13 or newer you can change the order of step 2 and 3 making the period when MV is detached and not working shorter (related issue https://github.com/ClickHouse/ClickHouse/issues/7878).\nSee also:\n https://github.com/ClickHouse/ClickHouse/issues/1226 https://github.com/ClickHouse/ClickHouse/pull/7533  ","categories":"","description":"MATERIALIZED VIEWS\n","excerpt":"MATERIALIZED VIEWS\n","ref":"/altinity-kb-schema-design/materialized-views/","tags":"","title":"MATERIALIZED VIEWS"},{"body":"max_memory_usage. Single query memory usage max_memory_usage - the maximum amount of memory allowed for a single query to take. By default, it’s 10Gb. The default value is good, don’t adjust it in advance.\nThere are scenarios when you need to relax the limit for particular queries (if you hit ‘Memory limit (for query) exceeded’), or use a lower limit if you need to discipline the users or increase the number of simultaneous queries.\nServer memory usage Server memory usage = constant memory footprint (used by different caches, dictionaries, etc) + sum of memory temporary used by running queries (a theoretical limit is a number of simultaneous queries multiplied by max_memory_usage).\nSince 20.4 you can set up a global limit using the max_server_memory_usage setting. If something will hit that limit you will see ‘Memory limit (total) exceeded’ in random places.\nBy default it 90% of the physical RAM of the server. https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#max_server_memory_usage https://github.com/ClickHouse/ClickHouse/blob/e5b96bd93b53d2c1130a249769be1049141ef386/programs/server/config.xml#L239-L250\nYou can decrease that in some scenarios (like you need to leave more free RAM for page cache or to some other software).\nHow to check what is using my RAM? altinity-kb-who-ate-my-memory.md\u0026quot; \nMark cache https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup39/mark-cache.pdf\n","categories":"","description":"memory configuration settings\n","excerpt":"memory configuration settings\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/","tags":"","title":"memory configuration settings"},{"body":"Internals:\nhttps://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup41/merge_tree.pdf\nhttps://youtu.be/1UIl7FpNo2M?t=2467\n","categories":"","description":"MergeTree table engine family\n","excerpt":"MergeTree table engine family\n","ref":"/engines/mergetree-table-engine-family/","tags":"","title":"MergeTree table engine family"},{"body":"Moved\n","categories":"","description":"Monitoring Considerations\n","excerpt":"Monitoring Considerations\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/monitoring-considerations/","tags":"","title":"Monitoring Considerations"},{"body":"Suppose we mount a new device at path /mnt/disk_1 and want to move table_4 to it.\n Create directory on new device for ClickHouse data. /in shell mkdir /mnt/disk_1/clickhouse Change ownership of created directory to ClickHouse user. /in shell chown -R clickhouse:clickhouse /mnt/disk_1/clickhouse Create a special storage policy which should include both disks: old and new. /in shell  nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdefault\u003e \u003cdisk\u003edefault\u003c/disk\u003e \u003cmax_data_part_size_bytes\u003e10000000\u003c/max_data_part_size_bytes\u003e \u003c/default\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e #########################################################################################  Update storage_policy setting of tables to new policy.  ALTERTABLEtable_4MODIFYSETTINGstorage_policy='move_from_default_to_disk_1'; Wait till all parts of tables change their disk_name to new disk.  SELECTname,disk_name,pathfromsystem.partsWHEREtable='table_4'andactive;SELECTdisk_name,path,sum(rows),sum(bytes_on_disk),uniq(partition),count()FROMsystem.partsWHEREtable='table_4'andactiveGROUPBYdisk_name,pathORDERBYdisk_name,path; Remove ‘default’ disk from new storage policy. In server shell:  nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### \u003cyandex\u003e \u003cstorage_configuration\u003e \u003cdisks\u003e \u003c!-- default disk is special, it always exists even if not explicitly configured here, but you can't change it's path here (you should use \u003cpath\u003e on top level config instead) --\u003e \u003cdefault\u003e \u003c!-- You can reserve some amount of free space on any disk (including default) by adding keep_free_space_bytes tag --\u003e \u003c/default\u003e \u003cdisk_1\u003e \u003c!-- disk name --\u003e \u003cpath\u003e/mnt/disk_1/clickhouse/\u003c/path\u003e \u003c/disk_1\u003e \u003c/disks\u003e \u003cpolicies\u003e \u003cmove_from_default_to_disk_1\u003e \u003c!-- name for new storage policy --\u003e \u003cvolumes\u003e \u003cdisk_1_vol\u003e \u003c!-- name of volume --\u003e \u003c!-- we have only one disk in that volume and we reference here the name of disk as configured above in \u003cdisks\u003e section --\u003e \u003cdisk\u003edisk_1\u003c/disk\u003e \u003c/disk_1_vol\u003e \u003c/volumes\u003e \u003cmove_factor\u003e0.99\u003c/move_factor\u003e \u003c/move_from_default_to_disk_1\u003e \u003c/policies\u003e \u003c/storage_configuration\u003e \u003c/yandex\u003e ######################################################################################### ClickHouse wouldn’t auto reload config, because we removed some disks from storage policy, so we need to restart it by hand.\n Restart ClickHouse server. Make sure that storage policy uses the right disks.  SELECT*FROMsystem.storage_policiesWHEREpolicy_name='move_from_default_to_disk_1';","categories":"","description":"Moving table to another device.\n","excerpt":"Moving table to another device.\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-moving-table-to-another-device./","tags":"","title":"Moving table to another device."},{"body":"Q. How to know if ALTER TABLE … DELETE/UPDATE mutation ON CLUSTER was finished successfully on all the nodes?\nA. mutation status in system.mutations is local to each replica, so use\nSELECThostname(),*FROMclusterAllReplicas('your_cluster_name',system.mutations);-- you can also add WHERE conditions to that query if needed. Look on is_done and latest_fail_reason columns\n","categories":"","description":"ALTER UPDATE / DELETE\n","excerpt":"ALTER UPDATE / DELETE\n","ref":"/altinity-kb-queries-and-syntax/mutations/","tags":"","title":"Mutations"},{"body":"Authorization MySQL8 used default authorization plugin caching_sha2_password. Unfortunately, libmysql which currently used (21.4-) in clickhouse is not.\nYou can fix it during create custom user with mysql_native_password authentication plugin.\nCREATEUSERIFNOTEXISTS'clickhouse'@'%'IDENTIFIEDWITHmysql_native_passwordBY'clickhouse_user_password';CREATEDATABASEIFNOTEXISTStest;GRANTALLPRIVILEGESONtest.*TO'clickhouse'@'%';Table schema changes As an example, in ClickHouse, run SHOW TABLE STATUS LIKE 'table_name' and try to figure out was table schema changed or not from MySQL response field Update_time.\nBy default, to properly data loading from MySQL8 source to dictionaries, please turn off the information_schema cache.\nYou can change default behavior with create /etc/mysql/conf.d/information_schema_cache.cnfwith following content:\n[mysqld] information_schema_stats_expiry=0 Or setup it via SQL query:\nSETGLOBALinformation_schema_stats_expiry=0;","categories":"","description":"MySQL8 source for dictionaries\n","excerpt":"MySQL8 source for dictionaries\n","ref":"/altinity-kb-dictionaries/mysql8-source-for-dictionaries/","tags":"","title":"MySQL8 source for dictionaries"},{"body":"Networking And Server Room Planning The network used for your ClickHouse cluster should be a fast network, ideally 10 Gbit or more. ClickHouse nodes generate a lot of traffic to exchange the data between nodes (port 9009 for replication, and 9000 for distributed queries). Zookeeper traffic in normal circumstanses is moderate, but in some special cases can also be very significant.\nFor the zookeeper low latency is more important than bandwidth.\nKeep the replicas isolated on the hardware level. This allows for cluster failover from possible outages.\n For Physical Environments: Avoid placing 2 ClickHouse replicas on the same server rack. Ideally, they should be on isolated network switches and an isolated power supply. For Clouds Environments: Use different availability zones between the ClickHouse replicas when possible (but be aware of the interzone traffic costs)  These considerations are the same as the Zookeeper nodes.\nFor example:\n   Rack Server Server Server Server     Rack 1 CH_SHARD1_R1 CH_SHARD2_R1 CH_SHARD3_R1 ZOO_1   Rack 2 CH_SHARD1_R2 CH_SHARD2_R2 CH_SHARD3_R2 ZOO_2   Rack 3 ZOO3       Network Ports And Firewall ClickHouse listens the following ports:\n 9000: clickhouse-client, native clients, other clickhouse-servers connect to here. 8123: HTTP clients 9009: Other replicas will connect here to download data.  For more information, see CLICKHOUSE NETWORKING, PART 1.\nZookeeper listens the following ports:\n 2181: Client connections. 2888: Inter-ensemble connections. 3888: Leader election.  Outbound traffic from ClickHouse connects to the following ports:\n ZooKeeper: On port 2181. Other CH nodes in the cluster: On port 9000 and 9009. Dictionary sources: Depending on what was configured such as HTTP, MySQL, Mongo, etc. Kafka or Hadoop: If those integrations were enabled.  SSL For non-trusted networks enable SSL/HTTPS. If acceptable, it is better to keep interserver communications unencrypted for performance reasons.\nNaming Schema The best time to start creating a naming schema for the servers is before they’re created and configured.\nThere are a few features based on good server naming in ClickHouse:\n clickhouse-client prompts: Allows a different prompt for clickhouse-client per server hostname. Nearest hostname load balancing: For more information, see Nearest Hostname.  A good option is to use the following:\n{datacenter}-{serverroom}-{rack identifier}-{clickhouse cluster identifier}-{shard number or server number}.\nOther examples:\n rxv-olap-ch-master-sh01-r01:  rxv - location (rack#15) olap - product name ch = clickhouse master = stage sh01 = shard 1 r01 = replica 1   hetnzerde1-ch-prod-01.local:  hetnzerde1 - location (also replica id) ch = clickhouse prod = stage 01 - server number / shard number in that DC   sh01.ch-front.dev.aws-east1a.example.com:  sh01 - shard 01 ch-front - cluster name dev = stage aws = cloud provider east1a = region and availability zone    Host Name References  What are the best practices for domain names (dev, staging, production)? 9 Best Practices and Examples for Working with Kubernetes Labels Thoughts On Hostname Nomenclature  Additional Hostname Tips  Hostnames configured on the server should not change. If you do need to change the host name, one reference to use is How to Change Hostname on Ubuntu 18.04. The server should be accessible to other servers in the cluster via it’s hostname. Otherwise you will need to configure interserver_hostname in your config. Ensure that hostname --fqdn and getent hosts $(hostname --fqdn) return the correct name and ip.  ","categories":"","description":"Network Configuration\n","excerpt":"Network Configuration\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/","tags":"","title":"Network Configuration"},{"body":" It is NOT RECOMMENDED for a general use Use on your own risk Use latest ClickHouse version if you need that.  CREATETABLEx(`a`Nullable(UInt32),`b`Nullable(UInt32),`cnt`UInt32)ENGINE=SummingMergeTreeORDERBY(a,b)SETTINGSallow_nullable_key=1;INSERTINTOxVALUES(Null,2,1),(Null,Null,1),(3,Null,1),(4,4,1);INSERTINTOxVALUES(Null,2,1),(Null,Null,1),(3,Null,1),(4,4,1);SELECT*FROMx;┌────a─┬────b─┬─cnt─┐│3│null│2││4│4│2││null│2│2││null│null│2│└──────┴──────┴─────┘","categories":"","description":"Nulls in order by\n","excerpt":"Nulls in order by\n","ref":"/engines/mergetree-table-engine-family/altinity-kb-nulls-in-order-by/","tags":"","title":"Nulls in order by"},{"body":"Q: Why do I have several active parts in a partition? Why Clickhouse does not merge them immediately? A: CH does not merge parts by time Merge scheduler selects parts by own algorithm based on the current node workload / number of parts / size of parts.\nCH merge scheduler balances between a big number of parts and a wasting resources on merges.\nMerges are CPU/DISK IO expensive. If CH will merge every new part then all resources will be spend on merges and will no resources remain on queries (selects ).\nCH will not merge parts with a combined size greater than 100 GB.\nSELECT database, table, partition, sum(rows) AS rows, count() AS part_count FROM system.parts WHERE (active = 1) AND (table LIKE '%') AND (database LIKE '%') GROUP BY database, table, partition ORDER BY part_count DESC limit 20 ","categories":"","description":"Number of active parts in a partition\n","excerpt":"Number of active parts in a partition\n","ref":"/altinity-kb-useful-queries/altinity-kb-number-of-active-parts-in-a-partition/","tags":"","title":"Number of active parts in a partition"},{"body":"List of missing tables\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,table,arrayFilter(i-\u003eNOThas(groupArray(host),i),hosts)miss_tableFROM(SELECTFQDN()host,database,nametableFROMclusterAllReplicas({cluster},system,tables)WHEREengineNOTIN('Log','Memory','TinyLog'))GROUPBYdatabase,tableHAVINGmiss_table\u003c\u003e[]SETTINGSskip_unavailable_shards=1;┌─database─┬─table─┬─miss_table────────────────┐│default│test│['host366.mynetwork.net']│└──────────┴───────┴───────────────────────────┘List of inconsistent tables\nSELECTdatabase,name,engine,uniqExact(create_table_query)ASddlFROMclusterAllReplicas({cluster},system.tables)GROUPBYdatabase,name,engineHAVINGddl\u003e1List of inconsistent columns\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,table,column,arrayStringConcat(arrayMap(i-\u003ei.2||': '||i.1,(groupArray((type,host))ASg)),', ')diffFROM(SELECTFQDN()host,database,table,namecolumn,typeFROMclusterAllReplicas({cluster},system,columns))GROUPBYdatabase,table,columnHAVINGlength(arrayDistinct(g.1))\u003e1ORlength(g.1)\u003c\u003elength(hosts)SETTINGSskip_unavailable_shards=1;┌─database─┬─table───┬─column────┬─diff────────────────────────────────┐│default│z│A│ch-host22:Int64,ch-host21:String│└──────────┴─────────┴───────────┴─────────────────────────────────────┘List of inconsistent dictionaries\nWITH(SELECTgroupArray(FQDN())FROMclusterAllReplicas({cluster},system,one))AShostsSELECTdatabase,dictionary,arrayFilter(i-\u003eNOThas(groupArray(host),i),hosts)miss_dict,arrayReduce('median',(groupArray((element_count,host))ASec).1)FROM(SELECTFQDN()host,database,namedictionary,element_countFROMclusterAllReplicas({cluster},system,dictionaries))GROUPBYdatabase,dictionaryHAVINGmiss_dict\u003c\u003e[]SETTINGSskip_unavailable_shards=1;","categories":"","description":"Object consistency in a cluster\n","excerpt":"Object consistency in a cluster\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-object-consistency-in-a-cluster/","tags":"","title":"Object consistency in a cluster"},{"body":"OPTIMIZE TABLE xyz – this initiates an unscheduled merge.\nExample You have 40 parts in 3 partitions. This unscheduled merge selects some partition (i.e. February) and selects 3 small parts to merge, then merge them into a single part. You get 38 parts in the result.\nOPTIMIZE TABLE xyz FINAL – initiates a cycle of unscheduled merges.\nClickHouse merges parts in this table until will remains 1 part in each partition (if a system has enough free disk space). As a result, you get 3 parts, 1 part per partition. In this case, CH rewrites parts even if they are already merged into a single part. It creates a huge CPU / Disk load if the table ( XYZ) is huge. ClickHouse reads / uncompress / merge / compress / writes all data in the table.\nIf this table has size 1TB it could take around 3 hours to complete.\nSo we don’t recommend running OPTIMIZE TABLE xyz FINAL against tables with more than 10million rows.\n","categories":"","description":"OPTIMIZE vs OPTIMIZE FINAL\n","excerpt":"OPTIMIZE vs OPTIMIZE FINAL\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-optimize-vs-optimize-final/","tags":"","title":"OPTIMIZE vs OPTIMIZE FINAL"},{"body":"Custom settings allows to emulate parameterized views.\nYou need to enable custom settings and define any prefixes for settings.\n$ cat /etc/clickhouse-server/config.d/custom_settings_prefixes.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e  \u003ccustom_settings_prefixes\u003emy,my2\u003c/custom_settings_prefixes\u003e \u003c/yandex\u003e You can also set the default value for user settings in the default section of the user configuration.\ncat /etc/clickhouse-server/users.d/custom_settings_default.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e  \u003cprofiles\u003e  \u003cdefault\u003e  \u003cmy2_category\u003e'hot deals'\u003c/my2_category\u003e  \u003c/default\u003e  \u003c/profiles\u003e \u003c/yandex\u003e See also: https://kb.altinity.com/altinity-kb-setup-and-maintenance/custom_settings/\nA server restart is required for the default value to be applied\n$ systemctl restart clickhouse-server Now you can set settings as any other settings, and query them using getSetting() function.\nSETmy2_category='hot deals';SELECTgetSetting('my2_category');┌─getSetting('my2_category')─┐│hotdeals│└────────────────────────────┘-- you can query ClickHouse settings as well SELECTgetSetting('max_threads')┌─getSetting('max_threads')─┐│8│└───────────────────────────┘Now we can create a view\nCREATEVIEWmy_new_viewASSELECT*FROMdealsWHEREcategory_idIN(SELECTcategory_idFROMdeal_categoriesWHEREcategory=getSetting('my2_category'));And query it\nSELECT*FROMmy_new_viewSETTINGSmy2_category='hot deals';If the custom setting is not set when the view is being created, you need to explicitly define the list of columns for the view:\nCREATEVIEWmy_new_view(c1Int,c2String,...)ASSELECT*FROMdealsWHEREcategory_idIN(SELECTcategory_idFROMdeal_categoriesWHEREcategory=getSetting('my2_category'));","categories":"","description":"Parameterized views\n","excerpt":"Parameterized views\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-parameterized-views/","tags":"","title":"Parameterized views"},{"body":"Clickhouse is able to fetch from a source only updated rows. You need to define update_field section.\nAs an example, We have a table in an external source MySQL, PG, HTTP, … defined with the following code sample:\nCREATETABLEcities(`polygon`Array(Tuple(Float64,Float64)),`city`String,`updated_at`DateTimeDEFAULTnow())ENGINE=MergeTreeORDERBYcityWhen you add new row and update some rows in this table you should update updated_at with the new timestamp.\n-- fetch updated rows every 30 seconds CREATEDICTIONARYcities_dict(polygonArray(Tuple(Float64,Float64)),cityString)PRIMARYKEYpolygonSOURCE(CLICKHOUSE(TABLEcitiesDB'default'update_field'updated_at'))LAYOUT(POLYGON())LIFETIME(MIN30MAX30)A dictionary with update_field updated_at will fetch only updated rows. A dictionary saves the current time (now) time of the last successful update and queries the source where updated_at \u003e= previous_update - 1 (shift = 1 sec.).\nIn case of HTTP source Clickhouse will send get requests with update_field as an URL parameter \u0026updated_at=2020-01-01%2000:01:01\n","categories":"","description":"Partial updates\n","excerpt":"Partial updates\n","ref":"/altinity-kb-dictionaries/partial-updates/","tags":"","title":"Partial updates"},{"body":"CREATETABLEdefault.metric(`key_a`UInt8,`key_b`UInt32,`date`Date,`value`UInt32,PROJECTIONmonthly(SELECTkey_a,key_b,min(date),sum(value)GROUPBYkey_a,key_b))ENGINE=MergeTreePARTITIONBYtoYYYYMM(date)ORDERBY(key_a,key_b,date)SETTINGSindex_granularity=8192;INSERTINTOmetricSELECTkey_a,key_b,date,rand()%100000ASvalueFROM(SELECTarrayJoin(range(8))ASkey_a,number%500000ASkey_b,today()-intDiv(number,500000)ASdateFROMnumbers_mt(1080000000));OPTIMIZETABLEmetricFINAL;SETmax_threads=8;WITHtoDate('2015-02-27')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROMmetricWHERE(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condGROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:6.561sec.Processed4.32billionrows,47.54GB(658.70millionrows/s.,7.25GB/s.)WITHtoDate('2015-02-27')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROM(SELECTkey_b,valueFROMmetricWHEREindexHint(_partition_idINCAST([toYYYYMM(start_date),toYYYYMM(end_date)],'Array(String)'))AND(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condUNIONALLSELECTkey_b,sum(value)ASvalueFROMmetricWHEREindexHint(_partition_idINCAST(range(toYYYYMM(start_date)+1,toYYYYMM(end_date)),'Array(String)'))ANDkey_a_condGROUPBYkey_b)GROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:1.038sec.Processed181.86millionrows,4.56GB(175.18millionrows/s.,4.40GB/s.)WITH(toDate('2016-02-27'),toDate('2017-02-15'))ASperiod_1,(toDate('2018-05-27'),toDate('2022-08-15'))ASperiod_2,(date\u003e(period_1.1))AND(date\u003c(period_1.2))ASperiod_1_cond,(date\u003e(period_2.1))AND(date\u003c(period_2.2))ASperiod_2_cond,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sumIf(value,period_1_cond)ASsum_per_1,sumIf(value,period_2_cond)ASsum_per_2FROMmetricWHERE(period_1_condORperiod_2_cond)ANDkey_a_condGROUPBYkey_bORDERBYsum_per_2/sum_per_1DESCLIMIT2525rowsinset.Elapsed:5.717sec.Processed3.47billionrows,38.17GB(606.93millionrows/s.,6.68GB/s.)WITH(toDate('2016-02-27'),toDate('2017-02-15'))ASperiod_1,(toDate('2018-05-27'),toDate('2022-08-15'))ASperiod_2,(date\u003e(period_1.1))AND(date\u003c(period_1.2))ASperiod_1_cond,(date\u003e(period_2.1))AND(date\u003c(period_2.2))ASperiod_2_cond,CAST([toYYYYMM(period_1.1),toYYYYMM(period_1.2),toYYYYMM(period_2.1),toYYYYMM(period_2.2)],'Array(String)')ASdaily_parts,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sumIf(value,period_1_cond)ASsum_per_1,sumIf(value,period_2_cond)ASsum_per_2FROM(SELECTkey_b,date,valueFROMmetricWHEREindexHint(_partition_idIN(daily_parts))AND(period_1_condORperiod_2_cond)ANDkey_a_condUNIONALLSELECTkey_b,min(date)ASdate,sum(value)ASvalueFROMmetricWHEREindexHint(_partition_idINCAST(arrayConcat(range(toYYYYMM(period_1.1)+1,toYYYYMM(period_1.2)),range(toYYYYMM(period_2.1)+1,toYYYYMM(period_2.1))),'Array(String)'))ANDindexHint(_partition_idNOTIN(daily_parts))ANDkey_a_condGROUPBYkey_b)GROUPBYkey_bORDERBYsum_per_2/sum_per_1DESCLIMIT2525rowsinset.Elapsed:0.444sec.Processed140.34millionrows,2.11GB(316.23millionrows/s.,4.77GB/s.)WITHtoDate('2022-01-03')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROMmetricWHERE(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condGROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:0.208sec.Processed100.06millionrows,1.10GB(481.06millionrows/s.,5.29GB/s.)WITHtoDate('2022-01-03')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROM(SELECTkey_b,valueFROMmetricWHEREindexHint(_partition_idINCAST([toYYYYMM(start_date),toYYYYMM(end_date)],'Array(String)'))AND(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condUNIONALLSELECTkey_b,sum(value)ASvalueFROMmetricWHEREindexHint(_partition_idINCAST(range(toYYYYMM(start_date)+1,toYYYYMM(end_date)),'Array(String)'))ANDkey_a_condGROUPBYkey_b)GROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:0.216sec.Processed100.06millionrows,1.10GB(462.68millionrows/s.,5.09GB/s.)WITHtoDate('2021-12-03')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROMmetricWHERE(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condGROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:0.308sec.Processed162.09millionrows,1.78GB(526.89millionrows/s.,5.80GB/s.)WITHtoDate('2021-12-03')ASstart_date,toDate('2022-02-15')ASend_date,key_aIN(1,3,5,7)ASkey_a_condSELECTkey_b,sum(value)ASsumFROM(SELECTkey_b,valueFROMmetricWHEREindexHint(_partition_idINCAST([toYYYYMM(start_date),toYYYYMM(end_date)],'Array(String)'))AND(date\u003estart_date)AND(date\u003cend_date)ANDkey_a_condUNIONALLSELECTkey_b,sum(value)ASvalueFROMmetricWHEREindexHint(_partition_idINCAST(range(toYYYYMM(start_date)+1,toYYYYMM(end_date)),'Array(String)'))ANDkey_a_condGROUPBYkey_b)GROUPBYkey_bORDERBYsumDESCLIMIT2525rowsinset.Elapsed:0.268sec.Processed102.08millionrows,1.16GB(381.46millionrows/s.,4.33GB/s.)","categories":"","description":"How to write queries, which will use both data from projection and raw table.\n","excerpt":"How to write queries, which will use both data from projection and raw …","ref":"/altinity-kb-queries-and-syntax/partial-projection-optimization/","tags":"","title":"Use both projection and raw data in single query"},{"body":"Check if there are blocks missing SELECTdatabase,table,partition_id,ranges.1ASprevious_part,ranges.2ASnext_part,ranges.3ASprevious_block_number,ranges.4ASnext_block_number,range(toUInt64(previous_block_number+1),toUInt64(next_block_number))ASmissing_block_numbersFROM(WITHarrayPopFront(groupArray(min_block_number)ASmin)ASmin_adj,arrayPopBack(groupArray(max_block_number)ASmax)ASmax_adj,arrayFilter((x,y,z)-\u003e(y!=(z+1)),arrayZip(arrayPopBack(groupArray(name)ASname_arr),arrayPopFront(name_arr),max_adj,min_adj),min_adj,max_adj)ASmissing_rangesSELECTdatabase,table,partition_id,missing_rangesFROM(SELECT*FROMsystem.partsWHEREactiveAND(table='query_thread_log')AND(partition_id='202108')ANDactiveORDERBYmin_block_numberASC)GROUPBYdatabase,table,partition_id)ARRAYJOINmissing_rangesASranges┌─database─┬─table────────────┬─partition_id─┬─previous_part───────┬─next_part──────────┬─previous_block_number─┬─next_block_number─┬─missing_block_numbers─┐│system│query_thread_log│202108│202108_864_1637_556│202108_1639_1639_0│1637│1639│[1638]│└──────────┴──────────────────┴──────────────┴─────────────────────┴────────────────────┴───────────────────────┴───────────────────┴───────────────────────┘Find the number of blocks in a table SELECTdatabase,table,partition_id,sum(max_block_number-min_block_number)ASblocks_countFROMsystem.partsWHEREactiveAND(table='query_thread_log')AND(partition_id='202108')ANDactiveGROUPBYdatabase,table,partition_id┌─database─┬─table────────────┬─partition_id─┬─blocks_count─┐│system│query_thread_log│202108│1635│└──────────┴──────────────────┴──────────────┴──────────────┘Compare the list of parts in ZooKeeper with the list of parts on disk selectzoo.p_pathaspart_zoo,zoo.ctime,zoo.mtime,disk.p_pathaspart_diskfrom(selectconcat(path,'/',name)asp_path,ctime,mtimefromsystem.zookeeperwherepathin(selectconcat(replica_path,'/parts')fromsystem.replicas))zooleftjoin(selectconcat(replica_path,'/parts/',name)asp_pathfromsystem.partsinnerjoinsystem.replicasusing(database,table))diskonzoo.p_path=disk.p_pathwherepart_disk=''orderbypart_zoo;","categories":"","description":"","excerpt":"Check if there are blocks missing …","ref":"/altinity-kb-useful-queries/parts-consistency/","tags":"","title":"Parts consistency"},{"body":"PIVOT CREATETABLEsales(suppkeyUInt8,categoryString,quantityUInt32)ENGINE=Memory();INSERTINTOsalesVALUES(2,'AA',7500),(1,'AB',4000),(1,'AA',6900),(1,'AB',8900),(1,'AC',8300),(1,'AA',7000),(1,'AC',9000),(2,'AA',9800),(2,'AB',9600),(1,'AC',8900),(1,'AD',400),(2,'AD',900),(2,'AD',1200),(1,'AD',2600),(2,'AC',9600),(1,'AC',6200);Using Map data type (starting from Clickhouse 21.1) WITHCAST(sumMap([category],[quantity]),'Map(String, UInt32)')ASmapSELECTsuppkey,map['AA']ASAA,map['AB']ASAB,map['AC']ASAC,map['AD']ASADFROMsalesGROUPBYsuppkeyORDERBYsuppkeyASC┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐│1│13900│12900│32400│3000││2│17300│9600│9600│2100│└─────────┴───────┴───────┴───────┴──────┘Using -If combinator SELECTsuppkey,sumIf(quantity,category='AA')ASAA,sumIf(quantity,category='AB')ASAB,sumIf(quantity,category='AC')ASAC,sumIf(quantity,category='AD')ASADFROMsalesGROUPBYsuppkeyORDERBYsuppkeyASC┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐│1│13900│12900│32400│3000││2│17300│9600│9600│2100│└─────────┴───────┴───────┴───────┴──────┘Using -Resample combinator WITHsumResample(0,4,1)(quantity,transform(category,['AA','AB','AC','AD'],[0,1,2,3],4))ASsumSELECTsuppkey,sum[1]ASAA,sum[2]ASAB,sum[3]ASAC,sum[4]ASADFROMsalesGROUPBYsuppkeyORDERBYsuppkeyASC┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐│1│13900│12900│32400│3000││2│17300│9600│9600│2100│└─────────┴───────┴───────┴───────┴──────┘UNPIVOT CREATETABLEsales_w(suppkeyUInt8,brandString,AAUInt32,ABUInt32,ACUInt32,ADUInt32)ENGINE=Memory();INSERTINTOsales_wVALUES(1,'BRAND_A',1500,4200,1600,9800),(2,'BRAND_B',6200,1300,5800,3100),(3,'BRAND_C',5000,8900,6900,3400);SELECTsuppkey,brand,category,quantityFROMsales_wARRAYJOIN[AA,AB,AC,AD]ASquantity,splitByString(', ','AA, AB, AC, AD')AScategoryORDERBYsuppkeyASC┌─suppkey─┬─brand───┬─category─┬─quantity─┐│1│BRAND_A│AA│1500││1│BRAND_A│AB│4200││1│BRAND_A│AC│1600││1│BRAND_A│AD│9800││2│BRAND_B│AA│6200││2│BRAND_B│AB│1300││2│BRAND_B│AC│5800││2│BRAND_B│AD│3100││3│BRAND_C│AA│5000││3│BRAND_C│AB│8900││3│BRAND_C│AC│6900││3│BRAND_C│AD│3400│└─────────┴─────────┴──────────┴──────────┘Using tupleToNameValuePairs (starting from ClickHouse 21.9) SELECTsuppkey,brand,tpl.1AScategory,tpl.2ASquantityFROMsales_wARRAYJOINtupleToNameValuePairs((AA,AB,AC,AD))AStplORDERBYsuppkeyASC┌─suppkey─┬─brand───┬─category─┬─quantity─┐│1│BRAND_A│AA│1500││1│BRAND_A│AB│4200││1│BRAND_A│AC│1600││1│BRAND_A│AD│9800││2│BRAND_B│AA│6200││2│BRAND_B│AB│1300││2│BRAND_B│AC│5800││2│BRAND_B│AD│3100││3│BRAND_C│AA│5000││3│BRAND_C│AB│8900││3│BRAND_C│AC│6900││3│BRAND_C│AD│3400│└─────────┴─────────┴──────────┴──────────┘","categories":"","description":"PIVOT / UNPIVOT\n","excerpt":"PIVOT / UNPIVOT\n","ref":"/altinity-kb-queries-and-syntax/pivot-unpivot/","tags":"","title":"PIVOT / UNPIVOT"},{"body":"In version 19.14 a serious issue was found: a race condition that can lead to server deadlock. The reason for that was quite fundamental, and a temporary workaround for that was added (“possible deadlock avoided”).\nThose locks are one of the fundamental things that the core team was actively working on in 2020.\nIn 20.3 some of the locks leading to that situation were removed as a part of huge refactoring.\nIn 20.4 more locks were removed, the check was made configurable (see lock_acquire_timeout ) so you can say how long to wait before returning that exception\nIn 20.5 heuristics of that check (“possible deadlock avoided”) was improved.\nIn 20.6 all table-level locks which were possible to remove were removed, so alters are totally lock-free.\n20.10 enables database=Atomic by default which allows running even DROP commands without locks.\nTypically issue was happening when doing some concurrent select on system.parts / system.columns / system.table with simultaneous table manipulations (doing some kind of ALTERS / TRUNCATES / DROP)I\nIf that exception happens often in your use-case: An update is recommended. In the meantime, check which queries are running (especially to system.tables / system.parts and other system tables) and check if killing them / avoiding them helps to solve the issue.\n","categories":"","description":"Possible deadlock avoided. Client should retry\n","excerpt":"Possible deadlock avoided. Client should retry\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-possible-deadlock-avoided.-client-should-retry/","tags":"","title":"Possible deadlock avoided. Client should retry"},{"body":"The biggest problem with running ClickHouse in k8s, happens when clickhouse-server can’t start for some reason and pod is falling in CrashloopBackOff, so you can’t easily get in the pod and check/fix/restart ClickHouse.\nThere is multiple possible reasons for this, some of them can be fixed without manual intervention in pod:\n Wrong configuration files Fix: Check templates which are being used for config file generation and fix them. While upgrade some backward incompatible changes prevents ClickHouse from start. Fix: Downgrade and check backward incompatible changes for all versions in between.  Next reasons would require to have manual intervention in pod/volume. There is two ways, how you can get access to data:\n Change entry point of ClickHouse pod to something else, so pod wouldn’t be terminated due ClickHouse error. Attach ClickHouse data volume to some generic pod (like Ubuntu). Unclear restart which produced broken files and/or state on disk is differs too much from state in zookeeper for replicated tables. Fix: Create force_restore_data flag. Wrong file permission for ClickHouse files in pod. Fix: Use chown to set right ownership for files and directories. Errors in ClickHouse table schema prevents ClickHouse from start. Fix: Rename problematic table.sql scripts to table.sql.bak Occasional failure of distributed queries because of wrong user/password. Due nature of k8s with dynamic ip allocations, it’s possible that ClickHouse would cache wrong ip-\u003e hostname combination and disallow connections because of mismatched hostname. Fix: run SYSTEM DROP DNS CACHE; \u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml.  Caveats:\n Not all configuration/state folders are being covered by persistent volumes. (geobases) Page cache belongs to k8s node and pv are being mounted to pod, in case of fast shutdown there is possibility to loss some data(needs to be clarified) Some cloud providers (GKE) can have slow unlink command, which is important for clickhouse because it’s needed for parts management. (max_part_removal_threads setting)  Useful commands:\nkubectl logs chi-chcluster-2-1-0 -c clickhouse-pod -n chcluster --previous kubectl describe pod chi-chcluster-2-1-0 -n chcluster Q. Clickhouse is caching the Kafka pod’s IP and trying to connect to the same ip even when there is a new Kafka pod running and the old one is deprecated. Is there some setting where we could refresh the connection\n\u003cdisable_internal_dns_cache\u003e1\u003c/disable_internal_dns_cache\u003e in config.xml\nClickHouse init process failed It’s due to low value for env CLICKHOUSE_INIT_TIMEOUT value. Consider increasing it up to 1 min. https://github.com/ClickHouse/ClickHouse/blob/9f5cd35a6963cc556a51218b46b0754dcac7306a/docker/server/entrypoint.sh#L120\n","categories":"","description":"Possible issues with running ClickHouse in k8s\n","excerpt":"Possible issues with running ClickHouse in k8s\n","ref":"/altinity-kb-kubernetes/altinity-kb-possible-issues-with-running-clickhouse-in-k8s/","tags":"","title":"Possible issues with running ClickHouse in k8s"},{"body":"Moving from a single ClickHouse server to a clustered format provides several benefits:\n Replication guarantees data integrity. Provides redundancy. Failover by being able to restart half of the nodes without encountering downtime.  Moving from an unsharded ClickHouse environment to a sharded cluster requires redesign of schema and queries. Starting with a sharded cluster from the beginning makes it easier in the future to scale the cluster up.\nSetting up a ClickHouse cluster for a production environment requires the following stages:\n Hardware Requirements Network Configuration Create Host Names Monitoring Considerations Configuration Steps Setting Up Backups Staging Plans Upgrading The Cluster  ","categories":"","description":"Production Cluster Configuration Guide\n","excerpt":"Production Cluster Configuration Guide\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/","tags":"","title":"Production Cluster Configuration Guide"},{"body":"Aggregating projections createtablez(BrowserString,CountryUInt8,FFloat64)Engine=MergeTreeorderbyBrowser;insertintozselecttoString(number%9999),number%33,1fromnumbers(100000000);--Q1) selectsum(F),BrowserfromzgroupbyBrowserformatNull;Elapsed:0.205sec.Processed100.00millionrows--Q2) selectsum(F),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.381sec.Processed100.00millionrows--Q3) selectsum(F),count(),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.398sec.Processed100.00millionrowsaltertablezaddprojectionpp(selectBrowser,Country,count(),sum(F)groupbyBrowser,Country);altertablezmaterializeprojectionpp;---- 0 = don't use proj, 1 = use projection setallow_experimental_projection_optimization=1;--Q1) selectsum(F),BrowserfromzgroupbyBrowserformatNull;Elapsed:0.003sec.Processed22.43thousandrows--Q2) selectsum(F),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.004sec.Processed22.43thousandrows--Q3) selectsum(F),count(),Browser,CountryfromzgroupbyBrowser,CountryformatNull;Elapsed:0.005sec.Processed22.43thousandrowsSee also  Amos Bird - kuaishou.com - Projections in ClickHouse. slides. video Documentation tinybird blog article  ","categories":"","description":"Projections examples\n","excerpt":"Projections examples\n","ref":"/altinity-kb-queries-and-syntax/projections-examples/","tags":"","title":"Projections examples"},{"body":"Main docs article https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/\nHardware requirements TLDR version:\n USE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup). use 3 nodes (more nodes = slower quorum, less = no HA). low network latency between zookeeper nodes is very important (latency, not bandwidth). have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings. ensure that zookeeper will not be CPU-starved by some other processes monitor zookeeper.  Side note: in many cases, the slowness of the zookeeper is actually a symptom of some issue with clickhouse schema/usage pattern (the most typical issues: an enormous number of partitions/tables/databases with real-time inserts, tiny \u0026 frequent inserts).\nSome doc about that subject:\n https://docs.confluent.io/platform/current/zookeeper/deployment.html https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.html#sc_commonProblems https://clickhouse.tech/docs/en/operations/tips/#zookeeper https://lucene.apache.org/solr/guide/7_4/setting-up-an-external-zookeeper-ensemble.html https://cwiki.apache.org/confluence/display/ZOOKEEPER/Troubleshooting  Cite from https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_commonProblems :\n Things to Avoid Here are some common problems you can avoid by configuring ZooKeeper correctly:\n inconsistent lists of servers : The list of ZooKeeper servers used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has. Things work okay if the client list is a subset of the real list, but things will really act strange if clients have a list of ZooKeeper servers that are in different ZooKeeper clusters. Also, the server lists in each Zookeeper server configuration file should be consistent with one another. incorrect placement of transaction log : The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log. incorrect Java heap size : You should take special care to set your Java max heap size correctly. In particular, you should not create a situation in which ZooKeeper swaps to disk. The disk is death to ZooKeeper. Everything is ordered, so if processing one request swaps the disk, all other queued requests will probably do the same. the disk. DON’T SWAP. Be conservative in your estimates: if you have 4G of RAM, do not set the Java max heap size to 6G or even 4G. For example, it is more likely you would use a 3G heap for a 4G machine, as the operating system and the cache also need memory. The best and only recommend practice for estimating the heap size your system needs is to run load tests, and then make sure you are well below the usage limit that would cause the system to swap. Publicly accessible deployment : A ZooKeeper ensemble is expected to operate in a trusted computing environment. It is thus recommended to deploy ZooKeeper behind a firewall.   ","categories":"","description":"Proper setup\n","excerpt":"Proper setup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/","tags":"","title":"Proper setup"},{"body":"The following example shows a range_hashed example at open intervals.\nDROPTABLEIFEXISTSrates;DROPDICTIONARYIFEXISTSrates_dict;CREATETABLErates(idUInt64,date_startNullable(Date),date_endNullable(Date),rateDecimal64(4))engine=Log;INSERTINTOratesVALUES(1,Null,'2021-03-13',99),(1,'2021-03-14','2021-03-16',100),(1,'2021-03-17',Null,101),(2,'2021-03-14',Null,200),(3,Null,'2021-03-14',300),(4,'2021-03-14','2021-03-14',400);CREATEDICTIONARYrates_dict(idUInt64,date_startDate,date_endDate,rateDecimal64(4))PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000USER'default'TABLE'rates'))LIFETIME(MIN1MAX1000)LAYOUT(RANGE_HASHED())RANGE(MINdate_startMAXdate_end);SELECT*FROMrates_dictorderbyid,date_start;┌─id─┬─date_start─┬───date_end─┬─────rate─┐│1│1970-01-01│2021-03-13│99.0000││1│2021-03-14│2021-03-16│100.0000││1│2021-03-17│1970-01-01│101.0000││2│2021-03-14│1970-01-01│200.0000││3│1970-01-01│2021-03-14│300.0000││4│2021-03-14│2021-03-14│400.0000│└────┴────────────┴────────────┴──────────┘WITHtoDate('2021-03-10')+INTERVALnumberDAYasdateselectdate,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(1),date)asrate1,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(2),date)asrate2,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(3),date)asrate3,dictGet(currentDatabase()||'.rates_dict','rate',toUInt64(4),date)asrate4FROMnumbers(10);┌───────date─┬────rate1─┬────rate2─┬────rate3─┬────rate4─┐│2021-03-10│99.0000│0.0000│300.0000│0.0000││2021-03-11│99.0000│0.0000│300.0000│0.0000││2021-03-12│99.0000│0.0000│300.0000│0.0000││2021-03-13│99.0000│0.0000│300.0000│0.0000││2021-03-14│100.0000│200.0000│300.0000│400.0000││2021-03-15│100.0000│200.0000│0.0000│0.0000││2021-03-16│100.0000│200.0000│0.0000│0.0000││2021-03-17│101.0000│200.0000│0.0000│0.0000││2021-03-18│101.0000│200.0000│0.0000│0.0000││2021-03-19│101.0000│200.0000│0.0000│0.0000│└────────────┴──────────┴──────────┴──────────┴──────────┘","categories":"","description":"range_hashed example - open intervals\n","excerpt":"range_hashed example - open intervals\n","ref":"/altinity-kb-dictionaries/altinity-kb-range_hashed-example-open-intervals/","tags":"","title":"range_hashed example - open intervals"},{"body":"Problem Every ClickHouse user experienced a loss of ZooKeeper one day. While the data is available and replicas respond to queries, inserts are no longer possible. ClickHouse uses ZooKeeper in order to store the reference version of the table structure and part of data, and when it is not available can not guarantee data consistency anymore. Replicated tables turn to the read-only mode. In this article we describe step-by-step instructions of how to restore ZooKeeper metadata and bring ClickHouse cluster back to normal operation.\nIn order to restore ZooKeeper we have to solve two tasks. First, we need to restore table metadata in ZooKeeper. Currently, the only way to do it is to recreate the table with the CREATE TABLE DDL statement.\nCREATETABLEtable_name...ENGINE=ReplicatedMergeTree('zookeeper_path','replica_name');The second and more difficult task is to populate zookeeper with information of clickhouse data parts. As mentioned above, ClickHouse stores the reference data about all parts of replicated tables in ZooKeeper, so we have to traverse all partitions and re-attach them to the recovered replicated table in order to fix that.\nInfo Starting from ClickHouse version 21.7 there is SYSTEM RESTORE REPLICA command  https://altinity.com/blog/a-new-way-to-restore-clickhouse-after-zookeeper-metadata-is-lost\nTest case Let’s say we have replicated table table_repl.\nCREATETABLEtable_repl(`number`UInt32)ENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl','{replica}')PARTITIONBYintDiv(number,1000)ORDERBYnumber;And populate it with some data\nSELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/cluster_1/tables/01/';INSERTINTOtable_replSELECT*FROMnumbers(1000,2000);SELECTpartition,sum(rows)ASrows,count()FROMsystem.partsWHEREtable='table_repl'ANDactiveGROUPBYpartition;Now let’s remove metadata in zookeeper using ZkCli.sh at ZooKeeper host:\ndeleteall /clickhouse/cluster_1/tables/01/table_repl And try to resync clickhouse replica state with zookeeper:\nSYSTEMRESTARTREPLICAtable_repl;If we try to insert some data in the table, error happens:\nINSERTINTOtable_replSELECTnumberASnumberFROMnumbers(1000,2000)WHEREnumber%2=0;And now we have an exception that we lost all metadata in zookeeper. It is time to recover!\nCurrent Solution   Detach replicated table.\nDETACHTABLEtable_repl;  Save the table’s attach script and change engine of replicated table to non-replicated *mergetree analogue. Table definition is located in the ‘metadata’ folder, ‘/var/lib/clickhouse/metadata/default/table_repl.sql’ in our example. Please make a backup copy and modify the file as follows:\nATTACHTABLEtable_repl(`number`UInt32)ENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl','{replica}')PARTITIONBYintDiv(number,1000)ORDERBYnumberSETTINGSindex_granularity=8192Needs to be replaced with this:\nATTACHTABLEtable_repl(`number`UInt32)ENGINE=MergeTree()PARTITIONBYintDiv(number,1000)ORDERBYnumberSETTINGSindex_granularity=8192  Attach non-replicated table.\nATTACHTABLEtable_repl;  Rename non-replicated table.\nRENAMETABLEtable_replTOtable_repl_old;  Create a new replicated table. Take the saved attach script and replace ATTACH with CREATE, and run it.\nCREATETABLEtable_repl(`number`UInt32)ENGINE=ReplicatedMergeTree('/clickhouse/{cluster}/tables/{shard}/table_repl','{replica}')PARTITIONBYintDiv(number,1000)ORDERBYnumberSETTINGSindex_granularity=8192  Attach parts from old table to new.\nALTERTABLEtable_replATTACHPARTITION1FROMtable_repl_old;ALTERTABLEtable_replATTACHPARTITION2FROMtable_repl_old;  If the table has many partitions, it may require some shell script to make it easier.\nAutomated approach For a large number of tables, you can use script https://github.com/Altinity/clickhouse-zookeeper-recovery which partially automates the above approach.\n","categories":"","description":"Recovering from complete metadata loss in ZooKeeper\n","excerpt":"Recovering from complete metadata loss in ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-recovering-from-complete-metadata-loss-in-zookeeper/","tags":"","title":"Recovering from complete metadata loss in ZooKeeper"},{"body":"remote(…) table function Suitable for moving up to hundreds of gigabytes of data.\nWith bigger tables recommended approach is to slice the original data by some WHERE condition, ideally - apply the condition on partitioning key, to avoid writing data to many partitions at once.\nINSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-13';INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-12';INSERTINTOstaging_tableSELECT*FROMremote(...)WHEREdate='2021-04-11';....Q. Can it create a bigger load on the source system? Yes, it may use disk read \u0026 network write bandwidth. But typically write speed is worse than the read speed, so most probably the receiver side will be a bottleneck, and the sender side will not be overloaded.\nWhile of course it should be checked, every case is different.\nQ. Can I tune INSERT speed to make it faster? Yes, by the cost of extra memory usage (on the receiver side).\nClickhouse tries to form blocks of data in memory and while one of limit: min_insert_block_size_rows or min_insert_block_size_bytes being hit, clickhouse dump this block on disk. If clickhouse tries to execute insert in parallel (max_insert_threads \u003e 1), it would form multiple blocks at one time.\nSo maximum memory usage can be calculated like this: max_insert_threads * first(min_insert_block_size_rows OR min_insert_block_size_bytes)\nDefault values:\n┌─name────────────────────────┬─value─────┐│min_insert_block_size_rows│1048545││min_insert_block_size_bytes│268427520││max_insert_threads│0│\u003c-Values0or1meansthatINSERTSELECTisnotruninparallel.└─────────────────────────────┴───────────┘Tune those settings depending on your table average row size and amount of memory which are safe to occupy by INSERT SELECT query.\nQ. I’ve got the error “All connection tries failed” SELECTcount()FROMremote('server.from.remote.dc:9440','default.table','admin','password')Receivedexceptionfromserver(version20.8.11):Code:519.DB::Exception:Receivedfromlocalhost:9000.DB::Exception:Allattemptstogettablestructurefailed.Log:Code:279,e.displayText()=DB::NetException:Allconnectiontriesfailed.Log:Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild))Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild))Code:209,e.displayText()=DB::NetException:Timeout:connecttimedout:192.0.2.1:9440(server.from.remote.dc:9440)(version20.8.11.17(officialbuild)) Using remote(…) table function with secure TCP port (default values is 9440). There is remoteSecure() function for that. High (\u003e50ms) ping between servers, values for connect_timeout_with_failover_ms, connect_timeout_with_failover_secure_ms need’s to be adjusted accordingly.  Default values:\n┌─name────────────────────────────────────┬─value─┐│connect_timeout_with_failover_ms│50││connect_timeout_with_failover_secure_ms│100│└─────────────────────────────────────────┴───────┘","categories":"","description":"Remote table function\n","excerpt":"Remote table function\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-table-function/","tags":"","title":"Remote table function"},{"body":"Removing of empty parts is a new feature introduced in 20.12. Earlier versions leave empty parts (with 0 rows) if TTL removes all rows from a part (https://github.com/ClickHouse/ClickHouse/issues/5491). If you set up TTL for your data it is likely that there are quite many empty parts in your system.\nThe new version notices empty parts and tries to remove all of them immediately. This is a one-time operation which runs right after an upgrade. After that TTL will remove empty parts on its own.\nThere is a problem when different replicas of the same table start to remove empty parts at the same time. Because of the bug they can block each other (https://github.com/ClickHouse/ClickHouse/issues/23292).\nWhat we can do to avoid this problem during an upgrade:\n  Drop empty partitions before upgrading to decrease the number of empty parts in the system.\nSELECTconcat('alter table ',database,'.',table,' drop partition id ''',partition_id,''';')FROMsystem.partsWHEREactiveGROUPBYdatabase,table,partition_idHAVINGcount()=countIf(rows=0)  Upgrade/restart one replica (in a shard) at a time. If only one replica is cleaning empty parts there will be no deadlock because of replicas waiting for one another. Restart one replica, wait for replication queue to process, then restart the next one.\n  Removing of empty parts can be disabled by adding remove_empty_parts=0 to the default profile.\n$ cat /etc/clickhouse-server/users.d/remove_empty_parts.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cremove_empty_parts\u003e0\u003c/remove_empty_parts\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e ","categories":"","description":"Removing empty parts\n","excerpt":"Removing empty parts\n","ref":"/upgrade/removing-empty-parts/","tags":"","title":"Removing empty parts"},{"body":"There might be parts left in ZooKeeper that don’t exist on disk The explanation is here https://github.com/ClickHouse/ClickHouse/pull/26716\nThe problem is introduced in 20.1.\nThe problem is fixed in 21.8 and backported to 21.3.16, 21.6.9, 21.7.6.\nRegarding the procedure to reproduce the issue: The procedure was not confirmed, but I think it should work.\n  Wait for a merge on a particular partition (or run an OPTIMIZE to trigger one) At this point you can collect the names of parts participating in the merge from the system.merges table, or the system.parts table.\n  When the merge finishes, stop one of the replicas before the inactive parts are dropped (or detach the table).\n  Bring the replica back up (or attach the table). Check that there are no inactive parts in system.parts, but they stayed in ZooKeeper. Also check that the inactive parts got removed from ZooKeeper for another replica. Here is the query to check ZooKeeper:\n  select name, ctime from system.zookeeper where path='\u003ctable_zpath\u003e/replicas/\u003creplica_name\u003e/parts/' and name like '\u003cput an expression for the parts that were merged\u003e' Drop the partition on the replica that DOES NOT have those extra parts in ZooKeeper. Check the list of parts in ZooKeeper. We hope that after this the parts on disk will be removed on all replicas, but one of the replicas will still have some parts left in ZooKeeper. If this happens, then we think that after a restart of the replica with extra parts in ZooKeeper it will try to download them from another replica.  A query to find ‘forgotten’ parts https://kb.altinity.com/altinity-kb-useful-queries/parts-consistency/#compare-the-list-of-parts-in-zookeeper-with-the-list-of-parts-on-disk\nA query to drop empty partitions with failing replication tasks select'alter table '||database||'.'||table||' drop partition id '''||partition_id||''';'from(selectdatabase,table,splitByChar('_',new_part_name)[1]partition_idfromsystem.replication_queuewheretype='GET_PART'andnotis_currently_executingandcreate_time\u003ctoStartOfDay(yesterday())groupbydatabase,table,partition_id)qleftjoin(selectdatabase,table,partition_id,countIf(active)cnt_active,count()cnt_totalfromsystem.partsgroupbydatabase,table,partition_id)pusingdatabase,table,partition_idwherecnt_active=0","categories":"","description":"Removing lost parts\n","excerpt":"Removing lost parts\n","ref":"/upgrade/removing-lost-parts/","tags":"","title":"Removing lost parts"},{"body":"Last state CREATETABLErepl_tbl(`key`UInt32,`val_1`UInt32,`val_2`String,`val_3`String,`val_4`String,`val_5`UUID,`ts`DateTime)ENGINE=ReplacingMergeTree(ts)ORDERBYkeySYSTEMSTOPMERGESrepl_tbl;INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);INSERTINTOrepl_tblSELECTnumberaskey,rand()asval_1,randomStringUTF8(10)asval_2,randomStringUTF8(5)asval_3,randomStringUTF8(4)asval_4,generateUUIDv4()asval_5,now()astsFROMnumbers(10000000);SELECTcount()FROMrepl_tbl┌──count()─┐│50000000│└──────────┘Single key -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblWHEREkey=10GROUPBYkey;1rowsinset.Elapsed:0.017sec.Processed40.96thousandrows,5.24MB(2.44millionrows/s.,312.31MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblWHEREkey=10ORDERBYtsDESCLIMIT1BYkey;1rowsinset.Elapsed:0.017sec.Processed40.96thousandrows,5.24MB(2.39millionrows/s.,305.41MB/s.)-- Subquery SELECT*FROMrepl_tblWHEREkey=10ANDts=(SELECTmax(ts)FROMrepl_tblWHEREkey=10);1rowsinset.Elapsed:0.019sec.Processed40.96thousandrows,1.18MB(2.20millionrows/s.,63.47MB/s.)-- FINAL SELECT*FROMrepl_tblFINALWHEREkey=10;1rowsinset.Elapsed:0.021sec.Processed40.96thousandrows,5.24MB(1.93millionrows/s.,247.63MB/s.)Multiple keys -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)GROUPBYkeyFORMATNull;Peakmemoryusage(forquery):2.31GiB.0rowsinset.Elapsed:3.264sec.Processed5.04millionrows,645.01MB(1.54millionrows/s.,197.60MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):1.11GiB.0rowsinset.Elapsed:1.772sec.Processed2.74millionrows,350.30MB(1.54millionrows/s.,197.73MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)ORDERBYtsDESCLIMIT1BYkeyFORMATNull;Peakmemoryusage(forquery):1.08GiB.0rowsinset.Elapsed:2.429sec.Processed5.04millionrows,645.01MB(2.07millionrows/s.,265.58MB/s.)-- Subquery SELECT*FROMrepl_tblWHERE(key,ts)IN(SELECTkey,max(ts)FROMrepl_tblWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)GROUPBYkey)FORMATNull;Peakmemoryusage(forquery):432.57MiB.0rowsinset.Elapsed:0.939sec.Processed5.04millionrows,160.33MB(5.36millionrows/s.,170.69MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):202.88MiB.0rowsinset.Elapsed:0.824sec.Processed5.04millionrows,160.33MB(6.11millionrows/s.,194.58MB/s.)-- FINAL SELECT*FROMrepl_tblFINALWHEREkeyIN(SELECTtoUInt32(number)FROMnumbers(1000000)WHEREnumber%100)FORMATNull;Peakmemoryusage(forquery):198.32MiB.0rowsinset.Elapsed:1.211sec.Processed5.04millionrows,645.01MB(4.16millionrows/s.,532.57MB/s.)Full table -- GROUP BY SELECTkey,argMax(val_1,ts)asval_1,argMax(val_2,ts)asval_2,argMax(val_3,ts)asval_3,argMax(val_4,ts)asval_4,argMax(val_5,ts)asval_5,max(ts)FROMrepl_tblGROUPBYkeyFORMATNull;Peakmemoryusage(forquery):15.02GiB.0rowsinset.Elapsed:19.164sec.Processed50.00millionrows,6.40GB(2.61millionrows/s.,334.02MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):4.44GiB.0rowsinset.Elapsed:9.700sec.Processed21.03millionrows,2.69GB(2.17millionrows/s.,277.50MB/s.)-- ORDER BY LIMIT BY SELECT*FROMrepl_tblORDERBYtsDESCLIMIT1BYkeyFORMATNull;Peakmemoryusage(forquery):10.46GiB.0rowsinset.Elapsed:21.264sec.Processed50.00millionrows,6.40GB(2.35millionrows/s.,301.03MB/s.)-- Subquery SELECT*FROMrepl_tblWHERE(key,ts)IN(SELECTkey,max(ts)FROMrepl_tblGROUPBYkey)FORMATNull;Peakmemoryusage(forquery):2.52GiB.0rowsinset.Elapsed:6.891sec.Processed50.00millionrows,1.60GB(7.26millionrows/s.,232.22MB/s.)-- set optimize_aggregation_in_order=1; Peakmemoryusage(forquery):1.05GiB.0rowsinset.Elapsed:4.427sec.Processed50.00millionrows,1.60GB(11.29millionrows/s.,361.49MB/s.)-- FINAL SELECT*FROMrepl_tblFINALFORMATNull;Peakmemoryusage(forquery):838.75MiB.0rowsinset.Elapsed:6.681sec.Processed50.00millionrows,6.40GB(7.48millionrows/s.,958.18MB/s.)FINAL Clickhouse merge parts only in scope of single partition, so if two rows with the same replacing key would land in different partitions, they would never be merged in single row. FINAL keyword works in other way, it merge all rows across all partitions. But that behavior can be changed viado_not_merge_across_partitions_select_final setting.\nhttps://kb.altinity.com\nFINAL clause speed\nCREATETABLErepl_tbl_part(`key`UInt32,`value`UInt32,`part_key`UInt32)ENGINE=ReplacingMergeTreePARTITIONBYpart_keyORDERBYkey;INSERTINTOrepl_tbl_partSELECT1ASkey,numberASvalue,number%2ASpart_keyFROMnumbers(4)SETTINGSoptimize_on_insert=0;SELECT*FROMrepl_tbl_part;┌─key─┬─value─┬─part_key─┐│1│1│1││1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│0│0││1│2│0│└─────┴───────┴──────────┘SELECT*FROMrepl_tbl_partFINAL;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘SELECT*FROMrepl_tbl_partFINALSETTINGSdo_not_merge_across_partitions_select_final=1;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│2│0│└─────┴───────┴──────────┘OPTIMIZETABLErepl_tbl_partFINAL;SELECT*FROMrepl_tbl_part;┌─key─┬─value─┬─part_key─┐│1│3│1│└─────┴───────┴──────────┘┌─key─┬─value─┬─part_key─┐│1│2│0│└─────┴───────┴──────────┘","categories":"","description":"ReplacingMergeTree\n","excerpt":"ReplacingMergeTree\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/","tags":"","title":"ReplacingMergeTree"},{"body":"Hi there, I have a question about replacing merge trees. I have set up a Materialized View with ReplacingMergeTree table, but even if I call optimize on it, the parts don’t get merged. I filled that table yesterday, nothing happened since then. What should I do?\nMerges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts. If the total size of input parts are greater than the maximum part size then they will never be merged.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings/#max-bytes-to-merge-at-max-space-in-pool\nhttps://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/replacingmergetree/ ReplacingMergeTree is suitable for clearing out duplicate data in the background in order to save space, but it doesn’t guarantee the absence of duplicates.\n","categories":"","description":"ReplacingMergeTree does not collapse duplicates\n","excerpt":"ReplacingMergeTree does not collapse duplicates\n","ref":"/engines/mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates/","tags":"","title":"ReplacingMergeTree does not collapse duplicates"},{"body":"SELECTdatabase,table,type,max(last_exception),max(postpone_reason),min(create_time),max(last_attempt_time),max(last_postpone_time),max(num_postponed)ASmax_postponed,max(num_tries)ASmax_tries,min(num_tries)ASmin_tries,countIf(last_exception!='')AScount_err,countIf(num_postponed\u003e0)AScount_postponed,countIf(is_currently_executing)AScount_executing,count()AScount_allFROMsystem.replication_queueGROUPBYdatabase,table,typeORDERBYcount_allDESC","categories":"","description":"Replication queue\n","excerpt":"Replication queue\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/","tags":"","title":"Replication queue"},{"body":" Step 1: Detach Kafka tables in ClickHouse Step 2: kafka-consumer-groups.sh --bootstrap-server kafka:9092 --topic topic:0,1,2 --group id1 --reset-offsets --to-latest --execute  More samples: https://gist.github.com/filimonov/1646259d18b911d7a1e8745d6411c0cc   Step: Attach Kafka tables back  See also these configuration settings:\n\u003ckafka\u003e \u003cauto_offset_reset\u003esmallest\u003c/auto_offset_reset\u003e \u003c/kafka\u003e ","categories":"","description":"Rewind / fast-forward / replay\n","excerpt":"Rewind / fast-forward / replay\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/","tags":"","title":"Rewind / fast-forward / replay"},{"body":"CREATETABLEtest_roaring_bitmapENGINE=MergeTreeORDERBYhASSELECTintDiv(number,5)ASh,groupArray(toUInt16(number-(2*intDiv(number,5))))ASvals,groupBitmapState(toUInt16(number-(2*intDiv(number,5))))ASvals_bitmapFROMnumbers(40)GROUPBYhSELECTh,vals,hex(vals_bitmap)FROMtest_roaring_bitmap┌─h─┬─vals─────────────┬─hex(vals_bitmap)─────────┐│0│[0,1,2,3,4]│000500000100020003000400││1│[3,4,5,6,7]│000503000400050006000700││2│[6,7,8,9,10]│000506000700080009000A00││3│[9,10,11,12,13]│000509000A000B000C000D00││4│[12,13,14,15,16]│00050C000D000E000F001000││5│[15,16,17,18,19]│00050F001000110012001300││6│[18,19,20,21,22]│000512001300140015001600││7│[21,22,23,24,25]│000515001600170018001900│└───┴──────────────────┴──────────────────────────┘SELECTgroupBitmapAnd(vals_bitmap)ASuniq,bitmapToArray(groupBitmapAndState(vals_bitmap))ASvalsFROMtest_roaring_bitmapWHEREhIN(0,1)┌─uniq─┬─vals──┐│2│[3,4]│└──────┴───────┘See also https://cdmana.com/2021/01/20210109005922716t.html\n","categories":"","description":"","excerpt":"CREATETABLEtest_roaring_bitmapENGINE=MergeTreeORDERBYhASSELECTintDiv(n …","ref":"/altinity-kb-queries-and-syntax/roaring-bitmaps-for-calculating-retention/","tags":"","title":"Roaring bitmaps for calculating retention"},{"body":"Short Instruction   Do FREEZE TABLE on needed table, partition. It would produce consistent snapshot of table data.\n  Run rsync command.\nrsync -ravlW --bwlimit=100000 /var/lib/clickhouse/data/shadow/N/database/table  root@remote_host:/var/lib/clickhouse/data/database/table/detached --bwlimit is transfer limit in KBytes per second.\n  Run ATTACH PARTITION for each partition from ./detached directory.\n  ","categories":"","description":"rsync\n","excerpt":"rsync\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/","tags":"","title":"rsync"},{"body":"The execution pipeline is embedded in the partition reading code.\nSo that works this way:\n ClickHouse does partition pruning based on WHERE conditions. For every partition, it picks a columns ranges (aka ‘marks’ / ‘granulas’) based on primary key conditions. Here the sampling logic is applied: a) in case of SAMPLE k (k in 0..1 range) it adds conditions WHERE sample_key \u003c k * max_int_of_sample_key_type b) in case of SAMPLE k OFFSET m it adds conditions WHERE sample_key BETWEEN m * max_int_of_sample_key_type AND (m + k) * max_int_of_sample_key_typec) in case of SAMPLE N (N\u003e1) if first estimates how many rows are inside the range we need to read and based on that convert it to 3a case (calculate k based on number of rows in ranges and desired number of rows) on the data returned by those other conditions are applied (so here the number of rows can be decreased here)  Source Code\nSAMPLE by Docs Source Code\nSAMPLE key Must be:\n Included in the primary key. Uniformly distributed in the domain of its data type:  Bad: Timestamp; Good: intHash32(UserID);   Cheap to calculate:  Bad: cityHash64(URL); Good: intHash32(UserID);   Not after high granular fields in primary key:  Bad: ORDER BY (Timestamp, sample_key); Good: ORDER BY (CounterID, Date, sample_key).    Sampling is:\n Deterministic Works in a consistent way for different tables. Allows reading less amount of data from disk.  SAMPLE key, bonus SAMPLE 1/10 Select data for 1/10 of all possible sample keys; SAMPLE 1000000   Select from about (not less than) 1 000 000 rows on each shard;  You can use _sample_factor virtual column to determine the relative sample factor; SAMPLE 1/10 OFFSET 1/10   Select second 1/10 of all possible sample keys; SET max_parallel_replicas = 3 Select from multiple replicas of each shard in parallel;  SAMPLE emulation via WHERE condition Sometimes, it’s easier to emulate sampling via conditions in WHERE clause instead of using SAMPLE key.\nSELECT count() FROM table WHERE ... AND cityHash64(some_high_card_key) % 10 = 0; -- Deterministic SELECT count() FROM table WHERE ... AND rand() % 10 = 0; -- Non-deterministic ClickHouse will read more data from disk compared to an example with a good SAMPLE key, but it’s more universal and can be used if you can’t change table ORDER BY key.\n","categories":"","description":"SAMPLE by\n","excerpt":"SAMPLE by\n","ref":"/altinity-kb-queries-and-syntax/altinity-kb-sample-by/","tags":"","title":"SAMPLE by"},{"body":"The most important idea about sampling that the primary index must have low cardinality. The following example demonstrates how sampling can be setup correctly, and an example if it being set up incorrectly as a comparison.\nSampling requires sample by expression . This ensures a range of sampled column types fit within a specified range, which ensures the requirement of low cardinality. In this example, I cannot use transaction_id because I can not ensure that the min value of transaction_id = 0 and max value = MAX_UINT64. Instead, I used cityHash64(transaction_id)to expand the range within the minimum and maximum values.\nFor example if all values of transaction_id are from 0 to 10000 sampling will be inefficient. But cityHash64(transaction_id) expands the range from 0 to 18446744073709551615:\nSELECTcityHash64(10000)┌────cityHash64(10000)─┐│14845905981091347439│└──────────────────────┘If I used transaction_id without knowing that they matched the allowable ranges, the results of sampled queries would be skewed. For example, when using sample 0.5, ClickHouse requests where sample_col \u003e= 0 and sample_col \u003c= MAX_UINT64/2.\nAlso you can include multiple columns into a hash function of the sampling expression to improve randomness of the distribution cityHash64(transaction_id, banner_id).\nSampling Friendly Table CREATETABLEtable_one(timestampUInt64,transaction_idUInt64,banner_idUInt16,valueUInt32)ENGINE=MergeTree()PARTITIONBYtoYYYYMMDD(toDateTime(timestamp))ORDERBY(banner_id,toStartOfHour(toDateTime(timestamp)),cityHash64(transaction_id))SAMPLEBYcityHash64(transaction_id)SETTINGSindex_granularity=8192insertintotable_oneselect1602809234+intDiv(number,100000),number,number%991,toUInt32(rand())fromnumbers(10000000000);I reduced the granularity of the timestamp column to one hour with toStartOfHour(toDateTime(timestamp)) , otherwise sampling will not work.\nVerifying Sampling Works The following shows that sampling works with the table and parameters described above. Notice the Elapsed time when invoking sampling:\n-- Q1. No where filters. -- The query is 10 times faster with SAMPLE 0.01 selectbanner_id,sum(value),count(value),max(value)fromtable_onegroupbybanner_idformatNull;0rowsinset.Elapsed:11.490sec.Processed10.00billionrows,60.00GB(870.30millionrows/s.,5.22GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_idformatNull;0rowsinset.Elapsed:1.316sec.Processed452.67millionrows,6.34GB(343.85millionrows/s.,4.81GB/s.)-- Q2. Filter by the first column in index (banner_id = 42) -- The query is 20 times faster with SAMPLE 0.01 -- reads 20 times less rows: 10.30 million rows VS Processed 696.32 thousand rows selectbanner_id,sum(value),count(value),max(value)fromtable_oneWHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.020sec.Processed10.30millionrows,61.78MB(514.37millionrows/s.,3.09GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01WHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.008sec.Processed696.32thousandrows,9.75MB(92.49millionrows/s.,1.29GB/s.)-- Q3. No filters -- The query is 10 times faster with SAMPLE 0.01 -- reads 20 times less rows. selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_onegroupbybanner_id,hrformatNull;0rowsinset.Elapsed:36.660sec.Processed10.00billionrows,140.00GB(272.77millionrows/s.,3.82GB/s.)selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_id,hrformatNull;0rowsinset.Elapsed:3.741sec.Processed452.67millionrows,9.96GB(121.00millionrows/s.,2.66GB/s.)-- Q4. Filter by not indexed column -- The query is 6 times faster with SAMPLE 0.01 -- reads 20 times less rows. selectcount()fromtable_onewherevalue=666formatNull;1rowsinset.Elapsed:6.056sec.Processed10.00billionrows,40.00GB(1.65billionrows/s.,6.61GB/s.)selectcount()fromtable_oneSAMPLE0.01wherevalue=666formatNull;1rowsinset.Elapsed:1.214sec.Processed452.67millionrows,5.43GB(372.88millionrows/s.,4.47GB/s.)Non-Sampling Friendly Table CREATETABLEtable_one(timestampUInt64,transaction_idUInt64,banner_idUInt16,valueUInt32)ENGINE=MergeTree()PARTITIONBYtoYYYYMMDD(toDateTime(timestamp))ORDERBY(banner_id,timestamp,cityHash64(transaction_id))SAMPLEBYcityHash64(transaction_id)SETTINGSindex_granularity=8192insertintotable_oneselect1602809234+intDiv(number,100000),number,number%991,toUInt32(rand())fromnumbers(10000000000);This is the same as our other table, BUT granularity of timestamp column is not reduced.\nVerifying Sampling Does Not Work The following tests shows that sampling is not working because of the lack of timestamp granularity. The Elapsed time is longer when sampling is used.\n-- Q1. No where filters. -- The query is 2 times SLOWER!!! with SAMPLE 0.01 -- Because it needs to read excessive column with sampling data! selectbanner_id,sum(value),count(value),max(value)fromtable_onegroupbybanner_idformatNull;0rowsinset.Elapsed:11.196sec.Processed10.00billionrows,60.00GB(893.15millionrows/s.,5.36GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_idformatNull;0rowsinset.Elapsed:24.378sec.Processed10.00billionrows,140.00GB(410.21millionrows/s.,5.74GB/s.)-- Q2. Filter by the first column in index (banner_id = 42) -- The query is SLOWER with SAMPLE 0.01 selectbanner_id,sum(value),count(value),max(value)fromtable_oneWHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.022sec.Processed10.27millionrows,61.64MB(459.28millionrows/s.,2.76GB/s.)selectbanner_id,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01WHEREbanner_id=42groupbybanner_idformatNull;0rowsinset.Elapsed:0.037sec.Processed10.27millionrows,143.82MB(275.16millionrows/s.,3.85GB/s.)-- Q3. No filters -- The query is SLOWER with SAMPLE 0.01 selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_onegroupbybanner_id,hrformatNull;0rowsinset.Elapsed:21.663sec.Processed10.00billionrows,140.00GB(461.62millionrows/s.,6.46GB/s.)selectbanner_id,toStartOfHour(toDateTime(timestamp))hr,sum(value),count(value),max(value)fromtable_oneSAMPLE0.01groupbybanner_id,hrformatNull;0rowsinset.Elapsed:26.697sec.Processed10.00billionrows,220.00GB(374.57millionrows/s.,8.24GB/s.)-- Q4. Filter by not indexed column -- The query is SLOWER with SAMPLE 0.01 selectcount()fromtable_onewherevalue=666formatNull;0rowsinset.Elapsed:7.679sec.Processed10.00billionrows,40.00GB(1.30billionrows/s.,5.21GB/s.)selectcount()fromtable_oneSAMPLE0.01wherevalue=666formatNull;0rowsinset.Elapsed:21.668sec.Processed10.00billionrows,120.00GB(461.51millionrows/s.,5.54GB/s.)","categories":"","description":"Clickhouse table sampling example\n","excerpt":"Clickhouse table sampling example\n","ref":"/altinity-kb-queries-and-syntax/sampling-example/","tags":"","title":"Sampling Example"},{"body":" golang-migrate tool - see golang-migrate bytebase  https://bytebase.com   Flyway - there are a lot of PRs introducing ClickHouse support, maintainer doesn’t merge them (maybe he will change his mind soon), but’s it’s not hard to build flyway from one of those PRs (latest at the top)  https://github.com/flyway/flyway/pull/3333 Сlickhouse support https://github.com/flyway/flyway/pull/3134 Сlickhouse support https://github.com/flyway/flyway/pull/3133 Add support clickhouse https://github.com/flyway/flyway/pull/2981 Clickhouse replicated https://github.com/flyway/flyway/pull/2640 Yet another ClickHouse support https://github.com/flyway/flyway/pull/2166 Clickhouse support (#1772) https://github.com/flyway/flyway/pull/1773 Fixed #1772: Add support for ClickHouse (https://clickhouse.yandex/)   liquibase  https://github.com/mediarithmics/liquibase-clickhouse https://johntipper.org/how-to-execute-liquibase-changesets-against-clickhouse/   custom tool for ClickHouse  https://github.com/delium/clickhouse-migrator   phpMigrations  https://github.com/smi2/phpMigrationsClickhouse https://habrahabr.ru/company/smi2/blog/317682/   dbmate  https://github.com/amacneil/dbmate#clickhouse    know more?\n","categories":"","description":"Schema migration tools for ClickHouse\n","excerpt":"Schema migration tools for ClickHouse\n","ref":"/altinity-kb-setup-and-maintenance/schema-migration-tools/","tags":"","title":"Schema migration tools for ClickHouse"},{"body":"Question What will happen, if we would run SELECT query from working Kafka table with MV attached? Would data showed in SELECT query appear later in MV destination table?\nAnswer  Most likely SELECT query would show nothing. If you lucky enough and something would show up, those rows wouldn’t appear in MV destination table.  So it’s not recommended to run SELECT queries on working Kafka tables.\nIn case of debug it’s possible to use another Kafka table with different consumer_group, so it wouldn’t affect your main pipeline.\n","categories":"","description":"SELECTs from engine=Kafka\n","excerpt":"SELECTs from engine=Kafka\n","ref":"/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/","tags":"","title":"SELECTs from engine=Kafka"},{"body":"Question I expect the sequence here to only match once as a is only directly after a once - but it matches with gaps. Why is that?\nSELECTsequenceCount('(?1)(?2)')(sequence,pageILIKE'%a%',pageILIKE'%a%')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('a',2),('b',3),('b',4),('a',5),('b',6),('a',7))2#??Answer sequenceMatch just ignores the events which don’t match the condition. Check that:\nSELECTsequenceMatch('(?1)(?2)')(sequence,page='a',page='b')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));1#??SELECTsequenceMatch('(?1).(?2)')(sequence,page='a',page='b')ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));0#???SELECTsequenceMatch('(?1)(?2)')(sequence,page='a',page='b',pageNOTIN('a','b'))ASsequencesfromvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));0#!SELECTsequenceMatch('(?1).(?2)')(sequence,page='a',page='b',pageNOTIN('a','b'))ASsequencesfromvalues('page String, sequence UInt16',('a',1),('c',2),('b',3));1#So for your example - just introduce one more ’nothing matched’ condition:\nSELECTsequenceCount('(?1)(?2)')(sequence,pageILIKE'%a%',pageILIKE'%a%',NOT(pageILIKE'%a%'))ASsequencesFROMvalues('page String, sequence UInt16',('a',1),('a',2),('b',3),('b',4),('a',5),('b',6),('a',7))","categories":"","description":"sequenceMatch\n","excerpt":"sequenceMatch\n","ref":"/altinity-kb-functions/altinity-kb-sequencematch/","tags":"","title":"sequenceMatch"},{"body":"Сonfig management (recommended structure) Clickhouse server config consists of two parts server settings (config.xml) and users settings (users.xml).\nBy default they are stored in the folder /etc/clickhouse-server/ in two files config.xml \u0026 users.xml.\nWe suggest never change vendor config files and place your changes into separate .xml files in sub-folders. This way is easier to maintain and ease Clickhouse upgrades.\n/etc/clickhouse-server/users.d – sub-folder for user settings.\n/etc/clickhouse-server/config.d – sub-folder for server settings.\n/etc/clickhouse-server/conf.d – sub-folder for any (both) settings.\nFile names of your xml files can be arbitrary but they are applied in alphabetical order.\nExamples:\n$ cat /etc/clickhouse-server/config.d/listen_host.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003clisten_host\u003e::\u003c/listen_host\u003e \u003c/yandex\u003e $ cat /etc/clickhouse-server/config.d/macros.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cmacros\u003e \u003ccluster\u003etest\u003c/cluster\u003e \u003creplica\u003ehost22\u003c/replica\u003e \u003cshard\u003e0\u003c/shard\u003e \u003cserver_id\u003e41295\u003c/server_id\u003e \u003cserver_name\u003ehost22.server.com\u003c/server_name\u003e \u003c/macros\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/config.d/zoo.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003czookeeper\u003e \u003cnode\u003e \u003chost\u003elocalhost\u003c/host\u003e \u003cport\u003e2181\u003c/port\u003e \u003c/node\u003e \u003c/zookeeper\u003e \u003cdistributed_ddl\u003e \u003cpath\u003e/clickhouse/test/task_queue/ddl\u003c/path\u003e \u003c/distributed_ddl\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/users.d/enable_access_management_for_user_default.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cusers\u003e \u003cdefault\u003e \u003caccess_management\u003e1\u003c/access_management\u003e \u003c/default\u003e \u003c/users\u003e \u003c/yandex\u003e cat /etc/clickhouse-server/users.d/memory_usage.xml \u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_bytes_before_external_group_by\u003e25290221568\u003c/max_bytes_before_external_group_by\u003e \u003cmax_memory_usage\u003e50580443136\u003c/max_memory_usage\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e BTW, you can define any macro in your configuration and use them in Zookeeper paths\n ReplicatedMergeTree('/clickhouse/{cluster}/tables/my_table','{replica}') or in your code using function getMacro:\nCREATEORREPLACEVIEWsrv_server_infoSELECT(SELECTgetMacro('shard'))ASshard_num,(SELECTgetMacro('server_name'))ASserver_name,(SELECTgetMacro('server_id'))ASserver_keySettings can be appended to an XML tree (default behaviour) or replaced or removed.\nExample how to delete tcp_port \u0026 http_port defined on higher level in the main config.xml (it disables open tcp \u0026 http ports if you configured secure ssl):\ncat /etc/clickhouse-server/config.d/disable_open_network.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003chttp_port remove=\"1\"/\u003e \u003ctcp_port remove=\"1\"/\u003e \u003c/yandex\u003e Example how to replace remote_servers section defined on higher level in the main config.xml (it allows to remove default test clusters.\n\u003c?xml version=\"1.0\" ?\u003e \u003cyandex\u003e \u003cremote_servers replace=\"1\"\u003e \u003cmycluster\u003e .... \u003c/mycluster\u003e \u003c/remote_servers\u003e \u003c/yandex\u003e Settings \u0026 restart General ‘rule of thumb’:\n server settings (config.xml and config.d) changes require restart; user settings (users.xml and users.d) changes don’t require restart.  But there are exceptions from those rules (see below).\nServer config (config.xml) sections which don’t require restart  \u003cmax_server_memory_usage\u003e \u003cmax_server_memory_usage_to_ram_ratio\u003e \u003cmax_table_size_to_drop\u003e \u003cmax_partition_size_to_drop\u003e \u003cmax_concurrent_queries\u003e \u003cmacros\u003e \u003cremote_servers\u003e \u003cdictionaries_config\u003e \u003cuser_defined_executable_functions_config\u003e \u003cmodels_config\u003e \u003ckeeper_server\u003e \u003czookeeper\u003e (but reconnect don’t happen automatically) \u003cstorage_configuration\u003e \u003cuser_directories\u003e \u003caccess_control_path\u003e \u003cencryption_codecs\u003e \u003clogger\u003e (since 21.11)  Those sections (live in separate files):\n \u003cdictionaries\u003e \u003cfunctions\u003e \u003cmodels\u003e  See also https://github.com/ClickHouse/ClickHouse/blob/445b0ba7cc6b82e69fef28296981fbddc64cd634/programs/server/Server.cpp#L809-L883\nUser settings which require restart. Most of user setting changes don’t require restart, but they get applied at the connect time, so existing connection may still use old user-level settings. That means that that new setting will be applied to new sessions / after reconnect.\nThe list of user setting which require server restart:\n \u003cbackground_buffer_flush_schedule_pool_size\u003e \u003cbackground_pool_size\u003e \u003cbackground_merges_mutations_concurrency_ratio\u003e \u003cbackground_move_pool_size\u003e \u003cbackground_fetches_pool_size\u003e \u003cbackground_common_pool_size\u003e \u003cbackground_schedule_pool_size\u003e \u003cbackground_message_broker_schedule_pool_size\u003e \u003cbackground_distributed_schedule_pool_size\u003e \u003cmax_replicated_fetches_network_bandwidth_for_server\u003e \u003cmax_replicated_sends_network_bandwidth_for_server\u003e  See also select * from system.settings where description ilike '%start%'\nAlso there are several ’long-running’ user sessions which are almost never restarted and can keep the setting from the server start (it’s DDLWorker, Kafka, and some other service things).\nDictionaries We suggest to store each dictionary description in a separate (own) file in a /etc/clickhouse-server/dict sub-folder.\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003chttp\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e and add to the configuration\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cdictionaries_config\u003edict/*.xml\u003c/dictionaries_config\u003e \u003cdictionaries_lazy_load\u003etrue\u003c/dictionaries_lazy_load\u003e \u003c/yandex\u003e dict/*.xml – relative path, servers seeks files in the folder /etc/clickhouse-server/dict. More info in Multiple Clickhouse instances.\nincl attribute \u0026 metrica.xml incl attribute allows to include some XML section from a special include file multiple times.\nBy default include file is /etc/metrika.xml. You can use many include files for each XML section.\nFor example to avoid repetition of user/password for each dictionary you can create an XML file:\n$ cat /etc/clickhouse-server/dict_sources.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cmysql_config\u003e \u003cport\u003e3306\u003c/port\u003e \u003cuser\u003euser\u003c/user\u003e \u003cpassword\u003e123\u003c/password\u003e \u003creplica\u003e \u003chost\u003emysql_host\u003c/host\u003e \u003cpriority\u003e1\u003c/priority\u003e \u003c/replica\u003e \u003cdb\u003emy_database\u003c/db\u003e \u003c/mysql_config\u003e \u003c/yandex\u003e Include this file:\n$ cat /etc/clickhouse-server/config.d/dictionaries.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e ... \u003cinclude_from\u003e/etc/clickhouse-server/dict_sources.xml\u003c/include_from\u003e \u003c/yandex\u003e And use in dictionary descriptions (incl=“mysql_config”):\n$ cat /etc/clickhouse-server/dict/country.xml \u003c?xml version=\"1.0\"?\u003e \u003cdictionaries\u003e \u003cdictionary\u003e \u003cname\u003ecountry\u003c/name\u003e \u003csource\u003e \u003cmysql incl=\"mysql_config\"\u003e \u003ctable\u003emy_table\u003c/table\u003e \u003cinvalidate_query\u003eselect max(id) from my_table\u003c/invalidate_query\u003e \u003c/mysql\u003e \u003c/source\u003e ... \u003c/dictionary\u003e \u003c/dictionaries\u003e Multiple Clickhouse instances at one host By default Clickhouse server configs are in /etc/clickhouse-server/ because clickhouse-server runs with a parameter –config-file /etc/clickhouse-server/config.xml\nconfig-file is defined in startup scripts:\n /etc/init.d/clickhouse-server – init-V /etc/systemd/system/clickhouse-server.service – systemd  Clickhouse uses the path from config-file parameter as base folder and seeks for other configs by relative path. All sub-folders users.d / config.d are relative.\nYou can start multiple clickhouse-server each with own –config-file.\nFor example:\n/usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node1/config.xml  /etc/clickhouse-server-node1/ config.xml ... users.xml  /etc/clickhouse-server-node1/config.d/disable_open_network.xml  /etc/clickhouse-server-node1/users.d/....  /usr/bin/clickhouse-server --config-file /etc/clickhouse-server-node2/config.xml  /etc/clickhouse-server-node2/ config.xml ... users.xml  /etc/clickhouse-server-node2/config.d/disable_open_network.xml  /etc/clickhouse-server-node2/users.d/.... If you need to run multiple servers for CI purposes you can combine all settings in a single fat XML file and start ClickHouse without config folders/sub-folders.\n/usr/bin/clickhouse-server --config-file /tmp/ch1.xml /usr/bin/clickhouse-server --config-file /tmp/ch2.xml /usr/bin/clickhouse-server --config-file /tmp/ch3.xml Each ClickHouse instance must work with own data-folder and tmp-folder.\nBy default ClickHouse uses /var/lib/clickhouse/. It can be overridden in path settings\n\u003cpath\u003e/data/clickhouse-ch1/\u003c/path\u003e  \u003ctmp_path\u003e/data/clickhouse-ch1/tmp/\u003c/tmp_path\u003e  \u003cuser_files_path\u003e/data/clickhouse-ch1/user_files/\u003c/user_files_path\u003e  \u003clocal_directory\u003e  \u003cpath\u003e/data/clickhouse-ch1/access/\u003c/path\u003e  \u003c/local_directory\u003e  \u003cformat_schema_path\u003e/data/clickhouse-ch1/format_schemas/\u003c/format_schema_path\u003e preprocessed_configs Clickhouse server watches config files and folders. When you change, add or remove XML files Clickhouse immediately assembles XML files into a combined file. These combined files are stored in /var/lib/clickhouse/preprocessed_configs/ folders.\nYou can verify that your changes are valid by checking /var/lib/clickhouse/preprocessed_configs/config.xml, /var/lib/clickhouse/preprocessed_configs/users.xml.\nIf something wrong with with your settings e.g. unclosed XML element or typo you can see alerts about this mistakes in /var/log/clickhouse-server/clickhouse-server.log\nIf you see your changes in preprocessed_configs it does not mean that changes are applied on running server, check Settings \u0026amp; restart\n","categories":"","description":"How to manage server config files in Clickhouse\n","excerpt":"How to manage server config files in Clickhouse\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-server-config-files/","tags":"","title":"Server config files"},{"body":"  query_log and other _log tables - set up TTL, or some other cleanup procedures.\ncat /etc/clickhouse-server/config.d/query_log.xml \u003cyandex\u003e \u003cquery_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY event_date ORDER BY (event_time) TTL event_date + interval 90 day SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/query_log\u003e \u003c/yandex\u003e   query_thread_log - typically is not too useful for end users, you can disable it (or set up TTL).\ncat /etc/clickhouse-server/config.d/disable_query_thread_log.xml \u003cyandex\u003e \u003cquery_thread_log remove=\"1\" /\u003e \u003c/yandex\u003e   If you have a good monitoring outside ClickHouse you don’t need to store the history of metrics in ClickHouse\ncat /etc/clickhouse-server/config.d/disable_metric_logs.xml \u003cyandex\u003e \u003cmetric_log remove=\"1\" /\u003e \u003casynchronous_metric_log remove=\"1\" /\u003e \u003c/yandex\u003e   part_log - may be nice, especially at the beginning / during system tuning/analyze.\ncat /etc/clickhouse-server/config.d/part_log.xml \u003cyandex\u003e \u003cpart_log replace=\"1\"\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003epart_log\u003c/table\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003cengine\u003e ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_time) TTL toStartOfMonth(event_date) + INTERVAL 3 MONTH SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003c/part_log\u003e \u003c/yandex\u003e   on older versions log_queries is disabled by default, it’s worth having it enabled always.\n$ cat /etc/clickhouse-server/users.d/log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e1\u003c/log_queries\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e   quite often you want to have on-disk group by / order by enabled (both disabled by default).\ncat /etc/clickhouse-server/users.d/enable_on_disk_operations.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003cmax_bytes_before_external_group_by\u003e2000000000\u003c/max_bytes_before_external_group_by\u003e \u003cmax_bytes_before_external_sort\u003e2000000000\u003c/max_bytes_before_external_sort\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e   quite often you want to create more users with different limitations. The most typical is \u003cmax_execution_time\u003e It’s actually also not a way to plan/share existing resources better, but it at least disciplines users.\nAlso introducing some restrictions on query complexity can be a good option to discipline users.\nYou can find the preset example here. Also, force_index_by_date + force_primary_key can be a nice idea to avoid queries that ‘accidentally’ do full scans, max_concurrent_queries_for_user\n  merge_tree settings: max_bytes_to_merge_at_max_space_in_pool (may be reduced in some scenarios), fsync_* , inactive_parts_to_throw_insert - can be enabled, replicated_deduplication_window - can be extended if single insert create lot of parts , merge_with_ttl_timeout - when you use ttl\n  settings default_database_engine / insert_distributed_sync / fsync_metadata / do_not_merge_across_partitions_select_final / fsync\n  memory usage per server / query / user: memory configuration settings\n  See also:\nhttps://docs.altinity.com/operationsguide/security/clickhouse-hardening-guide/\n","categories":"","description":"Settings to adjust\n","excerpt":"Settings to adjust\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/","tags":"","title":"Settings to adjust"},{"body":"It’s possible to shutdown server on fly, but that would lead to failure of some queries.\nMore safer way:\n  Remove server (which is going to be disabled) from remote_server section of config.xml on all servers.\n  Remove server from load balancer, so new queries wouldn’t hit it.\n  Detach Kafka / Rabbit / Buffer tables (if used), and Materialized* databases.\n  Wait until all already running queries would finish execution on it. It’s possible to check it via query:\nSHOWPROCESSLIST;  Ensure there is no pending data in distributed tables\nSELECT*FROMsystem.distribution_queue;SYSTEMFLUSHDISTRIBUTED\u003ctable_name\u003e;  Run sync replica query in related shard replicas (others than the one you remove) via query:\nSYSTEMSYNCREPLICAdb.table;  Shutdown server.\n  SYSTEM SHUTDOWN query doesn’t wait until query completion and tries to kill all queries immediately after receiving signal, even if setting shutdown_wait_unfinished being used.\nhttps://github.com/ClickHouse/ClickHouse/blob/master/programs/server/Server.cpp#L1353\n","categories":"","description":"Shutting down a node\n","excerpt":"Shutting down a node\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-shutting-down-a-node/","tags":"","title":"Shutting down a node"},{"body":"Q. What is SimpleAggregateFunction? Are there advantages to use it instead of AggregateFunction in AggregatingMergeTree? SimpleAggregateFunction can be used for those aggregations when the function state is exactly the same as the resulting function value. Typical example is max function: it only requires storing the single value which is already maximum, and no extra steps needed to get the final value. In contrast avg need to store two numbers - sum \u0026 count, which should be divided to get the final value of aggregation (done by the -Merge step at the very end).\n    SimpleAggregateFunction AggregateFunction     inserting accepts the value of underlying type OR\na value of corresponding SimpleAggregateFunction type CREATE TABLE saf_test\n( x SimpleAggregateFunction(max, UInt64) )\nENGINE=AggregatingMergeTree\nORDER BY tuple();\nINSERT INTO saf_test VALUES (1);\nINSERT INTO saf_test SELECT max(number) FROM numbers(10);\nINSERT INTO saf_test SELECT maxSimpleState(number) FROM numbers(20);  ONLY accepts the state of same aggregate function calculated using -State combinator   storing Internally store just a value of underlying type function-specific state   storage usage typically is much better due to better compression/codecs in very rare cases it can be more optimal than raw values\nadaptive granularity doesn't work for large states\n   reading raw value per row you can access it directly you need to use finalizeAgggregation function   using aggregated value just\nselect max(x) from test;  you need to use -Merge combinator select maxMerge(x) from test; \n   memory usage typically less memory needed (in some corner cases even 10 times) typically uses more memory, as every state can be quite complex   performance typically better, due to lower overhead worse    See also https://github.com/ClickHouse/ClickHouse/pull/4629 https://github.com/ClickHouse/ClickHouse/issues/3852\nQ. How maxSimpleState combinator result differs from plain max? They produce the same result, but types differ (the first have SimpleAggregateFunction datatype). Both can be pushed to SimpleAggregateFunction or to the underlying type. So they are interchangeable.\nInfo -SimpleState is useful for implicit Materialized View creation, like CREATE MATERIALIZED VIEW mv ENGINE = AggregatingMergeTree ORDER BY date AS SELECT date, sumSimpleState(1) AS cnt, sumSimpleState(revenue) AS rev FROM table GROUP BY date  Warning -SimpleState supported since 21.1. See https://github.com/ClickHouse/ClickHouse/pull/16853/  Q. Can I use -If combinator with SimpleAggregateFunction? Something like SimpleAggregateFunction(maxIf, UInt64, UInt8) is NOT possible. But is 100% ok to push maxIf (or maxSimpleStateIf) into SimpleAggregateFunction(max, UInt64)\nThere is one problem with that approach: -SimpleStateIf Would produce 0 as result in case of no-match, and it can mess up some aggregate functions state. It wouldn’t affect functions like max/argMax/sum, but could affect functions like min/argMin/any/anyLast\nSELECTminIfMerge(state_1),min(state_2)FROM(SELECTminIfState(number,number\u003e5)ASstate_1,minSimpleStateIf(number,number\u003e5)ASstate_2FROMnumbers(5)UNIONALLSELECTminIfState(toUInt64(2),2),minIf(2,2))┌─minIfMerge(state_1)─┬─min(state_2)─┐│2│0│└─────────────────────┴──────────────┘You can easily workaround that:\n Using Nullable datatype. Set result to some big number in case of no-match, which would be bigger than any possible value, so it would be safe to use. But it would work only for min/argMin  SELECTmin(state_1),min(state_2)FROM(SELECTminSimpleState(if(number\u003e5,number,1000))ASstate_1,minSimpleStateIf(toNullable(number),number\u003e5)ASstate_2FROMnumbers(5)UNIONALLSELECTminIf(2,2),minIf(2,2))┌─min(state_1)─┬─min(state_2)─┐│2│2│└──────────────┴──────────────┘Extra example WITHminIfState(number,number\u003e5)ASstate_1,minSimpleStateIf(number,number\u003e5)ASstate_2SELECTbyteSize(state_1),toTypeName(state_1),byteSize(state_2),toTypeName(state_2)FROMnumbers(10)FORMATVertical-- For UInt64 Row1:──────byteSize(state_1):24toTypeName(state_1):AggregateFunction(minIf,UInt64,UInt8)byteSize(state_2):8toTypeName(state_2):SimpleAggregateFunction(min,UInt64)-- For UInt32 ──────byteSize(state_1):16byteSize(state_2):4-- For UInt16 ──────byteSize(state_1):12byteSize(state_2):2-- For UInt8 ──────byteSize(state_1):10byteSize(state_2):1See also https://gist.github.com/filimonov/a4f6754497f02fcef78e9f23a4d170ee\n","categories":"","description":"Simple aggregate functions \u0026 combinators\n","excerpt":"Simple aggregate functions \u0026 combinators\n","ref":"/altinity-kb-queries-and-syntax/simplestateif-or-ifstate-for-simple-aggregate-functions/","tags":"","title":"Simple aggregate functions \u0026 combinators"},{"body":" Warning When you are creating skip indexes in non-regular (Replicated)MergeTree tables over non ORDER BY columns. ClickHouse applies index condition on the first step of query execution, so it’s possible to get outdated rows.  --(1) create test table droptableifexiststest;createtabletest(versionUInt32,idUInt32,stateUInt8,INDEXstate_idx(state)typeset(0)GRANULARITY1)ENGINEReplacingMergeTree(version)ORDERBY(id);--(2) insert sample data INSERTINTOtest(version,id,state)VALUES(1,1,1);INSERTINTOtest(version,id,state)VALUES(2,1,0);INSERTINTOtest(version,id,state)VALUES(3,1,1);--(3) check the result: -- expected 3, 1, 1 selectversion,id,statefromtestfinal;┌─version─┬─id─┬─state─┐│3│1│1│└─────────┴────┴───────┘-- expected empty result selectversion,id,statefromtestfinalwherestate=0;┌─version─┬─id─┬─state─┐│2│1│0│└─────────┴────┴───────┘","categories":"","description":"Skip index\n","excerpt":"Skip index\n","ref":"/engines/mergetree-table-engine-family/skip-index/","tags":"","title":"Skip index"},{"body":"tested with 20.8.17.25\nhttps://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes\nLet’s create test data createtablebftest(kInt64,xArray(Int64))Engine=MergeTreeorderbyk;insertintobftestselectnumber,arrayMap(i-\u003erand64()%565656,range(10))fromnumbers(10000000);insertintobftestselectnumber,arrayMap(i-\u003erand64()%565656,range(10))fromnumbers(100000000);Base point (no index) selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.495sec.Processed110.00millionrows,9.68GB(222.03millionrows/s.,19.54GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.505sec.Processed110.00millionrows,9.68GB(217.69millionrows/s.,19.16GB/s.)As you can see Clickhouse read 110.00 million rows and the query elapsed Elapsed: 0.505 sec.\nLet’s add an index altertablebftestaddindexix1(x)TYPEbloom_filterGRANULARITY3;-- GRANULARITY 3 means how many table granules will be in the one index granule -- In our case 1 granule of skip index allows to check and skip 3*8192 rows. -- Every dataset is unique sometimes GRANULARITY 1 is better, sometimes -- GRANULARITY 10. -- Need to test on the real data. optimizetablebftestfinal;-- I need to optimize my table because an index is created for only -- new parts (inserted or merged) -- optimize table final re-writes all parts, but with an index. -- probably in your production you don't need to optimize -- because your data is rotated frequently. -- optimize is a heavy operation, better never run optimize table final in a -- production. test bloom_filter GRANULARITY 3 selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.063sec.Processed5.41millionrows,475.79MB(86.42millionrows/s.,7.60GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.042sec.Processed1.13millionrows,99.48MB(26.79millionrows/s.,2.36GB/s.)As you can see I got 10 times boost.\nLet’s try to reduce GRANULARITY to drop by 1 table granule altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filterGRANULARITY1;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.051sec.Processed3.64millionrows,320.08MB(71.63millionrows/s.,6.30GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.050sec.Processed2.06millionrows,181.67MB(41.53millionrows/s.,3.65GB/s.)No improvement :(\nLet’s try to change the false/true probability of the bloom_filter bloom_filter(0.05) altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filter(0.05)GRANULARITY3;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.079sec.Processed8.95millionrows,787.22MB(112.80millionrows/s.,9.93GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.058sec.Processed3.86millionrows,339.54MB(66.83millionrows/s.,5.88GB/s.)No improvement.\nbloom_filter(0.01) altertablebftestdropindexix1;altertablebftestaddindexix1(x)TYPEbloom_filter(0.01)GRANULARITY3;optimizetablebftestfinal;selectcount()frombftestwherehas(x,42);┌─count()─┐│186│└─────────┘1rowsinset.Elapsed:0.069sec.Processed5.26millionrows,462.82MB(76.32millionrows/s.,6.72GB/s.)selectcount()frombftestwherehas(x,-42);┌─count()─┐│0│└─────────┘1rowsinset.Elapsed:0.047sec.Processed737.28thousandrows,64.88MB(15.72millionrows/s.,1.38GB/s.)Also no improvement :(\nOutcome: I would use TYPE bloom_filter GRANULARITY 3.\n2021 Altinity Inc. All rights reserved.\n","categories":"","description":"Example: skip index bloom_filter \u0026 array column\n","excerpt":"Example: skip index bloom_filter \u0026 array column\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/skip-index-bloom_filter-for-array-column/","tags":"","title":"Skip index bloom_filter Example"},{"body":"","categories":"","description":"Skip indexes\n","excerpt":"Skip indexes\n","ref":"/altinity-kb-queries-and-syntax/skip-indexes/","tags":"","title":"Skip indexes"},{"body":"bloom_filter createtablebftest(kInt64,xInt64)Engine=MergeTreeorderbyk;insertintobftestselectnumber,rand64()%565656fromnumbers(10000000);insertintobftestselectnumber,rand64()%565656fromnumbers(100000000);selectcount()frombftestwherex=42;┌─count()─┐│201│└─────────┘1rowsinset.Elapsed:0.243sec.Processed110.00millionrowsaltertablebftestaddindexix1(x)TYPEbloom_filterGRANULARITY1;altertablebftestmaterializeindexix1;selectcount()frombftestwherex=42;┌─count()─┐│201│└─────────┘1rowsinset.Elapsed:0.056sec.Processed3.68millionrowsminmax createtablebftest(kInt64,xInt64)Engine=MergeTreeorderbyk;-- data is in x column is correlated with the primary key insertintobftestselectnumber,number*2fromnumbers(100000000);altertablebftestaddindexix1(x)TYPEminmaxGRANULARITY1;altertablebftestmaterializeindexix1;selectcount()frombftestwherex=42;1rowsinset.Elapsed:0.004sec.Processed8.19thousandrowsprojection createtablebftest(kInt64,xInt64,SString)Engine=MergeTreeorderbyk;insertintobftestselectnumber,rand64()%565656,''fromnumbers(10000000);insertintobftestselectnumber,rand64()%565656,''fromnumbers(100000000);altertablebftestaddprojectionp1(selectk,xorderbyx);altertablebftestmaterializeprojectionp1settingsmutations_sync=1;setallow_experimental_projection_optimization=1;-- projection selectcount()frombftestwherex=42;1rowsinset.Elapsed:0.002sec.Processed24.58thousandrows-- no projection select*frombftestwherex=42formatNull;0rowsinset.Elapsed:0.432sec.Processed110.00millionrows-- projection select*frombftestwherekin(selectkfrombftestwherex=42)formatNull;0rowsinset.Elapsed:0.316sec.Processed1.50millionrows","categories":"","description":"","excerpt":"bloom_filter …","ref":"/altinity-kb-queries-and-syntax/skip-indexes/skip-indexes-examples/","tags":"","title":"Skip indexes examples"},{"body":"Sparse_hashed layout is supposed to save memory but has some downsides. We can test how much slower SPARSE_HASHED than HASHED is with the following:\ncreatetableorders(idUInt64,priceFloat64)Engine=MergeTree()orderbyid;insertintoordersselectnumber,0fromnumbers(5000000);CREATEDICTIONARYorders_hashed(idUInt64,priceFloat64)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEordersDB'default'USER'default'))LIFETIME(MIN0MAX0)LAYOUT(HASHED());CREATEDICTIONARYorders_sparse(idUInt64,priceFloat64)PRIMARYKEYidSOURCE(CLICKHOUSE(HOST'localhost'PORT9000TABLEordersDB'default'USER'default'))LIFETIME(MIN0MAX0)LAYOUT(SPARSE_HASHED());SELECTname,type,status,element_count,formatReadableSize(bytes_allocated)ASRAMFROMsystem.dictionariesWHEREnameLIKE'orders%'┌─name──────────┬─type─────────┬─status─┬─element_count─┬─RAM────────┐│orders_sparse│SparseHashed│LOADED│5000000│84.29MiB││orders_hashed│Hashed│LOADED│5000000│256.00MiB│└───────────────┴──────────────┴────────┴───────────────┴────────────┘SELECTsum(dictGet('default.orders_hashed','price',toUInt64(number)))ASresFROMnumbers(10000000)┌─res─┐│0│└─────┘1rowsinset.Elapsed:0.279sec.Processed10.02millionrows...SELECTsum(dictGet('default.orders_sparse','price',toUInt64(number)))ASresFROMnumbers(10000000)┌─res─┐│0│└─────┘1rowsinset.Elapsed:1.085sec.Processed10.02millionrows...As you can see SPARSE_HASHED is memory efficient and use about 3 times less memory (!!!) but is almost 4 times slower. But this is the ultimate case because this test does not read data from the disk (no MergeTree table involved).\nWe encourage you to test SPARSE_HASHED against your real queries, because it able to save a lot of memory and have larger (in rows) external dictionaries.\n","categories":"","description":"SPARSE_HASHED VS HASHED\n","excerpt":"SPARSE_HASHED VS HASHED\n","ref":"/altinity-kb-dictionaries/altinity-kb-sparse_hashed-vs-hashed/","tags":"","title":"SPARSE_HASHED VS HASHED"},{"body":"ClickHouse doesn’t probe CA path which is default on CentOS and Amazon Linux.\nClickHouse client cat /etc/clickhouse-client/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cclient\u003e \u003c!-- Used for connection to server's secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e ClickHouse server cat /etc/clickhouse-server/conf.d/openssl-ca.xml \u003cconfig\u003e \u003copenSSL\u003e \u003cserver\u003e \u003c!-- Used for https server AND secure tcp port --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/server\u003e \u003cclient\u003e \u003c!-- Used for connecting to https dictionary source and secured Zookeeper communication --\u003e \u003ccaConfig\u003e/etc/ssl/certs\u003c/caConfig\u003e \u003c/client\u003e \u003c/openSSL\u003e \u003c/config\u003e https://github.com/ClickHouse/ClickHouse/issues/17803\nhttps://github.com/ClickHouse/ClickHouse/issues/18869\n","categories":"","description":"SSL connection unexpectedly closed\n","excerpt":"SSL connection unexpectedly closed\n","ref":"/altinity-kb-setup-and-maintenance/ssl-connection-unexpectedly-closed/","tags":"","title":"SSL connection unexpectedly closed"},{"body":"Nested structures In certain conditions it could make sense to collapse one of dimensions to set of arrays. It’s usually profitable to do if this dimension is not commonly used in queries. It would reduce amount of rows in aggregated table and speed up queries which doesn’t care about this dimension in exchange of aggregation performance by collapsed dimension.\nCREATETABLEtraffic(`key1`UInt32,`key2`UInt32,`port`UInt16,`bits_in`UInt32CODEC(T64,LZ4),`bits_out`UInt32CODEC(T64,LZ4),`packets_in`UInt32CODEC(T64,LZ4),`packets_out`UInt32CODEC(T64,LZ4))ENGINE=SummingMergeTreeORDERBY(key1,key2,port);INSERTINTOtrafficSELECTnumber%1000,intDiv(number,10000),rand()%20,rand()%753,rand64()%800,rand()%140,rand64()%231FROMnumbers(100000000);CREATETABLEdefault.traffic_map(`key1`UInt32,`key2`UInt32,`bits_in`UInt32CODEC(T64,LZ4),`bits_out`UInt32CODEC(T64,LZ4),`packets_in`UInt32CODEC(T64,LZ4),`packets_out`UInt32CODEC(T64,LZ4),`portMap.port`Array(UInt16),`portMap.bits_in`Array(UInt32)CODEC(T64,LZ4),`portMap.bits_out`Array(UInt32)CODEC(T64,LZ4),`portMap.packets_in`Array(UInt32)CODEC(T64,LZ4),`portMap.packets_out`Array(UInt32)CODEC(T64,LZ4))ENGINE=SummingMergeTreeORDERBY(key1,key2);INSERTINTOtraffic_mapWITHrand()%20ASportSELECTnumber%1000ASkey1,intDiv(number,10000)ASkey2,rand()%753ASbits_in,rand64()%800ASbits_out,rand()%140ASpackets_in,rand64()%231ASpackets_out,[port],[bits_in],[bits_out],[packets_in],[packets_out]FROMnumbers(100000000);┌─table───────┬─column──────────────┬─────rows─┬─compressed─┬─uncompressed─┬──ratio─┐│traffic│bits_out│80252317│109.09MiB│306.14MiB│2.81││traffic│bits_in│80252317│108.34MiB│306.14MiB│2.83││traffic│port│80252317│99.21MiB│153.07MiB│1.54││traffic│packets_out│80252317│91.36MiB│306.14MiB│3.35││traffic│packets_in│80252317│84.61MiB│306.14MiB│3.62││traffic│key2│80252317│47.88MiB│306.14MiB│6.39││traffic│key1│80252317│1.38MiB│306.14MiB│221.42││traffic_map│portMap.bits_out│10000000│108.96MiB│306.13MiB│2.81││traffic_map│portMap.bits_in│10000000│108.32MiB│306.13MiB│2.83││traffic_map│portMap.port│10000000│92.00MiB│229.36MiB│2.49││traffic_map│portMap.packets_out│10000000│90.95MiB│306.13MiB│3.37││traffic_map│portMap.packets_in│10000000│84.19MiB│306.13MiB│3.64││traffic_map│key2│10000000│23.46MiB│38.15MiB│1.63││traffic_map│bits_in│10000000│15.59MiB│38.15MiB│2.45││traffic_map│bits_out│10000000│15.59MiB│38.15MiB│2.45││traffic_map│packets_out│10000000│13.22MiB│38.15MiB│2.89││traffic_map│packets_in│10000000│12.62MiB│38.15MiB│3.02││traffic_map│key1│10000000│180.29KiB│38.15MiB│216.66│└─────────────┴─────────────────────┴──────────┴────────────┴──────────────┴────────┘-- Queries SELECTkey1,sum(packets_in),sum(bits_out)FROMtrafficGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.488sec.Processed80.25millionrows,963.03MB(164.31millionrows/s.,1.97GB/s.)SELECTkey1,sum(packets_in),sum(bits_out)FROMtraffic_mapGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.063sec.Processed10.00millionrows,120.00MB(159.43millionrows/s.,1.91GB/s.)SELECTkey1,port,sum(packets_in),sum(bits_out)FROMtrafficGROUPBYkey1,portFORMAT`Null`0rowsinset.Elapsed:0.668sec.Processed80.25millionrows,1.12GB(120.14millionrows/s.,1.68GB/s.)WITHarrayJoin(arrayZip(untuple(sumMap(portMap.port,portMap.packets_in,portMap.bits_out))))AStplSELECTkey1,tpl.1ASport,tpl.2ASpackets_in,tpl.3ASbits_outFROMtraffic_mapGROUPBYkey1FORMAT`Null`0rowsinset.Elapsed:0.915sec.Processed10.00millionrows,1.08GB(10.93millionrows/s.,1.18GB/s.)","categories":"","description":"SummingMergeTree\n","excerpt":"SummingMergeTree\n","ref":"/engines/mergetree-table-engine-family/summingmergetree/","tags":"","title":"SummingMergeTree"},{"body":"Symptom: clickhouse don’t start with a message DB::Exception: Suspiciously many broken parts to remove.\nCause: That exception is just a safeguard check/circuit breaker, triggered when clickhouse detects a lot of broken parts during server startup.\nParts are considered broken if they have bad checksums or some files are missing or malformed. Usually, that means the data was corrupted on the disk.\nWhy data could be corrupted?\n  the most often reason is a hard restart of the system, leading to a loss of the data which was not fully flushed to disk from the system page cache. Please be aware that by default ClickHouse doesn’t do fsync, so data is considered inserted after it was passed to the Linux page cache. See fsync-related settings in ClickHouse.\n  it can also be caused by disk failures, maybe there are bad blocks on hard disk, or logical problems, or some raid issue. Check system journals, use fsck / mdadm and other standard tools to diagnose the disk problem.\n  other reasons: manual intervention/bugs etc, for example, the data files or folders are removed by mistake or moved to another folder.\n  Action:   If you ok to accept the data loss: set up force_restrore_data flag and clickhouse will move the parts to detached.\nsudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data then restart clickhouse, the table will be attached, and the broken parts will be detached, which means the data from those parts will not be available for the selects. You can see the list of those parts in the system.detached_parts table and drop them if needed using ALTER TABLE ... DROP DETACHED PART ... commands.\nIf you are ok to tolerate bigger losses automatically you can change that safeguard configuration to be less sensitive by increasing max_suspicious_broken_parts setting:\ncat /etc/clickhouse-server/config.d/max_suspicious_broken_parts.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cmerge_tree\u003e \u003cmax_suspicious_broken_parts\u003e50\u003c/max_suspicious_broken_parts\u003e \u003c/merge_tree\u003e \u003c/yandex\u003e this limit is set to 10 by default, we can set a bigger value (50 or 100 or more), but the data will lose because of the corruption.\nCheck also a similar setting max_suspicious_broken_parts_bytes.\nSee https://clickhouse.com/docs/en/operations/settings/merge-tree-settings/\n  If you can’t accept the data loss - you should recover data from backups / re-insert it once again etc.\nIf you don’t want to tolerate automatic detaching of broken parts, you can set max_suspicious_broken_parts_bytes and max_suspicious_broken_parts to 0.\n  Scenario illustrating / testing  Create table  create table t111(A UInt32) Engine=MergeTree order by A settings max_suspicious_broken_parts=1; insert into t111 select number from numbers(100000); Detach the table and make Data corruption  detach table t111; cd /var/lib/clickhouse/data/default/t111/all_*** make data file corruption:\n\u003e data.bin repeat for 2 or more data files.\nAttach the table:  attach table t111; Received exception from server (version 21.12.3): Code: 231. DB::Exception: Received from localhost:9000. DB::Exception: Suspiciously many (2) broken parts to remove.. (TOO_MANY_UNEXPEC setup force_restrore_data flag  sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data sudo service clickhouse-server restart then the table t111 will be attached lost the corrupted data.\n","categories":"","description":"Suspiciously many broken parts error during the server startup.\n","excerpt":"Suspiciously many broken parts error during the server startup.\n","ref":"/altinity-kb-setup-and-maintenance/suspiciously-many-broken-parts/","tags":"","title":"Suspiciously many broken parts"},{"body":" Note 1: System database stores virtual tables (parts, tables, columns, etc.) and *_log tables.\nVirtual tables do not persist on disk. They reflect ClickHouse memory (c++ structures). They cannot be changed or removed.\nLog tables are named with postfix *_log and have the MergeTree engine.\nYou can drop / rename / truncate *_log tables at any time. ClickHouse will recreate them in about 7 seconds (flush period).\n  Note 2: Log tables with numeric postfixes (_1 / 2 / 3 …) query_log_1 query_thread_log_3 are results of Clickhouse upgrades. When a new version of Clickhouse starts and discovers that a system log table’s schema is incompatible with a new schema, then Clickhouse renames the old *_log table to the name with the prefix and creates a table with the new schema. You can drop such tables if you don’t need such historic data.\n You can disable all / any of them Do not create log tables at all (a restart is needed for these changes to take effect).\n$ cat /etc/clickhouse-server/config.d/z_log_disable.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003casynchronous_metric_log remove=\"1\"/\u003e \u003cmetric_log remove=\"1\"/\u003e \u003cquery_thread_log remove=\"1\" /\u003e \u003cquery_log remove=\"1\" /\u003e \u003cquery_views_log remove=\"1\" /\u003e \u003cpart_log remove=\"1\"/\u003e \u003csession_log remove=\"1\"/\u003e \u003ctext_log remove=\"1\" /\u003e \u003ctrace_log remove=\"1\"/\u003e \u003ccrash_log remove=\"1\"/\u003e \u003copentelemetry_span_log remove=\"1\"/\u003e \u003czookeeper_log remove=\"1\"/\u003e \u003c/yandex\u003e We do not recommend removing query_log and query_thread_log as queries’ (they have very useful information for debugging), and logging can be easily turned off without a restart through user profiles:\n$ cat /etc/clickhouse-server/users.d/z_log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003c!-- normally it's better to keep it turned on! --\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e Hint: z_log_disable.xml is named with z_ in the beginning, it means this config will be applied the last and will override all other config files with these sections (config are applied in alphabetical order).\nYou can also configure these settings to reduce the amount of data in the system.query_log table:\nname | value | description ----------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------- log_queries_min_type | QUERY_START | Minimal type in query_log to log, possible values (from low to high): QUERY_START, QUERY_FINISH, EXCEPTION_BEFORE_START, EXCEPTION_WHILE_PROCESSING. log_queries_min_query_duration_ms | 0 | Minimal time for the query to run, to get to the query_log/query_thread_log. log_queries_cut_to_length | 100000 | If query length is greater than specified threshold (in bytes), then cut query when writing to query log. Also limit length of printed query in ordinary text log. log_profile_events | 1 | Log query performance statistics into the query_log and query_thread_log. log_query_settings | 1 | Log query settings into the query_log. log_queries_probability | 1 | Log queries with the specified probabality. You can configure TTL Example for query_log. It drops partitions with data older than 14 days:\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cquery_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cengine\u003eENGINE = MergeTree PARTITION BY (event_date) ORDER BY (event_time) TTL event_date + INTERVAL 14 DAY DELETE SETTINGS ttl_only_drop_parts=1 \u003c/engine\u003e \u003cflush_interval_milliseconds\u003e7500\u003c/flush_interval_milliseconds\u003e \u003c/query_log\u003e \u003c/yandex\u003e After that you need to restart ClickHouse and drop or rename the existing system.query_log table, then CH creates a new table with these settings.\nRENAMETABLEsystem.query_logTOsystem.query_log_1;Important part here is a daily partitioning PARTITION BY (event_date) and ttl_only_drop_parts=1. In this case ClickHouse drops whole partitions. Dropping of partitions is very easy operation for CPU / Disk I/O.\nUsual TTL (without ttl_only_drop_parts=1) is heavy CPU / Disk I/O consuming operation which re-writes data parts without expired rows.\nYou can add TTL without ClickHouse restart (and table dropping or renaming):\nALTERTABLEsystem.query_logMODIFYSETTINGttl_only_drop_parts=1;ALTERTABLEsystem.query_logMODIFYTTLevent_date+INTERVAL14DAY;But in this case ClickHouse will drop only whole monthly partitions (will store data older than 14 days).\nOne more way to configure TTL for system tables This way just adds TTL to a table and leaves monthly (default) partitioning (will store data older than 14 days).\n$ cat /etc/clickhouse-server/config.d/query_log_ttl.xml \u003c?xml version=\"1.0\"?\u003e \u003cyandex\u003e \u003cquery_log\u003e \u003cdatabase\u003esystem\u003c/database\u003e \u003ctable\u003equery_log\u003c/table\u003e \u003cttl\u003eevent_date + INTERVAL 30 DAY DELETE\u003c/ttl\u003e \u003c/query_log\u003e \u003c/yandex\u003e After that you need to restart ClickHouse and drop or rename the existing system.query_log table, then CH creates a new table with this TTL setting.\nYou can disable logging on a session level or in user’s profile (for all or specific users) But only for logs generated on session level (query_log / query_thread_log)\nIn this case a restart is not needed.\nLet’s disable query logging for all users (profile = default, all other profiles inherit it).\ncat /etc/clickhouse-server/users.d/log_queries.xml \u003cyandex\u003e \u003cprofiles\u003e \u003cdefault\u003e \u003clog_queries\u003e0\u003c/log_queries\u003e \u003clog_query_threads\u003e0\u003c/log_query_threads\u003e \u003c/default\u003e \u003c/profiles\u003e \u003c/yandex\u003e ","categories":"","description":"System tables eat my disk\n","excerpt":"System tables eat my disk\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-system-tables-eat-my-disk/","tags":"","title":"System tables eat my disk"},{"body":"Sometimes your Distributed DDL queries are being stuck, and not executing on all or subset of nodes, there are a lot of possible reasons for that kind of behavior, so it would take some time and effort to investigate.\nPossible reasons Clickhouse node can’t recognize itself SELECT*FROMsystem.clusters;-- check is_local column, it should have 1 for itself getent hosts clickhouse.local.net # or other name which should be local hostname --fqdn  cat /etc/hosts cat /etc/hostname Debian / Ubuntu There is an issue in Debian based images, when hostname being mapped to 127.0.1.1 address which doesn’t literally match network interface and clickhouse fails to detect this address as local.\nhttps://github.com/ClickHouse/ClickHouse/issues/23504\nPrevious task is being executed and taking some time It’s usually some heavy operations like merges, mutations, alter columns, so it make sense to check those tables:\nSHOWPROCESSLIST;SELECT*FROMsystem.merges;SELECT*FROMsystem.mutations;In that case, you can just wait completion of previous task.\nPrevious task is stuck because of some error In that case, the first step is to understand which exact task is stuck and why. There are some queries which can help with that.\n-- list of all distributed ddl queries, path can be different in your installation SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/';-- information about specific task. SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/';SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/'ANDname='query-0000001000';-- How many nodes executed this task SELECTname,numChildrenasfinished_nodesFROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/'ANDname='finished';┌─name─────┬─finished_nodes─┐│finished│0│└──────────┴────────────────┘-- The nodes that are running the task SELECTname,value,ctime,mtimeFROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/active/';-- What was the result for the finished nodes SELECTname,value,ctime,mtimeFROMsystem.zookeeperWHEREpath='/clickhouse/task_queue/ddl/query-0000001000/finished/';-- Latest successfull executed tasks from query_log. SELECTqueryFROMsystem.query_logWHEREqueryLIKE'%ddl_entry%'ANDtype=2ORDERBYevent_timeDESCLIMIT5;SELECTFQDN(),*FROMclusterAllReplicas('cluster',system.metrics)WHEREmetricLIKE'%MaxDDLEntryID%'┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐│chi-ab.svc.cluster.local│MaxDDLEntryID│1468│MaxprocessedDDLentryofDDLWorker.│└──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐│chi-ab.svc.cluster.local│MaxDDLEntryID│1468│MaxprocessedDDLentryofDDLWorker.│└──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘┌─FQDN()───────────────────┬─metric────────┬─value─┬─description───────────────────────────┐│chi-ab.svc.cluster.local│MaxDDLEntryID│1468│MaxprocessedDDLentryofDDLWorker.│└──────────────────────────┴───────────────┴───────┴───────────────────────────────────────┘-- Information about task execution from logs. grep-C40\"ddl_entry\"/var/log/clickhouse-server/clickhouse-server*.logIssues that can prevent the task execution Obsolete replicas left in zookeeper.\nSELECTdatabase,table,zookeeper_path,replica_pathzookeeperFROMsystem.replicasWHEREtotal_replicas!=active_replicas;SELECT*FROMsystem.zookeeperWHEREpath='/clickhouse/cluster/tables/01/database/table/replicas';SYSTEMDROPREPLICA'replica_name';SYSTEMSTOPREPLICATIONQUEUES;SYSTEMSTARTREPLICATIONQUEUES;https://clickhouse.tech/docs/en/sql-reference/statements/system/#query_language-system-drop-replica\nTask were removed from DDL queue, but left in Replicated*MergeTree table queue.\ngrep -C 40 \"ddl_entry\" /var/log/clickhouse-server/clickhouse-server*.log  /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:28.956888 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053555 [ 599 ] {} \u003cError\u003e DDLWorker: ZooKeeper error: Code: 999, e.displayText() = Coordination::Exception: No node, Stack trace (when copying this message, always include the lines below): /var/log/clickhouse-server/clickhouse-server.log- /var/log/clickhouse-server/clickhouse-server.log-0. Coordination::Exception::Exception(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, Coordination::Error, int) @ 0xfb2f6b3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-1. Coordination::Exception::Exception(Coordination::Error) @ 0xfb2fb56 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:2. DB::DDLWorker::createStatusDirs(std::__1::basic_string\u003cchar, std::__1::char_traits\u003cchar\u003e, std::__1::allocator\u003cchar\u003e \u003e const\u0026, std::__1::shared_ptr\u003czkutil::ZooKeeper\u003e const\u0026) @ 0xeb3127a in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:3. DB::DDLWorker::processTask(DB::DDLTask\u0026) @ 0xeb36c96 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log:4. DB::DDLWorker::enqueueTask(std::__1::unique_ptr\u003cDB::DDLTask, std::__1::default_delete\u003cDB::DDLTask\u003e \u003e) @ 0xeb35f22 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-5. ? @ 0xeb47aed in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-6. ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::worker(std::__1::__list_iterator\u003cThreadFromGlobalPool, void*\u003e) @ 0x8633bcd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-7. ThreadFromGlobalPool::ThreadFromGlobalPool\u003cvoid ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u003e(void\u0026\u0026, void ThreadPoolImpl\u003cThreadFromGlobalPool\u003e::scheduleImpl\u003cvoid\u003e(std::__1::function\u003cvoid ()\u003e, int, std::__1::optional\u003cunsigned long\u003e)::'lambda1'()\u0026\u0026...)::'lambda'()::operator()() @ 0x863612f in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-8. ThreadPoolImpl\u003cstd::__1::thread\u003e::worker(std::__1::__list_iterator\u003cstd::__1::thread, void*\u003e) @ 0x8630ffd in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-9. ? @ 0x8634bb3 in /usr/bin/clickhouse /var/log/clickhouse-server/clickhouse-server.log-10. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so /var/log/clickhouse-server/clickhouse-server.log-11. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so /var/log/clickhouse-server/clickhouse-server.log- (version 21.1.8.30 (official build)) /var/log/clickhouse-server/clickhouse-server.log:2021.05.04 12:41:29.053951 [ 599 ] {} \u003cDebug\u003e DDLWorker: Processing task query-0000211211 (ALTER TABLE db.table_local ON CLUSTER `all-replicated` DELETE WHERE id = 1) Context of this problem is:\n Constant pressure of cheap ON CLUSTER DELETE queries. One replica was down for a long amount of time (multiple days). Because of pressure on the DDL queue, it purged old records due to the task_max_lifetime setting. When a lagging replica comes up, it’s fail’s execute old queries from DDL queue, because at this point they were purged from it.  Solution:\n Reload/Restore this replica from scratch.  ","categories":"","description":"\"There are N unfinished hosts (0 of them are currently active).\"\n","excerpt":"\"There are N unfinished hosts (0 of them are currently active).\"\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/there-are-n-unfinished-hosts-0-of-them-are-currently-active/","tags":"","title":"There are N unfinished hosts (0 of them are currently active)."},{"body":"Count threads used by clickhouse-server cat /proc/$(pidof -s clickhouse-server)/status | grep Threads Threads: 103  ps hH $(pidof -s clickhouse-server) | wc -l 103  ps hH -AF | grep clickhouse | wc -l 116 Thread counts by type (using ps \u0026 clickhouse-local) ps H -o 'tid comm' $(pidof -s clickhouse-server) | tail -n +2 | awk '{ printf(\"%s\\t%s\\n\", $1, $2) }' | clickhouse-local -S \"threadid UInt16, name String\" -q \"SELECT name, count() FROM table GROUP BY name WITH TOTALS ORDER BY count() DESC FORMAT PrettyCompact\" Threads used by running queries: SELECTquery,length(thread_ids)ASthreads_countFROMsystem.processesORDERBYthreads_count;Thread pools limits \u0026 usage SELECTname,valueFROMsystem.settingsWHEREnameLIKE'%pool%'┌─name─────────────────────────────────────────┬─value─┐│connection_pool_max_wait_ms│0││distributed_connections_pool_size│1024││background_buffer_flush_schedule_pool_size│16││background_pool_size│16││background_move_pool_size│8││background_fetches_pool_size│8││background_schedule_pool_size│16││background_message_broker_schedule_pool_size│16││background_distributed_schedule_pool_size│16││postgresql_connection_pool_size│16││postgresql_connection_pool_wait_timeout│-1││odbc_bridge_connection_pool_size│16│└──────────────────────────────────────────────┴───────┘SELECTmetric,valueFROMsystem.metricsWHEREmetricLIKE'Background%'┌─metric──────────────────────────────────┬─value─┐│BackgroundPoolTask│0││BackgroundFetchesPoolTask│0││BackgroundMovePoolTask│0││BackgroundSchedulePoolTask│0││BackgroundBufferFlushSchedulePoolTask│0││BackgroundDistributedSchedulePoolTask│0││BackgroundMessageBrokerSchedulePoolTask│0│└─────────────────────────────────────────┴───────┘SELECT*FROMsystem.asynchronous_metricsWHERElower(metric)LIKE'%thread%'ORDERBYmetricASC┌─metric───────────────────────────────────┬─value─┐│HTTPThreads│0││InterserverThreads│0││MySQLThreads│0││OSThreadsRunnable│2││OSThreadsTotal│2910││PostgreSQLThreads│0││TCPThreads│1││jemalloc.background_thread.num_runs│0││jemalloc.background_thread.num_threads│0││jemalloc.background_thread.run_intervals│0│└──────────────────────────────────────────┴───────┘SELECT*FROMsystem.metricsWHERElower(metric)LIKE'%thread%'ORDERBYmetricASCQueryid:6acbb596-e28f-4f89-94b2-27dccfe88ee9┌─metric─────────────┬─value─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────┐│GlobalThread│151│Numberofthreadsinglobalthreadpool.││GlobalThreadActive│144│Numberofthreadsinglobalthreadpoolrunningatask.││LocalThread│0│Numberofthreadsinlocalthreadpools.Thethreadsinlocalthreadpoolsaretakenfromtheglobalthreadpool.││LocalThreadActive│0│Numberofthreadsinlocalthreadpoolsrunningatask.││QueryThread│0│Numberofqueryprocessingthreads│└────────────────────┴───────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘Stack traces of the working threads from the pools SETallow_introspection_functions=1;WITHarrayMap(x-\u003edemangle(addressToSymbol(x)),trace)ASallSELECTthread_id,query_id,arrayStringConcat(all,'\\n')ASresFROMsystem.stack_traceWHEREresILIKE'%Pool%'FORMATVertical;","categories":"","description":"Threads\n","excerpt":"Threads\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-threads/","tags":"","title":"Threads"},{"body":"Important things to know:\n DateTime inside clickhouse is actually UNIX timestamp always, i.e. number of seconds since 1970-01-01 00:00:00 GMT. Conversion from that UNIX timestamp to a human-readable form and reverse can happen on the client (for native clients) and on the server (for HTTP clients, and for some type of queries, like toString(ts)) Depending on the place where that conversion happened rules of different timezones may be applied. You can check server timezone using SELECT timezone() clickhouse-client also by default tries to use server timezone (see also --use_client_time_zone flag) If you want you can store the timezone name inside the data type, in that case, timestamp \u003c-\u003e human-readable time rules of that timezone will be applied.  SELECTtimezone(),toDateTime(now())ASt,toTypeName(t),toDateTime(now(),'UTC')ASt_utc,toTypeName(t_utc),toUnixTimestamp(t),toUnixTimestamp(t_utc)Row1:──────timezone():Europe/Warsawt:2021-07-1612:50:28toTypeName(toDateTime(now())):DateTimet_utc:2021-07-1610:50:28toTypeName(toDateTime(now(),'UTC')):DateTime('UTC')toUnixTimestamp(toDateTime(now())):1626432628toUnixTimestamp(toDateTime(now(),'UTC')):1626432628Since version 20.4 clickhouse uses embedded tzdata (see https://github.com/ClickHouse/ClickHouse/pull/10425 )\nYou get used tzdata version\nSELECT*FROMsystem.build_optionsWHEREname='TZDATA_VERSION'Queryid:0a9883f0-dadf-4fb1-8b42-8fe93f561430┌─name───────────┬─value─┐│TZDATA_VERSION│2020e│└────────────────┴───────┘and list of available time zones\nSELECT*FROMsystem.time_zonesWHEREtime_zoneLIKE'%Anta%'Queryid:855453d7-eccd-44cb-9631-f63bb02a273c┌─time_zone─────────────────┐│Antarctica/Casey││Antarctica/Davis││Antarctica/DumontDUrville││Antarctica/Macquarie││Antarctica/Mawson││Antarctica/McMurdo││Antarctica/Palmer││Antarctica/Rothera││Antarctica/South_Pole││Antarctica/Syowa││Antarctica/Troll││Antarctica/Vostok││Indian/Antananarivo│└───────────────────────────┘13rowsinset.Elapsed:0.002sec.When the conversion using different rules happen SELECTtimezone()┌─timezone()─┐│UTC│└────────────┘createtablet_with_dt_utc(tsDateTime64(3,'Europe/Moscow'))engine=Log;createtablex(tsString)engine=Null;creatematerializedviewx_mvtot_with_dt_utcasselectparseDateTime64BestEffort(ts)astsfromx;$echo'2021-07-15T05:04:23.733'|clickhouse-client-q'insert into t_with_dt_utc format CSV'-- here client checks the type of the columns, see that it's 'Europe/Moscow' and use conversion according to moscow rules $echo'2021-07-15T05:04:23.733'|clickhouse-client-q'insert into x format CSV'-- here client check tha type of the columns (it is string), and pass string value to the server. -- parseDateTime64BestEffort(ts) uses server default timezone (UTC in my case), and convert the value using UTC rules. -- and the result is 2 different timestamps (when i selecting from that is shows both in 'desired' timezone, forced by column type, i.e. Moscow): SELECT*FROMt_with_dt_utc┌──────────────────────ts─┐│2021-07-1505:04:23.733││2021-07-1508:04:23.733│└─────────────────────────┘Best practice here: use UTC timezone everywhere, OR use the same default timezone for clickhouse server as used by your data\n","categories":"","description":"Time zones\n","excerpt":"Time zones\n","ref":"/altinity-kb-queries-and-syntax/time-zones/","tags":"","title":"Time zones"},{"body":"DROPTABLEtest_ts_interpolation;--- generate test data CREATETABLEtest_ts_interpolationENGINE=LogASSELECT((number*100)+50)-(rand()%100)AStimestamp,transform(rand()%2,[0,1],['A','B'],'')ASts,if(ts='A',timestamp*10,timestamp*100)ASvalueFROMnumbers(1000000);SELECT*FROMtest_ts_interpolation;-- interpolation select with window functions SELECTtimestamp,if(ts='A',toFloat64(value),prev_a.2+(timestamp-prev_a.1)*(next_a.2-prev_a.2)/(next_a.1-prev_a.1))asa_value,if(ts='B',toFloat64(value),prev_b.2+(timestamp-prev_b.1)*(next_b.2-prev_b.2)/(next_b.1-prev_b.1))asb_valueFROM(SELECTtimestamp,ts,value,anyLastIf((timestamp,value),ts='A')OVER(ORDERBYtimestampROWSBETWEENUNBOUNDEDPRECEDINGAND1PRECEDING)ASprev_a,anyLastIf((timestamp,value),ts='A')OVER(ORDERBYtimestampDESCROWSBETWEENUNBOUNDEDPRECEDINGAND1PRECEDING)ASnext_a,anyLastIf((timestamp,value),ts='B')OVER(ORDERBYtimestampROWSBETWEENUNBOUNDEDPRECEDINGAND1PRECEDING)ASprev_b,anyLastIf((timestamp,value),ts='B')OVER(ORDERBYtimestampDESCROWSBETWEENUNBOUNDEDPRECEDINGAND1PRECEDING)ASnext_bFROMtest_ts_interpolation)","categories":"","description":"Time-series alignment with interpolation\n","excerpt":"Time-series alignment with interpolation\n","ref":"/altinity-kb-queries-and-syntax/ts-interpolation/","tags":"","title":"Time-series alignment with interpolation"},{"body":"CREATETABLEtop_with_rest(`k`String,`number`UInt64)ENGINE=Memory;INSERTINTOtop_with_restSELECTtoString(intDiv(number,10)),numberFROMnumbers_mt(10000);Using UNION ALL SELECT*FROM(SELECTk,sum(number)ASresFROMtop_with_restGROUPBYkORDERBYresDESCLIMIT10UNIONALLSELECTNULL,sum(number)ASresFROMtop_with_restWHEREkNOTIN(SELECTkFROMtop_with_restGROUPBYkORDERBYsum(number)DESCLIMIT10))ORDERBYresASC┌─k───┬───res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945│└─────┴───────┘┌─k────┬──────res─┐│null│49000050│└──────┴──────────┘Using arrays WITHtoUInt64(sumIf(sum,isNull(k))-sumIf(sum,isNotNull(k)))AStotalSELECT(arrayJoin(arrayPushBack(groupArrayIf(10)((k,sum),isNotNull(k)),(NULL,total)))AStpl).1ASkey,tpl.2ASresFROM(SELECTtoNullable(k)ASk,sum(number)ASsumFROMtop_with_restGROUPBYkWITHCUBEORDERBYsumDESCLIMIT11)ORDERBYresASC┌─key──┬──────res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945││null│49000050│└──────┴──────────┘Using window functions (starting from 21.1) SETallow_experimental_window_functions=1;SELECTkASkey,If(isNotNull(key),sum,toUInt64(sum-wind))ASresFROM(SELECT*,sumIf(sum,isNotNull(k))OVER()ASwindFROM(SELECTtoNullable(k)ASk,sum(number)ASsumFROMtop_with_restGROUPBYkWITHCUBEORDERBYsumDESCLIMIT11))ORDERBYresASC┌─key──┬──────res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945││null│49000050│└──────┴──────────┘SELECTk,sum(sum)ASresFROM(SELECTif(rn\u003e10,NULL,k)ASk,sumFROM(SELECTk,sum,row_number()OVER()ASrnFROM(SELECTk,sum(number)ASsumFROMtop_with_restGROUPBYkORDERBYsumDESC)))GROUPBYkORDERBYres┌─k────┬──────res─┐│990│99045││991│99145││992│99245││993│99345││994│99445││995│99545││996│99645││997│99745││998│99845││999│99945││null│49000050│└──────┴──────────┘","categories":"","description":"Top N \u0026 Remain\n","excerpt":"Top N \u0026 Remain\n","ref":"/altinity-kb-queries-and-syntax/top-n-and-remain/","tags":"","title":"Top N \u0026 Remain"},{"body":"Log of query execution Controlled by session level setting send_logs_level Possible values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none' Can be used with clickhouse-client in both interactive and non-interactive mode.\n$ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" [LAPTOP] 2021.04.29 00:05:31.425842 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e executeQuery: (from 127.0.0.1:42590, using production parser) SELECT sum(number) FROM numbers(1000) [LAPTOP] 2021.04.29 00:05:31.426281 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.* [LAPTOP] 2021.04.29 00:05:31.426648 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e InterpreterSelectQuery: FetchColumns -\u003e Complete [LAPTOP] 2021.04.29 00:05:31.427132 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e AggregatingTransform: Aggregating [LAPTOP] 2021.04.29 00:05:31.427187 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Aggregation method: without_key [LAPTOP] 2021.04.29 00:05:31.427220 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e AggregatingTransform: Aggregated. 1000 to 1 rows (from 7.81 KiB) in 0.0004469 sec. (2237637.0552696353 rows/sec., 17.07 MiB/sec.) [LAPTOP] 2021.04.29 00:05:31.427233 [ 25448 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cTrace\u003e Aggregator: Merging aggregated data [LAPTOP] 2021.04.29 00:05:31.427875 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cInformation\u003e executeQuery: Read 1000 rows, 7.81 KiB in 0.0019463 sec., 513795 rows/sec., 3.92 MiB/sec. [LAPTOP] 2021.04.29 00:05:31.427898 [ 25316 ] {14b0646d-8a6e-4b2f-9b13-52a218cf43ba} \u003cDebug\u003e MemoryTracker: Peak memory usage (for query): 0.00 B. 499500  $ clickhouse-client -mn --send_logs_level='trace' --query \"SELECT sum(number) FROM numbers(1000)\" 2\u003e ./query.log LAPTOP.localdomain:)SETsend_logs_level='trace';SETsend_logs_level='trace'Queryid:cbbffc02-283e-48ef-93e2-8b3baced6689Ok.0rowsinset.Elapsed:0.003sec.LAPTOP.localdomain:)SELECTsum(number)FROMnumbers(1000);SELECTsum(number)FROMnumbers(1000)Queryid:d3db767b-34e9-4252-9f90-348cf958f822[LAPTOP]2021.04.2900:06:51.673836[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eexecuteQuery:(from127.0.0.1:43116,usingproductionparser)SELECTsum(number)FROMnumbers(1000);[LAPTOP]2021.04.2900:06:51.674167[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eContextAccess(default):Accessgranted:CREATETEMPORARYTABLEON*.*[LAPTOP]2021.04.2900:06:51.674419[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eInterpreterSelectQuery:FetchColumns-\u003eComplete[LAPTOP]2021.04.2900:06:51.674748[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregatingTransform:Aggregating[LAPTOP]2021.04.2900:06:51.674781[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregator:Aggregationmethod:without_key[LAPTOP]2021.04.2900:06:51.674855[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eAggregatingTransform:Aggregated.1000to1rows(from7.81KiB)in0.0003299sec.(3031221.582297666rows/sec.,23.13MiB/sec.)[LAPTOP]2021.04.2900:06:51.674883[25449]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cTrace\u003eAggregator:Mergingaggregateddata┌─sum(number)─┐│499500│└─────────────┘[LAPTOP]2021.04.2900:06:51.675481[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cInformation\u003eexecuteQuery:Read1000rows,7.81KiBin0.0015799sec.,632951rows/sec.,4.83MiB/sec.[LAPTOP]2021.04.2900:06:51.675508[25316]{d3db767b-34e9-4252-9f90-348cf958f822}\u003cDebug\u003eMemoryTracker:Peakmemoryusage(forquery):0.00B.1rowsinset.Elapsed:0.007sec.Processed1.00thousandrows,8.00KB(136.43thousandrows/s.,1.09MB/s.)Flamegraph https://www.speedscope.app/\nWITH'95578e1c-1e93-463c-916c-a1a8cdd08198'ASquery,min(min)ASstart_value,max(max)ASend_value,groupUniqArrayArrayArray(trace_arr)ASuniq_frames,arrayMap((x,a,b)-\u003e('sampled',b,'none',start_value,end_value,arrayMap(s-\u003ereverse(arrayMap(y-\u003etoUInt32(indexOf(uniq_frames,y)-1),s)),x),a),groupArray(trace_arr),groupArray(weights),groupArray(trace_type))ASsamplesSELECTconcat('clickhouse-server@',version())ASexporter,'https://www.speedscope.app/file-format-schema.json'AS`$schema`,concat('Clickhouse query id: ',query)ASname,CAST(samples,'Array(Tuple(type String, name String, unit String, startValue UInt64, endValue UInt64, samples Array(Array(UInt32)), weights Array(UInt32)))')ASprofiles,CAST(tuple(arrayMap(x-\u003e(demangle(addressToSymbol(x)),addressToLine(x)),uniq_frames)),'Tuple(frames Array(Tuple(name String, line String)))')ASsharedFROM(SELECTmin(min_ns)ASmin,trace_type,max(max_ns)ASmax,groupArray(trace)AStrace_arr,groupArray(cnt)ASweightsFROM(SELECTmin(timestamp_ns)ASmin_ns,max(timestamp_ns)ASmax_ns,trace,trace_type,count()AScntFROMsystem.trace_logWHEREquery_id=queryGROUPBYtrace_type,trace)GROUPBYtrace_type)SETTINGSallow_introspection_functions=1,output_format_json_named_tuples_as_objects=1FORMATJSONEachRowSETTINGSoutput_format_json_named_tuples_as_objects=1","categories":"","description":"Troubleshooting\n","excerpt":"Troubleshooting\n","ref":"/altinity-kb-queries-and-syntax/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"","categories":"","description":"TTL\n","excerpt":"TTL\n","ref":"/altinity-kb-queries-and-syntax/ttl/","tags":"","title":"TTL"},{"body":"Example with MergeTree table CREATETABLEtest_ttl_group_by(`key`UInt32,`ts`DateTime,`value`UInt32,`min_value`UInt32DEFAULTvalue,`max_value`UInt32DEFAULTvalue)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,toStartOfDay(ts))TTLts+interval30dayGROUPBYkey,toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts));During TTL merges Clickhouse re-calculates values of columns in the SET section.\nGROUP BY section should be a prefix of a table’s ORDER BY.\n-- stop merges to demonstrate data before / after -- a rolling up SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()+number,1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval60day+number,2FROMnumbers(100);SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│100│200│2│2││202104│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│5│200│2│2││202104│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘As you can see 100 rows were rolled up into 5 rows (key has 5 values) for rows older than 30 days.\nExample with SummingMergeTree table CREATETABLEtest_ttl_group_by(`key1`UInt32,`key2`UInt32,`ts`DateTime,`value`UInt32,`min_value`SimpleAggregateFunction(min,UInt32)DEFAULTvalue,`max_value`SimpleAggregateFunction(max,UInt32)DEFAULTvalue)ENGINE=SummingMergeTreePARTITIONBYtoYYYYMM(ts)PRIMARYKEY(key1,key2,toStartOfDay(ts))ORDERBY(key1,key2,toStartOfDay(ts),ts)TTLts+interval30dayGROUPBYkey1,key2,toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts));-- stop merges to demonstrate data before / after -- a rolling up SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60),1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60),1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60-toIntervalDay(60)),2FROMnumbers(100);INSERTINTOtest_ttl_group_by(key1,key2,ts,value)SELECT1,1,toStartOfMinute(now()+number*60-toIntervalDay(60)),2FROMnumbers(100);SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│200│400│2│2││202104│200│200│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202102│1│400│2│2││202104│100│200│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘During merges Clickhouse re-calculates ts columns as min(toStartOfDay(ts)). It’s possible only for the last column of SummingMergeTree ORDER BY section ORDER BY (key1, key2, toStartOfDay(ts), ts) otherwise it will break the order of rows in the table.\nMultilevel TTL Group by CREATETABLEtest_ttl_group_by(`key`UInt32,`ts`DateTime,`value`UInt32,`min_value`UInt32DEFAULTvalue,`max_value`UInt32DEFAULTvalue)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,toStartOfWeek(ts),toStartOfDay(ts),toStartOfHour(ts))TTLts+interval1hourGROUPBYkey,toStartOfWeek(ts),toStartOfDay(ts),toStartOfHour(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfHour(ts)),ts+interval1dayGROUPBYkey,toStartOfWeek(ts),toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts)),ts+interval30dayGROUPBYkey,toStartOfWeek(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfWeek(ts));SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()+number,1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval2hour+number,2FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval2day+number,3FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval2month+number,4FROMnumbers(100);SELECTtoYYYYMMDD(ts)ASd,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYdORDERBYd;┌────────d─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│20210616│100│400│4│4││20210814│100│300│3│3││20210816│200│300│1│2│└──────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;SELECTtoYYYYMMDD(ts)ASd,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYdORDERBYd;┌────────d─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│20210613│5│400│4│4││20210814│5│300│3│3││20210816│105│300│1│2│└──────────┴─────────┴────────────┴────────────────┴────────────────┘TTL GROUP BY + DELETE CREATETABLEtest_ttl_group_by(`key`UInt32,`ts`DateTime,`value`UInt32,`min_value`UInt32DEFAULTvalue,`max_value`UInt32DEFAULTvalue)ENGINE=MergeTreePARTITIONBYtoYYYYMM(ts)ORDERBY(key,toStartOfDay(ts))TTLts+interval180day,ts+interval30dayGROUPBYkey,toStartOfDay(ts)SETvalue=sum(value),min_value=min(min_value),max_value=max(max_value),ts=min(toStartOfDay(ts));-- stop merges to demonstrate data before / after -- a rolling up SYSTEMSTOPTTLMERGEStest_ttl_group_by;SYSTEMSTOPMERGEStest_ttl_group_by;INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()+number,1FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval60day+number,2FROMnumbers(100);INSERTINTOtest_ttl_group_by(key,ts,value)SELECTnumber%5,now()-interval200day+number,3FROMnumbers(100);SELECTtoYYYYMM(ts)ASm,count(),sum(value),min(min_value),max(max_value)FROMtest_ttl_group_byGROUPBYm;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202101│100│300│3│3││202106│100│200│2│2││202108│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘SYSTEMSTARTTTLMERGEStest_ttl_group_by;SYSTEMSTARTMERGEStest_ttl_group_by;OPTIMIZETABLEtest_ttl_group_byFINAL;┌──────m─┬─count()─┬─sum(value)─┬─min(min_value)─┬─max(max_value)─┐│202106│5│200│2│2││202108│100│100│1│1│└────────┴─────────┴────────────┴────────────────┴────────────────┘","categories":"","description":"TTL GROUP BY Examples\n","excerpt":"TTL GROUP BY Examples\n","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/","tags":"","title":"TTL GROUP BY Examples"},{"body":"Example how to create a table and define recompression rules CREATETABLEhits(`banner_id`UInt64,`event_time`DateTimeCODEC(Delta,Default),`c_name`String,`c_cost`Float64)ENGINE=MergeTreePARTITIONBYtoYYYYMM(event_time)ORDERBY(banner_id,event_time)TTLevent_time+toIntervalMonth(1)RECOMPRESSCODEC(ZSTD(1)),event_time+toIntervalMonth(6)RECOMPRESSCODEC(ZSTD(6);Default comression is LZ4 https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-compression\nThese TTL rules recompress data after 1 and 6 months.\nCODEC(Delta, Default) – Default means to use default compression (LZ4 -\u003e ZSTD1 -\u003e ZSTD6) in this case.\nExample how to define recompression rules for an existing table CREATETABLEhits(`banner_id`UInt64,`event_time`DateTimeCODEC(Delta,LZ4),`c_name`String,`c_cost`Float64)ENGINE=MergeTreePARTITIONBYtoYYYYMM(event_time)ORDERBY(banner_id,event_time);ALTERTABLEhitsmodifycolumnevent_timeDateTimeCODEC(Delta,Default),modifyTTLevent_time+toIntervalMonth(1)RECOMPRESSCODEC(ZSTD(1)),event_time+toIntervalMonth(6)RECOMPRESSCODEC(ZSTD(6));All columns have implicite default compression from server config, except event_time, that’s why need to change to compression to Default for this column otherwise it won’t be recompressed.\n","categories":"","description":"TTL Recompress example\n","excerpt":"TTL Recompress example\n","ref":"/altinity-kb-queries-and-syntax/ttl/ttl-recompress-example/","tags":"","title":"TTL Recompress example"},{"body":"CREATETABLEtest_update(`key`UInt32,`value`String)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_updateSELECTnumber,concat('value ',toString(number))FROMnumbers(20);SELECT*FROMtest_update;┌─key─┬─value────┐│0│value0││1│value1││2│value2││3│value3││4│value4││5│value5││6│value6││7│value7││8│value8││9│value9││10│value10││11│value11││12│value12││13│value13││14│value14││15│value15││16│value16││17│value17││18│value18││19│value19│└─────┴──────────┘CREATETABLEtest_update_source(`key`UInt32,`value`String)ENGINE=MergeTreeORDERBYkey;INSERTINTOtest_update_sourceVALUES(1,'other value'),(10,'new value');CREATEDICTIONARYupdate_dict(`key`UInt32,`value`String)PRIMARYKEYkeySOURCE(CLICKHOUSE(TABLE'test_update_source'))LIFETIME(MIN0MAX10)LAYOUT(FLAT);SELECTdictGet('default.update_dict','value',toUInt64(1));┌─dictGet('default.update_dict','value',toUInt64(1))─┐│othervalue│└──────────────────────────────────────────────────────┘ALTERTABLEtest_updateUPDATEvalue=dictGet('default.update_dict','value',toUInt64(key))WHEREdictHas('default.update_dict',toUInt64(key));SELECT*FROMtest_update┌─key─┬─value───────┐│0│value0││1│othervalue││2│value2││3│value3││4│value4││5│value5││6│value6││7│value7││8│value8││9│value9││10│newvalue││11│value11││12│value12││13│value13││14│value14││15│value15││16│value16││17│value17││18│value18││19│value19│└─────┴─────────────┘ Info In case of Replicated installation, Dictionary should be created on all nodes and source tables should have ReplicatedMergeTree engine and be replicated across all nodes.  Info Starting from 20.4, ClickHouse forbid by default any potential non-deterministic mutations. This behavior controlled by setting allow_nondeterministic_mutations. You can apped it to query like this ALTER TABLE xxx UPDATE ... WHERE ... SETTINGS allow_nondeterministic_mutations = 1; For ON CLUSTER queries, you would need to put this setting in default profile and restart ClickHouse servers.  ","categories":"","description":"UPDATE via Dictionary\n","excerpt":"UPDATE via Dictionary\n","ref":"/altinity-kb-queries-and-syntax/update-via-dictionary/","tags":"","title":"UPDATE via Dictionary"},{"body":"SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(transform(number%3,[0,1,2,3],['aa','ab','ad','af'],'a0'))1rowsinset.Elapsed:4.668sec.Processed1.00billionrows,8.00GB(214.21millionrows/s.,1.71GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(multiIf((number%3)=0,'aa',(number%3)=1,'ab',(number%3)=2,'ad',(number%3)=3,'af','a0'))1rowsinset.Elapsed:7.333sec.Processed1.00billionrows,8.00GB(136.37millionrows/s.,1.09GB/s.)SELECTcount()FROMnumbers_mt(1000000000)WHERENOTignore(CAST(number%3ASEnum('aa'=0,'ab'=1,'ad'=2,'af'=3)')) 1 rows in set. Elapsed: 1.152 sec. Processed 1.00 billion rows, 8.00 GB (867.79 million rows/s., 6.94 GB/s.) ","categories":"","description":"Values mapping\n","excerpt":"Values mapping\n","ref":"/altinity-kb-queries-and-syntax/values-mapping/","tags":"","title":"Values mapping"},{"body":"Update itself is simple: update packages, restart clickhouse-server service afterwards.\n Check if the version you want to upgrade to is stable. We highly recommend the Altinity ClickHouse Stable Releases.  Review the changelog to ensure that no configuration changes are needed.   Update staging and test to verify all systems are working. Prepare and test downgrade procedures so the server can be returned to the previous version if necessary. Start with a “canary” update. This is one replica with one shard that is upgraded to make sure that the procedure works. Test and verify that everything works properly. Check for any errors in the log files. If everything is working well, update the rest of the cluster.  For small clusters, the BlueGreenDeployment technique is also a good option.\n ","categories":"","description":"Version Upgrades\n","excerpt":"Version Upgrades\n","ref":"/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/version-upgrades/","tags":"","title":"Version Upgrades"},{"body":"SELECT*,formatReadableSize(value)FROMsystem.asynchronous_metricsWHEREmetriclike'%Cach%'ormetriclike'%Mem%'orderbymetricformatPrettyCompactMonoBlock;SELECTevent_time,metric,value,formatReadableSize(value)FROMsystem.asynchronous_metric_logWHEREevent_time\u003enow()-600and(metriclike'%Cach%'ormetriclike'%Mem%')andvalue\u003c\u003e0orderbymetric,event_timeformatPrettyCompactMonoBlock;SELECTformatReadableSize(sum(bytes_allocated))FROMsystem.dictionaries;SELECTdatabase,name,formatReadableSize(total_bytes)FROMsystem.tablesWHEREengineIN('Memory','Set','Join');SELECTformatReadableSize(sum(memory_usage))FROMsystem.merges;SELECTformatReadableSize(sum(memory_usage))FROMsystem.processes;SELECTinitial_query_id,elapsed,formatReadableSize(memory_usage),formatReadableSize(peak_memory_usage),queryFROMsystem.processesORDERBYpeak_memory_usageDESCLIMIT10;SELECTmetric,formatReadableSize(value)FROMsystem.asynchronous_metricsWHEREmetricIN('UncompressedCacheBytes','MarkCacheBytes');SELECTformatReadableSize(sum(primary_key_bytes_in_memory))ASprimary_key_bytes_in_memory,formatReadableSize(sum(primary_key_bytes_in_memory_allocated))ASprimary_key_bytes_in_memory_allocatedFROMsystem.parts;SELECTtype,event_time,initial_query_id,formatReadableSize(memory_usage),queryFROMsystem.query_logWHERE(event_date\u003e=today())AND(event_time\u003e=(now()-7200))ORDERBYmemory_usageDESCLIMIT10;SELECTsum(data_uncompressed_bytes)FROMsystem.partsWHEREpart_type='InMemory';for i in `seq 1 600`; do clickhouse-client --empty_result_for_aggregation_by_empty_set=0 -q \"select (select 'Merges: \\ '||formatReadableSize(sum(memory_usage)) from system.merges), (select \\ 'Processes: '||formatReadableSize(sum(memory_usage)) from system.processes)\";\\ sleep 3; done  Merges: 96.57 MiB\tProcesses: 41.98 MiB Merges: 82.24 MiB\tProcesses: 41.91 MiB Merges: 66.33 MiB\tProcesses: 41.91 MiB Merges: 66.49 MiB\tProcesses: 37.13 MiB Merges: 67.78 MiB\tProcesses: 37.13 MiB echo \" Merges Processes PrimaryK TempTabs Dicts\"; \\ for i in `seq 1 600`; do clickhouse-client --empty_result_for_aggregation_by_empty_set=0 -q \"select \\ (select leftPad(formatReadableSize(sum(memory_usage)),15, ' ') from system.merges)|| (select leftPad(formatReadableSize(sum(memory_usage)),15, ' ') from system.processes)|| (select leftPad(formatReadableSize(sum(primary_key_bytes_in_memory_allocated)),15, ' ') from system.parts)|| \\ (select leftPad(formatReadableSize(sum(total_bytes)),15, ' ') from system.tables \\ WHERE engine IN ('Memory','Set','Join'))|| (select leftPad(formatReadableSize(sum(bytes_allocated)),15, ' ') FROM system.dictionaries) \"; sleep 3; done   Merges Processes PrimaryK TempTabs Dicts  0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB  0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB  0.00 B 0.00 B 21.35 MiB 1.58 GiB 911.07 MiB  0.00 B 0.00 B 21.36 MiB 1.58 GiB 911.07 MiB ","categories":"","description":"Who ate my memory\n","excerpt":"Who ate my memory\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/","tags":"","title":"Who ate my memory"},{"body":"   Link blog.tinybird.co/2021/03/16/c…     Date Mar 26, 2021    blog.tinybird.co/2021/03/16/c…\n An exploration on what’s possible to do with the most recent experimental feature on ClickHouse - window functions, and an overview of other interesting feat…\n Windows Functions Blog Link\nHow Do I Simulate Window Functions Using Arrays on older versions of clickhouse?  Group with groupArray. Calculate the needed metrics. Ungroup back using arrayJoin.  NTILE SELECTintDiv((num-1)-(cnt%3),3)ASntileFROM(SELECTrow_number()OVER(ORDERBYnumberASC)ASnum,count()OVER()AScntFROMnumbers(11))┌─ntile─┐│0││0││0││0││0││1││1││1││2││2││2│└───────┘","categories":"","description":"Window functions\n","excerpt":"Window functions\n","ref":"/altinity-kb-queries-and-syntax/window-functions/","tags":"","title":"Window functions"},{"body":" Warning The local set of parts of table doesn’t look like the set of parts in ZooKeeper. 100.00 rows of 150.00 total rows in filesystem are suspicious. There are 1 unexpected parts with 100 rows (1 of them is not just-written with 100 rows), 0 missing parts (with 0 blocks).: Cannot attach table.  ClickHouse has a registry of parts in ZooKeeper.\nAnd during the start ClickHouse compares that list of parts on a local disk is consistent with a list in ZooKeeper. If the lists are too different ClickHouse denies to start because it could be an issue with settings, wrong Shard or wrong Replica macros. But this safe-limiter throws an exception if the difference is more 50% (in rows).\nIn your case the table is very small and the difference \u003e50% ( 100.00 vs 150.00 ) is only a single part mismatch, which can be the result of hard restart.\nSELECT*FROMsystem.merge_tree_settingsWHEREname='replicated_max_ratio_of_wrong_parts'┌─name────────────────────────────────┬─value─┬─changed─┬─description──────────────────────────────────────────────────────────────────────────┬─type──┐│replicated_max_ratio_of_wrong_parts│0.5│0│Ifratioofwrongpartstototalnumberofpartsislessthanthis-allowtostart.│Float│└─────────────────────────────────────┴───────┴─────────┴──────────────────────────────────────────────────────────────────────────────────────┴───────┘You can set another value of replicated_max_ratio_of_wrong_parts for all MergeTree tables or per table.\nhttps://clickhouse.tech/docs/en/operations/settings/merge-tree-settings\nAfter manipulation with storage_policies and disks When storage policy changes (one disk was removed from it), ClickHouse compared parts on disk and this replica state in ZooKeeper and found out that a lot of parts (from removed disk) disappeared. So ClickHouse removed them from the replica state in ZooKeeper and scheduled to fetch them from other replicas.\nAfter we add the removed disk to storage_policy back, ClickHouse finds missing parts, but at this moment they are not registered for that replica. ClickHouse produce error message like this:\nWarning  Application: DB::Exception: The local set of parts of table default.tbl doesn’t look like the set of parts in ZooKeeper: 14.96 billion rows of 16.24 billion total rows in filesystem are suspicious. There are 45 unexpected parts with 14960302620 rows (43 of them is not just-written with 14959824636 rows), 0 missing parts (with 0 blocks).: Cannot attach table default.tbl from metadata file /var/lib/clickhouse/metadata/default/tbl.sql from query ATTACH TABLE default.tbl … ENGINE=ReplicatedMergeTree(’/clickhouse/tables/0/default/tbl’, ‘replica-0’)… SETTINGS index_granularity = 1024, storage_policy = ’ebs_hot_and_cold’: while loading database default from path /var/lib/clickhouse/metadata/data  At this point, it’s possible to either tune setting replicated_max_ratio_of_wrong_parts or do force restore, but it will end up downloading all “missing” parts from other replicas, which can take a lot of time for big tables.\nClickHouse 21.7+  Rename table SQL attach script in order to prevent ClickHouse from attaching it at startup.  mv /var/lib/clickhouse/metadata/default/tbl.sql /var/lib/clickhouse/metadata/default/tbl.sql.bak  Start ClickHouse server.\n  Remove metadata for this replica from ZooKeeper.\n  SYSTEM DROP REPLICA 'replica-0' FROM ZKPATH '/clickhouse/tables/0/default/tbl'; SELECT * FROM system.zookeeper WHERE path = '/clickhouse/tables/0/default/tbl/replicas'; Rename table SQL attach script back to normal name.  mv /var/lib/clickhouse/metadata/default/tbl.sql.bak /var/lib/clickhouse/metadata/default/tbl.sql Attach table to ClickHouse server, because there is no metadata in ZooKeeper, ClickHouse will attach it in read only state.  ATTACH TABLE default.tbl; Run SYSTEM RESTORE REPLICA in order to sync state on disk and in ZooKeeper.  SYSTEM RESTORE REPLICA default.tbl; Run SYSTEM SYNC REPLICA to download missing parts from other replicas.  SYSTEM SYNC REPLICA default.tbl; ","categories":"","description":"X rows of Y total rows in filesystem are suspicious\n","excerpt":"X rows of Y total rows in filesystem are suspicious\n","ref":"/altinity-kb-setup-and-maintenance/x-rows-of-y-total-rows-in-filesystem-are-suspicious/","tags":"","title":"X rows of Y total rows in filesystem are suspicious"},{"body":"Requirements TLDR version:\n USE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup). use 3 nodes (more nodes = slower quorum, less = no HA). low network latency between zookeeper nodes is very important (latency, not bandwidth). have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings ensure that zookeeper will not be CPU-starved by some other processes monitor zookeeper.  Side note: in many cases, the slowness of the zookeeper is actually a symptom of some issue with clickhouse schema/usage pattern (the most typical issues: an enormous number of partitions/tables/databases with real-time inserts, tiny \u0026 frequent inserts).\nHow to install  https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/ altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/  Random links on best practices  https://docs.confluent.io/platform/current/zookeeper/deployment.html https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.html#sc_commonProblems https://clickhouse.tech/docs/en/operations/tips/#zookeeper https://lucene.apache.org/solr/guide/7_4/setting-up-an-external-zookeeper-ensemble.html https://cwiki.apache.org/confluence/display/ZOOKEEPER/Troubleshooting  Cite from https://zookeeper.apache.org/doc/r3.5.7/zookeeperAdmin.html#sc_commonProblems :\n Things to Avoid Here are some common problems you can avoid by configuring ZooKeeper correctly:\n inconsistent lists of servers : The list of ZooKeeper servers used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has. Things work okay if the client list is a subset of the real list, but things will really act strange if clients have a list of ZooKeeper servers that are in different ZooKeeper clusters. Also, the server lists in each Zookeeper server configuration file should be consistent with one another. incorrect placement of transaction log : The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log. incorrect Java heap size : You should take special care to set your Java max heap size correctly. In particular, you should not create a situation in which ZooKeeper swaps to disk. The disk is death to ZooKeeper. Everything is ordered, so if processing one request swaps the disk, all other queued requests will probably do the same. the disk. DON’T SWAP. Be conservative in your estimates: if you have 4G of RAM, do not set the Java max heap size to 6G or even 4G. For example, it is more likely you would use a 3G heap for a 4G machine, as the operating system and the cache also need memory. The best and only recommend practice for estimating the heap size your system needs is to run load tests, and then make sure you are well below the usage limit that would cause the system to swap. Publicly accessible deployment : A ZooKeeper ensemble is expected to operate in a trusted computing environment. It is thus recommended to deploy ZooKeeper behind a firewall.   How to check number of followers: echo mntr | nc zookeeper 2187 | grep foll zk_synced_followers 2 zk_synced_non_voting_followers 0 zk_avg_follower_sync_time 0.0 zk_min_follower_sync_time 0 zk_max_follower_sync_time 0 zk_cnt_follower_sync_time 0 zk_sum_follower_sync_time 0 Tools https://github.com/apache/zookeeper/blob/master/zookeeper-docs/src/main/resources/markdown/zookeeperTools.md\nAlternatives for zkCli  https://github.com/go-zkcli/zkcli https://github.com/outbrain/zookeepercli https://idata.co.il/2018/07/a-day-at-the-zoo-graphic-uis-for-apache-zookeeper/  Web UI  https://github.com/elkozmon/zoonavigator-api https://github.com/tobilg/docker-zookeeper-webui https://github.com/vran-dev/PrettyZoo  ","categories":"","description":"ZooKeeper\n","excerpt":"ZooKeeper\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/","tags":"","title":"ZooKeeper"},{"body":"You may have a question: “Do I need to backup Zookeeper Database, because it’s pretty important for ClickHouse?”\nAnswer: ZK is in memory database. All nodes of ZK has exactly the same data.\nIf you have 3 ZK servers, then you have 3 copies of (3 backups) already.\nTo backup ZK has no sense because you need to have a snapshot of ZK + last ZK logs to exactly the last ZK transaction.\nYou cannot use ZK database backed up 3 hours ago or 3 minutes ago.\nZK restored from the backup will be inconsistent with CH database.\nAnswer2: Usually, it doesn’t have too much sense. It’s very hard to take zookeeper snapshot at exactly the same state as clickhouse. (well maybe if you will turn of clickhouses, then you can take snapshots of clickhouse AND zookeepers). So for example on clouds if you can stop all nodes and take disk snapshots - it will just work.\nBut while clickhouse is working it’s almost impossible to collect the current state of zookeeper.\nYou need to restore zookeeper and clickhouse snapshots from EXACTLY THE SAME moment of time - no procedure is needed. Just start \u0026 run.\nAlso, that allows only to snapshot of clickhouse \u0026 zookeeper as a whole. You can not do partial backups then.\nIf you lose zookeeper data while having clickhouse data (or backups of clickhouse data) - you can restore the zookeeper state from clickhouse state.\nWith a couple of tables, it can be done manually.\nOn scale, you can use https://github.com/Altinity/clickhouse-zookeeper-recovery\nIn future it will be even simpler https://github.com/ClickHouse/ClickHouse/pull/13652\n","categories":"","description":"ZooKeeper backup\n","excerpt":"ZooKeeper backup\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-backup/","tags":"","title":"ZooKeeper backup"},{"body":"Here is a plan for ZK 3.4.9 (no dynamic reconfiguration):\n Add the 3 new ZK nodes to the old cluster. No changes needed for the 3 old ZK nodes at this time.  Configure one of the new ZK nodes as a cluster of 4 nodes (3 old + 1 new), start it. Configure the other two new ZK nodes as a cluster of 6 nodes (3 old + 3 new), start them.   Make sure the 3 new ZK nodes connected to the old ZK cluster as followers (run echo stat | nc localhost 2181 on the 3 new ZK nodes) Confirm that the leader has 5 synced followers (run echo mntr | nc localhost 2181 on the leader, look for zk_synced_followers) Stop data ingestion in CH (this is to minimize errors when CH loses ZK). Change the zookeeper section in the configs on the CH nodes (remove the 3 old ZK servers, add the 3 new ZK servers) Make sure that there are no connections from CH to the 3 old ZK nodes (run echo stat | nc localhost 2181 on the 3 old nodes, check their Clients section). Restart all CH nodes if necessary (In some cases CH can reconnect to different ZK servers without a restart). Remove the 3 old ZK nodes from zoo.cfg on the 3 new ZK nodes. Restart the 3 new ZK nodes. They should form a cluster of 3 nodes. When CH reconnects to ZK, start data loading. Turn off the 3 old ZK nodes.  This plan works, but it is not the only way to do this, it can be changed if needed.\n","categories":"","description":"ZooKeeper cluster migration\n","excerpt":"ZooKeeper cluster migration\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration/","tags":"","title":"ZooKeeper cluster migration"},{"body":"ZooKeeper Monitoring ZooKeeper scrape metrics  embedded exporter since version 3.6.0  https://zookeeper.apache.org/doc/r3.6.2/zookeeperMonitor.html   standalone exporter  https://github.com/dabealu/zookeeper-exporter    Install dashboards  embedded exporter https://grafana.com/grafana/dashboards/10465 dabealu exporter https://grafana.com/grafana/dashboards/11442  See also https://grafana.com/grafana/dashboards?search=ZooKeeper\u0026amp;dataSource=prometheus\nsetup alert rules  embedded exporter link  See also  https://blog.serverdensity.com/how-to-monitor-zookeeper/ https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/#zookeeper-metrics https://dzone.com/articles/monitoring-apache-zookeeper-servers https://docs.signalfx.com/en/latest/integrations/integrations-reference/integrations.zookeeper.html https://github.com/samber/awesome-prometheus-alerts/blob/c3ba0cf1997c7e952369a090aeb10343cdca4878/_data/rules.yml#L1146-L1170 (or https://awesome-prometheus-alerts.grep.to/rules.html#zookeeper ) https://alex.dzyoba.com/blog/prometheus-alerts/ https://docs.datadoghq.com/integrations/zk/?tab=host  ","categories":"","description":"ZooKeeper Monitoring\n","excerpt":"ZooKeeper Monitoring\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring/","tags":"","title":"ZooKeeper Monitoring"},{"body":"/metadata Table schema.\ndate column -\u003e legacy MergeTree partition expression. sampling expression -\u003e SAMPLE BY index granularity -\u003e index_granularity mode -\u003e type of MergeTree table sign column -\u003e sign - CollapsingMergeTree / VersionedCollapsingMergeTree primary key -\u003e ORDER BY key if PRIMARY KEY not defined. sorting key -\u003e ORDER BY key if PRIMARY KEY defined. data format version -\u003e 1 partition key -\u003e PARTITION BY granularity bytes -\u003e index_granularity_bytes  types of MergeTree tables: Ordinary = 0 Collapsing = 1 Summing = 2 Aggregating = 3 Replacing = 5 Graphite = 6 VersionedCollapsing = 7 /mutations Log of latest mutations\n/columns List of columns for latest (reference) table version. Replicas would try to reach this state.\n/log Log of latest actions with table.\nRelated settings:\n┌─name────────────────────────┬─value─┬─changed─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─type───┐│max_replicated_logs_to_keep│1000│0│Howmanyrecordsmaybeinlog,ifthereisinactivereplica.Inactivereplicabecomeslostwhenwhenthisnumberexceed.│UInt64││min_replicated_logs_to_keep│10│0│KeepaboutthisnumberoflastrecordsinZooKeeperlog,eveniftheyareobsolete.Itdoesn't affect work of tables: used only to diagnose ZooKeeper log before cleaning. │ UInt64 │ └─────────────────────────────┴───────┴─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────┘ /replicas List of table replicas.\n/replicas/replica_name/ /replicas/replica_name/mutation_pointer Pointer to the latest mutation executed by replica\n/replicas/replica_name/log_pointer Pointer to the latest task from replication_queue executed by replica\n/replicas/replica_name/max_processed_insert_time /replica/replica_name/metadata Table schema of specific replica\n/replica/replica_name/columns Columns list of specific replica.\n/quorum Used for quorum inserts.\n","categories":"","description":"ZooKeeper schema\n","excerpt":"ZooKeeper schema\n","ref":"/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-schema/","tags":"","title":"ZooKeeper schema"}]