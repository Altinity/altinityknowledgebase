<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Altinity® Knowledge Base for ClickHouse®</title><link>http://kb.altinity.com/</link><description>Recent content on Altinity® Knowledge Base for ClickHouse®</description><generator>Hugo</generator><language>en</language><atom:link href="http://kb.altinity.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Mermaid Example</title><link>http://kb.altinity.com/using-this-knowledgebase/mermaid_example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/using-this-knowledgebase/mermaid_example/</guid><description>This Knowledge Base now supports Mermaid , a handy way to create charts from text. The following example shows a very simple chart, and the code to use.
To add a Mermaid chart, encase the Mermaid code between {{&amp;lt; mermaid &amp;gt;}}, as follows:
{{&amp;lt;mermaid&amp;gt;}} graph TD; A--&amp;gt;B; A--&amp;gt;C; B--&amp;gt;D; C--&amp;gt;D; {{&amp;lt;/mermaid&amp;gt;}} And it renders as so:
graph TD; A-->B; A-->C; B-->D; C-->D;</description></item><item><title>Adjustable table partitioning</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/variable-partitioning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/variable-partitioning/</guid><description>In that example, partitioning is being calculated via MATERIALIZED column expression toDate(toStartOfInterval(ts, toIntervalT(...))), but partition id also can be generated on application side and inserted to ClickHouse® as is.
CREATE TABLE tbl ( `ts` DateTime, `key` UInt32, `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalYear(1))) ) ENGINE = MergeTree PARTITION BY (partition_key, ignore(ts)) ORDER BY key; SET send_logs_level = &amp;#39;trace&amp;#39;; INSERT INTO tbl SELECT toDateTime(toDate(&amp;#39;2020-01-01&amp;#39;) + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20200101-0_1_1_0 to 20200101-0_1_1_0 INSERT INTO tbl SELECT toDateTime(toDate(&amp;#39;2021-01-01&amp;#39;) + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20210101-0_2_2_0 to 20210101-0_2_2_0 ALTER TABLE tbl MODIFY COLUMN `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalMonth(1))); INSERT INTO tbl SELECT toDateTime(toDate(&amp;#39;2022-01-01&amp;#39;) + number) as ts, number as key FROM numbers(300); Renaming temporary part tmp_insert_20220101-0_3_3_0 to 20220101-0_3_3_0 Renaming temporary part tmp_insert_20220201-0_4_4_0 to 20220201-0_4_4_0 Renaming temporary part tmp_insert_20220301-0_5_5_0 to 20220301-0_5_5_0 Renaming temporary part tmp_insert_20220401-0_6_6_0 to 20220401-0_6_6_0 Renaming temporary part tmp_insert_20220501-0_7_7_0 to 20220501-0_7_7_0 Renaming temporary part tmp_insert_20220601-0_8_8_0 to 20220601-0_8_8_0 Renaming temporary part tmp_insert_20220701-0_9_9_0 to 20220701-0_9_9_0 Renaming temporary part tmp_insert_20220801-0_10_10_0 to 20220801-0_10_10_0 Renaming temporary part tmp_insert_20220901-0_11_11_0 to 20220901-0_11_11_0 Renaming temporary part tmp_insert_20221001-0_12_12_0 to 20221001-0_12_12_0 ALTER TABLE tbl MODIFY COLUMN `partition_key` Date MATERIALIZED toDate(toStartOfInterval(ts, toIntervalDay(1))); INSERT INTO tbl SELECT toDateTime(toDate(&amp;#39;2023-01-01&amp;#39;) + number) as ts, number as key FROM numbers(5); Renaming temporary part tmp_insert_20230101-0_13_13_0 to 20230101-0_13_13_0 Renaming temporary part tmp_insert_20230102-0_14_14_0 to 20230102-0_14_14_0 Renaming temporary part tmp_insert_20230103-0_15_15_0 to 20230103-0_15_15_0 Renaming temporary part tmp_insert_20230104-0_16_16_0 to 20230104-0_16_16_0 Renaming temporary part tmp_insert_20230105-0_17_17_0 to 20230105-0_17_17_0 SELECT _partition_id, min(ts), max(ts), count() FROM tbl GROUP BY _partition_id ORDER BY _partition_id; ┌─_partition_id─┬─────────────min(ts)─┬─────────────max(ts)─┬─count()─┐ │ 20200101-0 │ 2020-01-01 00:00:00 │ 2020-10-26 00:00:00 │ 300 │ │ 20210101-0 │ 2021-01-01 00:00:00 │ 2021-10-27 00:00:00 │ 300 │ │ 20220101-0 │ 2022-01-01 00:00:00 │ 2022-01-31 00:00:00 │ 31 │ │ 20220201-0 │ 2022-02-01 00:00:00 │ 2022-02-28 00:00:00 │ 28 │ │ 20220301-0 │ 2022-03-01 00:00:00 │ 2022-03-31 00:00:00 │ 31 │ │ 20220401-0 │ 2022-04-01 00:00:00 │ 2022-04-30 00:00:00 │ 30 │ │ 20220501-0 │ 2022-05-01 00:00:00 │ 2022-05-31 00:00:00 │ 31 │ │ 20220601-0 │ 2022-06-01 00:00:00 │ 2022-06-30 00:00:00 │ 30 │ │ 20220701-0 │ 2022-07-01 00:00:00 │ 2022-07-31 00:00:00 │ 31 │ │ 20220801-0 │ 2022-08-01 00:00:00 │ 2022-08-31 00:00:00 │ 31 │ │ 20220901-0 │ 2022-09-01 00:00:00 │ 2022-09-30 00:00:00 │ 30 │ │ 20221001-0 │ 2022-10-01 00:00:00 │ 2022-10-27 00:00:00 │ 27 │ │ 20230101-0 │ 2023-01-01 00:00:00 │ 2023-01-01 00:00:00 │ 1 │ │ 20230102-0 │ 2023-01-02 00:00:00 │ 2023-01-02 00:00:00 │ 1 │ │ 20230103-0 │ 2023-01-03 00:00:00 │ 2023-01-03 00:00:00 │ 1 │ │ 20230104-0 │ 2023-01-04 00:00:00 │ 2023-01-04 00:00:00 │ 1 │ │ 20230105-0 │ 2023-01-05 00:00:00 │ 2023-01-05 00:00:00 │ 1 │ └───────────────┴─────────────────────┴─────────────────────┴─────────┘ SELECT count() FROM tbl WHERE ts &amp;gt; &amp;#39;2023-01-04&amp;#39;; Key condition: unknown MinMax index condition: (column 0 in [1672758001, +Inf)) Selected 1/17 parts by partition key, 1 parts by primary key, 1/1 marks by primary key, 1 marks to read from 1 ranges Spreading mark ranges among streams (default reading) Reading 1 ranges in order from part 20230105-0_17_17_0, approx.</description></item><item><title>AggregateFunction(uniq, UUID) doubled after ClickHouse® upgrade</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/uniq-uuid-doubled-clickhouse-upgrade/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/uniq-uuid-doubled-clickhouse-upgrade/</guid><description>What happened After ClickHouse® upgrade from version pre 21.6 to version after 21.6, count of unique UUID in AggregatingMergeTree tables nearly doubled in case of merging of data which was generated in different ClickHouse versions.
Why happened In pull request which changed the internal representation of big integers data types (and UUID). SipHash64 hash-function used for uniq aggregation function for UUID data type was replaced with intHash64, which leads to different result for the same UUID value across different ClickHouse versions.</description></item><item><title>AWS S3 Recipes</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/aws-s3-recipes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/aws-s3-recipes/</guid><description>Using AWS IAM — Identity and Access Management roles For EC2 instance, there is an option to configure an IAM role:
Role shall contain a policy with permissions like:
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Sid&amp;#34;: &amp;#34;allow-put-and-get&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Action&amp;#34;: [ &amp;#34;s3:PutObject&amp;#34;, &amp;#34;s3:GetObject&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:s3:::BUCKET_NAME/test_s3_disk/*&amp;#34; } ] } Corresponding configuration of ClickHouse®:
&amp;lt;clickhouse&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;disk_s3&amp;gt; &amp;lt;type&amp;gt;s3&amp;lt;/type&amp;gt; &amp;lt;endpoint&amp;gt;http://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/&amp;lt;/endpoint&amp;gt; &amp;lt;use_environment_credentials&amp;gt;true&amp;lt;/use_environment_credentials&amp;gt; &amp;lt;/disk_s3&amp;gt; &amp;lt;/disks&amp;gt; &amp;lt;policies&amp;gt; &amp;lt;policy_s3_only&amp;gt; &amp;lt;volumes&amp;gt; &amp;lt;volume_s3&amp;gt; &amp;lt;disk&amp;gt;disk_s3&amp;lt;/disk&amp;gt; &amp;lt;/volume_s3&amp;gt; &amp;lt;/volumes&amp;gt; &amp;lt;/policy_s3_only&amp;gt; &amp;lt;/policies&amp;gt; &amp;lt;/storage_configuration&amp;gt; &amp;lt;/clickhouse&amp;gt; Small check:</description></item><item><title>Can not connect to my ClickHouse® server</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/connection-problems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/connection-problems/</guid><description>Can not connect to my ClickHouse® server Errors like &amp;ldquo;Connection reset by peer, while reading from socket&amp;rdquo;
Ensure that the clickhouse-server is running
systemctl status clickhouse-server If server was restarted recently and don&amp;rsquo;t accept the connections after the restart - most probably it still just starting. During the startup sequence it need to iterate over all data folders in /var/lib/clickhouse-server In case if you have a very high number of folders there (usually caused by a wrong partitioning, or a very high number of tables / databases) that startup time can take a lot of time (same can happen if disk is very slow, for example NFS).</description></item><item><title>cgroups and kubernetes cloud providers</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cgroups_k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cgroups_k8s/</guid><description>Why my ClickHouse® is slow after upgrade to version 22.2 and higher?
The probable reason is that ClickHouse 22.2 started to respect cgroups (Respect cgroups limits in max_threads autodetection. #33342 (JaySon ).
You can observe that max_threads = 1
SELECT name, value FROM system.settings WHERE name = &amp;#39;max_threads&amp;#39; ┌─name────────┬─value─────┐ │ max_threads │ &amp;#39;auto(1)&amp;#39; │ └─────────────┴───────────┘ This makes ClickHouse to execute all queries with a single thread (normal behavior is half of available CPU cores, cores = 64, then &amp;lsquo;auto(32)&amp;rsquo;).</description></item><item><title>Check table metadata in zookeeper</title><link>http://kb.altinity.com/altinity-kb-useful-queries/table-meta-in-zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/table-meta-in-zookeeper/</guid><description>Compare table metadata of different replicas in zookeeper Check if a table is consistent across all zookeeper replicas. From each replica, returns metdadata, columns, and is_active nodes. Checks whether each replica&amp;rsquo;s value matches the previous replica&amp;rsquo;s value, and flags any mismatches (looks_good = 0).
SELECT *, if( prev_name = name AND name != &amp;#39;is_active&amp;#39;, prev_value = value, 1 ) AS looks_good FROM ( SELECT name, path, ctime, mtime, value, lagInFrame(name) OVER w AS prev_name, lagInFrame(value) OVER w AS prev_value FROM system.</description></item><item><title>Clean up orphaned objects on s3</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/clean-up-orphaned-objects-on-s3.md/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/clean-up-orphaned-objects-on-s3.md/</guid><description>Problems TRUNCATE and DROP TABLE remove metadata only. Long-running queries, merges or other replicas may still reference parts, so ClickHouse delays removal. There are bugs in Clickhouse that leave orphaned files, especially after failures. Solutions use our utility for garbage collection - https://github.com/Altinity/s3gc or create a separate path in the bucket for every table and every replica and remove the whole path in AWS console you can also use clickhouse-disk utility to delete s3 data: clickhouse-disks --disk s3 --query &amp;#34;remove /cluster/database/table/replica1&amp;#34;</description></item><item><title>ClickHouse® limitations</title><link>http://kb.altinity.com/altinity-kb-schema-design/how-much-is-too-much/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/how-much-is-too-much/</guid><description>In most of the cases ClickHouse® doesn&amp;rsquo;t have any hard limits. But obviously there there are some practical limitation / barriers for different things - often they are caused by some system / network / filesystem limitation.
So after reaching some limits you can get different kind of problems, usually it never a failures / errors, but different kinds of degradations (slower queries / high cpu/memory usage, extra load on the network / zookeeper etc).</description></item><item><title>Transforming ClickHouse logs to ndjson using Vector.dev</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/ch-logs-2-json-vectordev/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/ch-logs-2-json-vectordev/</guid><description>ClickHouse 22.8 Starting from 22.8 version, ClickHouse support writing logs in JSON format:
&amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt; &amp;lt;clickhouse&amp;gt; &amp;lt;logger&amp;gt; &amp;lt;!-- Structured log formatting: You can specify log format(for now, JSON only). In that case, the console log will be printed in specified format like JSON. For example, as below: {&amp;#34;date_time&amp;#34;:&amp;#34;1650918987.180175&amp;#34;,&amp;#34;thread_name&amp;#34;:&amp;#34;#1&amp;#34;,&amp;#34;thread_id&amp;#34;:&amp;#34;254545&amp;#34;,&amp;#34;level&amp;#34;:&amp;#34;Trace&amp;#34;,&amp;#34;query_id&amp;#34;:&amp;#34;&amp;#34;,&amp;#34;logger_name&amp;#34;:&amp;#34;BaseDaemon&amp;#34;,&amp;#34;message&amp;#34;:&amp;#34;Received signal 2&amp;#34;,&amp;#34;source_file&amp;#34;:&amp;#34;../base/daemon/BaseDaemon.cpp; virtual void SignalListener::run()&amp;#34;,&amp;#34;source_line&amp;#34;:&amp;#34;192&amp;#34;} To enable JSON logging support, just uncomment &amp;lt;formatting&amp;gt; tag below. --&amp;gt; &amp;lt;formatting&amp;gt;json&amp;lt;/formatting&amp;gt; &amp;lt;/logger&amp;gt; &amp;lt;/clickhouse&amp;gt; Transforming ClickHouse logs to ndjson using Vector.</description></item><item><title>Altinity Kubernetes Operator For ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-operator/</guid><description>Altinity Kubernetes Operator for ClickHouse® Documentation https://github.com/Altinity/clickhouse-operator/blob/master/docs/README.md</description></item><item><title>clickhouse-keeper-initd</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-initd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-initd/</guid><description>clickhouse-keeper-initd An init.d script for clickhouse-keeper. This example is based on zkServer.sh
#!/bin/bash ### BEGIN INIT INFO # Provides: clickhouse-keeper # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Required-Start: # Required-Stop: # Short-Description: Start keeper daemon # Description: Start keeper daemon ### END INIT INFO NAME=clickhouse-keeper ZOOCFGDIR=/etc/$NAME ZOOCFG=&amp;#34;$ZOOCFGDIR/keeper.xml&amp;#34; ZOO_LOG_DIR=/var/log/$NAME USER=clickhouse GROUP=clickhouse ZOOPIDDIR=/var/run/$NAME ZOOPIDFILE=$ZOOPIDDIR/$NAME.pid SCRIPTNAME=/etc/init.d/$NAME #echo &amp;#34;Using config: $ZOOCFG&amp;#34; &amp;gt;&amp;amp;2 ZOOCMD=&amp;#34;clickhouse-keeper -C ${ZOOCFG} start --daemon&amp;#34; # ensure PIDDIR exists, otw stop will fail mkdir -p &amp;#34;$(dirname &amp;#34;$ZOOPIDFILE&amp;#34;)&amp;#34; if [ !</description></item><item><title>clickhouse-keeper-service</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-service/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper-service/</guid><description>clickhouse-keeper-service installation Need to install clickhouse-common-static + clickhouse-keeper OR clickhouse-common-static + clickhouse-server. Both OK, use the first if you don&amp;rsquo;t need ClickHouse® server locally.
dpkg -i clickhouse-common-static_{%version}.deb clickhouse-keeper_{%version}.deb dpkg -i clickhouse-common-static_{%version}.deb clickhouse-server_{%version}.deb clickhouse-client_{%version}.deb Create directories
mkdir -p /etc/clickhouse-keeper/config.d mkdir -p /var/log/clickhouse-keeper mkdir -p /var/lib/clickhouse-keeper/coordination/log mkdir -p /var/lib/clickhouse-keeper/coordination/snapshots mkdir -p /var/lib/clickhouse-keeper/cores chown -R clickhouse.clickhouse /etc/clickhouse-keeper /var/log/clickhouse-keeper /var/lib/clickhouse-keeper config cat /etc/clickhouse-keeper/config.xml &amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt; &amp;lt;clickhouse&amp;gt; &amp;lt;logger&amp;gt; &amp;lt;!-- Possible levels [1]: - none (turns off logging) - fatal - critical - error - warning - notice - information - debug - trace - test (not for production usage) [1]: https://github.</description></item><item><title>ClickHouse® and different filesystems</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/filesystems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/filesystems/</guid><description>In general ClickHouse® should work with any POSIX-compatible filesystem.
hard links and soft links support is mandatory. ClickHouse can use O_DIRECT mode to bypass the cache (and async io) ClickHouse can use renameat2 command for some atomic operations (not all the filesystems support that). depending on the schema and details of the usage the filesystem load can vary between the setup. The most natural load - is high throughput, with low or moderate IOPS.</description></item><item><title>ClickHouse® python drivers</title><link>http://kb.altinity.com/altinity-kb-integrations/clickhouse_python_drivers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/clickhouse_python_drivers/</guid><description>There are two main python drivers that can be used with ClickHouse. They all have their different set of features and use cases:
ClickHouse driver AKA clickhouse-driver The clickhouse-driver is a Python library used for interacting with ClickHouse. Here&amp;rsquo;s a summary of its features:
Connectivity: clickhouse-driver allows Python applications to connect to ClickHouse servers over TCP/IP Native Interface (9000/9440 ports) and also HTTP interface but it is experimental. SQL Queries: It enables executing SQL queries against ClickHouse databases from Python scripts, including data manipulation (insertion, deletion, updating) and data retrieval (select queries).</description></item><item><title>ClickHouse® Access Control and Account Management (RBAC)</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/rbac/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/rbac/</guid><description>Documentation https://clickhouse.com/docs/en/operations/access-rights/ Enable ClickHouse® RBAC and create admin user Create an admin user like (root in MySQL or postgres in PostgreSQL) to do the DBA/admin ops in the user.xml file and set the access management property for the admin user &amp;lt;clickhouse&amp;gt; &amp;lt;users&amp;gt; &amp;lt;default&amp;gt; .... &amp;lt;/default&amp;gt; &amp;lt;admin&amp;gt; &amp;lt;!-- Password could be specified in plaintext or in SHA256 (in hex format). If you want to specify password in plaintext (not recommended), place it in &amp;#39;password&amp;#39; element.</description></item><item><title>ClickHouse® row-level deduplication</title><link>http://kb.altinity.com/altinity-kb-schema-design/row-level-deduplication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/row-level-deduplication/</guid><description>ClickHouse® row-level deduplication. (Block level deduplication exists in Replicated tables, and is not the subject of that article).
There is quite common requirement to do deduplication on a record level in ClickHouse.
Sometimes duplicates are appear naturally on collector side. Sometime they appear due the the fact that message queue system (Kafka/Rabbit/etc) offers at-least-once guarantees. Sometimes you just expect insert idempotency on row level. For now that problem has no good solution in general case using ClickHouse only.</description></item><item><title>Client Timeouts</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/client-timeouts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/client-timeouts/</guid><description>Timeout settings are related to the client, server, and network. They can be tuned to solve sporadic timeout issues.
It&amp;rsquo;s important to understand that network devices (routers, NATs, load balancers ) could have their own timeouts. Sometimes, they won&amp;rsquo;t respect TCP keep-alive and close the session due to inactivity. Only application-level keepalives could prevent TCP sessions from closing.
Below are the settings that will work only if you set them in the default user profile.</description></item><item><title>CollapsingMergeTree vs ReplacingMergeTree</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/collapsing-vs-replacing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/collapsing-vs-replacing/</guid><description>CollapsingMergeTree vs ReplacingMergeTree ReplacingMergeTree CollapsingMergeTree + very easy to use (always replace) - more complex (accounting-alike, put &amp;lsquo;rollback&amp;rsquo; records to fix something) + you don&amp;rsquo;t need to store the previous state of the row - you need to the store (somewhere) the previous state of the row, OR extract it from the table itself (point queries is not nice for ClickHouse®) - no deletes + support deletes - w/o FINAL - you can can always see duplicates, you need always to &amp;lsquo;pay&amp;rsquo; FINAL performance penalty + properly crafted query can give correct results without final (i.</description></item><item><title>Column backfilling with alter/update using a dictionary</title><link>http://kb.altinity.com/altinity-kb-schema-design/backfill_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/backfill_column/</guid><description>Column backfilling Sometimes you need to add a column into a huge table and backfill it with a data from another source, without reingesting all data.
Replicated setup In case of a replicated / sharded setup you need to have the dictionary and source table (dict_table / item_dict) on all nodes and they have to all have EXACTLY the same data. The easiest way to do this is to make dict_table replicated.</description></item><item><title>Compare query_log for 2 intervals</title><link>http://kb.altinity.com/altinity-kb-useful-queries/compare_query_log_for_2_intervals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/compare_query_log_for_2_intervals/</guid><description>Looks at unique query shapes (by normalized_query_hash) which occurred within two different time intervals (&amp;ldquo;before&amp;rdquo; and &amp;ldquo;after&amp;rdquo;), and returns performance metrics for each query pattern which performed worse in the &amp;ldquo;after&amp;rdquo; interval.
WITH toStartOfInterval(event_time, INTERVAL 5 MINUTE) = &amp;#39;2023-06-30 13:00:00&amp;#39; as before, toStartOfInterval(event_time, INTERVAL 5 MINUTE) = &amp;#39;2023-06-30 15:00:00&amp;#39; as after SELECT normalized_query_hash, anyIf(query, before) AS QueryBefore, anyIf(query, after) AS QueryAfter, countIf(before) as CountBefore, sumIf(query_duration_ms, before) / 1000 AS QueriesDurationBefore, sumIf(ProfileEvents.</description></item><item><title>Compatibility layer for the Altinity Kubernetes Operator for ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/monitoring-operator-exporter-compatibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/monitoring-operator-exporter-compatibility/</guid><description>It&amp;rsquo;s possible to expose clickhouse-server metrics in the style used by the Altinity Kubernetes Operator for ClickHouse®. It&amp;rsquo;s for the clickhouse-operator grafana dashboard.
CREATE VIEW system.operator_compatible_metrics ( `name` String, `value` Float64, `help` String, `labels` Map(String, String), `type` String ) AS SELECT concat(&amp;#39;chi_clickhouse_event_&amp;#39;, event) AS name, CAST(value, &amp;#39;Float64&amp;#39;) AS value, description AS help, map(&amp;#39;hostname&amp;#39;, hostName()) AS labels, &amp;#39;counter&amp;#39; AS type FROM system.events UNION ALL SELECT concat(&amp;#39;chi_clickhouse_metric_&amp;#39;, metric) AS name, CAST(value, &amp;#39;Float64&amp;#39;) AS value, description AS help, map(&amp;#39;hostname&amp;#39;, hostName()) AS labels, &amp;#39;gauge&amp;#39; AS type FROM system.</description></item><item><title>How to convert uniqExact states to approximate uniq functions states</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/uniqexact-to-uniq-combined/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/uniqexact-to-uniq-combined/</guid><description>uniqExactState uniqExactState is stored in two parts: a count of values in LEB128 format + list values without a delimiter.
In our case, the value is sipHash128 of strings passed to uniqExact function.
┌─hex(uniqExactState(toString(arrayJoin([1]))))─┐ │ 01E2756D8F7A583CA23016E03447724DE7 │ └───────────────────────────────────────────────┘ 01 E2756D8F7A583CA23016E03447724DE7 ^ ^ LEB128 sipHash128 ┌─hex(uniqExactState(toString(arrayJoin([1, 2]))))───────────────────┐ │ 024809CB4528E00621CF626BE9FA14E2BFE2756D8F7A583CA23016E03447724DE7 │ └────────────────────────────────────────────────────────────────────┘ 02 4809CB4528E00621CF626BE9FA14E2BF E2756D8F7A583CA23016E03447724DE7 ^ ^ ^ LEB128 sipHash128 sipHash128 So, our task is to find how we can generate such values by ourself.</description></item><item><title>Custom Settings</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/custom_settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/custom_settings/</guid><description>Using custom settings in config You can not use the custom settings in config file &amp;lsquo;as is&amp;rsquo;, because ClickHouse® don&amp;rsquo;t know which datatype should be used to parse it.
cat /etc/clickhouse-server/users.d/default_profile.xml &amp;lt;?xml version=&amp;#34;1.0&amp;#34;?&amp;gt; &amp;lt;yandex&amp;gt; &amp;lt;profiles&amp;gt; &amp;lt;default&amp;gt; &amp;lt;custom_data_version&amp;gt;1&amp;lt;/custom_data_version&amp;gt; &amp;lt;!-- will not work! see below --&amp;gt; &amp;lt;/default&amp;gt; &amp;lt;/profiles&amp;gt; &amp;lt;/yandex&amp;gt; That will end up with the following error:
2021.09.24 12:50:37.369259 [ 264905 ] {} &amp;lt;Error&amp;gt; ConfigReloader: Error updating configuration from &amp;#39;/etc/clickhouse-server/users.xml&amp;#39; config.: Code: 536.</description></item><item><title>DateTime64</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/datetime64/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/datetime64/</guid><description>Subtract fractional seconds WITH toDateTime64(&amp;#39;2021-09-07 13:41:50.926&amp;#39;, 3) AS time SELECT time - 1, time - 0.1 AS no_affect, time - toDecimal64(0.1, 3) AS uncorrect_result, time - toIntervalMillisecond(100) AS correct_result -- from 22.4 Query id: 696722bd-3c22-4270-babe-c6b124fee97f ┌──────────minus(time, 1)─┬───────────────no_affect─┬────────uncorrect_result─┬──────────correct_result─┐ │ 2021-09-07 13:41:49.926 │ 2021-09-07 13:41:50.926 │ 1970-01-01 00:00:00.000 │ 2021-09-07 13:41:50.826 │ └─────────────────────────┴─────────────────────────┴─────────────────────────┴─────────────────────────┘ WITH toDateTime64(&amp;#39;2021-03-03 09:30:00.100&amp;#39;, 3) AS time, fromUnixTimestamp64Milli(toInt64(toUnixTimestamp64Milli(time) + (1.25 * 1000))) AS first, toDateTime64(toDecimal64(time, 3) + toDecimal64(&amp;#39;1.25&amp;#39;, 3), 3) AS second, reinterpret(reinterpret(time, &amp;#39;Decimal64(3)&amp;#39;) + toDecimal64(&amp;#39;1.</description></item><item><title>Debug hanging thing</title><link>http://kb.altinity.com/altinity-kb-useful-queries/debug-hang/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/debug-hang/</guid><description>Debug hanging / freezing things If ClickHouse® is busy with something and you don&amp;rsquo;t know what&amp;rsquo;s happening, you can easily check the stacktraces of all the thread which are working
SELECT arrayStringConcat(arrayMap(x -&amp;gt; concat(&amp;#39;0x&amp;#39;, lower(hex(x)), &amp;#39;\t&amp;#39;, demangle(addressToSymbol(x))), trace), &amp;#39;\n&amp;#39;) as trace_functions, count() FROM system.stack_trace GROUP BY trace_functions ORDER BY count() DESC SETTINGS allow_introspection_functions=1 FORMAT Vertical; If you can&amp;rsquo;t start any queries, but you have access to the node, you can sent a signal</description></item><item><title>Description of asynchronous_metrics</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/asynchronous_metrics_descr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/asynchronous_metrics_descr/</guid><description>CompiledExpressionCacheCount -- number or compiled cached expression (if CompiledExpressionCache is enabled) jemalloc -- parameters of jemalloc allocator, they are not very useful, and not interesting MarkCacheBytes / MarkCacheFiles -- there are cache for .mrk files (default size is 5GB), you can see is it use all 5GB or not MemoryCode -- how much memory allocated for ClickHouse® executable MemoryDataAndStack -- virtual memory allocated for data and stack MemoryResident -- real memory used by ClickHouse ( the same as top RES/RSS) MemoryShared -- shared memory used by ClickHouse MemoryVirtual -- virtual memory used by ClickHouse ( the same as top VIRT) NumberOfDatabases NumberOfTables ReplicasMaxAbsoluteDelay -- important parameter - replica max absolute delay in seconds ReplicasMaxRelativeDelay -- replica max relative delay (from other replicas) in seconds ReplicasMaxInsertsInQueue -- max number of parts to fetch for a single Replicated table ReplicasSumInsertsInQueue -- sum of parts to fetch for all Replicated tables ReplicasMaxMergesInQueue -- max number of merges in queue for a single Replicated table ReplicasSumMergesInQueue -- total number of merges in queue for all Replicated tables ReplicasMaxQueueSize -- max number of tasks for a single Replicated table ReplicasSumQueueSize -- total number of tasks in replication queue UncompressedCacheBytes/UncompressedCacheCells -- allocated memory for uncompressed cache (disabled by default) Uptime -- uptime seconds</description></item><item><title>ClickHouse® data/disk encryption (at rest)</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/disk_encryption/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/disk_encryption/</guid><description>Create folder mkdir /data/clickhouse_encrypted chown clickhouse.clickhouse /data/clickhouse_encrypted Configure encrypted disk and storage https://clickhouse.com/docs/en/operations/storing-data/#encrypted-virtual-file-system https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server-settings-encryption cat /etc/clickhouse-server/config.d/encrypted_storage.xml &amp;lt;clickhouse&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;disk1&amp;gt; &amp;lt;type&amp;gt;local&amp;lt;/type&amp;gt; &amp;lt;path&amp;gt;/data/clickhouse_encrypted/&amp;lt;/path&amp;gt; &amp;lt;/disk1&amp;gt; &amp;lt;encrypted_disk&amp;gt; &amp;lt;type&amp;gt;encrypted&amp;lt;/type&amp;gt; &amp;lt;disk&amp;gt;disk1&amp;lt;/disk&amp;gt; &amp;lt;path&amp;gt;encrypted/&amp;lt;/path&amp;gt; &amp;lt;algorithm&amp;gt;AES_128_CTR&amp;lt;/algorithm&amp;gt; &amp;lt;key_hex id=&amp;#34;0&amp;#34;&amp;gt;00112233445566778899aabbccddeeff&amp;lt;/key_hex&amp;gt; &amp;lt;current_key_id&amp;gt;0&amp;lt;/current_key_id&amp;gt; &amp;lt;/encrypted_disk&amp;gt; &amp;lt;/disks&amp;gt; &amp;lt;policies&amp;gt; &amp;lt;encrypted&amp;gt; &amp;lt;volumes&amp;gt; &amp;lt;encrypted_volume&amp;gt; &amp;lt;disk&amp;gt;encrypted_disk&amp;lt;/disk&amp;gt; &amp;lt;/encrypted_volume&amp;gt; &amp;lt;/volumes&amp;gt; &amp;lt;/encrypted&amp;gt; &amp;lt;/policies&amp;gt; &amp;lt;/storage_configuration&amp;gt; &amp;lt;/clickhouse&amp;gt; systemctl restart clickhouse-server select name, path, type, is_encrypted from system.disks; ┌─name───────────┬─path──────────────────────────────────┬─type──┬─is_encrypted─┐ │ default │ /var/lib/clickhouse/ │ local │ 0 │ │ disk1 │ /data/clickhouse_encrypted/ │ local │ 0 │ │ encrypted_disk │ /data/clickhouse_encrypted/encrypted/ │ local │ 1 │ └────────────────┴───────────────────────────────────────┴───────┴──────────────┘ select * from system.</description></item><item><title>DISTINCT &amp; GROUP BY &amp; LIMIT 1 BY what the difference</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/distinct-vs-group-by-vs-limit-by/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/distinct-vs-group-by-vs-limit-by/</guid><description>DISTINCT SELECT DISTINCT number FROM numbers_mt(100000000) FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.00 GiB. 0 rows in set. Elapsed: 18.720 sec. Processed 100.03 million rows, 800.21 MB (5.34 million rows/s., 42.75 MB/s.) SELECT DISTINCT number FROM numbers_mt(100000000) SETTINGS max_threads = 1 FORMAT `Null` MemoryTracker: Peak memory usage (for query): 4.00 GiB. 0 rows in set. Elapsed: 18.349 sec. Processed 100.03 million rows, 800.21 MB (5.45 million rows/s., 43.61 MB/s.</description></item><item><title>DR two DC</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/dr-two-dc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/dr-two-dc/</guid><description>Clickhouse uses Keeper (or ZooKeeper) to inform other cluster nodes about changes. Clickhouse nodes then fetch new parts directly from other nodes in the cluster. The Keeper cluster is a key for building a DR schema. You can consider Keeper a “true” cluster while clickhouse-server nodes as storage access instruments.
To implement a disaster recovery (DR) setup for ClickHouse across two physically separated data centers (A and B), with only one side active at a time, you can create a single ClickHouse cluster spanning both data centers.</description></item><item><title>How to encode/decode quantileTDigest states from/to list of centroids</title><link>http://kb.altinity.com/altinity-kb-functions/how-to-encode-decode-quantiletdigest-state/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/how-to-encode-decode-quantiletdigest-state/</guid><description>quantileTDigestState quantileTDigestState is stored in two parts: a count of centroids in LEB128 format + list of centroids without a delimiter. Each centroid is represented as two Float32 values: Mean &amp;amp; Count.
SELECT hex(quantileTDigestState(1)), hex(toFloat32(1)) ┌─hex(quantileTDigestState(1))─┬─hex(toFloat32(1))─┐ │ 010000803F0000803F │ 0000803F │ └──────────────────────────────┴───────────────────┘ 01 0000803F 0000803F ^ ^ ^ LEB128 Float32 Mean Float32 Count We need to make two helper UDF functions:
cat /etc/clickhouse-server/decodeTDigestState_function.xml &amp;lt;yandex&amp;gt; &amp;lt;function&amp;gt; &amp;lt;type&amp;gt;executable&amp;lt;/type&amp;gt; &amp;lt;execute_direct&amp;gt;0&amp;lt;/execute_direct&amp;gt; &amp;lt;name&amp;gt;decodeTDigestState&amp;lt;/name&amp;gt; &amp;lt;return_type&amp;gt;Array(Tuple(mean Float32, count Float32))&amp;lt;/return_type&amp;gt; &amp;lt;argument&amp;gt; &amp;lt;type&amp;gt;AggregateFunction(quantileTDigest, UInt32)&amp;lt;/type&amp;gt; &amp;lt;/argument&amp;gt; &amp;lt;format&amp;gt;RowBinary&amp;lt;/format&amp;gt; &amp;lt;command&amp;gt;cat&amp;lt;/command&amp;gt; &amp;lt;send_chunk_header&amp;gt;0&amp;lt;/send_chunk_header&amp;gt; &amp;lt;/function&amp;gt; &amp;lt;/yandex&amp;gt; cat /etc/clickhouse-server/encodeTDigestState_function.</description></item><item><title>MSSQL bcp pipe to clickhouse-client</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/mssql-clickhouse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/mssql-clickhouse/</guid><description>How to pipe data to ClickHouse® from bcp export tool for MSSQL database Prepare tables LAPTOP.localdomain :) CREATE TABLE tbl(key UInt32) ENGINE=MergeTree ORDER BY key; root@LAPTOP:/home/user# sqlcmd -U sa -P Password78 1&amp;gt; WITH t0(i) AS (SELECT 0 UNION ALL SELECT 0), t1(i) AS (SELECT 0 FROM t0 a, t0 b), t2(i) AS (SELECT 0 FROM t1 a, t1 b), t3(i) AS (SELECT 0 FROM t2 a, t2 b), t4(i) AS (SELECT 0 FROM t3 a, t3 b), t5(i) AS (SELECT 0 FROM t4 a, t3 b),n(i) AS (SELECT ROW_NUMBER() OVER(ORDER BY (SELECT 0)) FROM t5) SELECT i INTO tbl FROM n WHERE i BETWEEN 1 AND 16777216 2&amp;gt; GO (16777216 rows affected) root@LAPTOP:/home/user# sqlcmd -U sa -P Password78 -Q &amp;#34;SELECT count(*) FROM tbl&amp;#34; ----------- 16777216 (1 rows affected) Piping root@LAPTOP:/home/user# mkfifo import_pipe root@LAPTOP:/home/user# bcp &amp;#34;SELECT * FROM tbl&amp;#34; queryout import_pipe -t, -c -b 200000 -U sa -P Password78 -S localhost &amp;amp; [1] 6038 root@LAPTOP:/home/user# Starting copy.</description></item><item><title>Functions to count uniqs</title><link>http://kb.altinity.com/altinity-kb-schema-design/uniq-functions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/uniq-functions/</guid><description>Functions to count uniqs Function Function(State) StateSize Result QPS uniqExact uniqExactState 1600003 100000 59.23 uniq uniqState 200804 100315 85.55 uniqCombined uniqCombinedState 98505 100314 108.09 uniqCombined(12) uniqCombinedState(12) 3291 98160 151.64 uniqCombined(15) uniqCombinedState(15) 24783 100768 110.18 uniqCombined(18) uniqCombinedState(18) 196805 100332 101.56 uniqCombined(20) uniqCombinedState(20) 786621 100088 65.05 uniqCombined64(12) uniqCombined64State(12) 3291 98160 164.96 uniqCombined64(15) uniqCombined64State(15) 24783 100768 133.96 uniqCombined64(18) uniqCombined64State(18) 196805 100332 110.85 uniqCombined64(20) uniqCombined64State(20) 786621 100088 66.48 uniqHLL12 uniqHLL12State 2651 101344 177.91 uniqTheta uniqThetaState 32795 98045 144.</description></item><item><title>GROUP BY tricks</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/group-by/tricks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/group-by/tricks/</guid><description>Tricks Testing dataset
CREATE TABLE sessions ( `app` LowCardinality(String), `user_id` String, `created_at` DateTime, `platform` LowCardinality(String), `clicks` UInt32, `session_id` UUID ) ENGINE = MergeTree PARTITION BY toYYYYMM(created_at) ORDER BY (app, user_id, session_id, created_at) INSERT INTO sessions WITH CAST(number % 4, &amp;#39;Enum8(\&amp;#39;Orange\&amp;#39; = 0, \&amp;#39;Melon\&amp;#39; = 1, \&amp;#39;Red\&amp;#39; = 2, \&amp;#39;Blue\&amp;#39; = 3)&amp;#39;) AS app, concat(&amp;#39;UID: &amp;#39;, leftPad(toString(number % 20000000), 8, &amp;#39;0&amp;#39;)) AS user_id, toDateTime(&amp;#39;2021-10-01 10:11:12&amp;#39;) + (number / 300) AS created_at, CAST((number + 14) % 3, &amp;#39;Enum8(\&amp;#39;Bat\&amp;#39; = 0, \&amp;#39;Mice\&amp;#39; = 1, \&amp;#39;Rat\&amp;#39; = 2)&amp;#39;) AS platform, number % 17 AS clicks, generateUUIDv4() AS session_id SELECT app, user_id, created_at, platform, clicks, session_id FROM numbers_mt(1000000000) 0 rows in set.</description></item><item><title>Handy queries for system.query_log</title><link>http://kb.altinity.com/altinity-kb-useful-queries/query_log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/query_log/</guid><description>The most cpu / write / read-intensive queries from query_log SELECT normalized_query_hash, any(query), count(), sum(query_duration_ms) / 1000 AS QueriesDuration, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;RealTimeMicroseconds&amp;#39;)]) / 1000000 AS RealTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;UserTimeMicroseconds&amp;#39;)]) / 1000000 AS UserTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;SystemTimeMicroseconds&amp;#39;)]) / 1000000 AS SystemTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;DiskReadElapsedMicroseconds&amp;#39;)]) / 1000000 AS DiskReadTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;DiskWriteElapsedMicroseconds&amp;#39;)]) / 1000000 AS DiskWriteTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;NetworkSendElapsedMicroseconds&amp;#39;)]) / 1000000 AS NetworkSendTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;NetworkReceiveElapsedMicroseconds&amp;#39;)]) / 1000000 AS NetworkReceiveTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;ZooKeeperWaitMicroseconds&amp;#39;)]) / 1000000 AS ZooKeeperWaitTime, sum(ProfileEvents.Values[indexOf(ProfileEvents.Names, &amp;#39;OSIOWaitMicroseconds&amp;#39;)]) / 1000000 AS OSIOWaitTime, sum(ProfileEvents.</description></item><item><title>How ALTERs work in ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/alters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/alters/</guid><description>How ALTERs work in ClickHouse®: ADD (COLUMN/INDEX/PROJECTION) Lightweight, will only change table metadata. So new entity will be added in case of creation of new parts during INSERT&amp;rsquo;s OR during merges of old parts.
In case of COLUMN, ClickHouse will calculate column value on fly in query context.
Warning CREATE TABLE test_materialization ( `key` UInt32, `value` UInt32 ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_materialization(key, value) SELECT 1, 1; INSERT INTO test_materialization(key, value) SELECT 2, 2; ALTER TABLE test_materialization ADD COLUMN inserted_at DateTime DEFAULT now(); SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:28:58 │ └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 2 │ 2022-09-01 03:28:58 │ └─────┴─────────────────────┘ SELECT key, inserted_at FROM test_materialization; ┌─key─┬─────────inserted_at─┐ │ 1 │ 2022-09-01 03:29:11 │ └─────┴─────────────────────┘ ┌─key─┬─────────inserted_at─┐ │ 2 │ 2022-09-01 03:29:11 │ └─────┴─────────────────────┘ Each query will return different inserted_at value, because each time now() function being executed.</description></item><item><title>How to change ORDER BY</title><link>http://kb.altinity.com/altinity-kb-schema-design/change-order-by/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/change-order-by/</guid><description>Create a new table and copy data through an intermediate table. Step by step procedure. We want to add column3 to the ORDER BY in this table:
CREATE TABLE example_table ( date Date, column1 String, column2 String, column3 String, column4 String ) ENGINE = ReplicatedMergeTree(&amp;#39;/clickhouse/{cluster}/tables/{shard}/default/example_table&amp;#39;, &amp;#39;{replica}&amp;#39;) PARTITION BY toYYYYMM(date) ORDER BY (column1, column2) Stop publishing/INSERT into example_table.
Rename table example_table to example_table_old
Create the new table with the old name. This will preserve all dependencies like materialized views.</description></item><item><title>How to Convert Ordinary to Atomic</title><link>http://kb.altinity.com/engines/altinity-kb-atomic-database-engine/how-to-convert-ordinary-to-atomic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/altinity-kb-atomic-database-engine/how-to-convert-ordinary-to-atomic/</guid><description>New, official way Implemented automatic conversion of database engine from Ordinary to Atomic (ClickHouse® Server 22.8+). Create empty convert_ordinary_to_atomic file in flags directory and all Ordinary databases will be converted automatically on next server start. The conversion is not automatic between upgrades, you need to set the flag as explained below: Warnings: * Server has databases (for example `test`) with Ordinary engine, which was deprecated. To convert this database to the new Atomic engine, create a flag /var/lib/clickhouse/flags/convert_ordinary_to_atomic and make sure that ClickHouse has write permission for it.</description></item><item><title>How to recreate a table in case of total corruption of the replication queue</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/how_to_recreate_table/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/how_to_recreate_table/</guid><description>How to fix a replication using hard-reset way Find the best replica (replica with the most fresh/consistent) data. Backup the table alter table mydatabase.mybadtable freeze; Stop all applications!!! Stop ingestion. Stop queries - table will be empty for some time. Check that detached folder is empty or clean it. SELECT concat(&amp;#39;alter table &amp;#39;, database, &amp;#39;.&amp;#39;, table, &amp;#39; drop detached part \&amp;#39;&amp;#39;, name, &amp;#39;\&amp;#39; settings allow_drop_detached=1;&amp;#39;) FROM system.detached_parts WHERE (database = &amp;#39;mydatabase&amp;#39;) AND (table = &amp;#39;mybadtable&amp;#39;) FORMAT TSVRaw; Make sure that detached folder is empty select count() from system.</description></item><item><title>http handler example</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/http_handlers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/http_handlers/</guid><description>http handler example (how to disable /play) # cat /etc/clickhouse-server/config.d/play_disable.xml &amp;lt;?xml version=&amp;#34;1.0&amp;#34; ?&amp;gt; &amp;lt;yandex&amp;gt; &amp;lt;http_handlers&amp;gt; &amp;lt;rule&amp;gt; &amp;lt;url&amp;gt;/play&amp;lt;/url&amp;gt; &amp;lt;methods&amp;gt;GET&amp;lt;/methods&amp;gt; &amp;lt;handler&amp;gt; &amp;lt;type&amp;gt;static&amp;lt;/type&amp;gt; &amp;lt;status&amp;gt;403&amp;lt;/status&amp;gt; &amp;lt;content_type&amp;gt;text/plain; charset=UTF-8&amp;lt;/content_type&amp;gt; &amp;lt;response_content&amp;gt;&amp;lt;/response_content&amp;gt; &amp;lt;/handler&amp;gt; &amp;lt;/rule&amp;gt; &amp;lt;defaults/&amp;gt; &amp;lt;!-- handler to save default handlers ?query / ping --&amp;gt; &amp;lt;/http_handlers&amp;gt; &amp;lt;/yandex&amp;gt;</description></item><item><title>Idempotent inserts into a materialized view</title><link>http://kb.altinity.com/altinity-kb-schema-design/materialized-views/idempotent_inserts_mv/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/materialized-views/idempotent_inserts_mv/</guid><description>Why inserts into materialized views are not idempotent? ClickHouse® still does not have transactions. They were to be implemented around 2022Q2 but still not in the roadmap.
Because of ClickHouse materialized view is a trigger. And an insert into a table and an insert into a subordinate materialized view it&amp;rsquo;s two different inserts so they are not atomic altogether.
And insert into a materialized view may fail after the successful insert into the table.</description></item><item><title>Imprecise parsing of literal Decimal or Float64</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/literal-decimal-or-float/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/literal-decimal-or-float/</guid><description>Decimal SELECT 9.2::Decimal64(2) AS postgresql_cast, toDecimal64(9.2, 2) AS to_function, CAST(9.2, &amp;#39;Decimal64(2)&amp;#39;) AS cast_float_literal, CAST(&amp;#39;9.2&amp;#39;, &amp;#39;Decimal64(2)&amp;#39;) AS cast_string_literal ┌─postgresql_cast─┬─to_function─┬─cast_float_literal─┬─cast_string_literal─┐ │ 9.2 │ 9.19 │ 9.19 │ 9.2 │ └─────────────────┴─────────────┴────────────────────┴─────────────────────┘ When we try to type cast 64.32 to Decimal128(2) the resulted value is 64.31.
When it sees a number with a decimal separator it interprets as Float64 literal (where 64.32 have no accurate representation, and actually you get something like 64.319999999999999999) and later that Float is casted to Decimal by removing the extra precision.</description></item><item><title>Ingestion metrics from system.part_log</title><link>http://kb.altinity.com/altinity-kb-useful-queries/ingestion-rate-part_log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/ingestion-rate-part_log/</guid><description>Insert rate Returns aggregated insert metrics, per table, for the current day (by default), including parts per insert, rows/bytes per insert, and rows/bytes per part.
select database, table, time_bucket, max(number_of_parts_per_insert) max_parts_pi, median(number_of_parts_per_insert) median_parts_pi, min(min_rows_per_part) min_rows_pp, max(max_rows_per_part) max_rows_pp, median(median_rows_per_part) median_rows_pp, min(rows_per_insert) min_rows_pi, median(rows_per_insert) median_rows_pi, max(rows_per_insert) max_rows_pi, sum(rows_per_insert) rows_inserted, sum(seconds_per_insert) parts_creation_seconds, count() inserts, sum(number_of_parts_per_insert) new_parts, max(last_part_pi) - min(first_part_pi) as insert_period, inserts*60/insert_period as inserts_per_minute from (SELECT database, table, toStartOfDay(event_time) AS time_bucket, count() AS number_of_parts_per_insert, min(rows) AS min_rows_per_part, max(rows) AS max_rows_per_part, median(rows) AS median_rows_per_part, sum(rows) AS rows_per_insert, min(size_in_bytes) AS min_bytes_per_part, max(size_in_bytes) AS max_bytes_per_part, median(size_in_bytes) AS median_bytes_per_part, sum(size_in_bytes) AS bytes_per_insert, median_bytes_per_part / median_rows_per_part AS avg_row_size, sum(duration_ms)/1000 as seconds_per_insert, max(event_time) as last_part_pi, min(event_time) as first_part_pi FROM system.</description></item><item><title>Ingestion of AggregateFunction</title><link>http://kb.altinity.com/altinity-kb-schema-design/ingestion-aggregate-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/ingestion-aggregate-function/</guid><description>How to insert AggregateFunction data Ephemeral column CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name_stub String Ephemeral, name AggregateFunction(argMax, String, DateTime) default arrayReduce(&amp;#39;argMaxState&amp;#39;, [name_stub], [updated]) ) ENGINE=AggregatingMergeTree order by uid; INSERT INTO users(uid, updated, name_stub) VALUES (1231, &amp;#39;2020-01-02 00:00:00&amp;#39;, &amp;#39;Jane&amp;#39;); INSERT INTO users(uid, updated, name_stub) VALUES (1231, &amp;#39;2020-01-01 00:00:00&amp;#39;, &amp;#39;John&amp;#39;); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘ Input function CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name AggregateFunction(argMax, String, DateTime) ) ENGINE=AggregatingMergeTree order by uid; INSERT INTO users SELECT uid, updated, arrayReduce(&amp;#39;argMaxState&amp;#39;, [name], [updated]) FROM input(&amp;#39;uid Int16, updated DateTime, name String&amp;#39;) FORMAT Values (1231, &amp;#39;2020-01-02 00:00:00&amp;#39;, &amp;#39;Jane&amp;#39;); INSERT INTO users SELECT uid, updated, arrayReduce(&amp;#39;argMaxState&amp;#39;, [name], [updated]) FROM input(&amp;#39;uid Int16, updated DateTime, name String&amp;#39;) FORMAT Values (1231, &amp;#39;2020-01-01 00:00:00&amp;#39;, &amp;#39;John&amp;#39;); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid; ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘ Materialized View And Null Engine CREATE TABLE users ( uid Int16, updated SimpleAggregateFunction(max, DateTime), name AggregateFunction(argMax, String, DateTime) ) ENGINE=AggregatingMergeTree order by uid; CREATE TABLE users_null ( uid Int16, updated DateTime, name String ) ENGINE=Null; CREATE MATERIALIZED VIEW users_mv TO users AS SELECT uid, updated, arrayReduce(&amp;#39;argMaxState&amp;#39;, [name], [updated]) name FROM users_null; INSERT INTO users_null Values (1231, &amp;#39;2020-01-02 00:00:00&amp;#39;, &amp;#39;Jane&amp;#39;); INSERT INTO users_null Values (1231, &amp;#39;2020-01-01 00:00:00&amp;#39;, &amp;#39;John&amp;#39;); SELECT uid, max(updated) AS updated, argMaxMerge(name) FROM users GROUP BY uid; ┌──uid─┬─────────────updated─┬─argMaxMerge(name)─┐ │ 1231 │ 2020-01-02 00:00:00 │ Jane │ └──────┴─────────────────────┴───────────────────┘</description></item><item><title>Insert Deduplication / Insert Idempotency</title><link>http://kb.altinity.com/altinity-kb-schema-design/insert_deduplication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/insert_deduplication/</guid><description>Replicated tables have a special feature insert deduplication (enabled by default).
Documentation: Data blocks are deduplicated. For multiple writes of the same data block (data blocks of the same size containing the same rows in the same order), the block is only written once. The reason for this is in case of network failures when the client application does not know if the data was written to the DB, so the INSERT query can simply be repeated.</description></item><item><title>MySQL</title><link>http://kb.altinity.com/altinity-kb-integrations/mysql-clickhouse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/mysql-clickhouse/</guid><description>Replication using MaterializeMySQL. https://clickhouse.com/docs/en/engines/database-engines/materialized-mysql https://translate.google.com/translate?sl=auto&amp;amp;tl=en&amp;amp;u=https://www.jianshu.com/p/d0d4306411b3 https://raw.githubusercontent.com/ClickHouse/clickhouse-presentations/master/meetup47/materialize_mysql.pdf It reads mysql binlog directly and transform queries into something which ClickHouse® can support. Supports updates and deletes (under the hood implemented via something like ReplacingMergeTree with enforced FINAL and &amp;lsquo;deleted&amp;rsquo; flag). Status is &amp;rsquo;experimental&amp;rsquo;, there are quite a lot of known limitations and issues, but some people use it. The original author of that went to another project, and the main team don&amp;rsquo;t have a lot of resource to improve that for now (more important thing in the backlog)</description></item><item><title>Istio Issues</title><link>http://kb.altinity.com/altinity-kb-kubernetes/altinity-kb-istio-user-issue-k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-kubernetes/altinity-kb-istio-user-issue-k8s/</guid><description>What is Istio? Per documentation on Istio Project&amp;#39;s website , Istio is &amp;ldquo;an open source service mesh that layers transparently onto existing distributed applications. Istio’s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring – with few or no service code changes.&amp;rdquo;
Istio works quite well at providing this functionality, and does so through controlling service-to-service communication in a Cluster, find-grained control of traffic behavior, routing rules, load-balancing, a policy layer and configuration API supporting access controls, rate limiting, etc.</description></item><item><title>Jemalloc heap profiling</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/jemalloc_heap_profiling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/jemalloc_heap_profiling/</guid><description>Config &amp;lt;!-- cat config.d/jemalloc_dict.xml --&amp;gt; &amp;lt;clickhouse&amp;gt; &amp;lt;dictionaries_config&amp;gt;/etc/clickhouse-server/config.d/*_dict.xml&amp;lt;/dictionaries_config&amp;gt; &amp;lt;http_handlers&amp;gt; &amp;lt;rule&amp;gt; &amp;lt;url&amp;gt;/pprof/heap&amp;lt;/url&amp;gt; &amp;lt;methods&amp;gt;GET,POST&amp;lt;/methods&amp;gt; &amp;lt;handler&amp;gt; &amp;lt;type&amp;gt;static&amp;lt;/type&amp;gt; &amp;lt;response_content&amp;gt;file://jemalloc_clickhouse.heap&amp;lt;/response_content&amp;gt; &amp;lt;/handler&amp;gt; &amp;lt;/rule&amp;gt; &amp;lt;rule&amp;gt; &amp;lt;url&amp;gt;/pprof/cmdline&amp;lt;/url&amp;gt; &amp;lt;methods&amp;gt;GET&amp;lt;/methods&amp;gt; &amp;lt;handler&amp;gt; &amp;lt;type&amp;gt;predefined_query_handler&amp;lt;/type&amp;gt; &amp;lt;query&amp;gt;SELECT &amp;#39;/var/lib/clickhouse&amp;#39; FORMAT TSVRaw&amp;lt;/query&amp;gt; &amp;lt;/handler&amp;gt; &amp;lt;/rule&amp;gt; &amp;lt;rule&amp;gt; &amp;lt;url&amp;gt;/pprof/symbol&amp;lt;/url&amp;gt; &amp;lt;methods&amp;gt;GET&amp;lt;/methods&amp;gt; &amp;lt;handler&amp;gt; &amp;lt;type&amp;gt;predefined_query_handler&amp;lt;/type&amp;gt; &amp;lt;query&amp;gt;SELECT &amp;#39;num_symbols: &amp;#39; || count() FROM system.symbols FORMAT TSVRaw SETTINGS allow_introspection_functions = 1&amp;lt;/query&amp;gt; &amp;lt;/handler&amp;gt; &amp;lt;/rule&amp;gt; &amp;lt;rule&amp;gt; &amp;lt;url&amp;gt;/pprof/symbol&amp;lt;/url&amp;gt; &amp;lt;methods&amp;gt;POST&amp;lt;/methods&amp;gt; &amp;lt;handler&amp;gt; &amp;lt;type&amp;gt;predefined_query_handler&amp;lt;/type&amp;gt; &amp;lt;query&amp;gt;WITH arrayJoin(splitByChar(&amp;#39;+&amp;#39;, {_request_body:String})) as addr SELECT addr || &amp;#39; &amp;#39; || demangle(addressToSymbol(reinterpretAsUInt64(reverse(substr(unhex(addr),2))))) SETTINGS allow_introspection_functions = 1 FORMAT TSVRaw&amp;lt;/query&amp;gt; &amp;lt;/handler&amp;gt; &amp;lt;/rule&amp;gt; &amp;lt;defaults/&amp;gt; &amp;lt;/http_handlers&amp;gt; &amp;lt;dictionary&amp;gt; &amp;lt;name&amp;gt;jemalloc_ls&amp;lt;/name&amp;gt; &amp;lt;structure&amp;gt; &amp;lt;key&amp;gt; &amp;lt;attribute&amp;gt; &amp;lt;name&amp;gt;id&amp;lt;/name&amp;gt; &amp;lt;type&amp;gt;String&amp;lt;/type&amp;gt; &amp;lt;/attribute&amp;gt; &amp;lt;/key&amp;gt; &amp;lt;attribute&amp;gt; &amp;lt;name&amp;gt;file&amp;lt;/name&amp;gt; &amp;lt;type&amp;gt;String&amp;lt;/type&amp;gt; &amp;lt;null_value /&amp;gt; &amp;lt;/attribute&amp;gt; &amp;lt;attribute&amp;gt; &amp;lt;name&amp;gt;size&amp;lt;/name&amp;gt; &amp;lt;type&amp;gt;UInt32&amp;lt;/type&amp;gt; &amp;lt;null_value /&amp;gt; &amp;lt;/attribute&amp;gt; &amp;lt;attribute&amp;gt; &amp;lt;name&amp;gt;time&amp;lt;/name&amp;gt; &amp;lt;type&amp;gt;DateTime&amp;lt;/type&amp;gt; &amp;lt;null_value /&amp;gt; &amp;lt;/attribute&amp;gt; &amp;lt;/structure&amp;gt; &amp;lt;source&amp;gt; &amp;lt;executable&amp;gt; &amp;lt;command&amp;gt;for f in /tmp/jemalloc_clickhouse.</description></item><item><title>JSONEachRow, Tuples, Maps and Materialized Views</title><link>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-jsoneachrow-tuples-and-mvs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-jsoneachrow-tuples-and-mvs/</guid><description>Using JSONEachRow with Tuple() in Materialized views Sometimes we can have a nested json message with a fixed size structure like this:
{&amp;#34;s&amp;#34;: &amp;#34;val1&amp;#34;, &amp;#34;t&amp;#34;: {&amp;#34;i&amp;#34;: 42, &amp;#34;d&amp;#34;: &amp;#34;2023-09-01 12:23:34.231&amp;#34;}} Values can be NULL but the structure should be fixed. In this case we can use Tuple() to parse the JSON message:
CREATE TABLE tests.nest_tuple_source ( `s` String, `t` Tuple(`i` UInt8, `d` DateTime64(3)) ) ENGINE = Null We can use the above table as a source for a materialized view, like it was a Kafka table and in case our message has unexpected keys we make the Kafka table ignore them with the setting (23.</description></item><item><title>Kafka engine Virtual columns</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-virtual-columns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-virtual-columns/</guid><description>Kafka engine virtual columns (built-in) From the Kafka engine docs , the supported virtual columns are:
_topic — Kafka topic (LowCardinality(String))
_key — message key (String)
_offset — message offset (UInt64)
_timestamp — message timestamp (Nullable(DateTime))
_timestamp_ms — timestamp with millisecond precision (Nullable(DateTime64(3)))
_partition — partition (UInt64)
_headers.name — header keys (Array(String))
_headers.value — header values (Array(String))
Extra virtual columns when you enable parse-error streaming:
If you set kafka_handle_error_mode='stream', ClickHouse adds:</description></item><item><title>kurt &amp; skew statistical functions in ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-functions/kurt_skew_statistics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/kurt_skew_statistics/</guid><description>from scipy.stats import skew, kurtosis # Creating a dataset dataset = [10,17,71,6,55,38,27,61,48,46,21,38,2,67,35,77,29,31,27,67,81,82,75,81,31,38,68,95,37,34,65,59,81,28,82,80,35,3,97,42,66,28,85,98,45,15,41,61,24,53,97,86,5,65,84,18,9,32,46,52,69,44,78,98,61,64,26,11,3,19,0,90,28,72,47,8,0,74,38,63,88,43,81,61,34,24,37,53,79,72,5,77,58,3,61,56,1,3,5,61] print(skew(dataset, axis=0, bias=True), skew(dataset)) # -0.05785361619432152 -0.05785361619432152 WITH arrayJoin([10,17,71,6,55,38,27,61,48,46,21,38,2,67,35,77,29,31,27,67,81,82,75,81,31,38,68,95,37,34,65,59,81,28,82,80,35,3,97,42,66,28,85,98,45,15,41,61,24,53,97,86,5,65,84,18,9,32,46,52,69,44,78,98,61,64,26,11,3,19,0,90,28,72,47,8,0,74,38,63,88,43,81,61,34,24,37,53,79,72,5,77,58,3,61,56,1,3,5,61]) AS value SELECT skewPop(value) AS ex_1 ┌──────────────────ex_1─┐ │ -0.057853616194321014 │ └───────────────────────┘ print(skew(dataset, bias=False)) # -0.05873838908626328 WITH arrayJoin([10, 17, 71, 6, 55, 38, 27, 61, 48, 46, 21, 38, 2, 67, 35, 77, 29, 31, 27, 67, 81, 82, 75, 81, 31, 38, 68, 95, 37, 34, 65, 59, 81, 28, 82, 80, 35, 3, 97, 42, 66, 28, 85, 98, 45, 15, 41, 61, 24, 53, 97, 86, 5, 65, 84, 18, 9, 32, 46, 52, 69, 44, 78, 98, 61, 64, 26, 11, 3, 19, 0, 90, 28, 72, 47, 8, 0, 74, 38, 63, 88, 43, 81, 61, 34, 24, 37, 53, 79, 72, 5, 77, 58, 3, 61, 56, 1, 3, 5, 61]) AS value SELECT skewSamp(value) AS ex_1, (pow(count(), 2) * ex_1) / ((count() - 1) * (count() - 2)) AS G ┌─────────────────ex_1─┬────────────────────G─┐ │ -0.</description></item><item><title>Logging</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/logging/</guid><description>Q. I get errors:
File not found: /var/log/clickhouse-server/clickhouse-server.log.0. File not found: /var/log/clickhouse-server/clickhouse-server.log.8.gz. ... File not found: /var/log/clickhouse-server/clickhouse-server.err.log.0, Stack trace (when copying this message, always include the lines below): 0. Poco::FileImpl::handleLastErrorImpl(std::__1::basic_string&amp;lt;char, std::__1::char_traits&amp;lt;char&amp;gt;, std::__1::allocator&amp;lt;char&amp;gt; &amp;gt; const&amp;amp;) @ 0x11c2b345 in /usr/bin/clickhouse 1. Poco::PurgeOneFileStrategy::purge(std::__1::basic_string&amp;lt;char, std::__1::char_traits&amp;lt;char&amp;gt;, std::__1::allocator&amp;lt;char&amp;gt; &amp;gt; const&amp;amp;) @ 0x11c84618 in /usr/bin/clickhouse 2. Poco::FileChannel::log(Poco::Message const&amp;amp;) @ 0x11c314cc in /usr/bin/clickhouse 3. DB::OwnFormattingChannel::logExtended(DB::ExtendedLogMessage const&amp;amp;) @ 0x8681402 in /usr/bin/clickhouse 4. DB::OwnSplitChannel::logSplit(Poco::Message const&amp;amp;) @ 0x8682fa8 in /usr/bin/clickhouse 5. DB::OwnSplitChannel::log(Poco::Message const&amp;amp;) @ 0x8682e41 in /usr/bin/clickhouse A.</description></item><item><title>High Memory Usage During Merge in system.metric_log</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/metric_log_ram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/metric_log_ram/</guid><description>Overview In recent versions of ClickHouse®, the merge process (part compaction) in the system.metric_log table can consume a large amount of memory. The issue arises due to an unfortunate combination of settings, where:
the merge is already large enough to produce wide parts, but not yet large enough to enable vertical merges. This problem has become more pronounced in newer ClickHouse® versions because the system.metric_log table has expanded significantly — many new metrics were added, increasing the total number of columns.</description></item><item><title>MODIFY (ADD) TTL in ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/modify-ttl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/modify-ttl/</guid><description>For a general overview of TTL, see the article Putting Things Where They Belong Using New TTL Moves .
ALTER TABLE tbl MODIFY (ADD) TTL: It&amp;rsquo;s 2 step process:
ALTER TABLE tbl MODIFY (ADD) TTL ... Update table metadata: schema .sql &amp;amp; metadata in ZK. It&amp;rsquo;s usually cheap and fast command. And any new INSERT after schema change will calculate TTL according to new rule.
ALTER TABLE tbl MATERIALIZE TTL Recalculate TTL for already exist parts.</description></item><item><title>Multiple aligned date columns in PARTITION BY expression</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/multiple-date-column-in-partition-key/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/multiple-date-column-in-partition-key/</guid><description>Alternative to doing that by minmax skip index .
CREATE TABLE part_key_multiple_dates ( `key` UInt32, `date` Date, `time` DateTime, `created_at` DateTime, `inserted_at` DateTime ) ENGINE = MergeTree PARTITION BY (toYYYYMM(date), ignore(created_at, inserted_at)) ORDER BY (key, time); INSERT INTO part_key_multiple_dates SELECT number, toDate(x), now() + intDiv(number, 10) AS x, x - (rand() % 100), x + (rand() % 100) FROM numbers(100000000); SELECT count() FROM part_key_multiple_dates WHERE date &amp;gt; (now() + toIntervalDay(105)); ┌─count()─┐ │ 8434210 │ └─────────┘ 1 rows in set.</description></item><item><title>ODBC Driver for ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-integrations/clickhouse-odbc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/clickhouse-odbc/</guid><description>ODBC interface for ClickHouse® RDBMS.
Licensed under the Apache 2.0 .
Installation and usage Windows Download the latest release . On 64bit system you usually need both 32 bit and 64 bit drivers. Install (usually you will need ANSI driver, but better to install both versions, see below). Configure ClickHouse DSN. Note: that install driver linked against MDAC (which is default for Windows), some non-windows native applications (cygwin / msys64 based) may require driver linked against unixodbc.</description></item><item><title>Part names &amp; MVCC</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/part-naming-and-mvcc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/part-naming-and-mvcc/</guid><description>Part names &amp;amp; multiversion concurrency control Part name format is:
&amp;lt;partitionid&amp;gt;_&amp;lt;min_block_number&amp;gt;_&amp;lt;max_block_number&amp;gt;_&amp;lt;level&amp;gt;_&amp;lt;data_version&amp;gt; system.parts contains all the information parsed.
partitionid is quite simple (it just comes from your partitioning key).
What are block_numbers?
DROP TABLE IF EXISTS part_names; create table part_names (date Date, n UInt8, m UInt8) engine=MergeTree PARTITION BY toYYYYMM(date) ORDER BY n; insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.parts where table = &amp;#39;part_names&amp;#39; and active; ┌─name─────────┬─partition_id─┬─min_block_number─┬─max_block_number─┬─level─┬─data_version─┐ │ 202203_1_1_0 │ 202203 │ 1 │ 1 │ 0 │ 1 │ └──────────────┴──────────────┴──────────────────┴──────────────────┴───────┴──────────────┘ insert into part_names VALUES (now(), 0, 0); select name, partition_id, min_block_number, max_block_number, level, data_version from system.</description></item><item><title>Pre-Aggregation approaches</title><link>http://kb.altinity.com/altinity-kb-schema-design/preaggregations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/preaggregations/</guid><description>Pre-Aggregation approaches: ETL vs Materialized Views vs Projections ETL MV Projections Realtime no yes yes How complex queries can be used to build the preaggregaton any complex very simple Impacts the insert speed no yes yes Are inconsistancies possible Depends on ETL. If it process the errors properly - no. yes (no transactions / atomicity) no Lifetime of aggregation any any Same as the raw data Requirements need external tools/scripting is a part of database schema is a part of table schema How complex to use in queries Depends on aggregation, usually simple, quering a separate table Depends on aggregation, sometimes quite complex, quering a separate table Very simple, quering the main table Can work correctly with ReplacingMergeTree as a source Yes No No Can work correctly with CollapsingMergeTree as a source Yes For simple aggregations For simple aggregations Can be chained Yes (Usually with DAGs / special scripts) Yes (but may be not straightforward, and often is a bad idea) No Resources needed to calculate the increment May be significant Usually tiny Usually tiny</description></item><item><title>Precreate parts using clickhouse-local</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/precreate_parts_using_clickhouse_local.sh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/precreate_parts_using_clickhouse_local.sh/</guid><description>Precreate parts using clickhouse-local the code below were testes on 23.3
## 1. Imagine we want to process this file: cat &amp;lt;&amp;lt;EOF &amp;gt; /tmp/data.csv 1,2020-01-01,&amp;#34;String&amp;#34; 2,2020-02-02,&amp;#34;Another string&amp;#34; 3,2020-03-03,&amp;#34;One more string&amp;#34; 4,2020-01-02,&amp;#34;String for first partition&amp;#34; EOF rm -rf /tmp/precreate_parts mkdir -p /tmp/precreate_parts cd /tmp/precreate_parts ## 2. that is the metadata for the table we want to fill ## schema should match the schema of the table from server ## (the easiest way is just to copy it from the server) ## I&amp;#39;ve added sleepEachRow(0.</description></item><item><title>How to pick an ORDER BY / PRIMARY KEY / PARTITION BY for the MergeTree family table</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/</guid><description>Good order by usually has 3 to 5 columns, from lowest cardinal on the left (and the most important for filtering) to highest cardinal (and less important for filtering).
Practical approach to create a good ORDER BY for a table:
Pick the columns you use in filtering always The most important for filtering and the lowest cardinal should be the left-most. Typically, it&amp;rsquo;s something like tenant_id Next column is more cardinal, less important.</description></item><item><title>Recovery after complete data loss</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/recovery-after-complete-data-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/recovery-after-complete-data-loss/</guid><description>Atomic &amp;amp; Ordinary databases. srv1 &amp;ndash; good replica
srv2 &amp;ndash; lost replica / we will restore it from srv1
test data (3 tables (atomic &amp;amp; ordinary databases)) srv1
create database testatomic on cluster &amp;#39;{cluster}&amp;#39; engine=Atomic; create table testatomic.test on cluster &amp;#39;{cluster}&amp;#39; (A Int64, D Date, s String) Engine = ReplicatedMergeTree(&amp;#39;/clickhouse/{cluster}/tables/{database}/{table}&amp;#39;,&amp;#39;{replica}&amp;#39;) partition by toYYYYMM(D) order by A; insert into testatomic.test select number, today(), &amp;#39;&amp;#39; from numbers(1000000); create database testordinary on cluster &amp;#39;{cluster}&amp;#39; engine=Ordinary; create table testordinary.</description></item><item><title>Remove block numbers from zookeeper for removed partitions</title><link>http://kb.altinity.com/altinity-kb-useful-queries/remove_unneeded_block_numbers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/remove_unneeded_block_numbers/</guid><description>Remove block numbers from zookeeper for removed partitions SELECT distinct concat(&amp;#39;delete &amp;#39;, zk.block_numbers_path, zk.partition_id) FROM ( SELECT r.database, r.table, zk.block_numbers_path, zk.partition_id, p.partition_id FROM ( SELECT path as block_numbers_path, name as partition_id FROM system.zookeeper WHERE path IN ( SELECT concat(zookeeper_path, &amp;#39;/block_numbers/&amp;#39;) as block_numbers_path FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system.replicas) ) ) as zk LEFT JOIN ( SELECT database, table, concat(zookeeper_path, &amp;#39;/block_numbers/&amp;#39;) as block_numbers_path FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system.replicas) ) as r ON (r.block_numbers_path = zk.block_numbers_path) LEFT JOIN ( SELECT DISTINCT partition_id, database, table FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system.</description></item><item><title>Removing tasks in the replication queue related to empty partitions</title><link>http://kb.altinity.com/altinity-kb-useful-queries/remove_empty_partitions_from_rq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/remove_empty_partitions_from_rq/</guid><description>Removing tasks in the replication queue related to empty partitions SELECT &amp;#39;ALTER TABLE &amp;#39; || database || &amp;#39;.&amp;#39; || table || &amp;#39; DROP PARTITION ID \&amp;#39;&amp;#39;|| partition_id || &amp;#39;\&amp;#39;;&amp;#39; FROM (SELECT DISTINCT database, table, extract(new_part_name, &amp;#39;^[^_]+&amp;#39;) as partition_id FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;, system.replication_queue) ) as rq LEFT JOIN (SELECT database, table, partition_id, sum(rows) as rows_count, count() as part_count FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;, system.parts) WHERE active GROUP BY database, table, partition_id ) as p USING (database, table, partition_id) WHERE p.</description></item><item><title>Replication: Can not resolve host of another ClickHouse® server</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/change-me/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/change-me/</guid><description>Symptom When configuring Replication the ClickHouse® cluster nodes are experiencing communication issues, and an error message appears in the log that states that the ClickHouse host cannot be resolved.
&amp;lt;Error&amp;gt; DNSResolver: Cannot resolve host (xxxxx), error 0: DNS error. auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &amp;amp;) const: Code: 198. DB::Exception: Not found address of host: xxxx. (DNS_ERROR), Cause: The error message indicates that the host name of the one of the nodes of the cluster cannot be resolved by other cluster members, causing communication issues between the nodes.</description></item><item><title>Row policies overhead (hiding 'removed' tenants)</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/row_policy_using_dictionary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/row_policy_using_dictionary/</guid><description>No row policy CREATE TABLE test_delete ( tenant Int64, key Int64, ts DateTime, value_a String ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (tenant, key, ts); INSERT INTO test_delete SELECT number%5, number, toDateTime(&amp;#39;2020-01-01&amp;#39;)+number/10, concat(&amp;#39;some_looong_string&amp;#39;, toString(number)), FROM numbers(1e8); INSERT INTO test_delete -- multiple small tenants SELECT number%5000, number, toDateTime(&amp;#39;2020-01-01&amp;#39;)+number/10, concat(&amp;#39;some_looong_string&amp;#39;, toString(number)), FROM numbers(1e8); Q1) SELECT tenant, count() FROM test_delete GROUP BY tenant ORDER BY tenant LIMIT 6; ┌─tenant─┬──count()─┐ │ 0 │ 20020000 │ │ 1 │ 20020000 │ │ 2 │ 20020000 │ │ 3 │ 20020000 │ │ 4 │ 20020000 │ │ 5 │ 20000 │ └────────┴──────────┘ 6 rows in set.</description></item><item><title>How much data are written to S3 during mutations</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_and_mutations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_and_mutations/</guid><description>Configuration S3 disk with disabled merges
&amp;lt;clickhouse&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;s3disk&amp;gt; &amp;lt;type&amp;gt;s3&amp;lt;/type&amp;gt; &amp;lt;endpoint&amp;gt;https://s3.us-east-1.amazonaws.com/mybucket/test/test/&amp;lt;/endpoint&amp;gt; &amp;lt;use_environment_credentials&amp;gt;1&amp;lt;/use_environment_credentials&amp;gt; &amp;lt;!-- use IAM AWS role --&amp;gt; &amp;lt;!--access_key_id&amp;gt;xxxx&amp;lt;/access_key_id&amp;gt; &amp;lt;secret_access_key&amp;gt;xxx&amp;lt;/secret_access_key--&amp;gt; &amp;lt;/s3disk&amp;gt; &amp;lt;/disks&amp;gt; &amp;lt;policies&amp;gt; &amp;lt;s3tiered&amp;gt; &amp;lt;volumes&amp;gt; &amp;lt;default&amp;gt; &amp;lt;disk&amp;gt;default&amp;lt;/disk&amp;gt; &amp;lt;/default&amp;gt; &amp;lt;s3disk&amp;gt; &amp;lt;disk&amp;gt;s3disk&amp;lt;/disk&amp;gt; &amp;lt;prefer_not_to_merge&amp;gt;true&amp;lt;/prefer_not_to_merge&amp;gt; &amp;lt;/s3disk&amp;gt; &amp;lt;/volumes&amp;gt; &amp;lt;/s3tiered&amp;gt; &amp;lt;/policies&amp;gt; &amp;lt;/storage_configuration&amp;gt; &amp;lt;/clickhouse&amp;gt; Let&amp;rsquo;s create a table and load some synthetic data.
CREATE TABLE test_s3 ( `A` Int64, `S` String, `D` Date ) ENGINE = MergeTree PARTITION BY D ORDER BY A SETTINGS storage_policy = &amp;#39;s3tiered&amp;#39;; insert into test_s3 select number, number, today() - intDiv(number, 10000000) from numbers(7e8); 0 rows in set.</description></item><item><title>Example of the table at s3 with cache</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_cache_example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3_cache_example/</guid><description>Storage configuration cat /etc/clickhouse-server/config.d/s3.xml &amp;lt;clickhouse&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;s3disk&amp;gt; &amp;lt;type&amp;gt;s3&amp;lt;/type&amp;gt; &amp;lt;endpoint&amp;gt;https://s3.us-east-1.amazonaws.com/mybucket/test/s3cached/&amp;lt;/endpoint&amp;gt; &amp;lt;use_environment_credentials&amp;gt;1&amp;lt;/use_environment_credentials&amp;gt; &amp;lt;!-- use IAM AWS role --&amp;gt; &amp;lt;!--access_key_id&amp;gt;xxxx&amp;lt;/access_key_id&amp;gt; &amp;lt;secret_access_key&amp;gt;xxx&amp;lt;/secret_access_key--&amp;gt; &amp;lt;/s3disk&amp;gt; &amp;lt;cache&amp;gt; &amp;lt;type&amp;gt;cache&amp;lt;/type&amp;gt; &amp;lt;disk&amp;gt;s3disk&amp;lt;/disk&amp;gt; &amp;lt;path&amp;gt;/var/lib/clickhouse/disks/s3_cache/&amp;lt;/path&amp;gt; &amp;lt;max_size&amp;gt;50Gi&amp;lt;/max_size&amp;gt; &amp;lt;!-- 50GB local cache to cache remote data --&amp;gt; &amp;lt;/cache&amp;gt; &amp;lt;/disks&amp;gt; &amp;lt;policies&amp;gt; &amp;lt;s3tiered&amp;gt; &amp;lt;volumes&amp;gt; &amp;lt;default&amp;gt; &amp;lt;disk&amp;gt;default&amp;lt;/disk&amp;gt; &amp;lt;max_data_part_size_bytes&amp;gt;50000000000&amp;lt;/max_data_part_size_bytes&amp;gt; &amp;lt;!-- only for parts less than 50GB after they moved to s3 during merges --&amp;gt; &amp;lt;/default&amp;gt; &amp;lt;s3cached&amp;gt; &amp;lt;disk&amp;gt;cache&amp;lt;/disk&amp;gt; &amp;lt;!-- sandwich cache plus s3disk --&amp;gt; &amp;lt;!-- prefer_not_to_merge&amp;gt;true&amp;lt;/prefer_not_to_merge&amp;gt; &amp;lt;perform_ttl_move_on_insert&amp;gt;false&amp;lt;/perform_ttl_move_on_insert--&amp;gt; &amp;lt;/s3cached&amp;gt; &amp;lt;/volumes&amp;gt; &amp;lt;/s3tiered&amp;gt; &amp;lt;/policies&amp;gt; &amp;lt;/storage_configuration&amp;gt; &amp;lt;/clickhouse&amp;gt; select * from system.</description></item><item><title>S3Disk</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3disk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-s3-object-storage/s3disk/</guid><description>Settings &amp;lt;clickhouse&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;s3&amp;gt; &amp;lt;type&amp;gt;s3&amp;lt;/type&amp;gt; &amp;lt;endpoint&amp;gt;http://s3.us-east-1.amazonaws.com/BUCKET_NAME/test_s3_disk/&amp;lt;/endpoint&amp;gt; &amp;lt;access_key_id&amp;gt;ACCESS_KEY_ID&amp;lt;/access_key_id&amp;gt; &amp;lt;secret_access_key&amp;gt;SECRET_ACCESS_KEY&amp;lt;/secret_access_key&amp;gt; &amp;lt;skip_access_check&amp;gt;true&amp;lt;/skip_access_check&amp;gt; &amp;lt;send_metadata&amp;gt;true&amp;lt;/send_metadata&amp;gt; &amp;lt;/s3&amp;gt; &amp;lt;/disks&amp;gt; &amp;lt;/storage_configuration&amp;gt; &amp;lt;/clickhouse&amp;gt; skip_access_check — if true, it&amp;rsquo;s possible to use read only credentials with regular MergeTree table. But you would need to disable merges (prefer_not_to_merge setting) on s3 volume as well.
send_metadata — if true, ClickHouse® will populate s3 object with initial part &amp;amp; file path, which allow you to recover metadata from s3 and make debug easier.</description></item><item><title>Inferring Schema from AvroConfluent Messages in Kafka for ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/kafka-schema-inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/kafka-schema-inference/</guid><description>To consume messages from Kafka within ClickHouse®, you need to define the ENGINE=Kafka table structure with all the column names and types. This task can be particularly challenging when dealing with complex Avro messages, as manually determining the exact schema for ClickHouse is both tricky and time-consuming. This complexity is particularly frustrating in the case of Avro formats, where the column names and their types are already clearly defined in the schema registry.</description></item><item><title>Setting the background message broker schedule pool size</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/background_message_broker_schedule_pool_size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/background_message_broker_schedule_pool_size/</guid><description>Overview When using Kafka, RabbitMQ, or NATS table engines in ClickHouse®, you may encounter issues related to a saturated background thread pool. One common symptom is a warning similar to the following:
2025.03.14 08:44:26.725868 [ 344 ] {} &amp;lt;Warning&amp;gt; StorageKafka (events_kafka): [rdk:MAXPOLL] [thrd:main]: Application maximum poll interval (60000ms) exceeded by 159ms (adjust max.poll.interval.ms for long-running message processing): leaving group This warning typically appears not because ClickHouse fails to poll, but because there are no available threads in the background pool to handle the polling in time.</description></item><item><title>Why is simple `SELECT count()` Slow in ClickHouse®?</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/slow_select_count/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/slow_select_count/</guid><description>ClickHouse is a columnar database that provides excellent performance for analytical queries. However, in some cases, a simple count query can be slow. In this article, we&amp;rsquo;ll explore the reasons why this can happen and how to optimize the query.
Three Strategies for Counting Rows in ClickHouse There are three ways to count rows in a table in ClickHouse:
optimize_trivial_count_query: This strategy extracts the number of rows from the table metadata.</description></item><item><title>SnowflakeID for Efficient Primary Keys</title><link>http://kb.altinity.com/altinity-kb-schema-design/snowflakeid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/snowflakeid/</guid><description>In data warehousing (DWH) environments, the choice of primary key (PK) can significantly impact performance, particularly in terms of RAM usage and query speed. This is where SnowflakeID comes into play, providing a robust solution for PK management. Here’s a deep dive into why and how Snowflake IDs are beneficial and practical implementation examples.
Why Snowflake ID? Natural IDs Suck: Natural keys derived from business data can lead to various issues like complexity and instability.</description></item><item><title>source parts size is greater than the current maximum</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/source-pars-size-is-greater-than-maximum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/source-pars-size-is-greater-than-maximum/</guid><description>Symptom I see messages like: source parts size (...) is greater than the current maximum (...) in the logs and/or inside system.replication_queue
Cause Usually that means that there are already few big merges running. You can see the running merges using the query:
SELECT * FROM system.merges That logic is needed to prevent picking a log of huge merges simultaneously (otherwise they will take all available slots and ClickHouse® will not be able to do smaller merges, which usually are important for keeping the number of parts stable).</description></item><item><title>ClickHouse® + Spark</title><link>http://kb.altinity.com/altinity-kb-integrations/spark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/spark/</guid><description>jdbc The trivial &amp;amp; natural way to talk to ClickHouse from Spark is using jdbc. There are 2 jdbc drivers:
https://github.com/ClickHouse/clickhouse-jdbc/ https://github.com/housepower/ClickHouse-Native-JDBC#integration-with-spark ClickHouse-Native-JDBC has some hints about integration with Spark even in the main README file.
&amp;lsquo;Official&amp;rsquo; driver does support some conversion of complex data types (Roaring bitmaps) for Spark-ClickHouse integration: https://github.com/ClickHouse/clickhouse-jdbc/pull/596 But proper partitioning of the data (to spark partitions) may be tricky with jdbc.
Some example snippets:
https://markelic.de/how-to-access-your-clickhouse-database-with-spark-in-python/ https://stackoverflow.</description></item><item><title>Successful ClickHouse® deployment plan</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-deployment-plan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-deployment-plan/</guid><description>Successful ClickHouse® deployment plan Stage 0. Build POC Install single node ClickHouse https://clickhouse.com/docs/en/getting-started/tutorial/ https://clickhouse.com/docs/en/getting-started/install/ https://docs.altinity.com/altinitystablebuilds/stablequickstartguide/ Start with creating a single table (the biggest one), use MergeTree engine. Create &amp;lsquo;some&amp;rsquo; schema (most probably it will be far from optimal). Prefer denormalized approach for all immutable dimensions, for mutable dimensions - consider dictionaries. Load some amount of data (at least 5 Gb, and 10 mln rows) - preferable the real one, or as close to real as possible.</description></item><item><title>sysall database (system tables on a cluster level)</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/sysall/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/sysall/</guid><description>Requirements The idea is that you have a macros cluster with cluster name.
For example you have a cluster named production and this cluster includes all ClickHouse® nodes.
$ cat /etc/clickhouse-server/config.d/clusters.xml &amp;lt;?xml version=&amp;#34;1.0&amp;#34; ?&amp;gt; &amp;lt;yandex&amp;gt; &amp;lt;remote_servers&amp;gt; &amp;lt;production&amp;gt; &amp;lt;shard&amp;gt; ... And you need to have a macro cluster set to production:
cat /etc/clickhouse-server/config.d/macros.xml &amp;lt;?xml version=&amp;#34;1.0&amp;#34; ?&amp;gt; &amp;lt;yandex&amp;gt; &amp;lt;macros&amp;gt; &amp;lt;cluster&amp;gt;production&amp;lt;/cluster&amp;gt; &amp;lt;replica&amp;gt;....&amp;lt;/replica&amp;gt; .... &amp;lt;/macros&amp;gt; &amp;lt;/yandex&amp;gt; Now you should be able to query all nodes using clusterAllReplicas:</description></item><item><title>Timeouts during OPTIMIZE FINAL</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/timeouts-during-optimize-final/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/timeouts-during-optimize-final/</guid><description>Timeout exceeded ... or executing longer than distributed_ddl_task_timeout during OPTIMIZE FINAL Timeout may occur
due to the fact that the client reach timeout interval.
in case of TCP / native clients - you can change send_timeout / receive_timeout + tcp_keep_alive_timeout + driver timeout settings in case of HTTP clients - you can change http_send_timeout / http_receive_timeout + tcp_keep_alive_timeout + driver timeout settings (in the case of ON CLUSTER queries) due to the fact that the timeout for query execution by shards ends</description></item><item><title>Collecting query execution flamegraphs using system.trace_log</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/trace_log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/trace_log/</guid><description>ClickHouse® has embedded functionality to analyze the details of query performance.
It&amp;rsquo;s system.trace_log table.
By default it collects information only about queries when runs longer than 1 sec (and collects stacktraces every second).
You can adjust that per query using settings query_profiler_real_time_period_ns &amp;amp; query_profiler_cpu_time_period_ns.
Both works very similar (with desired interval dump the stacktraces of all the threads which execute the query). real timer - allows to &amp;lsquo;see&amp;rsquo; the situations when cpu was not working much, but time was spend for example on IO.</description></item><item><title>Two columns indexing</title><link>http://kb.altinity.com/altinity-kb-schema-design/two-columns-indexing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/two-columns-indexing/</guid><description>Suppose we have telecom CDR data in which A party calls B party. Each data row consists of A party details: event_timestamp, A MSISDN , A IMEI, A IMSI , A start location, A end location , B MSISDN, B IMEI, B IMSI , B start location, B end location, and some other metadata.
Searches will use one of the A or B fields, for example, A IMSI, within the start and end time window.</description></item><item><title>Use an executable dictionary as cron task</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/executable-dictionary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/executable-dictionary/</guid><description>Rationale Imagine that we need to restart clickhouse-server every saturday at 10:00 AM. We can use an executable dictionary to do this. Here is the approach and code necessary to do this. It can be used for other operations like INSERT into tables or execute some other imaginative tasks that need an scheduled execution.
Let&amp;rsquo;s create a simple table to register all the restarts scheduled by this dictionary:
CREATE TABLE restart_table ( restart_datetime DateTime ) ENGINE = TinyLog Configuration This is the ClickHouse configuration file we will be using for executable dictionaries.</description></item><item><title>Useful settings to turn on/Defaults that should be reconsidered</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/useful-setting-to-turn-on/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/useful-setting-to-turn-on/</guid><description>Useful settings to turn on/Defaults that should be reconsidered Some setting that are not enabled by default.
ttl_only_drop_parts Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.
When ttl_only_drop_parts is disabled (by default), the ClickHouse® server only deletes expired rows according to their TTL.
When ttl_only_drop_parts is enabled, the ClickHouse server drops a whole part when all rows in it are expired.
Dropping whole parts instead of partial cleaning TTL-d rows allows having shorter merge_with_ttl_timeout times and lower impact on system performance.</description></item><item><title>Using array functions to mimic window-functions alike behavior</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/array-functions-as-window/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/array-functions-as-window/</guid><description>There are cases where you may need to mimic window functions using arrays in ClickHouse. This could be for optimization purposes, to better manage memory, or to enable on-disk spilling, especially if you’re working with an older version of ClickHouse that doesn&amp;rsquo;t natively support window functions.
Here’s an example demonstrating how to mimic a window function like runningDifference() using arrays:
Step 1: Create Sample Data We’ll start by creating a test table with some sample data:</description></item><item><title>Vulnerabilities</title><link>http://kb.altinity.com/upgrade/vulnerabilities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/upgrade/vulnerabilities/</guid><description>2022-03-15: 7 vulnerabilities in ClickHouse® were published. See the details https://jfrog.com/blog/7-rce-and-dos-vulnerabilities-found-in-clickhouse-dbms/ Those vulnerabilities were fixed by 2 PRs:
https://github.com/ClickHouse/ClickHouse/pull/27136 https://github.com/ClickHouse/ClickHouse/pull/27743 All releases starting from v21.10.2.15 have that problem fixed.
Also, the fix was backported to 21.3 and 21.8 branches - versions v21.8.11.4-lts and v21.3.19.1-lts accordingly have the problem fixed (and all newer releases in those branches).
The latest Altinity stable releases also contain the bugfix.
21.8.13 21.3.20 If you use some older version we recommend upgrading.</description></item><item><title>What are my TTL settings?</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/what-are-my-ttls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/what-are-my-ttls/</guid><description>Using SHOW CREATE TABLE If you just want to see the current TTL settings on a table, you can look at the schema definition.
SHOW CREATE TABLE events2_local FORMAT Vertical Query id: eba671e5-6b8c-4a81-a4d8-3e21e39fb76b Row 1: ────── statement: CREATE TABLE default.events2_local ( `EventDate` DateTime, `EventID` UInt32, `Value` String ) ENGINE = ReplicatedMergeTree(&amp;#39;/clickhouse/{cluster}/tables/{shard}/default/events2_local&amp;#39;, &amp;#39;{replica}&amp;#39;) PARTITION BY toYYYYMM(EventDate) ORDER BY (EventID, EventDate) TTL EventDate + toIntervalMonth(1) SETTINGS index_granularity = 8192 This works even when there&amp;rsquo;s no data in the table.</description></item><item><title>Who ate my CPU</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/who-ate-my-cpu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/who-ate-my-cpu/</guid><description>Merges SELECT table, round((elapsed * (1 / progress)) - elapsed, 2) AS estimate, elapsed, progress, is_mutation, formatReadableSize(total_size_bytes_compressed) AS size, formatReadableSize(memory_usage) AS mem FROM system.merges ORDER BY elapsed DESC Mutations SELECT database, table, substr(command, 1, 30) AS command, sum(parts_to_do) AS parts_to_do, anyIf(latest_fail_reason, latest_fail_reason != &amp;#39;&amp;#39;) FROM system.mutations WHERE NOT is_done GROUP BY database, table, command Current Processes select elapsed, query from system.processes where is_initial_query and elapsed &amp;gt; 2 Processes retrospectively SELECT normalizedQueryHash(query) hash, current_database, sum(ProfileEvents[&amp;#39;UserTimeMicroseconds&amp;#39;] as userCPUq)/1000 AS userCPUms, count(), sum(query_duration_ms) query_duration_ms, userCPUms/query_duration_ms cpu_per_sec, argMax(query, userCPUq) heaviest_query FROM system.</description></item><item><title>Install standalone Zookeeper for ClickHouse® on Ubuntu / Debian</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/install_ubuntu/</guid><description>Reference script to install standalone Zookeeper for Ubuntu / Debian Tested on Ubuntu 20.
# install java runtime environment sudo apt-get update sudo apt install default-jre # prepare folders, logs folder should be on the low-latency disk. sudo mkdir -p /var/lib/zookeeper/data /var/lib/zookeeper/logs /etc/zookeeper /var/log/zookeeper /opt # download and install files export ZOOKEEPER_VERSION=3.6.3 wget https://dlcdn.apache.org/zookeeper/zookeeper-${ZOOKEEPER_VERSION}/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -O /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz sudo tar -xvf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz -C /opt rm -rf /tmp/apache-zookeeper-${ZOOKEEPER_VERSION}-bin.tar.gz # create the user sudo groupadd -r zookeeper sudo useradd -r -g zookeeper --home-dir=/var/lib/zookeeper --shell=/bin/false zookeeper # symlink pointing to the used version of zookeeper distibution sudo ln -s /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin /opt/zookeeper sudo chown -R zookeeper:zookeeper /var/lib/zookeeper /var/log/zookeeper /etc/zookeeper /opt/apache-zookeeper-${ZOOKEEPER_VERSION}-bin sudo chown -h zookeeper:zookeeper /opt/zookeeper # shortcuts in /usr/local/bin/ echo -e &amp;#39;#!</description></item><item><title>Zookeeper session has expired</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/zookeeper-session-expired/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/zookeeper-session-expired/</guid><description>Q. I get &amp;ldquo;Zookeeper session has expired&amp;rdquo; once. What should i do? Should I worry?
Getting exceptions or lack of acknowledgement in distributed system from time to time is a normal situation. Your client should do the retry. If that happened once and your client do retries correctly - nothing to worry about.
It it happens often, or with every retry - it may be a sign of some misconfiguration / issue in cluster (see below).</description></item><item><title>Server configuration files</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-server-config-files/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-server-config-files/</guid><description>Сonfig management (recommended structure) ClickHouse® server config consists of two parts server settings (config.xml) and users settings (users.xml).
By default they are stored in the folder /etc/clickhouse-server/ in two files config.xml &amp;amp; users.xml.
We suggest never change vendor config files and place your changes into separate .xml files in sub-folders. This way is easier to maintain and ease ClickHouse upgrades.
/etc/clickhouse-server/users.d – sub-folder for user settings (derived from users.xml filename).</description></item><item><title/><link>http://kb.altinity.com/altinitycloud/altinity-cloud-connections/clickhouseclient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinitycloud/altinity-cloud-connections/clickhouseclient/</guid><description/></item><item><title/><link>http://kb.altinity.com/altinitykubernetesoperator/kubernetesinstallguide/minikubeonlinux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinitykubernetesoperator/kubernetesinstallguide/minikubeonlinux/</guid><description/></item><item><title/><link>http://kb.altinity.com/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/search/</guid><description/></item><item><title>-Resample vs -If vs -Map vs Subquery</title><link>http://kb.altinity.com/altinity-kb-functions/resample-vs-if-vs-map-vs-subquery/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/resample-vs-if-vs-map-vs-subquery/</guid><description>5 categories SELECT sumResample(0, 5, 1)(number, number % 5) AS sum FROM numbers_mt(1000000000) ┌─sum───────────────────────────────────────────────────────────────────────────────────────────┐ │ [99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000] │ └───────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 1.010 sec. Processed 1.00 billion rows, 8.00 GB (990.20 million rows/s., 7.92 GB/s.) SELECT sumMap([number % 5], [number]) AS sum FROM numbers_mt(1000000000) ┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ ([0,1,2,3,4],[99999999500000000,99999999700000000,99999999900000000,100000000100000000,100000000300000000]) │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 5.730 sec. Processed 1.00 billion rows, 8.00 GB (174.51 million rows/s., 1.40 GB/s.) SELECT sumMap(map(number % 5, number)) AS sum FROM numbers_mt(1000000000) ┌─sum─────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ {0:99999999500000000,1:99999999700000000,2:99999999900000000,3:100000000100000000,4:100000000300000000} │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 1 rows in set.</description></item><item><title>-State &amp; -Merge combinators</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/state-and-merge-combinators/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/state-and-merge-combinators/</guid><description>The -State combinator in ClickHouse® does not store additional information about the -If combinator, which means that aggregate functions with and without -If have the same serialized data structure. This can be verified through various examples, as demonstrated below.
Example 1: maxIfState and maxState In this example, we use the maxIfState and maxState functions on a dataset of numbers, serialize the result, and merge it using the maxMerge function.
$ clickhouse-local --query &amp;#34;SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(10) FORMAT RowBinary&amp;#34; | clickhouse-local --input-format RowBinary --structure=&amp;#34;x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)&amp;#34; --query &amp;#34;SELECT maxMerge(x), maxMerge(y) FROM table&amp;#34; 9 9 $ clickhouse-local --query &amp;#34;SELECT maxIfState(number,number % 2) as x, maxState(number) as y FROM numbers(11) FORMAT RowBinary&amp;#34; | clickhouse-local --input-format RowBinary --structure=&amp;#34;x AggregateFunction(max,UInt64), y AggregateFunction(max,UInt64)&amp;#34; --query &amp;#34;SELECT maxMerge(x), maxMerge(y) FROM table&amp;#34; 9 10 In both cases, the -State combinator results in identical serialized data footprints, regardless of the conditions in the -If variant.</description></item><item><title>Add/Remove a new replica to a ClickHouse® cluster</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/add_remove_replica/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/add_remove_replica/</guid><description>ADD nodes/replicas to a ClickHouse® cluster To add some ClickHouse® replicas to an existing cluster if -30TB then better to use replication:
don’t add the remote_servers.xml until replication is done. Add these files and restart to limit bandwidth and avoid saturation (70% total bandwidth): Core Settings | ClickHouse Docs 💡 Do the Gbps to Bps math correctly. For 10G —&amp;gt; 1250MB/s —&amp;gt; 1250000000 B/s and change max_replicated_* settings accordingly:
Nodes replicating from: &amp;lt;clickhouse&amp;gt; &amp;lt;profiles&amp;gt; &amp;lt;default&amp;gt; &amp;lt;max_replicated_sends_network_bandwidth_for_server&amp;gt;50000&amp;lt;/max_replicated_sends_network_bandwidth_for_server&amp;gt; &amp;lt;/default&amp;gt; &amp;lt;/profiles&amp;gt; &amp;lt;/clickhouse&amp;gt; Nodes replicating to: &amp;lt;clickhouse&amp;gt; &amp;lt;profiles&amp;gt; &amp;lt;default&amp;gt; &amp;lt;max_replicated_fetches_network_bandwidth_for_server&amp;gt;50000&amp;lt;/max_replicated_fetches_network_bandwidth_for_server&amp;gt; &amp;lt;/default&amp;gt; &amp;lt;/profiles&amp;gt; &amp;lt;/clickhouse&amp;gt; Manual method (DDL) Create tables manually and be sure macros in all replicas are aligned with the ZK path.</description></item><item><title>Adjusting librdkafka settings</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-adjusting-librdkafka-settings/</guid><description>To set rdkafka options - add to &amp;lt;kafka&amp;gt; section in config.xml or preferably use a separate file in config.d/: https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md Some random example using SSL certificates to authenticate:
&amp;lt;yandex&amp;gt; &amp;lt;kafka&amp;gt; &amp;lt;max_poll_interval_ms&amp;gt;60000&amp;lt;/max_poll_interval_ms&amp;gt; &amp;lt;session_timeout_ms&amp;gt;60000&amp;lt;/session_timeout_ms&amp;gt; &amp;lt;heartbeat_interval_ms&amp;gt;10000&amp;lt;/heartbeat_interval_ms&amp;gt; &amp;lt;reconnect_backoff_ms&amp;gt;5000&amp;lt;/reconnect_backoff_ms&amp;gt; &amp;lt;reconnect_backoff_max_ms&amp;gt;60000&amp;lt;/reconnect_backoff_max_ms&amp;gt; &amp;lt;request_timeout_ms&amp;gt;20000&amp;lt;/request_timeout_ms&amp;gt; &amp;lt;retry_backoff_ms&amp;gt;500&amp;lt;/retry_backoff_ms&amp;gt; &amp;lt;message_max_bytes&amp;gt;20971520&amp;lt;/message_max_bytes&amp;gt; &amp;lt;debug&amp;gt;all&amp;lt;/debug&amp;gt;&amp;lt;!-- only to get the errors --&amp;gt; &amp;lt;security_protocol&amp;gt;SSL&amp;lt;/security_protocol&amp;gt; &amp;lt;ssl_ca_location&amp;gt;/etc/clickhouse-server/ssl/kafka-ca-qa.crt&amp;lt;/ssl_ca_location&amp;gt; &amp;lt;ssl_certificate_location&amp;gt;/etc/clickhouse-server/ssl/client_clickhouse_client.pem&amp;lt;/ssl_certificate_location&amp;gt; &amp;lt;ssl_key_location&amp;gt;/etc/clickhouse-server/ssl/client_clickhouse_client.key&amp;lt;/ssl_key_location&amp;gt; &amp;lt;ssl_key_password&amp;gt;pass&amp;lt;/ssl_key_password&amp;gt; &amp;lt;/kafka&amp;gt; &amp;lt;/yandex&amp;gt; Authentication / connectivity Sometimes the consumer group needs to be explicitly allowed in the broker UI config.</description></item><item><title>ClickHouse® AggregatingMergeTree</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/aggregatingmergetree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/aggregatingmergetree/</guid><description>Q. What happens with columns which are not part of the ORDER BY key, nor have the AggregateFunction type?
A. it picks the first value met, (similar to any)
CREATE TABLE agg_test ( `a` String, `b` UInt8, `c` SimpleAggregateFunction(max, UInt8) ) ENGINE = AggregatingMergeTree ORDER BY a; INSERT INTO agg_test VALUES (&amp;#39;a&amp;#39;, 1, 1); INSERT INTO agg_test VALUES (&amp;#39;a&amp;#39;, 2, 2); SELECT * FROM agg_test FINAL; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 2 │ └───┴───┴───┘ INSERT INTO agg_test VALUES (&amp;#39;a&amp;#39;, 3, 3); SELECT * FROM agg_test; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 2 │ └───┴───┴───┘ ┌─a─┬─b─┬─c─┐ │ a │ 3 │ 3 │ └───┴───┴───┘ OPTIMIZE TABLE agg_test FINAL; SELECT * FROM agg_test; ┌─a─┬─b─┬─c─┐ │ a │ 1 │ 3 │ └───┴───┴───┘ Last non-null value for each column CREATE TABLE test_last ( `col1` Int32, `col2` SimpleAggregateFunction(anyLast, Nullable(DateTime)), `col3` SimpleAggregateFunction(anyLast, Nullable(DateTime)) ) ENGINE = AggregatingMergeTree ORDER BY col1 Ok.</description></item><item><title>Aggressive merges</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-aggressive_merges/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-aggressive_merges/</guid><description>Q: Is there any way I can dedicate more resources to the merging process when running ClickHouse® on pretty beefy machines (like 36 cores, 1TB of RAM, and large NVMe disks)?
Mostly such things doing by changing the level of parallelism:
1. background_pool_size - how many threads will be actually doing the merge (if you can push all the server resources to do the merges, i.e. no selects will be running - you can give all the cores to that, so try increasing to 36).</description></item><item><title>ALTER MODIFY COLUMN is stuck, the column is inaccessible.</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-alter-modify-column-is-stuck-the-column-is-inaccessible/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-alter-modify-column-is-stuck-the-column-is-inaccessible/</guid><description>Problem You’ve created a table in ClickHouse with the following structure:
CREATE TABLE modify_column(column_n String) ENGINE=MergeTree() ORDER BY tuple(); You populated the table with some data:
INSERT INTO modify_column VALUES (&amp;#39;key_a&amp;#39;); INSERT INTO modify_column VALUES (&amp;#39;key_b&amp;#39;); INSERT INTO modify_column VALUES (&amp;#39;key_c&amp;#39;); Next, you attempted to change the column type using this query:
ALTER TABLE modify_column MODIFY COLUMN column_n Enum8(&amp;#39;key_a&amp;#39;=1, &amp;#39;key_b&amp;#39;=2); However, the operation failed, and you encountered an error when inspecting the system.</description></item><item><title>Altinity Backup for ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-backup/</guid><description>Installation and configuration Download the latest clickhouse-backup.tar.gz from assets from https://github.com/Altinity/clickhouse-backup/releases This tar.gz contains a single binary of clickhouse-backup and an example of config file.
Backblaze has s3 compatible API but requires empty acl parameter acl: &amp;quot;&amp;quot;.
https://www.backblaze.com/ has 15 days and free 10Gb S3 trial.
$ mkdir clickhouse-backup $ cd clickhouse-backup $ wget https://github.com/Altinity/clickhouse-backup/releases/download/v2.5.20/clickhouse-backup.tar.gz $ tar zxf clickhouse-backup.tar.gz $ rm clickhouse-backup.tar.gz $ cat config.yml general: remote_storage: s3 disable_progress_bar: false backups_to_keep_local: 0 backups_to_keep_remote: 0 log_level: info allow_empty_backups: false clickhouse: username: default password: &amp;#34;&amp;#34; host: localhost port: 9000 disk_mapping: {} skip_tables: - system.</description></item><item><title>Altinity packaging compatibility >21.x and earlier</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-packaging-compatibility-greater-than-21.x-and-earlier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-packaging-compatibility-greater-than-21.x-and-earlier/</guid><description>Working with Altinity &amp;amp; Yandex packaging together Since ClickHouse® version 21.1 Altinity switches to the same packaging as used by Yandex. That is needed for syncing things and introduces several improvements (like adding systemd service file).
Unfortunately, that change leads to compatibility issues - automatic dependencies resolution gets confused by the conflicting package names: both when you update ClickHouse to the new version (the one which uses older packaging) and when you want to install older altinity packages (20.</description></item><item><title>ANSI SQL mode</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ansi-sql-mode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ansi-sql-mode/</guid><description>To make ClickHouse® more compatible with ANSI SQL standards (at the expense of some performance), you can adjust several settings. These configurations will bring ClickHouse closer to ANSI SQL behavior but may introduce a slowdown in query performance:
join_use_nulls=1 Introduced in: early versions Ensures that JOIN operations return NULL for non-matching rows, aligning with standard SQL behavior.
cast_keep_nullable=1 Introduced in: v20.5 Preserves the NULL flag when casting between data types, which is typical in ANSI SQL.</description></item><item><title>arrayFold</title><link>http://kb.altinity.com/altinity-kb-functions/arrayfold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/arrayfold/</guid><description>EWMA example WITH [40, 45, 43, 31, 20] AS data, 0.3 AS alpha SELECT arrayFold((acc, x) -&amp;gt; arrayPushBack(acc, (alpha * x) + ((1 - alpha) * (acc[-1]))), arrayPopFront(data), [CAST(data[1], &amp;#39;Float64&amp;#39;)]) as ewma ┌─ewma─────────────────────────────────────────────────────────────┐ │ [40,41.5,41.949999999999996,38.66499999999999,33.06549999999999] │ └──────────────────────────────────────────────────────────────────┘</description></item><item><title>arrayMap, arrayJoin or ARRAY JOIN memory usage</title><link>http://kb.altinity.com/altinity-kb-functions/array-like-memory-usage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/array-like-memory-usage/</guid><description>arrayMap-like functions memory usage calculation. In order to calculate arrayMap or similar array* functions ClickHouse® temporarily does arrayJoin-like operation, which in certain conditions can lead to huge memory usage for big arrays.
So for example, you have 2 columns:
SELECT * FROM ( SELECT [1, 2, 3, 4, 5] AS array_1, [1, 2, 3, 4, 5] AS array_2 ) ┌─array_1─────┬─array_2─────┐ │ [1,2,3,4,5] │ [1,2,3,4,5] │ └─────────────┴─────────────┘ Let&amp;rsquo;s say we want to multiply array elements at corresponding positions.</description></item><item><title>assumeNotNull and friends</title><link>http://kb.altinity.com/altinity-kb-functions/assumenotnull-and-friends/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/assumenotnull-and-friends/</guid><description>assumeNotNull result is implementation specific:
WITH CAST(NULL, &amp;#39;Nullable(UInt8)&amp;#39;) AS column SELECT column, assumeNotNull(column + 999) AS x; ┌─column─┬─x─┐ │ null │ 0 │ └────────┴───┘ WITH CAST(NULL, &amp;#39;Nullable(UInt8)&amp;#39;) AS column SELECT column, assumeNotNull(materialize(column) + 999) AS x; ┌─column─┬───x─┐ │ null │ 999 │ └────────┴─────┘ CREATE TABLE test_null ( `key` UInt32, `value` Nullable(String) ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_null SELECT number, concat(&amp;#39;value &amp;#39;, toString(number)) FROM numbers(4); SELECT * FROM test_null; ┌─key─┬─value───┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ └─────┴─────────┘ ALTER TABLE test_null UPDATE value = NULL WHERE key = 3; SELECT * FROM test_null; ┌─key─┬─value───┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ null │ └─────┴─────────┘ SELECT key, assumeNotNull(value) FROM test_null; ┌─key─┬─assumeNotNull(value)─┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ └─────┴──────────────────────┘ WITH CAST(NULL, &amp;#39;Nullable(Enum8(\&amp;#39;a\&amp;#39; = 1, \&amp;#39;b\&amp;#39; = 0))&amp;#39;) AS test SELECT assumeNotNull(test) ┌─assumeNotNull(test)─┐ │ b │ └─────────────────────┘ WITH CAST(NULL, &amp;#39;Nullable(Enum8(\&amp;#39;a\&amp;#39; = 1))&amp;#39;) AS test SELECT assumeNotNull(test) Error on processing query &amp;#39;with CAST(null, &amp;#39;Nullable(Enum8(\&amp;#39;a\&amp;#39; = 1))&amp;#39;) as test select assumeNotNull(test); ;&amp;#39;: Code: 36, e.</description></item><item><title>Async INSERTs</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/async-inserts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/async-inserts/</guid><description>Overview Async INSERTs is a ClickHouse® feature that enables automatic server-side batching of data. While we generally recommend batching at the application/ingestor level for better control and decoupling, async inserts are valuable when you have hundreds or thousands of clients performing small inserts and client-side batching is not feasible.
Key Documentation: Official Async Inserts Documentation How Async Inserts Work When async_insert=1 is enabled, ClickHouse buffers incoming inserts and flushes them to disk when one of these conditions is met:</description></item><item><title>Atomic insert</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/atomic-insert/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/atomic-insert/</guid><description>An insert is atomic if it creates only one part.
An insert will create one part if:
Data is inserted directly into a MergeTree table Data is inserted into a single partition. Smaller blocks are properly squashed up to the configured block size (min_insert_block_size_rows and min_insert_block_size_bytes) For INSERT FORMAT: Number of rows is less than max_insert_block_size (default is 1048545) Parallel formatting is disabled (For TSV, TSKV, CSV, and JSONEachRow formats setting input_format_parallel_parsing=0 is set).</description></item><item><title>AWS EC2 Storage</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/aws-ec2-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/aws-ec2-storage/</guid><description>EBS Most native choose for ClickHouse® as fast storage, because it usually guarantees best throughput, IOPS, latency for reasonable price.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html General Purpose SSD volumes In usual conditions ClickHouse being limited by throughput of volumes and amount of provided IOPS doesn&amp;rsquo;t make any big difference for performance starting from a certain number. So the most native choice for ClickHouse is gp3 and gp2 volumes.
‌EC2 instances also have an EBS throughput limit, it depends on the size of the EC2 instance.</description></item><item><title>Backfill/populate MV in a controlled manner</title><link>http://kb.altinity.com/altinity-kb-schema-design/materialized-views/backfill-populate-mv-in-a-controlled-manner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/materialized-views/backfill-populate-mv-in-a-controlled-manner/</guid><description>Q. How to populate MV create with TO syntax? INSERT INTO mv SELECT * FROM huge_table? Will it work if the source table has billions of rows?
A. single huge insert ... select ... actually will work, but it will take A LOT of time, and during that time lot of bad things can happen (lack of disk space, hard restart etc). Because of that, it&amp;rsquo;s better to do such backfill in a more controlled manner and in smaller pieces.</description></item><item><title>Backups</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardening-clickhouse-security/</guid><description>ClickHouse® is currently at the design stage of creating some universal backup solution. Some custom backup strategies are:
Each shard is backed up separately. FREEZE the table/partition. For more information, see Alter Freeze Partition . This creates hard links in shadow subdirectory. rsync that directory to a backup location, then remove that subfolder from shadow. Cloud users are recommended to use Rclone . Always add the full contents of the metadata subfolder that contains the current DB schema and ClickHouse configs to your backup.</description></item><item><title>Best schema for storing many metrics registered from the single source</title><link>http://kb.altinity.com/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/</guid><description>Picking the best schema for storing many metrics registered from single source is quite a common problem.
1 One row per metric i.e.: timestamp, sourceid, metric_name, metric_value
Pros and cons:
Pros: simple well normalized schema easy to extend that is quite typical pattern for timeseries databases Cons different metrics values stored in same columns (worse compression) to use values of different datatype you need to cast everything to string or introduce few columns for values of different types.</description></item><item><title>BI Tools</title><link>http://kb.altinity.com/altinity-kb-integrations/bi-tools/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/bi-tools/</guid><description> Superset: https://superset.apache.org/docs/databases/clickhouse Metabase: https://github.com/enqueue/metabase-clickhouse-driver Querybook: https://www.querybook.org/docs/setup_guide/connect_to_query_engines/#all-query-engines Tableau: Altinity Tableau Connector for ClickHouse® support both JDBC &amp;amp; ODBC drivers Looker: https://docs.looker.com/setup-and-management/database-config/clickhouse Apache Zeppelin SeekTable ReDash Mondrian: https://altinity.com/blog/accessing-clickhouse-from-excel-using-mondrian-rolap-engine Grafana: Integrating Grafana with ClickHouse Cumul.io Tablum: https://tablum.io</description></item><item><title>Can detached parts in ClickHouse® be dropped?</title><link>http://kb.altinity.com/altinity-kb-useful-queries/detached-parts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/detached-parts/</guid><description>Overview This article explains what detached parts are in ClickHouse® (why they appear, what each detached prefix means, and how to safely clean them up). Use this guide when investigating missing data, replication issues, or disk growth related to the detached directory.
Detached parts act like the “Recycle Bin” in Windows. When ClickHouse® deems some data unneeded—often during internal reconciliations at server startup—it moves the data to the detached area instead of deleting it immediately.</description></item><item><title>CatBoost / MindsDB / Fast.ai</title><link>http://kb.altinity.com/altinity-kb-integrations/catboost-mindsdb-fast.ai/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/catboost-mindsdb-fast.ai/</guid><description>Info Article is based on feedback provided by one of Altinity clients. CatBoost:
It uses gradient boosting - a hard to use technique which can outperform neural networks. Gradient boosting is powerful but it&amp;rsquo;s easy to shoot yourself in the foot using it. The documentation on how to use it is quite lacking. The only good source of information on how to properly configure a model to yield good results is this video: https://www.</description></item><item><title>ClickHouse® Projections</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/projections-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/projections-examples/</guid><description>Projections in ClickHouse act as inner tables within a main table, functioning as a mechanism to optimize queries by using these inner tables when only specific columns are needed. Essentially, a projection is similar to a Materialized View with an AggregatingMergeTree engine , designed to be automatically populated with relevant data.
However, too many projections can lead to excess storage, much like overusing Materialized Views. Projections share the same lifecycle as the main table, meaning they are automatically backfilled and don’t require query rewrites, which is particularly advantageous when integrating with BI tools.</description></item><item><title>clickhouse-client</title><link>http://kb.altinity.com/altinity-kb-interfaces/altinity-kb-clickhouse-client/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-interfaces/altinity-kb-clickhouse-client/</guid><description>Q. How can I input multi-line SQL code? can you guys give me an example?
A. Just run clickhouse-client with -m switch, and it starts executing only after you finish the line with a semicolon.
Q. How can i use pager with clickhouse-client
A. Here is an example: clickhouse-client --pager 'less -RS'
Q. Data is returned in chunks / several tables.
A. Data get streamed from the server in blocks, every block is formatted individually when the default PrettyCompact format is used.</description></item><item><title>clickhouse-copier 20.3 and earlier</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.3-and-earlier/</guid><description>clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.</description></item><item><title>clickhouse-copier 20.4 - 21.6</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4_21.6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-20.4_21.6/</guid><description>clickhouse-copier was created to move data between clusters. It runs simple INSERT…SELECT queries and can copy data between tables with different engine parameters and between clusters with different number of shards. In the task configuration file you need to describe the layout of the source and the target cluster, and list the tables that you need to copy. You can copy whole tables or specific partitions. clickhouse-copier uses temporary distributed tables to select from the source cluster and insert into the target cluster.</description></item><item><title>ClickHouse® Function/Engines/Settings Report</title><link>http://kb.altinity.com/upgrade/clickhouse-feature-report/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/upgrade/clickhouse-feature-report/</guid><description>Follow this link for a complete report on ClickHouse® features with their availability: https://github.com/anselmodadams/ChMisc/blob/main/report/report.md . It is frequently updated (at least once a month).</description></item><item><title>ClickHouse® in Docker</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-clickhouse-in-docker/</guid><description>Do you have documentation on Docker deployments? Check
https://hub.docker.com/r/clickhouse/clickhouse-server https://docs.altinity.com/clickhouseonkubernetes/ sources of entry point - https://github.com/ClickHouse/ClickHouse/blob/master/docker/server/entrypoint.sh Important things:
use concrete version tag (avoid using latest) if possible use --network=host (due to performance reasons) you need to mount the folder /var/lib/clickhouse to have persistency. you MAY also mount the folder /var/log/clickhouse-server to have logs accessible outside of the container. Also, you may mount in some files or folders in the configuration folder: /etc/clickhouse-server/config.</description></item><item><title>ClickHouse® Monitoring</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-monitoring/</guid><description>What to read / watch on the subject:
Altinity webinar &amp;ldquo;ClickHouse Monitoring 101: What to monitor and how&amp;rdquo;. Watch the video or download the slides . The ClickHouse docs What should be monitored The following metrics should be collected / monitored
For Host Machine:
CPU Memory Network (bytes/packets) Storage (iops) Disk Space (free / used) For ClickHouse:
Connections (Number of queries running) DDL queue length RWLocks Read / Write / Return (bytes/rows) Merges (queue length, memory used) Mutations Query duration (optional) Replication queue length and lag Read only tables ZooKeeper latencies Zookeeper operations (count) S3 errors (if used) For Zookeeper:</description></item><item><title>ClickHouse® versions</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-versions/</guid><description>ClickHouse® versioning schema Example:
21.3.10.1-lts
21 is the year of release. 3 indicates a Feature Release. This is an increment where features are delivered. 10 is the bugfix / maintenance version. When that version is incremented it means that some bugs was fixed comparing to 21.3.9. 1 - build number, means nothing for end users. lts - type of release. (long time support). What is Altinity Stable version? It is one of general / public version of ClickHouse which has passed some extra testings, the upgrade path and changelog was analyzed, known issues are documented, and at least few big companies use it on production.</description></item><item><title>Cluster Configuration FAQ</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-faq/</guid><description>ClickHouse® does not start, some other unexpected behavior happening Check ClickHouse logs, they are your friends:
tail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log | less tail -n 10000 /var/log/clickhouse-server/clickhouse-server.log | less
How Do I Restrict Memory Usage? See our knowledge base article and official documentation for more information.
ClickHouse died during big query execution Misconfigured ClickHouse can try to allocate more RAM than is available on the system.
In that case an OS component called oomkiller can kill the ClickHouse process.</description></item><item><title>Cluster Configuration Process</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/cluster-configuration-process/</guid><description>So you set up 3 nodes with zookeeper (zookeeper1, zookeeper2, zookeeper3 - How to install zookeeper? ), and and 4 nodes with ClickHouse® (clickhouse-sh1r1,clickhouse-sh1r2,clickhouse-sh2r1,clickhouse-sh2r2 - how to install ClickHouse? ). Now we need to make them work together.
Use ansible/puppet/salt or other systems to control the servers’ configurations.
Configure ClickHouse access to Zookeeper by adding the file zookeeper.xml in /etc/clickhouse-server/config.d/ folder. This file must be placed on all ClickHouse servers. &amp;lt;yandex&amp;gt; &amp;lt;zookeeper&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper1&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper2&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;node&amp;gt; &amp;lt;host&amp;gt;zookeeper3&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;2181&amp;lt;/port&amp;gt; &amp;lt;/node&amp;gt; &amp;lt;/zookeeper&amp;gt; &amp;lt;/yandex&amp;gt; On each server put the file macros.</description></item><item><title>Codecs on array columns</title><link>http://kb.altinity.com/altinity-kb-schema-design/codecs/codecs-on-array-columns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/codecs/codecs-on-array-columns/</guid><description>Info Supported since 20.10 (PR #15089 ). On older versions you will get exception: DB::Exception: Codec Delta is not applicable for Array(UInt64) because the data type is not of fixed size. DROP TABLE IF EXISTS array_codec_test SYNC create table array_codec_test( number UInt64, arr Array(UInt64) ) Engine=MergeTree ORDER BY number; INSERT INTO array_codec_test SELECT number, arrayMap(i -&amp;gt; number + i, range(100)) from numbers(10000000); /**** Default LZ4 *****/ OPTIMIZE TABLE array_codec_test FINAL; --- Elapsed: 3.</description></item><item><title>Codecs speed</title><link>http://kb.altinity.com/altinity-kb-schema-design/codecs/codecs-speed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/codecs/codecs-speed/</guid><description>create table test_codec_speed engine=MergeTree ORDER BY tuple() as select cast(now() + rand()%2000 + number, &amp;#39;DateTime&amp;#39;) as x from numbers(1000000000); option 1: CODEC(LZ4) (same as default) option 2: CODEC(DoubleDelta) (`alter table test_codec_speed modify column x DateTime CODEC(DoubleDelta)`); option 3: CODEC(T64, LZ4) (`alter table test_codec_speed modify column x DateTime CODEC(T64, LZ4)`) option 4: CODEC(Delta, LZ4) (`alter table test_codec_speed modify column x DateTime CODEC(Delta, LZ4)`) option 5: CODEC(ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(ZSTD(1))`) option 6: CODEC(T64, ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(T64, ZSTD(1))`) option 7: CODEC(Delta, ZSTD(1)) (`alter table test_codec_speed modify column x DateTime CODEC(Delta, ZSTD(1))`) option 8: CODEC(T64, LZ4HC(1)) (`alter table test_codec_speed modify column x DateTime CODEC(T64, LZ4HC(1))`) option 9: CODEC(Gorilla) (`alter table test_codec_speed modify column x DateTime CODEC(Gorilla)`) Result may be not 100% reliable (checked on my laptop, need to be repeated in lab environment) OPTIMIZE TABLE test_codec_speed FINAL (second run - i.</description></item><item><title>Configure ClickHouse® for low memory environments</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/configure_clickhouse_for_low_mem_envs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/configure_clickhouse_for_low_mem_envs/</guid><description>While Clickhouse® it&amp;rsquo;s typically deployed on powerful servers with ample memory and CPU, it can be deployed in resource-constrained environments like a Raspberry Pi. Whether you&amp;rsquo;re working on edge computing, IoT data collection, or simply experimenting with ClickHouse in a small-scale setup, running it efficiently on low-memory hardware can be a rewarding challenge.
TLDR;
&amp;lt;!-- config.xml --&amp;gt; &amp;lt;!-- These settinsg should allow to run clickhouse in nodes with 4GB/8GB RAM --&amp;gt; &amp;lt;clickhouse&amp;gt; &amp;lt;!</description></item><item><title>Converting MergeTree to Replicated</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-converting-mergetree-to-replicated/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-converting-mergetree-to-replicated/</guid><description>To enable replication in a table that uses the MergeTree engine, you need to convert the engine to ReplicatedMergeTree. Options here are:
UseINSERT INTO foo_replicated SELECT * FROM foo. (suitable for small tables) Create table aside and attach all partition from the existing table then drop original table (uses hard links don&amp;rsquo;t require extra disk space). ALTER TABLE foo_replicated ATTACH PARTITION ID 'bar' FROM 'foo' You can easily auto generate those commands using a query like: SELECT DISTINCT 'ALTER TABLE foo_replicated ATTACH PARTITION ID \'' || partition_id || '\' FROM foo;' from system.</description></item><item><title>Cumulative Anything</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/cumulative-unique/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/cumulative-unique/</guid><description>Sample data CREATE TABLE events ( `ts` DateTime, `user_id` UInt32 ) ENGINE = Memory; INSERT INTO events SELECT toDateTime(&amp;#39;2021-04-29 10:10:10&amp;#39;) + toIntervalHour(7 * number) AS ts, toDayOfWeek(ts) + (number % 2) AS user_id FROM numbers(15); Using window functions (starting from ClickHouse® 21.3) SELECT toStartOfDay(ts) AS ts, uniqExactMerge(uniqExactState(user_id)) OVER (ORDER BY ts ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS uniq FROM events GROUP BY ts ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ SELECT ts, uniqExactMerge(state) OVER (ORDER BY ts ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS uniq FROM ( SELECT toStartOfDay(ts) AS ts, uniqExactState(user_id) AS state FROM events GROUP BY ts ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ Using arrays WITH groupArray(_ts) AS ts_arr, groupArray(state) AS state_arr SELECT arrayJoin(ts_arr) AS ts, arrayReduce(&amp;#39;uniqExactMerge&amp;#39;, arrayFilter((x, y) -&amp;gt; (y &amp;lt;= ts), state_arr, ts_arr)) AS uniq FROM ( SELECT toStartOfDay(ts) AS _ts, uniqExactState(user_id) AS state FROM events GROUP BY _ts ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ WITH arrayJoin(range(toUInt32(_ts) AS int, least(int + toUInt32((3600 * 24) * 5), toUInt32(toDateTime(&amp;#39;2021-05-04 00:00:00&amp;#39;))), 3600 * 24)) AS ts_expanded SELECT toDateTime(ts_expanded) AS ts, uniqExactMerge(state) AS uniq FROM ( SELECT toStartOfDay(ts) AS _ts, uniqExactState(user_id) AS state FROM events GROUP BY _ts ) GROUP BY ts ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘ Using runningAccumulate (incorrect result over blocks) SELECT ts, runningAccumulate(state) AS uniq FROM ( SELECT toStartOfDay(ts) AS ts, uniqExactState(user_id) AS state FROM events GROUP BY ts ORDER BY ts ASC ) ORDER BY ts ASC ┌──────────────────ts─┬─uniq─┐ │ 2021-04-29 00:00:00 │ 2 │ │ 2021-04-30 00:00:00 │ 3 │ │ 2021-05-01 00:00:00 │ 4 │ │ 2021-05-02 00:00:00 │ 5 │ │ 2021-05-03 00:00:00 │ 7 │ └─────────────────────┴──────┘</description></item><item><title>Data types on disk and in RAM</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/data-types-on-disk-and-in-ram/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/data-types-on-disk-and-in-ram/</guid><description>DataType RAM size (=byteSize) Disk Size String string byte length + 9 string length: 64 bit integer
zero-byte terminator: 1 byte.
string length prefix (varint) + string itself:
string shorter than 128 - string byte length + 1 string shorter than 16384 - string byte length + 2 string shorter than 2097152 - string byte length + 2 string shorter than 268435456 - string byte length + 4
AggregateFunction(count, .</description></item><item><title>Database Size - Table - Column size</title><link>http://kb.altinity.com/altinity-kb-useful-queries/altinity-kb-database-size-table-column-size/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/altinity-kb-database-size-table-column-size/</guid><description>Tables Table size Returns table size, compression rates, and row and part counts, by table
SELECT database, table, formatReadableSize(sum(data_compressed_bytes) AS size) AS compressed, formatReadableSize(sum(data_uncompressed_bytes) AS usize) AS uncompressed, round(usize / size, 2) AS compr_rate, sum(rows) AS rows, count() AS part_count FROM system.parts WHERE (active = 1) AND (database LIKE &amp;#39;%&amp;#39;) AND (table LIKE &amp;#39;%&amp;#39;) GROUP BY database, table ORDER BY size DESC; Table size + inner MatView (Atomic) As above, but resolves Materialized View inner table names (for Materialized Views created using implicit inner table)</description></item><item><title>DELETE via tombstone column</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/delete-via-tombstone-column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/delete-via-tombstone-column/</guid><description>This article provides an overview of the different methods to handle row deletion in ClickHouse, using tombstone columns and ALTER UPDATE or DELETE. The goal is to highlight the performance impacts of different techniques and storage settings, including a scenario using S3 for remote storage.
Creating a Test Table We will start by creating a simple MergeTree table with a tombstone column (is_active) to track active rows: CREATE TABLE test_delete ( `key` UInt32, `ts` UInt32, `value_a` String, `value_b` String, `value_c` String, `is_active` UInt8 DEFAULT 1 ) ENGINE = MergeTree ORDER BY key; Inserting Data Insert sample data into the table: INSERT INTO test_delete (key, ts, value_a, value_b, value_c) SELECT number, 1, concat(&amp;#39;some_looong_string&amp;#39;, toString(number)), concat(&amp;#39;another_long_str&amp;#39;, toString(number)), concat(&amp;#39;string&amp;#39;, toString(number)) FROM numbers(10000000); INSERT INTO test_delete (key, ts, value_a, value_b, value_c) VALUES (400000, 2, &amp;#39;totally different string&amp;#39;, &amp;#39;another totally different string&amp;#39;, &amp;#39;last string&amp;#39;); Querying the Data To verify the inserted data: SELECT * FROM test_delete WHERE key = 400000; ┌────key─┬─ts─┬─value_a──────────────────┬─value_b──────────────────────────┬─value_c─────┬─is_active─┐ │ 400000 │ 2 │ totally different string │ another totally different string │ last string │ 1 │ └────────┴────┴──────────────────────────┴──────────────────────────────────┴─────────────┴───────────┘ ┌────key─┬─ts─┬─value_a──────────────────┬─value_b────────────────┬─value_c──────┬─is_active─┐ │ 400000 │ 1 │ some_looong_string400000 │ another_long_str400000 │ string400000 │ 1 │ └────────┴────┴──────────────────────────┴────────────────────────┴──────────────┴───────────┘ This should return two rows with different ts values.</description></item><item><title>Dictionaries &amp; arrays</title><link>http://kb.altinity.com/altinity-kb-dictionaries/dictionaries-and-arrays/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/dictionaries-and-arrays/</guid><description>Dictionary with ClickHouse® table as a source Test data DROP TABLE IF EXISTS arr_src; CREATE TABLE arr_src ( key UInt64, array_int Array(Int64), array_str Array(String) ) ENGINE = MergeTree order by key; INSERT INTO arr_src SELECT number, arrayMap(i -&amp;gt; (number * i), range(5)), arrayMap(i -&amp;gt; concat(&amp;#39;str&amp;#39;, toString(number * i)), range(5)) FROM numbers(1000); Dictionary DROP DICTIONARY IF EXISTS arr_dict; CREATE DICTIONARY arr_dict ( key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT [&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;] ) PRIMARY KEY key SOURCE(CLICKHOUSE(DATABASE &amp;#39;default&amp;#39; TABLE &amp;#39;arr_src&amp;#39;)) LIFETIME(120) LAYOUT(HASHED()); SELECT dictGet(&amp;#39;arr_dict&amp;#39;, &amp;#39;array_int&amp;#39;, toUInt64(42)) AS res_int, dictGetOrDefault(&amp;#39;arr_dict&amp;#39;, &amp;#39;array_str&amp;#39;, toUInt64(424242), [&amp;#39;none&amp;#39;]) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ [&amp;#39;none&amp;#39;] │ └───────────────────┴──────────┘ Dictionary with PostgreSQL as a source Test data in PG create user ch; create database ch; GRANT ALL PRIVILEGES ON DATABASE ch TO ch; ALTER USER ch WITH PASSWORD &amp;#39;chch&amp;#39;; CREATE TABLE arr_src ( key int, array_int integer[], array_str text[] ); INSERT INTO arr_src VALUES (42, &amp;#39;{0,42,84,126,168}&amp;#39;,&amp;#39;{&amp;#34;str0&amp;#34;,&amp;#34;str42&amp;#34;,&amp;#34;str84&amp;#34;,&amp;#34;str126&amp;#34;,&amp;#34;str168&amp;#34;}&amp;#39;), (66, &amp;#39;{0,66,132,198,264}&amp;#39;,&amp;#39;{&amp;#34;str0&amp;#34;,&amp;#34;str66&amp;#34;,&amp;#34;str132&amp;#34;,&amp;#34;str198&amp;#34;,&amp;#34;str264&amp;#34;}&amp;#39;); Dictionary Example CREATE DICTIONARY pg_arr_dict ( key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT [&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;] ) PRIMARY KEY key SOURCE(POSTGRESQL(PORT 5432 HOST &amp;#39;pg-host&amp;#39; user &amp;#39;ch&amp;#39; password &amp;#39;chch&amp;#39; DATABASE &amp;#39;ch&amp;#39; TABLE &amp;#39;arr_src&amp;#39;)) LIFETIME(120) LAYOUT(HASHED()); select * from pg_arr_dict; ┌─key─┬─array_int──────────┬─array_str───────────────────────────────────┐ │ 66 │ [0,66,132,198,264] │ [&amp;#39;str0&amp;#39;,&amp;#39;str66&amp;#39;,&amp;#39;str132&amp;#39;,&amp;#39;str198&amp;#39;,&amp;#39;str264&amp;#39;] │ │ 42 │ [0,42,84,126,168] │ [&amp;#39;str0&amp;#39;,&amp;#39;str42&amp;#39;,&amp;#39;str84&amp;#39;,&amp;#39;str126&amp;#39;,&amp;#39;str168&amp;#39;] │ └─────┴────────────────────┴─────────────────────────────────────────────┘ SELECT dictGet(&amp;#39;pg_arr_dict&amp;#39;, &amp;#39;array_int&amp;#39;, toUInt64(42)) AS res_int, dictGetOrDefault(&amp;#39;pg_arr_dict&amp;#39;, &amp;#39;array_str&amp;#39;, toUInt64(424242), [&amp;#39;none&amp;#39;]) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ [&amp;#39;none&amp;#39;] │ └───────────────────┴──────────┘ Dictionary with MySQL as a source Test data in MySQL -- casted into CH Arrays create table arr_src( _key bigint(20) NOT NULL, _array_int text, _array_str text, PRIMARY KEY(_key) ); INSERT INTO arr_src VALUES (42, &amp;#39;[0,42,84,126,168]&amp;#39;,&amp;#39;[&amp;#39;&amp;#39;str0&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str42&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str84&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str126&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str168&amp;#39;&amp;#39;]&amp;#39;), (66, &amp;#39;[0,66,132,198,264]&amp;#39;,&amp;#39;[&amp;#39;&amp;#39;str0&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str66&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str132&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str198&amp;#39;&amp;#39;,&amp;#39;&amp;#39;str264&amp;#39;&amp;#39;]&amp;#39;); Dictionary in MySQL -- supporting table to cast data CREATE TABLE arr_src ( `_key` UInt8, `_array_int` String, `array_int` Array(Int32) ALIAS cast(_array_int, &amp;#39;Array(Int32)&amp;#39;), `_array_str` String, `array_str` Array(String) ALIAS cast(_array_str, &amp;#39;Array(String)&amp;#39;) ) ENGINE = MySQL(&amp;#39;mysql_host&amp;#39;, &amp;#39;ch&amp;#39;, &amp;#39;arr_src&amp;#39;, &amp;#39;ch&amp;#39;, &amp;#39;pass&amp;#39;); -- dictionary fetches data from the supporting table CREATE DICTIONARY mysql_arr_dict ( _key UInt64, array_int Array(Int64) DEFAULT [1,2,3], array_str Array(String) DEFAULT [&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;] ) PRIMARY KEY _key SOURCE(CLICKHOUSE(DATABASE &amp;#39;default&amp;#39; TABLE &amp;#39;arr_src&amp;#39;)) LIFETIME(120) LAYOUT(HASHED()); select * from mysql_arr_dict; ┌─_key─┬─array_int──────────┬─array_str───────────────────────────────────┐ │ 66 │ [0,66,132,198,264] │ [&amp;#39;str0&amp;#39;,&amp;#39;str66&amp;#39;,&amp;#39;str132&amp;#39;,&amp;#39;str198&amp;#39;,&amp;#39;str264&amp;#39;] │ │ 42 │ [0,42,84,126,168] │ [&amp;#39;str0&amp;#39;,&amp;#39;str42&amp;#39;,&amp;#39;str84&amp;#39;,&amp;#39;str126&amp;#39;,&amp;#39;str168&amp;#39;] │ └──────┴────────────────────┴─────────────────────────────────────────────┘ SELECT dictGet(&amp;#39;mysql_arr_dict&amp;#39;, &amp;#39;array_int&amp;#39;, toUInt64(42)) AS res_int, dictGetOrDefault(&amp;#39;mysql_arr_dict&amp;#39;, &amp;#39;array_str&amp;#39;, toUInt64(424242), [&amp;#39;none&amp;#39;]) AS res_str ┌─res_int───────────┬─res_str──┐ │ [0,42,84,126,168] │ [&amp;#39;none&amp;#39;] │ └───────────────────┴──────────┘ SELECT dictGet(&amp;#39;mysql_arr_dict&amp;#39;, &amp;#39;array_int&amp;#39;, toUInt64(66)) AS res_int, dictGetOrDefault(&amp;#39;mysql_arr_dict&amp;#39;, &amp;#39;array_str&amp;#39;, toUInt64(66), [&amp;#39;none&amp;#39;]) AS res_str ┌─res_int────────────┬─res_str─────────────────────────────────────┐ │ [0,66,132,198,264] │ [&amp;#39;str0&amp;#39;,&amp;#39;str66&amp;#39;,&amp;#39;str132&amp;#39;,&amp;#39;str198&amp;#39;,&amp;#39;str264&amp;#39;] │ └────────────────────┴─────────────────────────────────────────────┘</description></item><item><title>Dictionaries vs LowCardinality</title><link>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-dictionaries-vs-lowcardinality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-dictionaries-vs-lowcardinality/</guid><description>Q. I think I&amp;rsquo;m still trying to understand how de-normalized is okay - with my relational mindset, I want to move repeated string fields into their own table, but I&amp;rsquo;m not sure to what extent this is necessary
I will look at LowCardinality in more detail - I think it may work well here
A. If it&amp;rsquo;s a simple repetition, which you don&amp;rsquo;t need to manipulate/change in future - LowCardinality works great, and you usually don&amp;rsquo;t need to increase the system complexity by introducing dicts.</description></item><item><title>Dictionary on the top of several tables using VIEW</title><link>http://kb.altinity.com/altinity-kb-dictionaries/dictionary-on-top-tables/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/dictionary-on-top-tables/</guid><description>DROP TABLE IF EXISTS dictionary_source_en; DROP TABLE IF EXISTS dictionary_source_ru; DROP TABLE IF EXISTS dictionary_source_view; DROP DICTIONARY IF EXISTS flat_dictionary; CREATE TABLE dictionary_source_en ( id UInt64, value String ) ENGINE = TinyLog; INSERT INTO dictionary_source_en VALUES (1, &amp;#39;One&amp;#39;), (2,&amp;#39;Two&amp;#39;), (3, &amp;#39;Three&amp;#39;); CREATE TABLE dictionary_source_ru ( id UInt64, value String ) ENGINE = TinyLog; INSERT INTO dictionary_source_ru VALUES (1, &amp;#39;Один&amp;#39;), (2,&amp;#39;Два&amp;#39;), (3, &amp;#39;Три&amp;#39;); CREATE VIEW dictionary_source_view AS SELECT id, dictionary_source_en.value as value_en, dictionary_source_ru.</description></item><item><title>differential backups using clickhouse-backup</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-backup-diff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-backup-diff/</guid><description>differential backups using clickhouse-backup Download the latest version of Altinity Backup for ClickHouse®: https://github.com/Altinity/clickhouse-backup/releases # ubuntu / debian wget https://github.com/Altinity/clickhouse-backup/releases/download/v2.5.20/clickhouse-backup_2.5.20_amd64.deb sudo dpkg -i clickhouse-backup_2.5.20_amd64.deb # centos / redhat / fedora sudo yum install https://github.com/Altinity/clickhouse-backup/releases/download/v2.5.20/clickhouse-backup-2.5.20-1.x86_64.rpm # other platforms wget https://github.com/Altinity/clickhouse-backup/releases/download/v2.5.20/clickhouse-backup.tar.gz sudo mkdir /etc/clickhouse-backup/ sudo mv clickhouse-backup/config.yml /etc/clickhouse-backup/config.yml.example sudo mv clickhouse-backup/clickhouse-backup /usr/bin/ rm -rf clickhouse-backup clickhouse-backup.tar.gz Create a runner script for the crontab mkdir /opt/clickhouse-backup-diff/ cat &amp;lt;&amp;lt; &amp;#39;END&amp;#39; &amp;gt; /opt/clickhouse-backup-diff/clickhouse-backup-cron.sh #!/bin/bash set +x command_line_argument=$1 backup_name=$(date +%Y-%M-%d-%H-%M-%S) echo &amp;#34;Creating local backup &amp;#39;${backup_name}&amp;#39; (full, using hardlinks).</description></item><item><title>Dimension table design</title><link>http://kb.altinity.com/altinity-kb-dictionaries/dimension_table_desing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/dimension_table_desing/</guid><description>Dimension table design considerations Choosing storage Engine To optimize the performance of reporting queries, dimensional tables should be loaded into RAM as ClickHouse Dictionaries whenever feasible. It&amp;rsquo;s becoming increasingly common to allocate 100-200GB of RAM per server specifically for these Dictionaries. Implementing sharding by tenant can further reduce the size of these dimension tables, enabling a greater portion of them to be stored in RAM and thus enhancing query speed.</description></item><item><title>Distributed table to ClickHouse® Cluster</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/distributed-table-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/distributed-table-cluster/</guid><description>In order to shift INSERTS to a standby cluster (for example increase zone availability or disaster recovery ) some ClickHouse® features can be used.
Basically we need to create a distributed table, a MV, rewrite the remote_servers.xml config file and tune some parameters.
Distributed engine information and parameters: https://clickhouse.com/docs/en/engines/table-engines/special/distributed/ Steps Create a Distributed table in the source cluster For example, we should have a ReplicatedMergeTree table in which all inserts are falling.</description></item><item><title>EmbeddedRocksDB &amp; dictionary</title><link>http://kb.altinity.com/engines/altinity-kb-embeddedrocksdb-and-dictionary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/altinity-kb-embeddedrocksdb-and-dictionary/</guid><description>RocksDB is faster than MergeTree on Key/Value queries because MergeTree primary key index is sparse. Probably it&amp;rsquo;s possible to speedup MergeTree by reducing index_granularity.
NVMe disk is used for the tests.
The main feature of RocksDB is instant updates. You can update a row instantly (microseconds):
select * from rocksDB where A=15645646; ┌────────A─┬─B────────────────────┐ │ 15645646 │ 12517841379565221195 │ └──────────┴──────────────────────┘ 1 rows in set. Elapsed: 0.001 sec. insert into rocksDB values (15645646, &amp;#39;xxxx&amp;#39;); 1 rows in set.</description></item><item><title>Encrypt</title><link>http://kb.altinity.com/altinity-kb-functions/encrypt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/encrypt/</guid><description>WHERE over encrypted column CREATE TABLE encrypt ( `key` UInt32, `value` FixedString(4) ) ENGINE = MergeTree ORDER BY key; INSERT INTO encrypt SELECT number, encrypt(&amp;#39;aes-256-ctr&amp;#39;, reinterpretAsString(number + 0.3), &amp;#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;#39;, &amp;#39;xxxxxxxxxxxxxxxx&amp;#39;) FROM numbers(100000000); SET max_threads = 1; SELECT count() FROM encrypt WHERE value IN encrypt(&amp;#39;aes-256-ctr&amp;#39;, reinterpretAsString(toFloat32(1.3)), &amp;#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;#39;, &amp;#39;xxxxxxxxxxxxxxxx&amp;#39;) ┌─count()─┐ │ 1 │ └─────────┘ 1 rows in set. Elapsed: 0.666 sec. Processed 100.00 million rows, 400.01 MB (150.23 million rows/s., 600.93 MB/s.) SELECT count() FROM encrypt WHERE reinterpretAsFloat32(encrypt(&amp;#39;aes-256-ctr&amp;#39;, value, &amp;#39;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&amp;#39;, &amp;#39;xxxxxxxxxxxxxxxx&amp;#39;)) IN toFloat32(1.</description></item><item><title>Error handling</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/error-handling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/error-handling/</guid><description>Pre 21.6 There are couple options:
Certain formats which has schema in built in them (like JSONEachRow) could silently skip any unexpected fields after enabling setting input_format_skip_unknown_fields
It&amp;rsquo;s also possible to skip up to N malformed messages for each block, with used setting kafka_skip_broken_messages but it&amp;rsquo;s also does not support all possible formats.
After 21.6 It&amp;rsquo;s possible to stream messages which could not be parsed, this behavior could be enabled via setting: kafka_handle_error_mode='stream' and ClickHouse® wil write error and message from Kafka itself to two new virtual columns: _error, _raw_message.</description></item><item><title>Exactly once semantics</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-exactly-once-semantics/</guid><description>EOS consumer (isolation.level=read_committed) is enabled by default since librdkafka 1.2.0, so for ClickHouse® - since 20.2
See:
edenhill/librdkafka@6b2a155 9de5dff BUT: while EOS semantics will guarantee you that no duplicates will happen on the Kafka side (i.e. even if you produce the same messages few times it will be consumed once), but ClickHouse as a Kafka client can currently guarantee only at-least-once. And in some corner cases (connection lost etc) you can get duplicates.</description></item><item><title>Example of PostgreSQL dictionary</title><link>http://kb.altinity.com/altinity-kb-dictionaries/example-of-postgresql-dictionary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/example-of-postgresql-dictionary/</guid><description>CREATE DICTIONARY postgres_dict ( id UInt32, value String ) PRIMARY KEY id SOURCE( POSTGRESQL( port 5432 host &amp;#39;postgres1&amp;#39; user &amp;#39;postgres&amp;#39; password &amp;#39;mysecretpassword&amp;#39; db &amp;#39;clickhouse&amp;#39; table &amp;#39;test_schema.test_table&amp;#39; ) ) LIFETIME(MIN 300 MAX 600) LAYOUT(HASHED()); and later do
SELECT dictGetString(postgres_dict, &amp;#39;value&amp;#39;, toUInt64(1))</description></item><item><title>Example: minmax</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/minmax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/minmax/</guid><description>Use cases Strong correlation between column from table ORDER BY / PARTITION BY key and other column which is regularly being used in WHERE condition Good example is incremental ID which increasing with time.
CREATE TABLE skip_idx_corr ( `key` UInt32, `id` UInt32, `ts` DateTime ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, id); INSERT INTO skip_idx_corr SELECT rand(), number, now() + intDiv(number, 10) FROM numbers(100000000); SELECT count() FROM skip_idx_corr WHERE id = 6000000 1 rows in set.</description></item><item><title>EXPLAIN query</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/explain-query/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/explain-query/</guid><description>EXPLAIN types EXPLAIN AST SYNTAX PLAN indexes = 0, header = 0, description = 1, actions = 0, optimize = 1 json = 0 PIPELINE header = 0, graph = 0, compact = 1 ESTIMATE SELECT ... AST - abstract syntax tree SYNTAX - query text after AST-level optimizations PLAN - query execution plan PIPELINE - query execution pipeline ESTIMATE - See Estimates for select query , available since ClickHouse® 21.</description></item><item><title>Fetch Alter Table</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/fetch_alter_table/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/fetch_alter_table/</guid><description>FETCH Parts from Zookeeper This is a detailed explanation on how to move data by fetching partitions or parts between replicas
Get partitions by database and table: SELECT hostName() AS host, database, table partition_id, name as part_id FROM cluster(&amp;#39;{cluster}&amp;#39;, system.parts) WHERE database IN (&amp;#39;db1&amp;#39;,&amp;#39;db2&amp;#39; ... &amp;#39;dbn&amp;#39;) AND active This query will return all the partitions and parts stored in this node for the databases and their tables.
Fetch the partitions: Prior starting with the fetching process it is recommended to check the system.</description></item><item><title>Fill missing values at query time</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/fill-missing-values-at-query-time/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/fill-missing-values-at-query-time/</guid><description>CREATE TABLE event_table ( `key` UInt32, `created_at` DateTime, `value_a` UInt32, `value_b` String ) ENGINE = MergeTree ORDER BY (key, created_at) INSERT INTO event_table SELECT 1 AS key, toDateTime(&amp;#39;2020-10-11 10:10:10&amp;#39;) + number AS created_at, if((number = 0) OR ((number % 5) = 1), number + 1, 0) AS value_a, if((number = 0) OR ((number % 3) = 1), toString(number), &amp;#39;&amp;#39;) AS value_b FROM numbers(10) SELECT main.key, main.created_at, a.value_a, b.value_b FROM event_table AS main ASOF INNER JOIN ( SELECT key, created_at, value_a FROM event_table WHERE value_a !</description></item><item><title>FINAL clause speed</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-final-clause-speed/</guid><description>SELECT * FROM table FINAL
History Before ClickHouse® 20.5 - always executed in a single thread and slow. Since 20.5 - final can be parallel, see https://github.com/ClickHouse/ClickHouse/pull/10463 Since 20.10 - you can use do_not_merge_across_partitions_select_final setting. See https://github.com/ClickHouse/ClickHouse/pull/15938 and https://github.com/ClickHouse/ClickHouse/issues/11722 Since 22.6 - final even more parallel, see https://github.com/ClickHouse/ClickHouse/pull/36396 Since 22.8 - final doesn&amp;rsquo;t read excessive data, see https://github.com/ClickHouse/ClickHouse/pull/47801 Since 23.5 - final use less memory, see https://github.com/ClickHouse/ClickHouse/pull/50429 Since 23.9 - final doesn&amp;rsquo;t read PK columns if unneeded ie only one part in partition, see https://github.</description></item><item><title>Flattened table</title><link>http://kb.altinity.com/altinity-kb-schema-design/flattened-table/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/flattened-table/</guid><description>It&amp;rsquo;s possible to use dictionaries for populating columns of fact table.
CREATE TABLE customer ( `customer_id` UInt32, `first_name` String, `birth_date` Date, `sex` Enum(&amp;#39;M&amp;#39; = 1, &amp;#39;F&amp;#39; = 2) ) ENGINE = MergeTree ORDER BY customer_id CREATE TABLE order ( `order_id` UInt32, `order_date` DateTime DEFAULT now(), `cust_id` UInt32, `amount` Decimal(12, 2) ) ENGINE = MergeTree PARTITION BY toYYYYMM(order_date) ORDER BY (order_date, cust_id, order_id) INSERT INTO customer VALUES(1, &amp;#39;Mike&amp;#39;, now() - INTERVAL 30 YEAR, &amp;#39;M&amp;#39;); INSERT INTO customer VALUES(2, &amp;#39;Boris&amp;#39;, now() - INTERVAL 40 YEAR, &amp;#39;M&amp;#39;); INSERT INTO customer VALUES(3, &amp;#39;Sofie&amp;#39;, now() - INTERVAL 24 YEAR, &amp;#39;F&amp;#39;); INSERT INTO order (order_id, cust_id, amount) VALUES(50, 1, 15); INSERT INTO order (order_id, cust_id, amount) VALUES(30, 1, 10); SELECT * EXCEPT &amp;#39;order_date&amp;#39; FROM order ┌─order_id─┬─cust_id─┬─amount─┐ │ 30 │ 1 │ 10.</description></item><item><title>Floats vs Decimals</title><link>http://kb.altinity.com/altinity-kb-schema-design/floats-vs-decimals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/floats-vs-decimals/</guid><description>Float arithmetics is not accurate: https://floating-point-gui.de/ In case you need accurate calculations you should use Decimal datatypes.
Operations on floats are not associative SELECT (toFloat64(100000000000000000.) + toFloat64(7.5)) - toFloat64(100000000000000000.) AS res ┌─res─┐ │ 0 │ └─────┘ SELECT (toFloat64(100000000000000000.) - toFloat64(100000000000000000.)) + toFloat64(7.5) AS res ┌─res─┐ │ 7.5 │ └─────┘ No problem with Decimals: SELECT (toDecimal64(100000000000000000., 1) + toDecimal64(7.5, 1)) - toDecimal64(100000000000000000., 1) AS res ┌─res─┐ │ 7.5 │ └─────┘ SELECT (toDecimal64(100000000000000000.</description></item><item><title>golang-migrate</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/schema-migration-tools/golang-migrate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/schema-migration-tools/golang-migrate/</guid><description>migrate migrate is a simple schema migration tool written in golang. No external dependencies are required (like interpreter, jre), only one platform-specific executable. golang-migrate/migrate migrate supports several databases, including ClickHouse® (support was introduced by @kshvakov ).
To store information about migrations state migrate creates one additional table in target database, by default that table is called schema_migrations.
Install download the migrate executable for your platform and put it to the folder listed in your %PATH.</description></item><item><title>Google S3 (GCS)</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-google-s3-gcs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-google-s3-gcs/</guid><description>GCS with the table function - seems to work correctly for simple scenarios.
Essentially you can follow the steps from the Migrating from Amazon S3 to Cloud Storage .
Set up a GCS bucket. This bucket must be set as part of the default project for the account. This configuration can be found in settings -&amp;gt; interoperability. Generate a HMAC key for the account, can be done in settings -&amp;gt; interoperability, in the section for user account access keys.</description></item><item><title>Hardware Requirements</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/hardware-requirements/</guid><description>ClickHouse® ClickHouse will use all available hardware to maximize performance. So the more hardware - the better. As of this publication, the hardware requirements are:
Minimum Hardware: 4-core CPU with support of SSE4.2, 16 Gb RAM, 1Tb HDD. Recommended for development and staging environments. SSE4.2 is required, and going below 4 Gb of RAM is not recommended. Recommended Hardware: &amp;gt;=16-cores, &amp;gt;=64Gb RAM, HDD-raid or SSD. For processing up to hundreds of millions / billions of rows.</description></item><item><title>High CPU usage in ClickHouse®</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/high-cpu-usage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/high-cpu-usage/</guid><description>In general, it is a NORMAL situation for ClickHouse® that while processing a huge dataset it can use a lot of (or all of) the server resources. It is &amp;lsquo;by design&amp;rsquo; - just to make the answers faster.
The main directions to reduce the CPU usage is to review the schema / queries to limit the amount of the data which need to be processed, and to plan the resources in a way when single running query will not impact the others.</description></item><item><title>How to check the list of watches</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-how-to-check-the-list-of-watches/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-how-to-check-the-list-of-watches/</guid><description>Zookeeper use watches to notify a client on znode changes. This article explains how to check watches set by ZooKeeper servers and how it is used.
Solution:
Zookeeper uses the 'wchc' command to list all watches set on the Zookeeper server.
# echo wchc | nc zookeeper 2181
Reference
https://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html The wchp and wchc commands are not enabled by default because of their known DOS vulnerability. For more information, see ZOOKEEPER-2693 and Zookeeper 3.</description></item><item><title>How to Convert Atomic to Ordinary</title><link>http://kb.altinity.com/engines/altinity-kb-atomic-database-engine/altinity-kb-how-to-convert-atomic-to-ordinary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/altinity-kb-atomic-database-engine/altinity-kb-how-to-convert-atomic-to-ordinary/</guid><description>The following instructions are an example on how to convert a database with the Engine type Atomic to a database with the Engine type Ordinary.
Warning That can be used only for simple schemas. Schemas with MATERIALIZED views will require extra manipulations. CREATE DATABASE atomic_db ENGINE = Atomic; CREATE DATABASE ordinary_db ENGINE = Ordinary; CREATE TABLE atomic_db.x ENGINE = MergeTree ORDER BY tuple() AS system.numbers; INSERT INTO atomic_db.x SELECT number FROM numbers(100000); RENAME TABLE atomic_db.</description></item><item><title>How to test different compression codecs</title><link>http://kb.altinity.com/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/codecs/altinity-kb-how-to-test-different-compression-codecs/</guid><description>Example Create test_table based on the source table.
CREATE TABLE test_table AS source_table ENGINE=MergeTree() PARTITION BY ...; If the source table has Replicated*MergeTree engine, you would need to change it to non-replicated.
Attach one partition with data from the source table to test_table.
ALTER TABLE test_table ATTACH PARTITION ID &amp;#39;20210120&amp;#39; FROM source_table; You can modify the column or create a new one based on the old column value.
ALTER TABLE test_table MODIFY COLUMN column_a CODEC(ZSTD(2)); ALTER TABLE test_table ADD COLUMN column_new UInt32 DEFAULT toUInt32OrZero(column_old) CODEC(T64,LZ4); After that, you would need to populate changed columns with data.</description></item><item><title>index &amp; column files</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/index-and-column-files/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/index-and-column-files/</guid><description>
https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup27/adaptive_index_granularity.pdf</description></item><item><title>Ingestion performance and formats</title><link>http://kb.altinity.com/altinity-kb-schema-design/ingestion-performance-and-formats/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/ingestion-performance-and-formats/</guid><description>clickhouse-client -q &amp;#39;select toString(number) s, number n, number/1000 f from numbers(100000000) format TSV&amp;#39; &amp;gt; speed.tsv clickhouse-client -q &amp;#39;select toString(number) s, number n, number/1000 f from numbers(100000000) format RowBinary&amp;#39; &amp;gt; speed.RowBinary clickhouse-client -q &amp;#39;select toString(number) s, number n, number/1000 f from numbers(100000000) format Native&amp;#39; &amp;gt; speed.Native clickhouse-client -q &amp;#39;select toString(number) s, number n, number/1000 f from numbers(100000000) format CSV&amp;#39; &amp;gt; speed.csv clickhouse-client -q &amp;#39;select toString(number) s, number n, number/1000 f from numbers(100000000) format JSONEachRow&amp;#39; &amp;gt; speed.</description></item><item><title>IPs/masks</title><link>http://kb.altinity.com/altinity-kb-schema-design/how-to-store-ips/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/how-to-store-ips/</guid><description>How do I Store IPv4 and IPv6 Address In One Field? There is a clean and simple solution for that. Any IPv4 has its unique IPv6 mapping:
IPv4 IP address: 191.239.213.197 IPv4-mapped IPv6 address: ::ffff:191.239.213.197 Find IPs matching CIDR/network mask (IPv4) WITH IPv4CIDRToRange( toIPv4(&amp;#39;10.0.0.1&amp;#39;), 8 ) as range SELECT * FROM values(&amp;#39;ip IPv4&amp;#39;, toIPv4(&amp;#39;10.2.3.4&amp;#39;), toIPv4(&amp;#39;192.0.2.1&amp;#39;), toIPv4(&amp;#39;8.8.8.8&amp;#39;)) WHERE ip BETWEEN range.1 AND range.2; Find IPs matching CIDR/network mask (IPv6) WITH IPv6CIDRToRange ( toIPv6(&amp;#39;2001:0db8:0000:85a3:0000:0000:ac1f:8001&amp;#39;), 32 ) as range SELECT * FROM values(&amp;#39;ip IPv6&amp;#39;, toIPv6(&amp;#39;2001:db8::8a2e:370:7334&amp;#39;), toIPv6(&amp;#39;::ffff:192.</description></item><item><title>JOIN optimization tricks</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/joins/joins-tricks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/joins/joins-tricks/</guid><description>All tests below were done with default hash join. ClickHouse joins are evolving rapidly and behavior varies with other join types.
Data For our exercise, we will use two tables from a well known TPS-DS benchmark: store_sales and customer. Table sizes are the following:
store_sales = 2 billion rows customer = 12 millions rows
So there are 200 rows in store_sales table per each customer on average. Also 90% of customers made 1-10 purchases.</description></item><item><title>Join with Calendar using Arrays</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/join-with-calendar-using-arrays/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/join-with-calendar-using-arrays/</guid><description>Sample data CREATE TABLE test_metrics (counter_id Int64, timestamp DateTime, metric UInt64) Engine=Log; INSERT INTO test_metrics SELECT number % 3, toDateTime(&amp;#39;2021-01-01 00:00:00&amp;#39;), 1 FROM numbers(20); INSERT INTO test_metrics SELECT number % 3, toDateTime(&amp;#39;2021-01-03 00:00:00&amp;#39;), 1 FROM numbers(20); SELECT counter_id, toDate(timestamp) dt, sum(metric) FROM test_metrics GROUP BY counter_id, dt ORDER BY counter_id, dt; ┌─counter_id─┬─────────dt─┬─sum(metric)─┐ │ 0 │ 2021-01-01 │ 7 │ │ 0 │ 2021-01-03 │ 7 │ │ 1 │ 2021-01-01 │ 7 │ │ 1 │ 2021-01-03 │ 7 │ │ 2 │ 2021-01-01 │ 6 │ │ 2 │ 2021-01-03 │ 6 │ └────────────┴────────────┴─────────────┘ Calendar WITH arrayMap(i -&amp;gt; (toDate(&amp;#39;2021-01-01&amp;#39;) + i), range(4)) AS Calendar SELECT arrayJoin(Calendar); ┌─arrayJoin(Calendar)─┐ │ 2021-01-01 │ │ 2021-01-02 │ │ 2021-01-03 │ │ 2021-01-04 │ └─────────────────────┘ Join with Calendar using arrayJoin SELECT counter_id, tuple.</description></item><item><title>JSONAsString and Mat. View as JSON parser</title><link>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/</guid><description>Tables with engine Null don’t store data but can be used as a source for materialized views.
JSONAsString a special input format which allows to ingest JSONs into a String column. If the input has several JSON objects (comma separated) they will be interpreted as separate rows. JSON can be multiline.
create table entrypoint(J String) Engine=Null; create table datastore(a String, i Int64, f Float64) Engine=MergeTree order by a; create materialized view jsonConverter to datastore as select (JSONExtract(J, &amp;#39;Tuple(String,Tuple(Int64,Float64))&amp;#39;) as x), x.</description></item><item><title>JSONExtract to parse many attributes at a time</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/jsonextract-to-parse-many-attributes-at-a-time/</guid><description>Don&amp;rsquo;t use several JSONExtract for parsing big JSON. It&amp;rsquo;s very ineffective, slow, and consumes CPU. Try to use one JSONExtract to parse String to Tupes and next get the needed elements:
WITH JSONExtract(json, &amp;#39;Tuple(name String, id String, resources Nested(description String, format String, tracking_summary Tuple(total UInt32, recent UInt32)), extras Nested(key String, value String))&amp;#39;) AS parsed_json SELECT tupleElement(parsed_json, &amp;#39;name&amp;#39;) AS name, tupleElement(parsed_json, &amp;#39;id&amp;#39;) AS id, tupleElement(tupleElement(parsed_json, &amp;#39;resources&amp;#39;), &amp;#39;description&amp;#39;) AS `resources.description`, tupleElement(tupleElement(parsed_json, &amp;#39;resources&amp;#39;), &amp;#39;format&amp;#39;) AS `resources.</description></item><item><title>JVM sizes and garbage collector settings</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/jvm-sizes-and-garbage-collector-settings/</guid><description>TLDR version use fresh Java version (11 or newer), disable swap and set up (for 4 Gb node):
JAVA_OPTS=&amp;#34;-Xms512m -Xmx3G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50&amp;#34; If you have a node with more RAM - change it accordingly, for example for 8Gb node:
JAVA_OPTS=&amp;#34;-Xms512m -Xmx7G -XX:+AlwaysPreTouch -Djute.maxbuffer=8388608 -XX:MaxGCPauseMillis=50&amp;#34; Details ZooKeeper runs as in JVM. Depending on version different garbage collectors are available.
Recent JVM versions (starting from 10) use G1 garbage collector by default (should work fine).</description></item><item><title>Kafka main parsing loop</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/</guid><description>One of the threads from scheduled_pool (pre ClickHouse® 20.9) / background_message_broker_schedule_pool (after 20.9) do that in infinite loop:
Batch poll (time limit: kafka_poll_timeout_ms 500ms, messages limit: kafka_poll_max_batch_size 65536) Parse messages. If we don&amp;rsquo;t have enough data (rows limit: kafka_max_block_size 1048576) or time limit reached (kafka_flush_interval_ms 7500ms) - continue polling (goto p.1) Write a collected block of data to MV Do commit (commit after write = at-least-once). On any error, during that process, Kafka client is restarted (leading to rebalancing - leave the group and get back in few seconds).</description></item><item><title>Kafka parallel consuming</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-parallel-consuming/</guid><description>For very large topics when you need more parallelism (especially on the insert side) you may use several tables with the same pipeline (pre ClickHouse® 20.9) or enable kafka_thread_per_consumer (after 20.9).
kafka_num_consumers = N, kafka_thread_per_consumer=1 Notes:
the inserts will happen in parallel (without that setting inserts happen linearly) enough partitions are needed. kafka_num_consumers is limited by number of physical cores (half of vCPUs). kafka_disable_num_consumers_limit can be used to override the limit.</description></item><item><title>KILL QUERY</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-kill-query/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-kill-query/</guid><description>Unfortunately not all queries can be killed. KILL QUERY only sets a flag that must be checked by the query. A query pipeline is checking this flag before a switching to next block. If the pipeline has stuck somewhere in the middle it cannot be killed. If a query does not stop, the only way to get rid of it is to restart ClickHouse®.
See also:
https://github.com/ClickHouse/ClickHouse/issues/3964 https://github.com/ClickHouse/ClickHouse/issues/1576 How to replace a running query Q.</description></item><item><title>Kubernetes job for clickhouse-copier</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-kubernetes-job/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/altinity-kb-clickhouse-copier-kubernetes-job/</guid><description>clickhouse-copier deployment in kubernetes clickhouse-copier can be deployed in a kubernetes environment to automate some simple backups or copy fresh data between clusters.
Some documentation to read:
https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/altinity-kb-clickhouse-copier/ https://github.com/clickhouse/copier/ Deployment Use a kubernetes job is recommended but a simple pod can be used if you only want to execute the copy one time.
Just edit/change all the yaml files to your needs.
1) Create the PVC: First create a namespace in which all the pods and resources are going to be deployed</description></item><item><title>Lag / Lead</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/lag-lead/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/lag-lead/</guid><description>Sample data CREATE TABLE llexample ( g Int32, a Date ) ENGINE = Memory; INSERT INTO llexample SELECT number % 3, toDate(&amp;#39;2020-01-01&amp;#39;) + number FROM numbers(10); SELECT * FROM llexample ORDER BY g,a; ┌─g─┬──────────a─┐ │ 0 │ 2020-01-01 │ │ 0 │ 2020-01-04 │ │ 0 │ 2020-01-07 │ │ 0 │ 2020-01-10 │ │ 1 │ 2020-01-02 │ │ 1 │ 2020-01-05 │ │ 1 │ 2020-01-08 │ │ 2 │ 2020-01-03 │ │ 2 │ 2020-01-06 │ │ 2 │ 2020-01-09 │ └───┴────────────┘ Using arrays select g, (arrayJoin(tuple_ll) as ll).</description></item><item><title>Load balancers</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/load-balancers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/load-balancers/</guid><description>In general - one of the simplest option to do load balancing is to implement it on the client side.
I.e. list several endpoints for ClickHouse® connections and add some logic to pick one of the nodes.
Many client libraries support that.
ClickHouse native protocol (port 9000) Currently there are no protocol-aware proxies for ClickHouse protocol, so the proxy / load balancer can work only on TCP level.
One of the best option for TCP load balancer is haproxy, also nginx can work in that mode.</description></item><item><title>LowCardinality</title><link>http://kb.altinity.com/altinity-kb-schema-design/lowcardinality/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-schema-design/lowcardinality/</guid><description>Settings allow_suspicious_low_cardinality_types In CREATE TABLE statement allows specifying LowCardinality modifier for types of small fixed size (8 or less). Enabling this may increase merge times and memory consumption.
low_cardinality_max_dictionary_size default - 8192
Maximum size (in rows) of shared global dictionary for LowCardinality type.
low_cardinality_use_single_dictionary_for_part LowCardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.
low_cardinality_allow_in_native_format Use LowCardinality type in Native format.</description></item><item><title>Machine learning in ClickHouse</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/machine-learning-in-clickhouse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/machine-learning-in-clickhouse/</guid><description>Resources
Machine Learning in ClickHouse - Presentation from 2019 (Meetup 31) ML discussion: CatBoost / MindsDB / Fast.ai - Brief article from 2021 Machine Learning Forecase (Russian) - Presentation from 2019 (Meetup 38)</description></item><item><title>memory configuration settings</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/</guid><description>max_memory_usage. Single query memory usage max_memory_usage - the maximum amount of memory allowed for a single query to take. By default, it&amp;rsquo;s 10Gb. The default value is good, don&amp;rsquo;t adjust it in advance.
There are scenarios when you need to relax the limit for particular queries (if you hit &amp;lsquo;Memory limit (for query) exceeded&amp;rsquo;), or use a lower limit if you need to discipline the users or increase the number of simultaneous queries.</description></item><item><title>Memory Overcommiter</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-memory-overcommit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-memory-overcommit/</guid><description>Memory Overcommiter From version 22.2+ ClickHouse® was updated with enhanced Memory overcommit capabilities . In the past, queries were constrained by the max_memory_usage setting, imposing a rigid limitation. Users had the option to increase this limit, but it came at the potential expense of impacting other users during a single query. With the introduction of Memory overcommit, more memory-intensive queries can now execute, granted there are ample resources available. When the server reaches its maximum memory limit , ClickHouse identifies the most overcommitted queries and attempts to terminate them.</description></item><item><title>Merge performance and OPTIMIZE FINAL</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/merge-performance-final-optimize-by/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/merge-performance-final-optimize-by/</guid><description>Merge Performance Main things affecting the merge speed are:
Schema (especially compression codecs, some bad types, sorting order&amp;hellip;) Horizontal vs Vertical merge Horizontal = reads all columns at once, do merge sort, write new part Vertical = first read columns from order by, do merge sort, write them to disk, remember permutation, then process the rest of columns on by one, applying permutation. compact vs wide parts Other things like server load, concurrent merges&amp;hellip; SELECT name, value FROM system.</description></item><item><title>Moving a table to another device</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-moving-table-to-another-device./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-moving-table-to-another-device./</guid><description>Suppose we mount a new device at path /mnt/disk_1 and want to move table_4 to it.
Create directory on new device for ClickHouse® data. /in shell mkdir /mnt/disk_1/clickhouse Change ownership of created directory to ClickHouse user. /in shell chown -R clickhouse:clickhouse /mnt/disk_1/clickhouse Create a special storage policy which should include both disks: old and new. /in shell nano /etc/clickhouse-server/config.d/storage.xml ###################/etc/clickhouse-server/config.d/storage.xml########################### &amp;lt;yandex&amp;gt; &amp;lt;storage_configuration&amp;gt; &amp;lt;disks&amp;gt; &amp;lt;!-- default disk is special, it always exists even if not explicitly configured here, but you can&amp;#39;t change it&amp;#39;s path here (you should use &amp;lt;path&amp;gt; on top level config instead) --&amp;gt; &amp;lt;default&amp;gt; &amp;lt;!</description></item><item><title>Multiple MVs attached to Kafka table</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-mv-consuming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-mv-consuming/</guid><description>Kafka Consumer is a thread inside the Kafka Engine table that is visible by Kafka monitoring tools like kafka-consumer-groups and in Clickhouse in system.kafka_consumers table.
Having multiple consumers increases ingesting parallelism and can significantly speed up event processing. However, it comes with a trade-off: it&amp;rsquo;s a CPU-intensive task, especially under high event load and/or complicated parsing of incoming data. Therefore, it&amp;rsquo;s crucial to create as many consumers as you really need and ensure you have enough CPU cores to handle them.</description></item><item><title>Mutations</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/mutations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/mutations/</guid><description>How to know if ALTER TABLE … DELETE/UPDATE mutation ON CLUSTER was finished successfully on all the nodes? A. mutation status in system.mutations is local to each replica, so use
SELECT hostname(), * FROM clusterAllReplicas(&amp;#39;your_cluster_name&amp;#39;, system.mutations); -- you can also add WHERE conditions to that query if needed. Look on is_done and latest_fail_reason columns
Are mutations being run in parallel or they are sequential in ClickHouse® (in scope of one table) ClickHouse runs mutations sequentially, but it can combine several mutations in a single and apply all of them in one merge.</description></item><item><title>MySQL8 source for dictionaries</title><link>http://kb.altinity.com/altinity-kb-dictionaries/mysql8-source-for-dictionaries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/mysql8-source-for-dictionaries/</guid><description>Authorization MySQL8 used default authorization plugin caching_sha2_password. Unfortunately, libmysql which currently used (21.4-) in ClickHouse® is not.
You can fix it during create custom user with mysql_native_password authentication plugin.
CREATE USER IF NOT EXISTS &amp;#39;clickhouse&amp;#39;@&amp;#39;%&amp;#39; IDENTIFIED WITH mysql_native_password BY &amp;#39;clickhouse_user_password&amp;#39;; CREATE DATABASE IF NOT EXISTS test; GRANT ALL PRIVILEGES ON test.* TO &amp;#39;clickhouse&amp;#39;@&amp;#39;%&amp;#39;; Table schema changes As an example, in ClickHouse, run SHOW TABLE STATUS LIKE 'table_name' and try to figure out was table schema changed or not from MySQL response field Update_time.</description></item><item><title>Network Configuration</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/cluster-production-configuration-guide/network-configuration/</guid><description>Networking And Server Room Planning The network used for your ClickHouse® cluster should be a fast network, ideally 10 Gbit or more. ClickHouse nodes generate a lot of traffic to exchange the data between nodes (port 9009 for replication, and 9000 for distributed queries). Zookeeper traffic in normal circumstances is moderate, but in some special cases can also be very significant.
For the zookeeper low latency is more important than bandwidth.</description></item><item><title>Notes on Various Errors with respect to replication and distributed connections</title><link>http://kb.altinity.com/altinity-kb-useful-queries/connection-issues-distributed-parts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/connection-issues-distributed-parts/</guid><description>ClickHouseDistributedConnectionExceptions This alert usually indicates that one of the nodes isn’t responding or that there’s an interconnectivity issue. Debug steps:
1. Check Cluster Connectivity Verify connectivity inside the cluster by running:
SELECT count() FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;, cluster(&amp;#39;{cluster}&amp;#39;, system.one)) 2. Check for Errors Run the following queries to see if any nodes report errors:
SELECT hostName(), * FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;, system.clusters) WHERE errors_count &amp;gt; 0; SELECT hostName(), * FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;, system.errors) WHERE last_error_time &amp;gt; now() - 3600 ORDER BY value; Depending on the results, ensure that the affected node is up and responding to queries.</description></item><item><title>Nulls in order by</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/altinity-kb-nulls-in-order-by/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/altinity-kb-nulls-in-order-by/</guid><description> It is NOT RECOMMENDED for a general use Use on your own risk Use latest ClickHouse® version if you need that. CREATE TABLE x ( `a` Nullable(UInt32), `b` Nullable(UInt32), `cnt` UInt32 ) ENGINE = SummingMergeTree ORDER BY (a, b) SETTINGS allow_nullable_key = 1; INSERT INTO x VALUES (Null,2,1), (Null,Null,1), (3, Null, 1), (4,4,1); INSERT INTO x VALUES (Null,2,1), (Null,Null,1), (3, Null, 1), (4,4,1); SELECT * FROM x; ┌────a─┬────b─┬─cnt─┐ │ 3 │ null │ 2 │ │ 4 │ 4 │ 2 │ │ null │ 2 │ 2 │ │ null │ null │ 2 │ └──────┴──────┴─────┘</description></item><item><title>Number of active parts in a partition</title><link>http://kb.altinity.com/altinity-kb-useful-queries/altinity-kb-number-of-active-parts-in-a-partition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/altinity-kb-number-of-active-parts-in-a-partition/</guid><description>Q: Why do I have several active parts in a partition? Why ClickHouse® does not merge them immediately? A: CH does not merge parts by time Merge scheduler selects parts by own algorithm based on the current node workload / number of parts / size of parts.
CH merge scheduler balances between a big number of parts and a wasting resources on merges.
Merges are CPU/DISK IO expensive. If CH will merge every new part then all resources will be spend on merges and will no resources remain on queries (selects ).</description></item><item><title>Object consistency in a cluster</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-object-consistency-in-a-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-object-consistency-in-a-cluster/</guid><description>List of missing tables
WITH ( SELECT groupArray(FQDN()) FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system,one) ) AS hosts SELECT database, table, arrayFilter( i-&amp;gt; NOT has(groupArray(host),i), hosts) miss_table FROM ( SELECT FQDN() host, database, name table FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system,tables) WHERE engine NOT IN (&amp;#39;Log&amp;#39;,&amp;#39;Memory&amp;#39;,&amp;#39;TinyLog&amp;#39;) ) GROUP BY database, table HAVING miss_table &amp;lt;&amp;gt; [] SETTINGS skip_unavailable_shards=1; ┌─database─┬─table─┬─miss_table────────────────┐ │ default │ test │ [&amp;#39;host366.mynetwork.net&amp;#39;] │ └──────────┴───────┴───────────────────────────┘ List of inconsistent tables
SELECT database, name, engine, uniqExact(create_table_query) AS ddl FROM clusterAllReplicas(&amp;#39;{cluster}&amp;#39;,system.tables) GROUP BY database, name, engine HAVING ddl &amp;gt; 1 List of inconsistent columns</description></item><item><title>OPTIMIZE vs OPTIMIZE FINAL</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-optimize-vs-optimize-final/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-optimize-vs-optimize-final/</guid><description>OPTIMIZE TABLE xyz &amp;ndash; this initiates an unscheduled merge.
Example You have 40 parts in 3 partitions. This unscheduled merge selects some partition (i.e. February) and selects 3 small parts to merge, then merge them into a single part. You get 38 parts in the result.
OPTIMIZE TABLE xyz FINAL &amp;ndash; initiates a cycle of unscheduled merges.
ClickHouse® merges parts in this table until will remains 1 part in each partition (if a system has enough free disk space).</description></item><item><title>Parameterized views</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-parameterized-views/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-parameterized-views/</guid><description>ClickHouse® versions 23.1+ (23.1.6.42, 23.2.5.46, 23.3.1.2823) have inbuilt support for parametrized views :
CREATE VIEW my_new_view AS SELECT * FROM deals WHERE category_id IN ( SELECT category_id FROM deal_categories WHERE category = {category:String} ) SELECT * FROM my_new_view(category = &amp;#39;hot deals&amp;#39;); One more example CREATE OR REPLACE VIEW v AS SELECT 1::UInt32 x WHERE x IN ({xx:Array(UInt32)}); select * from v(xx=[1,2,3]); ┌─x─┐ │ 1 │ └───┘ ClickHouse versions pre 23.1 Custom settings allows to emulate parameterized views.</description></item><item><title>Partial updates</title><link>http://kb.altinity.com/altinity-kb-dictionaries/partial-updates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/partial-updates/</guid><description>ClickHouse® is able to fetch from a source only updated rows. You need to define update_field section.
As an example, We have a table in an external source MySQL, PG, HTTP, &amp;hellip; defined with the following code sample:
CREATE TABLE cities ( `polygon` Array(Tuple(Float64, Float64)), `city` String, `updated_at` DateTime DEFAULT now() ) ENGINE = MergeTree ORDER BY city When you add new row and update some rows in this table you should update updated_at with the new timestamp.</description></item><item><title>Use both projection and raw data in single query</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/partial-projection-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/partial-projection-optimization/</guid><description>CREATE TABLE default.metric ( `key_a` UInt8, `key_b` UInt32, `date` Date, `value` UInt32, PROJECTION monthly ( SELECT key_a, key_b, min(date), sum(value) GROUP BY key_a, key_b ) ) ENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY (key_a, key_b, date) SETTINGS index_granularity = 8192; INSERT INTO metric SELECT key_a, key_b, date, rand() % 100000 AS value FROM ( SELECT arrayJoin(range(8)) AS key_a, number % 500000 AS key_b, today() - intDiv(number, 500000) AS date FROM numbers_mt(1080000000) ); OPTIMIZE TABLE metric FINAL; SET max_threads = 8; WITH toDate(&amp;#39;2015-02-27&amp;#39;) AS start_date, toDate(&amp;#39;2022-02-15&amp;#39;) AS end_date, key_a IN (1, 3, 5, 7) AS key_a_cond SELECT key_b, sum(value) AS sum FROM metric WHERE (date &amp;gt; start_date) AND (date &amp;lt; end_date) AND key_a_cond GROUP BY key_b ORDER BY sum DESC LIMIT 25 25 rows in set.</description></item><item><title>Parts consistency</title><link>http://kb.altinity.com/altinity-kb-useful-queries/parts-consistency/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-useful-queries/parts-consistency/</guid><description>Check if there are blocks missing SELECT database, table, partition_id, ranges.1 AS previous_part, ranges.2 AS next_part, ranges.3 AS previous_block_number, ranges.4 AS next_block_number, range(toUInt64(previous_block_number + 1), toUInt64(next_block_number)) AS missing_block_numbers FROM ( WITH arrayPopFront(groupArray(min_block_number) AS min) AS min_adj, arrayPopBack(groupArray(max_block_number) AS max) AS max_adj, arrayFilter((x, y, z) -&amp;gt; (y != (z + 1)), arrayZip(arrayPopBack(groupArray(name) AS name_arr), arrayPopFront(name_arr), max_adj, min_adj), min_adj, max_adj) AS missing_ranges SELECT database, table, partition_id, missing_ranges FROM ( SELECT * FROM system.</description></item><item><title>PIVOT / UNPIVOT</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/pivot-unpivot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/pivot-unpivot/</guid><description>PIVOT CREATE TABLE sales(suppkey UInt8, category String, quantity UInt32) ENGINE=Memory(); INSERT INTO sales VALUES (2, &amp;#39;AA&amp;#39; ,7500),(1, &amp;#39;AB&amp;#39; , 4000),(1, &amp;#39;AA&amp;#39; , 6900),(1, &amp;#39;AB&amp;#39;, 8900), (1, &amp;#39;AC&amp;#39;, 8300), (1, &amp;#39;AA&amp;#39;, 7000), (1, &amp;#39;AC&amp;#39;, 9000), (2,&amp;#39;AA&amp;#39;, 9800), (2,&amp;#39;AB&amp;#39;, 9600), (1,&amp;#39;AC&amp;#39;, 8900),(1, &amp;#39;AD&amp;#39;, 400), (2,&amp;#39;AD&amp;#39;, 900), (2,&amp;#39;AD&amp;#39;, 1200), (1,&amp;#39;AD&amp;#39;, 2600), (2, &amp;#39;AC&amp;#39;, 9600),(1, &amp;#39;AC&amp;#39;, 6200); Using Map data type (starting from ClickHouse® 21.1) WITH CAST(sumMap([category], [quantity]), &amp;#39;Map(String, UInt32)&amp;#39;) AS map SELECT suppkey, map[&amp;#39;AA&amp;#39;] AS AA, map[&amp;#39;AB&amp;#39;] AS AB, map[&amp;#39;AC&amp;#39;] AS AC, map[&amp;#39;AD&amp;#39;] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ WITH CAST(sumMap(map(category, quantity)), &amp;#39;Map(LowCardinality(String), UInt32)&amp;#39;) AS map SELECT suppkey, map[&amp;#39;AA&amp;#39;] AS AA, map[&amp;#39;AB&amp;#39;] AS AB, map[&amp;#39;AC&amp;#39;] AS AC, map[&amp;#39;AD&amp;#39;] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ Using -If combinator SELECT suppkey, sumIf(quantity, category = &amp;#39;AA&amp;#39;) AS AA, sumIf(quantity, category = &amp;#39;AB&amp;#39;) AS AB, sumIf(quantity, category = &amp;#39;AC&amp;#39;) AS AC, sumIf(quantity, category = &amp;#39;AD&amp;#39;) AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ Using -Resample combinator WITH sumResample(0, 4, 1)(quantity, transform(category, [&amp;#39;AA&amp;#39;, &amp;#39;AB&amp;#39;, &amp;#39;AC&amp;#39;, &amp;#39;AD&amp;#39;], [0, 1, 2, 3], 4)) AS sum SELECT suppkey, sum[1] AS AA, sum[2] AS AB, sum[3] AS AC, sum[4] AS AD FROM sales GROUP BY suppkey ORDER BY suppkey ASC ┌─suppkey─┬────AA─┬────AB─┬────AC─┬───AD─┐ │ 1 │ 13900 │ 12900 │ 32400 │ 3000 │ │ 2 │ 17300 │ 9600 │ 9600 │ 2100 │ └─────────┴───────┴───────┴───────┴──────┘ UNPIVOT CREATE TABLE sales_w(suppkey UInt8, brand String, AA UInt32, AB UInt32, AC UInt32, AD UInt32) ENGINE=Memory(); INSERT INTO sales_w VALUES (1, &amp;#39;BRAND_A&amp;#39;, 1500, 4200, 1600, 9800), (2, &amp;#39;BRAND_B&amp;#39;, 6200, 1300, 5800, 3100), (3, &amp;#39;BRAND_C&amp;#39;, 5000, 8900, 6900, 3400); SELECT suppkey, brand, category, quantity FROM sales_w ARRAY JOIN [AA, AB, AC, AD] AS quantity, splitByString(&amp;#39;, &amp;#39;, &amp;#39;AA, AB, AC, AD&amp;#39;) AS category ORDER BY suppkey ASC ┌─suppkey─┬─brand───┬─category─┬─quantity─┐ │ 1 │ BRAND_A │ AA │ 1500 │ │ 1 │ BRAND_A │ AB │ 4200 │ │ 1 │ BRAND_A │ AC │ 1600 │ │ 1 │ BRAND_A │ AD │ 9800 │ │ 2 │ BRAND_B │ AA │ 6200 │ │ 2 │ BRAND_B │ AB │ 1300 │ │ 2 │ BRAND_B │ AC │ 5800 │ │ 2 │ BRAND_B │ AD │ 3100 │ │ 3 │ BRAND_C │ AA │ 5000 │ │ 3 │ BRAND_C │ AB │ 8900 │ │ 3 │ BRAND_C │ AC │ 6900 │ │ 3 │ BRAND_C │ AD │ 3400 │ └─────────┴─────────┴──────────┴──────────┘ SELECT suppkey, brand, tpl.</description></item><item><title>Possible deadlock avoided. Client should retry</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-possible-deadlock-avoided.-client-should-retry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-possible-deadlock-avoided.-client-should-retry/</guid><description>In ClickHouse® version 19.14 a serious issue was found: a race condition that can lead to server deadlock. The reason for that was quite fundamental, and a temporary workaround for that was added (&amp;ldquo;possible deadlock avoided&amp;rdquo;).
Those locks are one of the fundamental things that the core team was actively working on in 2020.
In 20.3 some of the locks leading to that situation were removed as a part of huge refactoring.</description></item><item><title>Proper setup</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-proper-setup/</guid><description>Main docs article https://docs.altinity.com/operationsguide/clickhouse-zookeeper/zookeeper-installation/ Hardware requirements TLDR version:
USE DEDICATED FAST DISKS for the transaction log! (crucial for performance due to write-ahead-log, NVMe is preferred for heavy load setup). use 3 nodes (more nodes = slower quorum, less = no HA). low network latency between zookeeper nodes is very important (latency, not bandwidth). have at least 4Gb of RAM, disable swap, tune JVM sizes, and garbage collector settings. ensure that zookeeper will not be CPU-starved by some other processes monitor zookeeper.</description></item><item><title>RabbitMQ Error handling</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-rabbitmq/error-handling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-rabbitmq/error-handling/</guid><description>Same approach as in Kafka but virtual columns are different. Check https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq#virtual-columns CREATE TABLE IF NOT EXISTS rabbitmq.broker_errors_queue ( exchange_name String, channel_id String, delivery_tag UInt64, redelivered UInt8, message_id String, timestamp UInt64 ) engine = RabbitMQ SETTINGS rabbitmq_host_port = &amp;#39;localhost:5672&amp;#39;, rabbitmq_exchange_name = &amp;#39;exchange-test&amp;#39;, -- required parameter even though this is done via the rabbitmq config rabbitmq_queue_consume = true, rabbitmq_queue_base = &amp;#39;test-errors&amp;#39;, rabbitmq_format = &amp;#39;JSONEachRow&amp;#39;, rabbitmq_username = &amp;#39;guest&amp;#39;, rabbitmq_password = &amp;#39;guest&amp;#39;, rabbitmq_handle_error_mode = &amp;#39;stream&amp;#39;; CREATE MATERIALIZED VIEW IF NOT EXISTS rabbitmq.</description></item><item><title>range_hashed example - open intervals</title><link>http://kb.altinity.com/altinity-kb-dictionaries/altinity-kb-range_hashed-example-open-intervals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/altinity-kb-range_hashed-example-open-intervals/</guid><description>The following example shows a range_hashed example at open intervals.
DROP TABLE IF EXISTS rates; DROP DICTIONARY IF EXISTS rates_dict; CREATE TABLE rates ( id UInt64, date_start Nullable(Date), date_end Nullable(Date), rate Decimal64(4) ) engine=Log; INSERT INTO rates VALUES (1, Null, &amp;#39;2021-03-13&amp;#39;,99), (1, &amp;#39;2021-03-14&amp;#39;,&amp;#39;2021-03-16&amp;#39;,100), (1, &amp;#39;2021-03-17&amp;#39;, Null, 101), (2, &amp;#39;2021-03-14&amp;#39;, Null, 200), (3, Null, &amp;#39;2021-03-14&amp;#39;, 300), (4, &amp;#39;2021-03-14&amp;#39;, &amp;#39;2021-03-14&amp;#39;, 400); CREATE DICTIONARY rates_dict ( id UInt64, date_start Date, date_end Date, rate Decimal64(4) ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST &amp;#39;localhost&amp;#39; PORT 9000 USER &amp;#39;default&amp;#39; TABLE &amp;#39;rates&amp;#39;)) LIFETIME(MIN 1 MAX 1000) LAYOUT(RANGE_HASHED()) RANGE(MIN date_start MAX date_end); SELECT * FROM rates_dict order by id, date_start; ┌─id─┬─date_start─┬───date_end─┬─────rate─┐ │ 1 │ 1970-01-01 │ 2021-03-13 │ 99.</description></item><item><title>Recovering from complete metadata loss in ZooKeeper</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-recovering-from-complete-metadata-loss-in-zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-recovering-from-complete-metadata-loss-in-zookeeper/</guid><description>Problem Every ClickHouse® user experienced a loss of ZooKeeper one day. While the data is available and replicas respond to queries, inserts are no longer possible. ClickHouse uses ZooKeeper in order to store the reference version of the table structure and part of data, and when it is not available can not guarantee data consistency anymore. Replicated tables turn to the read-only mode. In this article we describe step-by-step instructions of how to restore ZooKeeper metadata and bring ClickHouse cluster back to normal operation.</description></item><item><title>System tables ate my disk</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-system-tables-eat-my-disk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-system-tables-eat-my-disk/</guid><description>Note 1: System database stores virtual tables (parts, tables, columns, etc.) and *_log tables.
Virtual tables do not persist on disk. They reflect ClickHouse® memory (c++ structures). They cannot be changed or removed.
Log tables are named with postfix *_log and have the MergeTree engine . ClickHouse does not use information stored in these tables, this data is for you only.
You can drop / rename / truncate *_log tables at any time.</description></item><item><title>Remote table function</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-table-function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/remote-table-function/</guid><description>remote(&amp;hellip;) table function Suitable for moving up to hundreds of gigabytes of data.
With bigger tables recommended approach is to slice the original data by some WHERE condition, ideally - apply the condition on partitioning key, to avoid writing data to many partitions at once.
INSERT INTO staging_table SELECT * FROM remote(...) WHERE date=&amp;#39;2021-04-13&amp;#39;; INSERT INTO staging_table SELECT * FROM remote(...) WHERE date=&amp;#39;2021-04-12&amp;#39;; INSERT INTO staging_table SELECT * FROM remote(...) WHERE date=&amp;#39;2021-04-11&amp;#39;; .</description></item><item><title>Removing empty parts</title><link>http://kb.altinity.com/upgrade/removing-empty-parts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/upgrade/removing-empty-parts/</guid><description>Removing of empty parts is a new feature introduced in ClickHouse® 20.12. Earlier versions leave empty parts (with 0 rows) if TTL removes all rows from a part (https://github.com/ClickHouse/ClickHouse/issues/5491 ). If you set up TTL for your data it is likely that there are quite many empty parts in your system.
The new version notices empty parts and tries to remove all of them immediately. This is a one-time operation which runs right after an upgrade.</description></item><item><title>Removing lost parts</title><link>http://kb.altinity.com/upgrade/removing-lost-parts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/upgrade/removing-lost-parts/</guid><description>There might be parts left in ZooKeeper that don&amp;rsquo;t exist on disk The explanation is here https://github.com/ClickHouse/ClickHouse/pull/26716 The problem is introduced in ClickHouse® 20.1.
The problem is fixed in 21.8 and backported to 21.3.16, 21.6.9, 21.7.6.
Regarding the procedure to reproduce the issue: The procedure was not confirmed, but I think it should work.
Wait for a merge on a particular partition (or run an OPTIMIZE to trigger one) At this point you can collect the names of parts participating in the merge from the system.</description></item><item><title>ReplacingMergeTree does not collapse duplicates</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/replacingmergetree/altinity-kb-replacingmergetree-does-not-collapse-duplicates/</guid><description>Hi there, I have a question about replacing merge trees. I have set up a Materialized View with ReplacingMergeTree table, but even if I call optimize on it, the parts don&amp;rsquo;t get merged. I filled that table yesterday, nothing happened since then. What should I do?
Merges are eventual and may never happen. It depends on the number of inserts that happened after, the number of parts in the partition, size of parts.</description></item><item><title>ClickHouse® Replication problems</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-check-replication-ddl-queue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-check-replication-ddl-queue/</guid><description>Common problems &amp;amp; solutions If the replication queue does not have any Exceptions only postponed reasons without exceptions just leave ClickHouse® do Merges/Mutations and it will eventually catch up and reduce the number of tasks in replication_queue. Number of concurrent merges and fetches can be tuned but if it is done without an analysis of your workload then you may end up in a worse situation. If Delay in queue is going up actions may be needed:</description></item><item><title>Replication queue</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-replication-queue/</guid><description>SELECT database, table, type, max(last_exception), max(postpone_reason), min(create_time), max(last_attempt_time), max(last_postpone_time), max(num_postponed) AS max_postponed, max(num_tries) AS max_tries, min(num_tries) AS min_tries, countIf(last_exception != &amp;#39;&amp;#39;) AS count_err, countIf(num_postponed &amp;gt; 0) AS count_postponed, countIf(is_currently_executing) AS count_executing, count() AS count_all FROM system.replication_queue GROUP BY database, table, type ORDER BY count_all DESC</description></item><item><title>Rewind / fast-forward / replay</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-rewind-fast-forward-replay/</guid><description>Step 1: Detach Kafka tables in ClickHouse® DETACH TABLE db.kafka_table_name ON CLUSTER &amp;#39;{cluster}&amp;#39;; Step 2: kafka-consumer-groups.sh --bootstrap-server kafka:9092 --topic topic:0,1,2 --group id1 --reset-offsets --to-latest --execute More samples: https://gist.github.com/filimonov/1646259d18b911d7a1e8745d6411c0cc Step 3: Attach Kafka tables back ATTACH TABLE db.kafka_table_name ON CLUSTER &amp;#39;{cluster}&amp;#39;; See also these configuration settings:
&amp;lt;kafka&amp;gt; &amp;lt;auto_offset_reset&amp;gt;smallest&amp;lt;/auto_offset_reset&amp;gt; &amp;lt;/kafka&amp;gt; About Offset Consuming When a consumer joins the consumer group, the broker will check if it has a committed offset. If that is the case, then it will start from the latest offset.</description></item><item><title>Roaring bitmaps for calculating retention</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/roaring-bitmaps-for-calculating-retention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/roaring-bitmaps-for-calculating-retention/</guid><description>CREATE TABLE test_roaring_bitmap ENGINE = MergeTree ORDER BY h AS SELECT intDiv(number, 5) AS h, groupArray(toUInt16(number - (2 * intDiv(number, 5)))) AS vals, groupBitmapState(toUInt16(number - (2 * intDiv(number, 5)))) AS vals_bitmap FROM numbers(40) GROUP BY h SELECT h, vals, hex(vals_bitmap) FROM test_roaring_bitmap ┌─h─┬─vals─────────────┬─hex(vals_bitmap)─────────┐ │ 0 │ [0,1,2,3,4] │ 000500000100020003000400 │ │ 1 │ [3,4,5,6,7] │ 000503000400050006000700 │ │ 2 │ [6,7,8,9,10] │ 000506000700080009000A00 │ │ 3 │ [9,10,11,12,13] │ 000509000A000B000C000D00 │ │ 4 │ [12,13,14,15,16] │ 00050C000D000E000F001000 │ │ 5 │ [15,16,17,18,19] │ 00050F001000110012001300 │ │ 6 │ [18,19,20,21,22] │ 000512001300140015001600 │ │ 7 │ [21,22,23,24,25] │ 000515001600170018001900 │ └───┴──────────────────┴──────────────────────────┘ SELECT groupBitmapAnd(vals_bitmap) AS uniq, bitmapToArray(groupBitmapAndState(vals_bitmap)) AS vals FROM test_roaring_bitmap WHERE h IN (0, 1) ┌─uniq─┬─vals──┐ │ 2 │ [3,4] │ └──────┴───────┘ See also A primer on roaring bitmaps</description></item><item><title>Moving ClickHouse to Another Server</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-data-migration/rsync/</guid><description>When migrating a large, live ClickHouse cluster (multi-terabyte scale) to a new server or cluster, the goal is to minimize downtime while ensuring data consistency. A practical method is to use incremental rsync in multiple passes, combined with ClickHouse’s replication features.
Prepare the new cluster Ensure the new cluster is set up with its own ZooKeeper (or Keeper). Configure ClickHouse but keep it stopped initially. For clickhouse-operator instances, you can stop all pods by CHI definition: spec: stop: &amp;#34;true&amp;#34; and attach volumes (PVC) to a service pod.</description></item><item><title>SAMPLE by</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-sample-by/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-sample-by/</guid><description>The execution pipeline is embedded in the partition reading code.
So that works this way:
ClickHouse® does partition pruning based on WHERE conditions. For every partition, it picks a columns ranges (aka &amp;lsquo;marks&amp;rsquo; / &amp;lsquo;granulas&amp;rsquo;) based on primary key conditions. Here the sampling logic is applied: a) in case of SAMPLE k (k in 0..1 range) it adds conditions WHERE sample_key &amp;lt; k * max_int_of_sample_key_type b) in case of SAMPLE k OFFSET m it adds conditions WHERE sample_key BETWEEN m * max_int_of_sample_key_type AND (m + k) * max_int_of_sample_key_typec) in case of SAMPLE N (N&amp;gt;1) if first estimates how many rows are inside the range we need to read and based on that convert it to 3a case (calculate k based on number of rows in ranges and desired number of rows) on the data returned by those other conditions are applied (so here the number of rows can be decreased here) Source Code SAMPLE by Docs Source Code SAMPLE key Must be:</description></item><item><title>Sampling Example</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/sampling-example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/sampling-example/</guid><description>The most important idea about sampling that the primary index must have LowCardinality. (For more information, see the Altinity Knowledge Base article on LowCardinality or a ClickHouse® user&amp;#39;s lessons learned from LowCardinality ).
The following example demonstrates how sampling can be setup correctly, and an example if it being set up incorrectly as a comparison.
Sampling requires sample by expression . This ensures a range of sampled column types fit within a specified range, which ensures the requirement of low cardinality.</description></item><item><title>Security named collections</title><link>http://kb.altinity.com/altinity-kb-dictionaries/security-named-collections/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/security-named-collections/</guid><description>Dictionary with ClickHouse® table as a source with named collections Data for connecting to external sources can be stored in named collections &amp;lt;clickhouse&amp;gt; &amp;lt;named_collections&amp;gt; &amp;lt;local_host&amp;gt; &amp;lt;host&amp;gt;localhost&amp;lt;/host&amp;gt; &amp;lt;port&amp;gt;9000&amp;lt;/port&amp;gt; &amp;lt;database&amp;gt;default&amp;lt;/database&amp;gt; &amp;lt;user&amp;gt;ch_dict&amp;lt;/user&amp;gt; &amp;lt;password&amp;gt;mypass&amp;lt;/password&amp;gt; &amp;lt;/local_host&amp;gt; &amp;lt;/named_collections&amp;gt; &amp;lt;/clickhouse&amp;gt; Dictionary DROP DICTIONARY IF EXISTS named_coll_dict; CREATE DICTIONARY named_coll_dict ( key UInt64, val String ) PRIMARY KEY key SOURCE(CLICKHOUSE(NAME local_host TABLE my_table DB default)) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); INSERT INTO my_table(key, val) VALUES(1, &amp;#39;first row&amp;#39;); SELECT dictGet(&amp;#39;named_coll_dict&amp;#39;, &amp;#39;b&amp;#39;, 1); ┌─dictGet(&amp;#39;named_coll_dict&amp;#39;, &amp;#39;b&amp;#39;, 1)─┐ │ first row │ └────────────────────────────────────┘</description></item><item><title>SELECTs from engine=Kafka</title><link>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-selects-from-engine-kafka/</guid><description>Question What will happen, if we would run SELECT query from working Kafka table with MV attached? Would data showed in SELECT query appear later in MV destination table?
Answer Most likely SELECT query would show nothing. If you lucky enough and something would show up, those rows wouldn&amp;rsquo;t appear in MV destination table. So it&amp;rsquo;s not recommended to run SELECT queries on working Kafka tables.
In case of debug it&amp;rsquo;s possible to use another Kafka table with different consumer_group, so it wouldn&amp;rsquo;t affect your main pipeline.</description></item><item><title>sequenceMatch</title><link>http://kb.altinity.com/altinity-kb-functions/altinity-kb-sequencematch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-functions/altinity-kb-sequencematch/</guid><description>Question I expect the sequence here to only match once as a is only directly after a once - but it matches with gaps. Why is that?
SELECT sequenceCount(&amp;#39;(?1)(?2)&amp;#39;)(sequence, page ILIKE &amp;#39;%a%&amp;#39;, page ILIKE &amp;#39;%a%&amp;#39;) AS sequences FROM values(&amp;#39;page String, sequence UInt16&amp;#39;, (&amp;#39;a&amp;#39;, 1), (&amp;#39;a&amp;#39;, 2), (&amp;#39;b&amp;#39;, 3), (&amp;#39;b&amp;#39;, 4), (&amp;#39;a&amp;#39;, 5), (&amp;#39;b&amp;#39;, 6), (&amp;#39;a&amp;#39;, 7)) 2 # ?? Answer sequenceMatch just ignores the events which don&amp;rsquo;t match the condition. Check that:</description></item><item><title>Settings to adjust</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/</guid><description>query_log and other _log tables - set up TTL, or some other cleanup procedures.
cat /etc/clickhouse-server/config.d/query_log.xml &amp;lt;clickhouse&amp;gt; &amp;lt;query_log replace=&amp;#34;1&amp;#34;&amp;gt; &amp;lt;database&amp;gt;system&amp;lt;/database&amp;gt; &amp;lt;table&amp;gt;query_log&amp;lt;/table&amp;gt; &amp;lt;flush_interval_milliseconds&amp;gt;7500&amp;lt;/flush_interval_milliseconds&amp;gt; &amp;lt;engine&amp;gt; ENGINE = MergeTree PARTITION BY event_date ORDER BY (event_time) TTL event_date + interval 90 day SETTINGS ttl_only_drop_parts=1 &amp;lt;/engine&amp;gt; &amp;lt;/query_log&amp;gt; &amp;lt;/clickhouse&amp;gt; query_thread_log - typically is not too useful for end users, you can disable it (or set up TTL). We do not recommend removing this table completely as you might need it for debug one day and the threads&amp;rsquo; logging can be easily disabled/enabled without a restart through user profiles:</description></item><item><title>Shutting down a node</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-shutting-down-a-node/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-shutting-down-a-node/</guid><description>It’s possible to shutdown server on fly, but that would lead to failure of some queries.
More safer way:
Remove server (which is going to be disabled) from remote_server section of config.xml on all servers.
avoid removing the last replica of the shard (that can lead to incorrect data placement if you use non-random distribution) Remove server from load balancer, so new queries wouldn’t hit it.
Detach Kafka / Rabbit / Buffer tables (if used), and Materialized* databases.</description></item><item><title>Simple aggregate functions &amp; combinators</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/simplestateif-or-ifstate-for-simple-aggregate-functions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/simplestateif-or-ifstate-for-simple-aggregate-functions/</guid><description>Q. What is SimpleAggregateFunction? Are there advantages to use it instead of AggregateFunction in AggregatingMergeTree? The ClickHouse® SimpleAggregateFunction can be used for those aggregations when the function state is exactly the same as the resulting function value. Typical example is max function: it only requires storing the single value which is already maximum, and no extra steps needed to get the final value. In contrast avg need to store two numbers - sum &amp;amp; count, which should be divided to get the final value of aggregation (done by the -Merge step at the very end).</description></item><item><title>Skip index</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/skip-index/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/skip-index/</guid><description> Warning When you are creating skip indexes in non-regular (Replicated)MergeTree tables over non ORDER BY columns. ClickHouse® applies index condition on the first step of query execution, so it’s possible to get outdated rows.
--(1) create test table drop table if exists test; create table test ( version UInt32 ,id UInt32 ,state UInt8 ,INDEX state_idx (state) type set(0) GRANULARITY 1 ) ENGINE ReplacingMergeTree(version) ORDER BY (id); --(2) insert sample data INSERT INTO test (version, id, state) VALUES (1,1,1); INSERT INTO test (version, id, state) VALUES (2,1,0); INSERT INTO test (version, id, state) VALUES (3,1,1); --(3) check the result: -- expected 3, 1, 1 select version, id, state from test final; ┌─version─┬─id─┬─state─┐ │ 3 │ 1 │ 1 │ └─────────┴────┴───────┘ -- expected empty result select version, id, state from test final where state=0; ┌─version─┬─id─┬─state─┐ │ 2 │ 1 │ 0 │ └─────────┴────┴───────┘</description></item><item><title>Skip index bloom_filter Example</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/skip-index-bloom_filter-for-array-column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/skip-index-bloom_filter-for-array-column/</guid><description>tested with ClickHouse® 20.8.17.25
https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes Let&amp;rsquo;s create test data create table bftest (k Int64, x Array(Int64)) Engine=MergeTree order by k; insert into bftest select number, arrayMap(i-&amp;gt;rand64()%565656, range(10)) from numbers(10000000); insert into bftest select number, arrayMap(i-&amp;gt;rand64()%565656, range(10)) from numbers(100000000); Base point (no index) select count() from bftest where has(x, 42); ┌─count()─┐ │ 186 │ └─────────┘ 1 rows in set. Elapsed: 0.495 sec. Processed 110.00 million rows, 9.68 GB (222.03 million rows/s., 19.</description></item><item><title>Skip indexes examples</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/skip-indexes-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/skip-indexes/skip-indexes-examples/</guid><description>bloom_filter create table bftest (k Int64, x Int64) Engine=MergeTree order by k; insert into bftest select number, rand64()%565656 from numbers(10000000); insert into bftest select number, rand64()%565656 from numbers(100000000); select count() from bftest where x = 42; ┌─count()─┐ │ 201 │ └─────────┘ 1 rows in set. Elapsed: 0.243 sec. Processed 110.00 million rows alter table bftest add index ix1(x) TYPE bloom_filter GRANULARITY 1; alter table bftest materialize index ix1; select count() from bftest where x = 42; ┌─count()─┐ │ 201 │ └─────────┘ 1 rows in set.</description></item><item><title>SPARSE_HASHED VS HASHED vs HASHED_ARRAY</title><link>http://kb.altinity.com/altinity-kb-dictionaries/altinity-kb-sparse_hashed-vs-hashed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-dictionaries/altinity-kb-sparse_hashed-vs-hashed/</guid><description>Sparse_hashed and hashed_array layouts are supposed to save memory but has some downsides. We can test it with the following:
create table orders(id UInt64, price Float64) Engine = MergeTree() order by id; insert into orders select number, 0 from numbers(5000000); CREATE DICTIONARY orders_hashed (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST &amp;#39;localhost&amp;#39; PORT 9000 TABLE orders DB &amp;#39;default&amp;#39; USER &amp;#39;default&amp;#39;)) LIFETIME(MIN 0 MAX 0) LAYOUT(HASHED()); CREATE DICTIONARY orders_sparse (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST &amp;#39;localhost&amp;#39; PORT 9000 TABLE orders DB &amp;#39;default&amp;#39; USER &amp;#39;default&amp;#39;)) LIFETIME(MIN 0 MAX 0) LAYOUT(SPARSE_HASHED()); CREATE DICTIONARY orders_hashed_array (id UInt64, price Float64) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST &amp;#39;localhost&amp;#39; PORT 9000 TABLE orders DB &amp;#39;default&amp;#39; USER &amp;#39;default&amp;#39;)) LIFETIME(MIN 0 MAX 0) LAYOUT(HASHED_ARRAY()); SELECT name, type, status, element_count, formatReadableSize(bytes_allocated) AS RAM FROM system.</description></item><item><title>SSL connection unexpectedly closed</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/ssl-connection-unexpectedly-closed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/ssl-connection-unexpectedly-closed/</guid><description>ClickHouse doesn&amp;rsquo;t probe CA path which is default on CentOS and Amazon Linux.
ClickHouse client cat /etc/clickhouse-client/conf.d/openssl-ca.xml &amp;lt;config&amp;gt; &amp;lt;openSSL&amp;gt; &amp;lt;client&amp;gt; &amp;lt;!-- Used for connection to server&amp;#39;s secure tcp port --&amp;gt; &amp;lt;caConfig&amp;gt;/etc/ssl/certs&amp;lt;/caConfig&amp;gt; &amp;lt;/client&amp;gt; &amp;lt;/openSSL&amp;gt; &amp;lt;/config&amp;gt; ClickHouse server cat /etc/clickhouse-server/conf.d/openssl-ca.xml &amp;lt;config&amp;gt; &amp;lt;openSSL&amp;gt; &amp;lt;server&amp;gt; &amp;lt;!-- Used for https server AND secure tcp port --&amp;gt; &amp;lt;caConfig&amp;gt;/etc/ssl/certs&amp;lt;/caConfig&amp;gt; &amp;lt;/server&amp;gt; &amp;lt;client&amp;gt; &amp;lt;!-- Used for connecting to https dictionary source and secured Zookeeper communication --&amp;gt; &amp;lt;caConfig&amp;gt;/etc/ssl/certs&amp;lt;/caConfig&amp;gt; &amp;lt;/client&amp;gt; &amp;lt;/openSSL&amp;gt; &amp;lt;/config&amp;gt; https://github.</description></item><item><title>SummingMergeTree</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/summingmergetree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/summingmergetree/</guid><description>Nested structures In certain conditions it could make sense to collapse one of dimensions to set of arrays. It&amp;rsquo;s usually profitable to do if this dimension is not commonly used in queries. It would reduce amount of rows in aggregated table and speed up queries which doesn&amp;rsquo;t care about this dimension in exchange of aggregation performance by collapsed dimension.
CREATE TABLE traffic ( `key1` UInt32, `key2` UInt32, `port` UInt16, `bits_in` UInt32 CODEC (T64,LZ4), `bits_out` UInt32 CODEC (T64,LZ4), `packets_in` UInt32 CODEC (T64,LZ4), `packets_out` UInt32 CODEC (T64,LZ4) ) ENGINE = SummingMergeTree ORDER BY (key1, key2, port); INSERT INTO traffic SELECT number % 1000, intDiv(number, 10000), rand() % 20, rand() % 753, rand64() % 800, rand() % 140, rand64() % 231 FROM numbers(100000000); CREATE TABLE default.</description></item><item><title>Suspiciously many broken parts</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/suspiciously-many-broken-parts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/suspiciously-many-broken-parts/</guid><description>Symptom: clickhouse fails to start with a message DB::Exception: Suspiciously many broken parts to remove.
Cause: That exception is just a safeguard check/circuit breaker, triggered when clickhouse detects a lot of broken parts during server startup.
Parts are considered broken if they have bad checksums or some files are missing or malformed. Usually, that means the data was corrupted on the disk.
Why data could be corrupted?
the most often reason is a hard restart of the system, leading to a loss of the data which was not fully flushed to disk from the system page cache.</description></item><item><title>There are N unfinished hosts (0 of them are currently active).</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/there-are-n-unfinished-hosts-0-of-them-are-currently-active/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-ddlworker/there-are-n-unfinished-hosts-0-of-them-are-currently-active/</guid><description>Sometimes your Distributed DDL queries are being stuck, and not executing on all or subset of nodes, there are a lot of possible reasons for that kind of behavior, so it would take some time and effort to investigate.
Possible reasons ClickHouse® node can&amp;rsquo;t recognize itself SELECT * FROM system.clusters; -- check is_local column, it should have 1 for itself getent hosts clickhouse.local.net # or other name which should be local hostname --fqdn cat /etc/hosts cat /etc/hostname Debian / Ubuntu There is an issue in Debian based images, when hostname being mapped to 127.</description></item><item><title>Threads</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-threads/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-threads/</guid><description>Count threads used by clickhouse-server cat /proc/$(pidof -s clickhouse-server)/status | grep Threads Threads: 103 ps hH $(pidof -s clickhouse-server) | wc -l 103 ps hH -AF | grep clickhouse | wc -l 116 Thread counts by type (using ps &amp;amp; clickhouse-local) ps H -o &amp;#39;tid comm&amp;#39; $(pidof -s clickhouse-server) | tail -n +2 | awk &amp;#39;{ printf(&amp;#34;%s\t%s\n&amp;#34;, $1, $2) }&amp;#39; | clickhouse-local -S &amp;#34;threadid UInt16, name String&amp;#34; -q &amp;#34;SELECT name, count() FROM table GROUP BY name WITH TOTALS ORDER BY count() DESC FORMAT PrettyCompact&amp;#34; Threads used by running queries: SELECT query, length(thread_ids) AS threads_count FROM system.</description></item><item><title>Time zones</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/time-zones/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/time-zones/</guid><description>Important things to know:
DateTime inside ClickHouse® is actually UNIX timestamp always, i.e. number of seconds since 1970-01-01 00:00:00 GMT. Conversion from that UNIX timestamp to a human-readable form and reverse can happen on the client (for native clients) and on the server (for HTTP clients, and for some type of queries, like toString(ts)) Depending on the place where that conversion happened rules of different timezones may be applied. You can check server timezone using SELECT timezone() clickhouse-client also by default tries to use server timezone (see also --use_client_time_zone flag) If you want you can store the timezone name inside the data type, in that case, timestamp &amp;lt;-&amp;gt; human-readable time rules of that timezone will be applied.</description></item><item><title>Time-series alignment with interpolation</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ts-interpolation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ts-interpolation/</guid><description>This article demonstrates how to perform time-series data alignment with interpolation using window functions in ClickHouse. The goal is to align two different time-series (A and B) on the same timestamp axis and fill the missing values using linear interpolation.
Step-by-Step Implementation We begin by creating a table with test data that simulates two time-series (A and B) with randomly distributed timestamps and values. Then, we apply interpolation to fill missing values for each time-series based on the surrounding data points.</description></item><item><title>Top N &amp; Remain</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/top-n-and-remain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/top-n-and-remain/</guid><description>When working with large datasets, you may often need to compute the sum of values for the top N groups and aggregate the remainder separately. This article demonstrates several methods to achieve that in ClickHouse.
Dataset Setup We&amp;rsquo;ll start by creating a table top_with_rest and inserting data for demonstration purposes:
CREATE TABLE top_with_rest ( `k` String, `number` UInt64 ) ENGINE = Memory; INSERT INTO top_with_rest SELECT toString(intDiv(number, 10)), number FROM numbers_mt(10000); This creates a table with 10,000 numbers, grouped by dividing the numbers into tens.</description></item><item><title>Troubleshooting</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/troubleshooting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/troubleshooting/</guid><description>Query Execution Logging When troubleshooting query execution in ClickHouse®, one of the most useful tools is logging the query execution details. This can be controlled using the session-level setting send_logs_level. Here are the different log levels you can use: Possible values: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'
This can be used with clickhouse-client in both interactive and non-interactive mode.
The logs provide detailed information about query execution, making it easier to identify issues or bottlenecks.</description></item><item><title>TTL GROUP BY Examples</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/</guid><description>Example with MergeTree table CREATE TABLE test_ttl_group_by ( `key` UInt32, `ts` DateTime, `value` UInt32, `min_value` UInt32 DEFAULT value, `max_value` UInt32 DEFAULT value ) ENGINE = MergeTree PARTITION BY toYYYYMM(ts) ORDER BY (key, toStartOfDay(ts)) TTL ts + interval 30 day GROUP BY key, toStartOfDay(ts) SET value = sum(value), min_value = min(min_value), max_value = max(max_value), ts = min(toStartOfDay(ts)); During TTL merges ClickHouse® re-calculates values of columns in the SET section.
GROUP BY section should be a prefix of a table&amp;rsquo;s PRIMARY KEY (the same as ORDER BY, if no separate PRIMARY KEY defined).</description></item><item><title>TTL Recompress example</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/ttl-recompress-example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/ttl-recompress-example/</guid><description>See also the Altinity Knowledge Base article on testing different compression codecs .
Example how to create a table and define recompression rules CREATE TABLE hits ( `banner_id` UInt64, `event_time` DateTime CODEC(Delta, Default), `c_name` String, `c_cost` Float64 ) ENGINE = MergeTree PARTITION BY toYYYYMM(event_time) ORDER BY (banner_id, event_time) TTL event_time + toIntervalMonth(1) RECOMPRESS CODEC(ZSTD(1)), event_time + toIntervalMonth(6) RECOMPRESS CODEC(ZSTD(6); Default compression is LZ4. See the ClickHouse® documentation for more information.</description></item><item><title>UPDATE via Dictionary</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/update-via-dictionary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/update-via-dictionary/</guid><description>CREATE TABLE test_update ( `key` UInt32, `value` String ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_update SELECT number, concat(&amp;#39;value &amp;#39;, toString(number)) FROM numbers(20); SELECT * FROM test_update; ┌─key─┬─value────┐ │ 0 │ value 0 │ │ 1 │ value 1 │ │ 2 │ value 2 │ │ 3 │ value 3 │ │ 4 │ value 4 │ │ 5 │ value 5 │ │ 6 │ value 6 │ │ 7 │ value 7 │ │ 8 │ value 8 │ │ 9 │ value 9 │ │ 10 │ value 10 │ │ 11 │ value 11 │ │ 12 │ value 12 │ │ 13 │ value 13 │ │ 14 │ value 14 │ │ 15 │ value 15 │ │ 16 │ value 16 │ │ 17 │ value 17 │ │ 18 │ value 18 │ │ 19 │ value 19 │ └─────┴──────────┘ CREATE TABLE test_update_source ( `key` UInt32, `value` String ) ENGINE = MergeTree ORDER BY key; INSERT INTO test_update_source VALUES (1,&amp;#39;other value&amp;#39;), (10, &amp;#39;new value&amp;#39;); CREATE DICTIONARY update_dict ( `key` UInt32, `value` String ) PRIMARY KEY key SOURCE(CLICKHOUSE(TABLE &amp;#39;test_update_source&amp;#39;)) LIFETIME(MIN 0 MAX 10) LAYOUT(FLAT); SELECT dictGet(&amp;#39;default.</description></item><item><title>Using clickhouse-keeper</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/clickhouse-keeper/</guid><description>Since 2021 the development of built-in ClickHouse® alternative for Zookeeper is happening, whose goal is to address several design pitfalls, and get rid of extra dependency.
See slides: https://presentations.clickhouse.com/meetup54/keeper.pdf and video https://youtu.be/IfgtdU1Mrm0?t=2682 Current status (last updated: July 2023) Since version 23.3 we recommend using clickhouse-keeper for new installations.
Even better if you will use the latest version of clickhouse-keeper (currently it&amp;rsquo;s 23.7), and it&amp;rsquo;s not necessary to use the same version of clickhouse-keeper as ClickHouse itself.</description></item><item><title>Values mapping</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/values-mapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/values-mapping/</guid><description>SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(transform(number % 3, [0, 1, 2, 3], [&amp;#39;aa&amp;#39;, &amp;#39;ab&amp;#39;, &amp;#39;ad&amp;#39;, &amp;#39;af&amp;#39;], &amp;#39;a0&amp;#39;)) 1 rows in set. Elapsed: 4.668 sec. Processed 1.00 billion rows, 8.00 GB (214.21 million rows/s., 1.71 GB/s.) SELECT count() FROM numbers_mt(1000000000) WHERE NOT ignore(multiIf((number % 3) = 0, &amp;#39;aa&amp;#39;, (number % 3) = 1, &amp;#39;ab&amp;#39;, (number % 3) = 2, &amp;#39;ad&amp;#39;, (number % 3) = 3, &amp;#39;af&amp;#39;, &amp;#39;a0&amp;#39;)) 1 rows in set.</description></item><item><title>UPSERT by VersionedCollapsingMergeTree</title><link>http://kb.altinity.com/engines/mergetree-table-engine-family/versioned-collapsing-mergetree/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/engines/mergetree-table-engine-family/versioned-collapsing-mergetree/</guid><description>Challenges with mutated data When you have an incoming event stream with duplicates, updates, and deletes, building a consistent row state inside the ClickHouse® table is a big challenge.
The UPDATE/DELETE approach in the OLTP world won’t help with OLAP databases tuned to handle big batches. UPDATE/DELETE operations in ClickHouse are executed as “mutations,” rewriting a lot of data and being relatively slow. You can’t run such operations very often, as for OLTP databases.</description></item><item><title>Who ate my ClickHouse® memory?</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-who-ate-my-memory/</guid><description>SYSTEM JEMALLOC PURGE; SELECT &amp;#39;OS&amp;#39; as group, metric as name, toInt64(value) as val FROM system.asynchronous_metrics WHERE metric like &amp;#39;OSMemory%&amp;#39; UNION ALL SELECT &amp;#39;Caches&amp;#39; as group, metric as name, toInt64(value) FROM system.asynchronous_metrics WHERE metric LIKE &amp;#39;%CacheBytes&amp;#39; UNION ALL SELECT &amp;#39;Caches&amp;#39; as group, metric as name, toInt64(value) FROM system.metrics WHERE metric LIKE &amp;#39;%CacheBytes&amp;#39; UNION ALL SELECT &amp;#39;MMaps&amp;#39; as group, metric as name, toInt64(value) FROM system.metrics WHERE metric LIKE &amp;#39;MMappedFileBytes&amp;#39; UNION ALL SELECT &amp;#39;Process&amp;#39; as group, metric as name, toInt64(value) FROM system.</description></item><item><title>Window functions</title><link>http://kb.altinity.com/altinity-kb-queries-and-syntax/window-functions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-queries-and-syntax/window-functions/</guid><description>Resources: Tutorial: ClickHouse® Window Functions Video: Fun with ClickHouse Window Functions Blog: Battle of the Views: ClickHouse Window View vs. Live View How Do I Simulate Window Functions Using Arrays on older versions of ClickHouse? Group with groupArray. Calculate the needed metrics. Ungroup back using arrayJoin. NTILE SELECT intDiv((num - 1) - (cnt % 3), 3) AS ntile FROM ( SELECT row_number() OVER (ORDER BY number ASC) AS num, count() OVER () AS cnt FROM numbers(11) ) ┌─ntile─┐ │ 0 │ │ 0 │ │ 0 │ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ │ 2 │ └───────┘</description></item><item><title>X rows of Y total rows in filesystem are suspicious</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/x-rows-of-y-total-rows-in-filesystem-are-suspicious/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/x-rows-of-y-total-rows-in-filesystem-are-suspicious/</guid><description>Warning The local set of parts of table doesn&amp;rsquo;t look like the set of parts in ZooKeeper. 100.00 rows of 150.00 total rows in filesystem are suspicious. There are 1 unexpected parts with 100 rows (1 of them is not just-written with 100 rows), 0 missing parts (with 0 blocks).: Cannot attach table. ClickHouse has a registry of parts in ZooKeeper.
And during the start ClickHouse compares that list of parts on a local disk is consistent with a list in ZooKeeper.</description></item><item><title>ZooKeeper backup</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-backup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-backup/</guid><description>Question: Do I need to backup Zookeeper Database, because it’s pretty important for ClickHouse®?
TLDR answer: NO, just backup ClickHouse data itself, and do SYSTEM RESTORE REPLICA during recovery to recreate zookeeper data
Details:
Zookeeper does not store any data, it stores the STATE of the distributed system (&amp;ldquo;that replica have those parts&amp;rdquo;, &amp;ldquo;still need 2 merges to do&amp;rdquo;, &amp;ldquo;alter is being applied&amp;rdquo; etc). That state always changes, and you can not capture / backup / and recover that state in a safe manner.</description></item><item><title>ZooKeeper cluster migration</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/altinity-kb-zookeeper-cluster-migration/</guid><description>Here is a plan for ZK 3.4.9 (no dynamic reconfiguration):
Add the 3 new ZK nodes to the old cluster. No changes needed for the 3 old ZK nodes at this time. Configure one of the new ZK nodes as a cluster of 4 nodes (3 old + 1 new), start it. Configure the other two new ZK nodes as a cluster of 6 nodes (3 old + 3 new), start them.</description></item><item><title>ZooKeeper cluster migration when using K8s node local storage</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-cluster-migration-k8s-node-storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-cluster-migration-k8s-node-storage/</guid><description>Describes how to migrate a ZooKeeper cluster when using K8s node-local storage such as static PV, local-path, TopoLVM.
Requires HA setup (3+ pods).
This solution is more risky than migration by adding followers because it reduces the number of active consensus members but is operationally simpler. When running with clickhouse-keeper, it can be performed gracefully so that quorum is maintained during the whole operation.
Find the leader pod and note its name To detect leader run echo stat | nc 127.</description></item><item><title>ZooKeeper Monitoring</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-monitoring/</guid><description>ZooKeeper scrape metrics embedded exporter since version 3.6.0 https://zookeeper.apache.org/doc/r3.6.2/zookeeperMonitor.html standalone exporter https://github.com/dabealu/zookeeper-exporter Install dashboards embedded exporter https://grafana.com/grafana/dashboards/10465 dabealu exporter https://grafana.com/grafana/dashboards/11442 See also https://grafana.com/grafana/dashboards?search=ZooKeeper&amp;amp;amp;dataSource=prometheus setup alert rules embedded exporter link See also https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/#zookeeper-metrics https://dzone.com/articles/monitoring-apache-zookeeper-servers - note exhibitor is no longer maintained https://github.com/samber/awesome-prometheus-alerts/blob/c3ba0cf1997c7e952369a090aeb10343cdca4878/_data/rules.yml#L1146-L1170 (or https://awesome-prometheus-alerts.grep.to/rules.html#zookeeper ) https://alex.dzyoba.com/blog/prometheus-alerts/ https://docs.datadoghq.com/integrations/zk/?tab=host https://statuslist.app/uptime-monitoring/zookeeper/</description></item><item><title>ZooKeeper schema</title><link>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-schema/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-zookeeper/zookeeper-schema/</guid><description>/metadata Table schema.
date column -&amp;gt; legacy MergeTree partition expression. sampling expression -&amp;gt; SAMPLE BY index granularity -&amp;gt; index_granularity mode -&amp;gt; type of MergeTree table sign column -&amp;gt; sign - CollapsingMergeTree / VersionedCollapsingMergeTree primary key -&amp;gt; ORDER BY key if PRIMARY KEY not defined. sorting key -&amp;gt; ORDER BY key if PRIMARY KEY defined. data format version -&amp;gt; 1 partition key -&amp;gt; PARTITION BY granularity bytes -&amp;gt; index_granularity_bytes types of MergeTree tables: Ordinary = 0 Collapsing = 1 Summing = 2 Aggregating = 3 Replacing = 5 Graphite = 6 VersionedCollapsing = 7 /mutations Log of latest mutations</description></item></channel></rss>